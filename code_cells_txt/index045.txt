-

import vsketch

class MySketch(vsketch.SketchClass):
    def draw(self, vsk: vsketch.Vsketch) -> None:
        vsk.size("a4", landscape=False)

    def finalize(self, vsk: vsketch.Vsketch) -> None:
        vsk.vpype("linemerge linesimplify reloop linesort")


if __name__ == "__main__":
    MySketch().display()

class MySketch(vsketch.SketchClass):
    def draw(self, vsk: vsketch.Vsketch) -> None:
        vsk.size("a4", landscape=False)

    def finalize(self, vsk: vsketch.Vsketch) -> None:
        vsk.vpype("linemerge linesimplify reloop linesort")



import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.line(0, 0, 100, 202)
vsk.rect(100, 100, 500, 180)
vsk.circle(2, 2, radius=300)
vsk.triangle(0, 0, 1, 1, 0, 1)

vsk.display()

import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=False)
for i in range(0,150,15):
    #vsk.line(0, 0, 100, 202)
    vsk.rect(-150+i, 60+i, 10+i*3, 250+i)
    vsk.circle(200, -200, radius=150+i)
    #vsk.triangle(i, 10+i, 100, 100, 500, 200)
vsk.save("my_file.svg")
vsk.display()

!vsk run examples/quick_draw

!ls 




//1.
for (var i = 0; i <= 1000; i+=100) {
  console.log(i);
}

//2.
for (var i = 1; i <= 128; i*=2) {
  console.log(i);
}

//.3
for (var i = 0; i <= 10; i+=2) {
  console.log(i);
}

//.4
for (var i = 3; i <= 15; i+=3)
  console.log(i);


//5.
for ( var i = 9; i >= 0; i-=1) {
  console.log(i);
}

//6.
for ( var i = 1; i <= 4; i++) {
  for (g = 0; g < 3; g++)
      console.log(i);
}

//7.
for ( var g = 0; g < 3; g++) {
  for (i = 1; i <= 4; i++)
      console.log(i);
}

console.log("scope!");

// var globalVar = "Crystal!";
//
// var sum = function(x, y) {
//   // hiding variable inside function
//   var result = x +y;
//   return result;
// };
//
// sum(4, 6);
//
// console.log(result);
//
// global variable
// var hello = scoped("Bob");
// console.log(hello);

// var result1 = scoped("Bob");
// console.log(result);
// console.log(greeting);

// var keyword and scope

var greeting = "Hello!";

var scoped = function(name) {
  // hiding
  // local variable
  var greeting = "Hello" + name;
  return greeting;
};

// function arguments - optional

function multiply(num1, num2) {
  if (num2 === undefined) {
    num2 = 2;
  }
  var result = num1 * num2;
  return result;
}

var result1 = multiply(4,5)
console.log(result1);

var result2 = multiply(5);
console.log(result2)



https://github.com/jixiaozhong/RealSR

You can prepend the env variable to the build command:

USE_NNPACK=0 python setup.py install

or set it via:

export USE_NNPACK=0

before building PyTorch from source.

Kai Zhang (cskaizhang@gmail.com)
github: https://github.com/cszn/BSRGAN
        https://github.com/cszn/KAIR
@inproceedings{zhang2021designing,
Designing a Practical Degradation Model 
for Deep Blind Image Super-Resolution},
author={Zhang, Kai and Liang, Jingyun and Van Gool, Luc and Timofte, Radu}            

os.environ["CUDA_VISIBLE_DEVICES"]=""

import os.path
import logging
import torch
import sys
import os
sys.path.append('/home/jack/Desktop/BSRGAN')
#from utils import utils_image
import utils_image
#sys.path.remove('/home/jack/hidden')
#sys.path
from python_utils.logger import Logged
import python_utils.logger
#import python_utils
#from utils import utils_logger
#from utils import utils_image as util
# from utils import utils_model
from models.network_rrdbnet import RRDBNet as net
os.environ["CUDA_VISIBLE_DEVICES"]=""

"""
Spyder (Python 3.6-3.7)
PyTorch 1.4.0-1.8.1
Windows 10 or Linux
Kai Zhang (cskaizhang@gmail.com)
github: https://github.com/cszn/BSRGAN
        https://github.com/cszn/KAIR
If you have any question, please feel free to contact with me.
Kai Zhang (e-mail: cskaizhang@gmail.com)
by Kai Zhang ( March/2020 --> March/2021 --> )
This work was previously submitted to CVPR2021.

# --------------------------------------------
@inproceedings{zhang2021designing,
  title={Designing a Practical Degradation Model for Deep Blind Image Super-Resolution},
  author={Zhang, Kai and Liang, Jingyun and Van Gool, Luc and Timofte, Radu},
  booktitle={arxiv},
  year={2021}
}
# --------------------------------------------

"""


def main(cnt):

    #utils_logger.logger_info('blind_sr_log', log_path='blind_sr_log.log')
    Logged.info('blind_sr_log', log_path='blind_sr_log.log')
    #python_utils.logger('blind_sr_log', log_path='blind_sr_log.log')
    #logger = logging.getLogger('blind_sr_log')

#    print(torch.__version__)               # pytorch version
#    print(torch.version.cuda)              # cuda version
#    print(torch.backends.cudnn.version())  # cudnn version

    testsets = 'testsets'       # fixed, set path of testsets
    testset_Ls = ['RealSRSet']  # ['RealSRSet','DPED']

    model_names = ['RRDB','ESRGAN','FSSR_DPED','FSSR_JPEG','RealSR_DPED','RealSR_JPEG','BSRGANx2']
    #model_names = ['FSSR_DPED']    # 'BSRGANx2' for scale factor 2



    save_results = True
    sf = 4
    device = torch.device('cpu')# if torch.cuda.is_available() else 'cpu')

    for model_name in model_names:
        if model_name in ['BSRGANx2']:
            sf = 2
        model_path = os.path.join('model_zoo', model_name+'.pth')          # set model path
        Logged.logger.info('{:>16s} : {:s}'.format('Model Name', model_name))
        # torch.cuda.set_device(0)      # set GPU ID
        #Logged.logger.info('{:>16s} : {:<d}'.format('GPU ID', torch.cuda.current_device()))
        #torch.cuda.empty_cache()

        # --------------------------------
        # define network and load model
        # --------------------------------
        model = net(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, sf=sf)  # define network

#            model_old = torch.load(model_path)
#            state_dict = model.state_dict()
#            for ((key, param),(key2, param2)) in zip(model_old.items(), state_dict.items()):
#                state_dict[key2] = param
#            model.load_state_dict(state_dict, strict=True)

        model.load_state_dict(torch.load(model_path), strict=True)
        model.eval()
        for k, v in model.named_parameters():
            v.requires_grad = False
        model = model.to(device)
        torch.cuda.empty_cache()

        for testset_L in testset_Ls:

            L_path = os.path.join(testsets, testset_L)
            #E_path = os.path.join(testsets, testset_L+'_'+model_name)
            E_path = os.path.join(testsets, testset_L+'_results_x'+str(sf))
            try:
                os.makedirs(E_path)
            except FileExistsError:
                # directory already exists
                pass
 
            Logged.logger.info('{:>16s} : {:s}'.format('Input Path', L_path))
            Logged.logger.info('{:>16s} : {:s}'.format('Output Path', E_path))
            idx = 0

            for img in utils_image.get_image_paths(L_path):

                # --------------------------------
                # (1) img_L
                # --------------------------------
                idx += 1
                img_name, ext = os.path.splitext(os.path.basename(img))
                Logged.logger.info('{:->4d} --> {:<s} --> x{:<d}--> {:<s}'.format(idx, model_name, sf, img_name+ext))

                img_L = utils_image.imread_uint(img, n_channels=3)
                img_L = utils_image.uint2tensor4(img_L)
                img_L = img_L.to(device)

                # --------------------------------
                # (2) inference
                # --------------------------------
                img_E = model(img_L)

                # --------------------------------
                # (3) img_E
                # --------------------------------
                img_E = utils_image.tensor2uint(img_E)
                if save_results:
                    utils_image.imsave(img_E, os.path.join(E_path, img_name+'_'+str(cnt)+'_'+model_name+'.png'))


#if __name__ == '__main__':

cnt = 110
main(cnt)


import os.path
import logging
import torch

from utils import utils_logger
from utils import utils_image as util
# from utils import utils_model
from models.network_rrdbnet import RRDBNet as net


"""
Spyder (Python 3.6-3.7)
PyTorch 1.4.0-1.8.1
Windows 10 or Linux
Kai Zhang (cskaizhang@gmail.com)
github: https://github.com/cszn/BSRGAN
        https://github.com/cszn/KAIR
If you have any question, please feel free to contact with me.
Kai Zhang (e-mail: cskaizhang@gmail.com)
by Kai Zhang ( March/2020 --> March/2021 --> )
This work was previously submitted to CVPR2021.

# --------------------------------------------
@inproceedings{zhang2021designing,
  title={Designing a Practical Degradation Model for Deep Blind Image Super-Resolution},
  author={Zhang, Kai and Liang, Jingyun and Van Gool, Luc and Timofte, Radu},
  booktitle={arxiv},
  year={2021}
}
# --------------------------------------------

"""


def main():

    utils_logger.logger_info('blind_sr_log', log_path='blind_sr_log.log')
    logger = logging.getLogger('blind_sr_log')

#    print(torch.__version__)               # pytorch version
#    print(torch.version.cuda)              # cuda version
#    print(torch.backends.cudnn.version())  # cudnn version

    testsets = 'testsets'       # fixed, set path of testsets
    testset_Ls = ['RealSRSet']  # ['RealSRSet','DPED']

    model_names = ['RRDB','ESRGAN','FSSR_DPED','FSSR_JPEG','RealSR_DPED','RealSR_JPEG']
    model_names = ['BSRGANx2']    # 'BSRGANx2' for scale factor 2
    model_names = ['RealSR_DPED']
    model_names = ['RRDB']
    model_names = ['BSRGANx2']
    model_names = ['RealSR_JPEG']
    model_names = ['BSRGANx2'] 
    save_results = True
    sf = 4
    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    device = torch.device('cpu')# if torch.cuda.is_available() else 'cpu')

    for model_name in model_names:
        if model_name in ['BSRGANx2']:
            sf = 2
        model_path = os.path.join('model_zoo', model_name+'.pth')          # set model path
        logger.info('{:>16s} : {:s}'.format('Model Name', model_name))

        # torch.cuda.set_device(0)      # set GPU ID
        #logger.info('{:>16s} : {:<d}'.format('GPU ID', torch.cuda.current_device()))
        #torch.cuda.empty_cache()

        # --------------------------------
        # define network and load model
        # --------------------------------
        model = net(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, sf=sf)  # define network

#            model_old = torch.load(model_path)
#            state_dict = model.state_dict()
#            for ((key, param),(key2, param2)) in zip(model_old.items(), state_dict.items()):
#                state_dict[key2] = param
#            model.load_state_dict(state_dict, strict=True)

        model.load_state_dict(torch.load(model_path), strict=True)
        model.eval()
        for k, v in model.named_parameters():
            v.requires_grad = False
        model = model.to(device)
        torch.cuda.empty_cache()

        for testset_L in testset_Ls:

            L_path = os.path.join(testsets, testset_L)
            #E_path = os.path.join(testsets, testset_L+'_'+model_name)
            E_path = os.path.join(testsets, testset_L+'_results_x'+str(sf))
            util.mkdir(E_path)

            logger.info('{:>16s} : {:s}'.format('Input Path', L_path))
            logger.info('{:>16s} : {:s}'.format('Output Path', E_path))
            idx = 0

            for img in util.get_image_paths(L_path):

                # --------------------------------
                # (1) img_L
                # --------------------------------
                idx += 1
                img_name, ext = os.path.splitext(os.path.basename(img))
                logger.info('{:->4d} --> {:<s} --> x{:<d}--> {:<s}'.format(idx, model_name, sf, img_name+ext))

                img_L = util.imread_uint(img, n_channels=3)
                img_L = util.uint2tensor4(img_L)
                img_L = img_L.to(device)

                # --------------------------------
                # (2) inference
                # --------------------------------
                img_E = model(img_L)

                # --------------------------------
                # (3) img_E
                # --------------------------------
                img_E = util.tensor2uint(img_E)
                if save_results:
                    util.imsave(img_E, os.path.join(E_path, img_name+'_'+model_name+'XX.png'))


if __name__ == '__main__':

    main()

sys.path.remove('/home/jack/hidden')
sys.path.append('/home/jack/Desktop/BSRGAN')
from utils import utils_logger

f=open("/home/jack/Desktop/BSRGAN/blind_sr_log.log").readlines()
for line in f:
    line = line.replace("\n","")
    print(line)



/media/jack/HDD500/fonts

import os
import shutil

target = '/media/jack/HDD500/fonts/'
for source in FONTS:
    
    try:
        shutil.copy(source, target)
    except IOError as e:
        print("Unable to copy file. %s" % e)
    except:
        print("Unexpected error:", sys.exc_info())
        pass



#assert not os.path.isabs(source)
#target = os.path.join(target, os.path.dirname(source))

# create the folders if not already exists
#os.makedirs(target)

# adding exception handling


FONTS = []
for fonts in open("fonts").readlines():
    fonts = str(fonts).replace("\n", "")
    FONTS.append(fonts)

print(len(FONTS))

words = ['docker','jack']
def locateem(words,fonts):
    TXT = []
    if any((words in words) for words in FONTS):
        TXT.append(TXT)
        TXT = fonts
        return TXT   

cnt = 0
words = ['docker','jack']
for fonts in FONTS:
    cnt = cnt + 1
    fnt = locateem(words,fonts)
  
    

print (len(fnt))

print(fnt)

lookingfor = "docker"
for cnt in range(0, len(FONTS)):
    if lookingfor in FONTS[cnt]:
        cnt = cnt+1
        if cnt<100:
            print(str(cnt) + ": " + FONT[cnt])

print(FONTS[35])

lookingfor = ["docker","config"]
cnts= 0
items = 0
for fonts in FONTS:
    for items in range(len(lookingfor)):
                dis = lookingfor[items]
                if dis not in fonts:
                    print (dis,fonts)
                    cnts = cnts+1
                    if cnts<10:print (fonts)

item = ["docker","config"]
cnt = 0
for font in FONTS:
    for element in item:
        if "config" not in font:
                cnt = cnt+1
                if cnt<100:print(font)

                


#[x for x in item if x not in z]
#or (if you don't mind losing duplicates of non-unique elements):

#set(item) - set(z)


sFONTS = set(FONTS)
len (set(sFONTS))

item = ["docker","config"]
set(FONTS) - set(item)

item = ["config",1,2,3,4,5,6,7,8,9]
SFONTS = FONTS[:200]
print(len((SFONTS)))
for cnt in range(0,len(SFONTS)):
    if "config" not in FONTS[cnt]:
          print(FONTS[cnt])#+"\n")

#But you could easily do this like:

#[x for x in item if x not in z]

#or (if you don't mind losing duplicates of non-unique elements):

#set(FONTS) - set(items)


item = [0,1,2,3,4,5,6,7,8,9]

print(item[:5])



from transformers import ViTFeatureExtractor, ViTModel
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')
#model = ViTModel.from_pretrained('edumunozsala/vit_base-224-in21k-ft-cifar100')
model = ViTModel.from_pretrained('/home/jack/data/cifar-100-python')
inputs = feature_extractor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state


import time
import sqlite3

# Create a connection to the database
conn = sqlite3.connect('countdown.db')

# Create a table to store the countdown time
conn.execute('CREATE TABLE countdown (time INTEGER)')

# Start the stopwatch countdown
start_time = time.time()

# Save the start time to the database
conn.execute('INSERT INTO countdown VALUES (?)', (start_time,))
conn.commit()

# Close the database connection
conn.close()


# import the time module
import time
# define the countdown func.
def countdown(t):
    while t:
        mins, secs = divmod(t, 60)
        timer = '{:02d}:{:02d}'.format(mins, secs)
        print(timer, end="\r")
        time.sleep(1)
        t -= 1
print('Timeout!!')
# input time in seconds
t = input("Input the Countdown time in seconds: ")
# function call
countdown(int(t))

/home/jack/miniconda3/envs/cloned_base/lib/python3.9/site-packages/gin/torch/__init__.py


%%writefile train2.py
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import argparse
import sresnet
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.manual_seed(100)
torch.cuda.manual_seed(100)

parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')
parser.add_argument('--depth', default=18, type=int)
parser.add_argument('--class_num', default=100, type=int)
parser.add_argument('--epoch', default=200, type=int)
parser.add_argument('--lambda_KD', default=0.5, type=float)
args = parser.parse_args()
print(args)


def CrossEntropy(outputs, targets):
    log_softmax_outputs = F.log_softmax(outputs/3.0, dim=1)
    softmax_targets = F.softmax(targets/3.0, dim=1)
    return -(log_softmax_outputs * softmax_targets).sum(dim=1).mean()


BATCH_SIZE = 128
LR = 0.1

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
trainset, testset = None, None
if args.class_num == 100:
    print("dataset: CIFAR100")
    trainset = torchvision.datasets.CIFAR100(
        root='/home/jack/data',
        train=True,
        download=False,
        transform=transform_train
    )
    testset = torchvision.datasets.CIFAR100(
        root='/home/jack/data',
        train=False,
        download=False,
        transform=transform_test
    )
if args.class_num == 10:
    print("dataset: CIFAR10")
    trainset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root='./data',
        train=False,
        download=True,
        transform=transform_test
    )
trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=4
)
testloader = torch.utils.data.DataLoader(
    testset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=4
)

net = None
if args.depth == 18:
    net = sresnet.resnet18(num_classes=args.class_num, align="CONV")
    print("using resnet 18")
if args.depth == 50:
    net = sresnet.resnet50(num_classes=args.class_num, align="CONV")
    print("using resnet 50")
if args.depth == 101:
    net = sresnet.resnet101(num_classes=args.class_num, align="CONV")
    print("using resnet 101")
if args.depth == 152:
    net = sresnet.resnet152(num_classes=args.class_num, align="CONV")
    print("using resnet 152")

net.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=LR, weight_decay=5e-4, momentum=0.9)

if __name__ == "__main__":
    best_acc = 0
    print("Start Training")  # 定义遍历数据集的次数
    with open("acc.txt", "w") as f:
        with open("log.txt", "w")as f2:
            for epoch in range(args.epoch):
                correct4, correct3, correct2, correct1, correct0 = 0, 0, 0, 0, 0
                predicted4, predicted3, predicted2, predicted1, predicted0 = 0, 0, 0, 0, 0
                if epoch in [75, 130, 180]:
                    for param_group in optimizer.param_groups:
                        param_group['lr'] /= 10
                net.train()
                sum_loss = 0.0
                correct = 0.0
                total = 0.0
                for i, data in enumerate(trainloader, 0):
                    length = len(trainloader)
                    inputs, labels = data
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs, feature_loss = net(inputs)

                    ensemble = sum(outputs[:-1])/len(outputs)
                    ensemble.detach_()
                    ensemble.requires_grad = False

                    #   compute loss
                    loss = torch.FloatTensor([0.]).to(device)

                    #   for deepest classifier
                    loss += criterion(outputs[0], labels)

                    #   for soft & hard target
                    teacher_output = outputs[0].detach()
                    teacher_output.requires_grad = False

                    for index in range(1, len(outputs)):
                        loss += CrossEntropy(outputs[index], teacher_output) * args.lambda_KD * 9
                        loss += criterion(outputs[index], labels) * (1 - args.lambda_KD)

                    #   for faeture align loss
                    if args.lambda_KD != 0:
                        loss += feature_loss * 5e-7

                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    total += float(labels.size(0))
                    sum_loss += loss.item()

                    _0, predicted0 = torch.max(outputs[0].data, 1)
                    _1, predicted1 = torch.max(outputs[1].data, 1)
                    _2, predicted2 = torch.max(outputs[2].data, 1)
                    _3, predicted3 = torch.max(outputs[3].data, 1)
                    _4, predicted4 = torch.max(ensemble.data, 1)

                    correct0 += float(predicted0.eq(labels.data).cpu().sum())
                    correct1 += float(predicted1.eq(labels.data).cpu().sum())
                    correct2 += float(predicted2.eq(labels.data).cpu().sum())
                    correct3 += float(predicted3.eq(labels.data).cpu().sum())
                    correct4 += float(predicted4.eq(labels.data).cpu().sum())

                    print('[epoch:%d, iter:%d] Loss: %.03f | Acc: 4/4: %.2f%% 3/4: %.2f%% 2/4: %.2f%%  1/4: %.2f%%'
                          ' Ensemble: %.2f%%' % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1),
                                                  100 * correct0 / total, 100 * correct1 / total,
                                                  100 * correct2 / total, 100 * correct3 / total,
                                                  100 * correct4 / total))

                print("Waiting Test!")
                with torch.no_grad():
                    correct4, correct3, correct2, correct1, correct0 = 0, 0, 0, 0, 0
                    predicted4, predicted3, predicted2, predicted1, predicted0 = 0, 0, 0, 0, 0
                    correct = 0.0
                    total = 0.0
                    for data in testloader:
                        net.eval()
                        images, labels = data
                        images, labels = images.to(device), labels.to(device)
                        outputs, feature_loss = net(images)
                        ensemble = sum(outputs) / len(outputs)
                        _0, predicted0 = torch.max(outputs[0].data, 1)
                        _1, predicted1 = torch.max(outputs[1].data, 1)
                        _2, predicted2 = torch.max(outputs[2].data, 1)
                        _3, predicted3 = torch.max(outputs[3].data, 1)
                        _4, predicted4 = torch.max(ensemble.data, 1)

                        correct0 += float(predicted0.eq(labels.data).cpu().sum())
                        correct1 += float(predicted1.eq(labels.data).cpu().sum())
                        correct2 += float(predicted2.eq(labels.data).cpu().sum())
                        correct3 += float(predicted3.eq(labels.data).cpu().sum())
                        correct4 += float(predicted4.eq(labels.data).cpu().sum())
                        total += float(labels.size(0))

                    print('Test Set AccuracyAcc: 4/4: %.4f%% 3/4: %.4f%% 2/4: %.4f%%  1/4: %.4f%%'
                          ' Ensemble: %.4f%%' % (100 * correct0 / total, 100 * correct1 / total,
                                                 100 * correct2 / total, 100 * correct3 / total,
                                                 100 * correct4 / total))
                    if correct0/total > best_acc:
                        torch.save(net.state_dict(), "./4att/bestmodel.pth")
                        print("model saved")
                        best_acc = correct0/total

            print("Training Finished, TotalEPOCH=%d" % args.epoch)





!python train2.py




'''
Implementation of Compositional Pattern Producing Networks in Tensorflow

https://en.wikipedia.org/wiki/Compositional_pattern-producing_network

@hardmaru, 2016

'''

import numpy as np
import tensorflow as tf
from ops import *

class CPPN():
  def __init__(self, batch_size=1, z_dim = 32, c_dim = 1, scale = 8.0, net_size = 32):
    """

    Args:
    z_dim: how many dimensions of the latent space vector (R^z_dim)
    c_dim: 1 for mono, 3 for rgb.  dimension for output space.  you can modify code to do HSV rather than RGB.
    net_size: number of nodes for each fully connected layer of cppn
    scale: the bigger, the more zoomed out the picture becomes

    """

    self.batch_size = batch_size
    self.net_size = net_size
    x_dim = 256
    y_dim = 256
    self.x_dim = x_dim
    self.y_dim = y_dim
    self.scale = scale
    self.c_dim = c_dim
    self.z_dim = z_dim

    # tf Graph batch of image (batch_size, height, width, depth)
    self.batch = tf.placeholder(tf.float32, [batch_size, x_dim, y_dim, c_dim])

    n_points = x_dim * y_dim
    self.n_points = n_points

    self.x_vec, self.y_vec, self.r_vec = self._coordinates(x_dim, y_dim, scale)

    # latent vector
    self.z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim])
    # inputs to cppn, like coordinates and radius from centre
    self.x = tf.placeholder(tf.float32, [self.batch_size, None, 1])
    self.y = tf.placeholder(tf.float32, [self.batch_size, None, 1])
    self.r = tf.placeholder(tf.float32, [self.batch_size, None, 1])

    # builds the generator network
    self.G = self.generator(x_dim = self.x_dim, y_dim = self.y_dim)

    self.init()

  def init(self):

    # Initializing the tensor flow variables
    init = tf.global_variables_initializer()
    # Launch the session
    self.sess = tf.Session()
    self.sess.run(init)

  def reinit(self):
    init = tf.initialize_variables(tf.trainable_variables())
    self.sess.run(init)

  def _coordinates(self, x_dim = 32, y_dim = 32, scale = 1.0):
    '''
    calculates and returns a vector of x and y coordintes, and corresponding radius from the centre of image.
    '''
    n_points = x_dim * y_dim
    x_range = scale*(np.arange(x_dim)-(x_dim-1)/2.0)/(x_dim-1)/0.5
    y_range = scale*(np.arange(y_dim)-(y_dim-1)/2.0)/(y_dim-1)/0.5
    x_mat = np.matmul(np.ones((y_dim, 1)), x_range.reshape((1, x_dim)))
    y_mat = np.matmul(y_range.reshape((y_dim, 1)), np.ones((1, x_dim)))
    r_mat = np.sqrt(x_mat*x_mat + y_mat*y_mat)
    x_mat = np.tile(x_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)
    y_mat = np.tile(y_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)
    r_mat = np.tile(r_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)
    return x_mat, y_mat, r_mat

  def generator(self, x_dim, y_dim, reuse = False):

    if reuse:
        tf.get_variable_scope().reuse_variables()

    net_size = self.net_size
    n_points = x_dim * y_dim

    # note that latent vector z is scaled to self.scale factor.
    z_scaled = tf.reshape(self.z, [self.batch_size, 1, self.z_dim]) * \
                    tf.ones([n_points, 1], dtype=tf.float32) * self.scale
    z_unroll = tf.reshape(z_scaled, [self.batch_size*n_points, self.z_dim])
    x_unroll = tf.reshape(self.x, [self.batch_size*n_points, 1])
    y_unroll = tf.reshape(self.y, [self.batch_size*n_points, 1])
    r_unroll = tf.reshape(self.r, [self.batch_size*n_points, 1])

    U = fully_connected(z_unroll, net_size, 'g_0_z') + \
        fully_connected(x_unroll, net_size, 'g_0_x', with_bias = False) + \
        fully_connected(y_unroll, net_size, 'g_0_y', with_bias = False) + \
        fully_connected(r_unroll, net_size, 'g_0_r', with_bias = False)


    '''
    Below are a bunch of examples of different CPPN configurations.
    Feel free to comment out and experiment!
    '''

    ###
    ### Example: 3 layers of tanh() layers, with net_size = 32 activations/layer
    ###
    #'''
    H = tf.nn.tanh(U)
    for i in range(3):
      H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_'+str(i)))
    output = tf.sigmoid(fully_connected(H, self.c_dim, 'g_final'))
    #'''

    ###
    ### Similar to example above, but instead the output is
    ### a weird function rather than just the sigmoid
    '''
    H = tf.nn.tanh(U)
    for i in range(3):
      H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_'+str(i)))
    output = tf.sqrt(1.0-tf.abs(tf.tanh(fully_connected(H, self.c_dim, 'g_final'))))
    '''

    ###
    ### Example: mixing softplus and tanh layers, with net_size = 32 activations/layer
    ###
    '''
    H = tf.nn.tanh(U)
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_1'))
    H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_2'))
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_2'))
    H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_2'))
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_2'))
    output = tf.sigmoid(fully_connected(H, self.c_dim, 'g_final'))
    '''

    ###
    ### Example: mixing sinusoids, tanh and multiple softplus layers
    ###
    '''
    H = tf.nn.tanh(U)
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_1'))
    H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_2'))
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_2'))
    output = 0.5 * tf.sin(fully_connected(H, self.c_dim, 'g_final')) + 0.5
    '''

    ###
    ### Example: residual network of 4 tanh() layers
    ###
    '''
    H = tf.nn.tanh(U)
    for i in range(3):
      H = H+tf.nn.tanh(fully_connected(H, net_size, g_tanh_'+str(i)))
    output = tf.sigmoid(fully_connected(H, self.c_dim, 'g_final'))
    '''

    '''
    The final hidden later is pass thru a fully connected sigmoid later, so outputs -> (0, 1)
    Also, the output has a dimention of c_dim, so can be monotone or RGB
    '''
    result = tf.reshape(output, [self.batch_size, y_dim, x_dim, self.c_dim])

    return result

  def generate(self, z=None, x_dim = 26, y_dim = 26, scale = 8.0):
    """ Generate data by sampling from latent space.

    If z is not None, data for this point in latent space is
    generated. Otherwise, z is drawn from prior in latent
    space.
    """
    if z is None:
        z = np.random.uniform(-1.0, 1.0, size=(self.batch_size, self.z_dim)).astype(np.float32)
    # Note: This maps to mean of distribution, we could alternatively
    # sample from Gaussian distribution

    G = self.generator(x_dim = x_dim, y_dim = y_dim, reuse = True)
    x_vec, y_vec, r_vec = self._coordinates(x_dim, y_dim, scale = scale)
    image = self.sess.run(G, feed_dict={self.z: z, self.x: x_vec, self.y: y_vec, self.r: r_vec})
    return image

  def close(self):
    self.sess.close()


generate()

'''
Implementation of Compositional Pattern Producing Networks in Tensorflow

https://en.wikipedia.org/wiki/Compositional_pattern-producing_network

@hardmaru, 2016

Sampler Class

This file is meant to be run inside an IPython session, as it is meant
to be used interacively for experimentation.

It shouldn't be that hard to take bits of this code into a normal
command line environment though if you want to use outside of IPython.

usage:

%run -i sampler.py

sampler = Sampler(z_dim = 4, c_dim = 1, scale = 8.0, net_size = 32)

'''
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

import numpy as np
#import tensorflow as tf
import math
import random
import PIL
from PIL import Image
import pylab
from model import CPPN
import matplotlib.pyplot as plt
import images2gif
from images2gif import writeGif

#mgc = get_ipython().magic
%matplotlib inline
pylab.rcParams['figure.figsize'] = (10.0, 10.0)

class Sampler():
    def __init__(self, z_dim = 8, c_dim = 1, scale = 10.0, net_size = 32):
        self.cppn = CPPN(z_dim = z_dim, c_dim = c_dim, scale = scale, net_size = net_size)
        self.z = self.generate_z() # saves most recent z here, in case we find a nice image and want the z-vec
    def reinit(self):
        self.cppn.reinit()
    def generate_z(self):
        z = np.random.uniform(-1.0, 1.0, size=(1, self.cppn.z_dim)).astype(np.float32)
        return z
    def generate(self, z=None, x_dim=1080, y_dim=1060, scale = 10.0):
        if z is None:
            z = self.generate_z()
        else:
            z = np.reshape(z, (1, self.cppn.z_dim))
            self.z = z
            return self.cppn.generate(z, x_dim, y_dim, scale)[0]
    def show_image(self, image_data):
        '''
        image_data is a tensor, in [height width depth]
        image_data is NOT the PIL.Image class
        '''
        plt.subplot(1, 1, 1)
        y_dim = image_data.shape[0]
        x_dim = image_data.shape[1]
        c_dim = self.cppn.c_dim
        if c_dim > 1:
            plt.imshow(image_data, interpolation='nearest')
        else:
            plt.imshow(image_data.reshape(y_dim, x_dim), cmap='Greys', interpolation='nearest')
            plt.axis('off')
            plt.show()
    def save_png(self, image_data, filename):
        img_data = np.array(1-image_data)
        y_dim = image_data.shape[0]
        x_dim = image_data.shape[1]
        c_dim = self.cppn.c_dim
        if c_dim > 1:
            img_data = np.array(img_data.reshape((y_dim, x_dim, c_dim))*255.0, dtype=np.uint8)
        else:
            img_data = np.array(img_data.reshape((y_dim, x_dim))*255.0, dtype=np.uint8)
        im = Image.fromarray(img_data)
        im.save(filename)
    def to_image(self, image_data):
        # convert to PIL.Image format from np array (0, 1)
        img_data = np.array(1-image_data)
        y_dim = image_data.shape[0]
        x_dim = image_data.shape[1]
        c_dim = self.cppn.c_dim
        if c_dim > 1:
            img_data = np.array(img_data.reshape((y_dim, x_dim, c_dim))*255.0, dtype=np.uint8)
        else:
            img_data = np.array(img_data.reshape((y_dim, x_dim))*255.0, dtype=np.uint8)
            im = Image.fromarray(img_data)
        return im
    def save_anim_gif(self, z1, z2, filename, n_frame = 10, duration1 = 0.5, \
                    duration2 = 1.0, duration = 0.1, x_dim = 512, y_dim = 512, scale = 10.0, reverse = True):
        '''
        this saves an animated gif from two latent states z1 and z2
        n_frame: number of states in between z1 and z2 morphing effect, exclusive of z1 and z2
        duration1, duration2, control how long z1 and z2 are shown.  duration controls frame speed, in seconds
        '''
        delta_z = (z2-z1) / (n_frame+1)
        total_frames = n_frame + 2
        images = []
        for i in range(total_frames):
            z = z1 + delta_z*float(i)
            images.append(self.to_image(self.generate(z, x_dim, y_dim, scale)))
            print ("processing image ", i)
            durations = [duration1]+[duration]*n_frame+[duration2]
        if reverse == True: # go backwards in time back to the first state
            revImages = list(images)
            revImages.reverse()
            revImages = revImages[1:]
            images = images+revImages
            durations = durations + [duration]*n_frame + [duration1]
            print ("writing gif file...")
            writeGif(filename, images, duration = durations)


Sampler()

Changing tf.variables_initializer to tf.compat.v1.variables_initializer worked for me. I am using Tensorflow 2.3.

#Instead of tf.placeholder(shape=[None, 2], dtype=tf.float32) 
tf.compat.v1.placeholder(shape=[None, 2],
 dtype=tf.float32) if you don't want to disable v2 completely.

#replace import tensorflow as tf by following
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

#Source:stackoverflow.com
#1
#module 'tensorflow' has no attribute 'placeholder' tf 2.0
#Python By Marc Tolkmitt on May 14 2022 ThankComment

#change the <tf.placeholder> as <tf.compat.v1.placeholder>

#such as

x = tf.placeholder(shape = [None, image_pixels], dtype = tf.float32)
#change as

#x = tf.compat.v1.placeholder(shape = [None, image_pixels], dtype = tf.float32)
#but there would be another problem about runtime error with eager execution add <tf.compat.v1.disable_eager_execution()> after import part

import tensorflow as tf
tf.compat.v1.disable_eager_execution()



%run -i sampler.py

sampler = Sampler(z_dim = 4, c_dim = 1, scale = 8.0, net_size = 32)





https://psysh.org/#install

c.execute("CREATE VIRTUAL TABLE PROJECT using FTS4 (input,sqltime TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL)")

//$dir = 'sqlite:/[YOUR-PATH]/combadd.sqlite';
$dir = 'sqlite:/home/jack/Desktop/pas.bak/ALLnotes.db';
$dbh  = new PDO($dir) or die("cannot open the database");
$query =  "SELECT rowid, * FROM PROJECT";
foreach ($dbh->query($query) as $row)
{
    if (strpos($row[1],'command:') !== false) {
    echo $row[0],": ",$row[1],"\n";
    echo "-----------------------\n";
}
}
$dbh = null; //This is how you close a PDO connection

//$dir = 'sqlite:/[YOUR-PATH]/combadd.sqlite';
$dirn = 'sqlite:/home/jack/Desktop/pas.bak/ALLnotes.db';
$dbhn  = new PDO($dirn) or die("cannot open the database");
$query =  "SELECT rowid, * FROM PROJECT;
foreach ($dbhn->query($query) as $row)
{
    if (row[1] contains "www"){ 
    echo $row[0],": ",$row[1],"\n";
    echo "-----------------------\n";
}
}
$dbh = null; //This is how you close a PDO connection

//$dir = 'sqlite:/[YOUR-PATH]/combadd.sqlite';
$dir = 'sqlite:/home/jack/Desktop/pas.bak/ALLnotes.db';
$dbh  = new PDO($dir) or die("cannot open the database");
$query =  "SELECT rowid, * FROM PROJECT;
foreach ($dbh->query($query) as $row)
{
    $DATA =row[1];
    echo $DATA;
    if (strpos(row[1],'www') !== false) {
    echo 'Car exists.';
} else {
    echo 'No cars.';
}

}

$dbh = null; //This is how you close a PDO connection

var img1 = $$("input[name='img1']")[0].value;

$a = readline('Enter a string: ');
 
// For output
echo $a;   

echo "What do you want to input? ";
$input = rtrim(fgets(STDIN));
//echo "I got it:\n" . $input;

%lsmagic

echo "Are you sure you want to do this?  Type 'yes' to continue: ";
$handle = fopen ("php://stdin","r");
$line = fgets($handle);
if(trim($line) != 'yes'){
    echo "ABORTING!\n";
    exit;
}
echo "\n";
echo "Thank you, continuing...\n";

file_get_contents('php://input');

readline('input your name');

$a = readline();

$val = "
<script>
document.write(document.querySelector('#user').value);
</script>
";
echo $val;

//    if (row[1] contains "www"){ 
//    echo $row[0],": ",$row[1],"\n";
//    echo "-----------------------\n";

%%writefile process.py
#
# Module providing the `Process` class which emulates `threading.Thread`
#
# multiprocessing/process.py
#
# Copyright (c) 2006-2008, R Oudkerk
# Licensed to PSF under a Contributor Agreement.
#

__all__ = ['BaseProcess', 'current_process', 'active_children',
           'parent_process']

#
# Imports
#

import os
import sys
import signal
import itertools
import threading
from _weakrefset import WeakSet

#
#
#

try:
    ORIGINAL_DIR = os.path.abspath(os.getcwd())
except OSError:
    ORIGINAL_DIR = None

#
# Public functions
#

def current_process():
    '''
    Return process object representing the current process
    '''
    return _current_process

def active_children():
    '''
    Return list of process objects corresponding to live child processes
    '''
    _cleanup()
    return list(_children)


def parent_process():
    '''
    Return process object representing the parent process
    '''
    return _parent_process

#
#
#

def _cleanup():
    # check for processes which have finished
    for p in list(_children):
        if p._popen.poll() is not None:
            _children.discard(p)

#
# The `Process` class
#

class BaseProcess(object):
    '''
    Process objects represent activity that is run in a separate process

    The class is analogous to `threading.Thread`
    '''
    def _Popen(self):
        raise NotImplementedError

    def __init__(self, group=None, target=None, name=None, args=(), kwargs={},
                 *, daemon=None):
        assert group is None, 'group argument must be None for now'
        count = next(_process_counter)
        self._identity = _current_process._identity + (count,)
        self._config = _current_process._config.copy()
        self._parent_pid = os.getpid()
        self._parent_name = _current_process.name
        self._popen = None
        self._closed = False
        self._target = target
        self._args = tuple(args)
        self._kwargs = dict(kwargs)
        self._name = name or type(self).__name__ + '-' + \
                     ':'.join(str(i) for i in self._identity)
        if daemon is not None:
            self.daemon = daemon
        _dangling.add(self)

    def _check_closed(self):
        if self._closed:
            raise ValueError("process object is closed")

    def run(self):
        '''
        Method to be run in sub-process; can be overridden in sub-class
        '''
        if self._target:
            self._target(*self._args, **self._kwargs)

    def start(self):
        '''
        Start child process
        '''
        self._check_closed()
        assert self._popen is None, 'cannot start a process twice'
        assert self._parent_pid == os.getpid(), \
               'can only start a process object created by current process'
        assert not _current_process._config.get('daemon'), \
               'daemonic processes are not allowed to have children'
        _cleanup()
        self._popen = self._Popen(self)
        self._sentinel = self._popen.sentinel
        # Avoid a refcycle if the target function holds an indirect
        # reference to the process object (see bpo-30775)
        del self._target, self._args, self._kwargs
        _children.add(self)

    def terminate(self):
        '''
        Terminate process; sends SIGTERM signal or uses TerminateProcess()
        '''
        self._check_closed()
        self._popen.terminate()

    def kill(self):
        '''
        Terminate process; sends SIGKILL signal or uses TerminateProcess()
        '''
        self._check_closed()
        self._popen.kill()

    def join(self, timeout=None):
        '''
        Wait until child process terminates
        '''
        self._check_closed()
        assert self._parent_pid == os.getpid(), 'can only join a child process'
        assert self._popen is not None, 'can only join a started process'
        res = self._popen.wait(timeout)
        if res is not None:
            _children.discard(self)

    def is_alive(self):
        '''
        Return whether process is alive
        '''
        self._check_closed()
        if self is _current_process:
            return True
        assert self._parent_pid == os.getpid(), 'can only test a child process'

        if self._popen is None:
            return False

        returncode = self._popen.poll()
        if returncode is None:
            return True
        else:
            _children.discard(self)
            return False

    def close(self):
        '''
        Close the Process object.

        This method releases resources held by the Process object.  It is
        an error to call this method if the child process is still running.
        '''
        if self._popen is not None:
            if self._popen.poll() is None:
                raise ValueError("Cannot close a process while it is still running. "
                                 "You should first call join() or terminate().")
            self._popen.close()
            self._popen = None
            del self._sentinel
            _children.discard(self)
        self._closed = True

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, name):
        assert isinstance(name, str), 'name must be a string'
        self._name = name

    @property
    def daemon(self):
        '''
        Return whether process is a daemon
        '''
        return self._config.get('daemon', False)

    @daemon.setter
    def daemon(self, daemonic):
        '''
        Set whether process is a daemon
        '''
        assert self._popen is None, 'process has already started'
        self._config['daemon'] = daemonic

    @property
    def authkey(self):
        return self._config['authkey']

    @authkey.setter
    def authkey(self, authkey):
        '''
        Set authorization key of process
        '''
        self._config['authkey'] = AuthenticationString(authkey)

    @property
    def exitcode(self):
        '''
        Return exit code of process or `None` if it has yet to stop
        '''
        self._check_closed()
        if self._popen is None:
            return self._popen
        return self._popen.poll()

    @property
    def ident(self):
        '''
        Return identifier (PID) of process or `None` if it has yet to start
        '''
        self._check_closed()
        if self is _current_process:
            return os.getpid()
        else:
            return self._popen and self._popen.pid

    pid = ident

    @property
    def sentinel(self):
        '''
        Return a file descriptor (Unix) or handle (Windows) suitable for
        waiting for process termination.
        '''
        self._check_closed()
        try:
            return self._sentinel
        except AttributeError:
            raise ValueError("process not started") from None

    def __repr__(self):
        exitcode = None
        if self is _current_process:
            status = 'started'
        elif self._closed:
            status = 'closed'
        elif self._parent_pid != os.getpid():
            status = 'unknown'
        elif self._popen is None:
            status = 'initial'
        else:
            exitcode = self._popen.poll()
            if exitcode is not None:
                status = 'stopped'
            else:
                status = 'started'

        info = [type(self).__name__, 'name=%r' % self._name]
        if self._popen is not None:
            info.append('pid=%s' % self._popen.pid)
        info.append('parent=%s' % self._parent_pid)
        info.append(status)
        if exitcode is not None:
            exitcode = _exitcode_to_name.get(exitcode, exitcode)
            info.append('exitcode=%s' % exitcode)
        if self.daemon:
            info.append('daemon')
        return '<%s>' % ' '.join(info)

    ##

    def _bootstrap(self, parent_sentinel=None):
        from . import util, context
        global _current_process, _parent_process, _process_counter, _children

        try:
            if self._start_method is not None:
                context._force_start_method(self._start_method)
            _process_counter = itertools.count(1)
            _children = set()
            util._close_stdin()
            old_process = _current_process
            _current_process = self
            _parent_process = _ParentProcess(
                self._parent_name, self._parent_pid, parent_sentinel)
            if threading._HAVE_THREAD_NATIVE_ID:
                threading.main_thread()._set_native_id()
            try:
                self._after_fork()
            finally:
                # delay finalization of the old process object until after
                # _run_after_forkers() is executed
                del old_process
            util.info('child process calling self.run()')
            try:
                self.run()
                exitcode = 0
            finally:
                util._exit_function()
        except SystemExit as e:
            if e.code is None:
                exitcode = 0
            elif isinstance(e.code, int):
                exitcode = e.code
            else:
                sys.stderr.write(str(e.code) + '\n')
                exitcode = 1
        except:
            exitcode = 1
            import traceback
            sys.stderr.write('Process %s:\n' % self.name)
            traceback.print_exc()
        finally:
            threading._shutdown()
            util.info('process exiting with exitcode %d' % exitcode)
            util._flush_std_streams()

        return exitcode

    @staticmethod
    def _after_fork():
        from . import util
        util._finalizer_registry.clear()
        util._run_after_forkers()


#
# We subclass bytes to avoid accidental transmission of auth keys over network
#

class AuthenticationString(bytes):
    def __reduce__(self):
        from .context import get_spawning_popen
        if get_spawning_popen() is None:
            raise TypeError(
                'Pickling an AuthenticationString object is '
                'disallowed for security reasons'
                )
        return AuthenticationString, (bytes(self),)


#
# Create object representing the parent process
#

class _ParentProcess(BaseProcess):

    def __init__(self, name, pid, sentinel):
        self._identity = ()
        self._name = name
        self._pid = pid
        self._parent_pid = None
        self._popen = None
        self._closed = False
        self._sentinel = sentinel
        self._config = {}

    def is_alive(self):
        from multiprocessing.connection import wait
        return not wait([self._sentinel], timeout=0)

    @property
    def ident(self):
        return self._pid

    def join(self, timeout=None):
        '''
        Wait until parent process terminates
        '''
        from multiprocessing.connection import wait
        wait([self._sentinel], timeout=timeout)

    pid = ident

#
# Create object representing the main process
#

class _MainProcess(BaseProcess):

    def __init__(self):
        self._identity = ()
        self._name = 'MainProcess'
        self._parent_pid = None
        self._popen = None
        self._closed = False
        self._config = {'authkey': AuthenticationString(os.urandom(32)),
                        'semprefix': '/mp'}
        # Note that some versions of FreeBSD only allow named
        # semaphores to have names of up to 14 characters.  Therefore
        # we choose a short prefix.
        #
        # On MacOSX in a sandbox it may be necessary to use a
        # different prefix -- see #19478.
        #
        # Everything in self._config will be inherited by descendant
        # processes.

    def close(self):
        pass


_parent_process = None
_current_process = _MainProcess()
_process_counter = itertools.count(1)
_children = set()
del _MainProcess

#
# Give names to some return codes
#

_exitcode_to_name = {}

for name, signum in list(signal.__dict__.items()):
    if name[:3]=='SIG' and '_' not in name:
        _exitcode_to_name[-signum] = f'-{name}'
del name, signum

# For debug and leak testing
_dangling = WeakSet()


!pwd

!ls /home/jack/processing/processing-py-site-master/process.py

from multiprocessing import *
import multiprocessing
children = multiprocessing.active_children()
children



from PIL import Image, ImageSequence

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    bg = bg.resize((640,640), Image.BICUBIC)
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((bg.size), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    result_images = []
    for i in range(10):
        size = (int(fg_copy.width * (i+1)/10), int(fg_copy.height * (i+1)/10))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/10))
        fg_copy_resized = fg_copy_resized.convert('RGBA')
        fg_copy_resized.putalpha(int((i+1)*255/10))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
    # Save the resulting images as a GIF animation
    result_images[0].save('gifs/zoom_effect.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)
#zoom_effect(bg_file, fg_file)
zoom_effect("gifs/background.jpg", "gifs/focus.png")


from PIL import Image, ImageSequence

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((bg.size), Image.BICUBIC)
    fg.putalpha(0)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    fg_copy.putalpha(0)
    result_images = []
    for i in range(10):
        size = (int(fg_copy.width * (i+1)/10), int(fg_copy.height * (i+1)/10))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/10))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        result = Image.blend(bg, result, (i+1)/10)
        result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
    # Save the resulting images as a GIF animation
    result_images[0].save('gifs/zoom_effect.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)

zoom_effect("gifs/background.jpg", "gifs/focus.png")
#Problem: Notice the line, result.save("gifs/_"+str(i)+".png") I did that to check the #images generated that all have black 'focus' overlays in various opacities the same as #the resulting zoom_effect.gif. 

import os
total_size = 0
for root, dirs, files in os.walk('/home/jack'):
    for file in files:
        if file.endswith('.ipynb'):
            filepath = os.path.join(root, file)
            total_size += os.path.getsize(filepath)
total_size_mb = total_size / (1024 * 1024)
print(f"Total size of Jupyter notebooks: {total_size_mb:.2f} MB")

import os
import shutil

src_dir = '/home/jack'  # Replace this with the root directory where you want to search for Jupyter notebooks
dst_dir = '/path/to/destination/directory/'  # Replace this with the destination directory where you want to copy the notebooks
notebooks = {}  # Dictionary to keep track of notebook names and sequence numbers

for root, dirs, files in os.walk(src_dir):
    for file in files:
        if file.endswith('.ipynb'):
            src_path = os.path.join(root, file)
            dst_path = os.path.join(dst_dir, file)
            if file in notebooks:
                # Increment sequence number for duplicate file names
                notebooks[file] += 1
                name, ext = os.path.splitext(file)
                new_name = f"{name}_{notebooks[file]}{ext}"
                dst_path = os.path.join(dst_dir, new_name)
            else:
                notebooks[file] = 0
            try:
                shutil.copy2(src_path, dst_path)
            except PermissionError:
                # Skip directories that we don't have permission to read
                continue

print("Done copying Jupyter notebooks!")


from PIL import Image, ImageSequence

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((bg.size), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    fg_copy.putalpha(0)
    result_images = []
    for i in range(10):
        size = (int(fg_copy.width * (i+1)/10), int(fg_copy.height * (i+1)/10))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/10))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        result_images.append(result)
    # Save the resulting images as a GIF animation
    result_images[0].save('gifs/zoom_effect.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)

if __name__ == "__main__":
    zoom_effect("gifs/background.jpg", "gifs/focus.png")


from PIL import Image, ImageSequence
def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    fg = Image.open(fg_file)
    fg = fg.resize((bg.size), Image.BICUBIC)
    fg = fg.convert('RGBA')
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    fg_copy.putalpha(0)
    result_images = []
    for i in range(10):
        size = (int(fg_copy.width * (i+1)/10), int(fg_copy.height * (i+1)/10))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/10))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        result_images.append(result)
    # Save the resulting images as a GIF animation
    result_images[0].save('gifs/zoom_effect.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)
bg_file = "gifs/background.jpg"
fg_file = "gifs/focus.jpg"
zoom_effect(bg_file, fg_file)


from PIL import Image, ImageSequence

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    fg = Image.open(fg_file).convert('RGBA')
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    fg_copy.putalpha(0)
    result_images = []
    for i in range(10):
        size = (int(fg_copy.width * (i+1)/10), int(fg_copy.height * (i+1)/10))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/10))
        result = bg.copy()
        result.alpha_composite(fg_copy_resized)
        result_images.append(result)

    # Save the resulting images as a GIF animation
    result_images[0].save('zoom_effect.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)

#if __name__ == "__main__":
bg_file = "/home/jack/Downloads/00de3/lexica/e12768a8-747e-4fd0-98d4-6f094f2962b3.jpeg"
fg_file = "/home/jack/Desktop/dockercommands/gypsy-princess11.jpg"
zoom_effect(bg_file, fg_file)


from PIL import Image, ImageSequence

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file)
    fg = Image.open(fg_file).convert('RGBA')
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width/2), int(fg_copy.height/2)), resample=Image.BICUBIC)
    fg_copy.putalpha(0)
    result_images = []
    for i in range(10):
        size = (int(fg_copy.width * (i+1)/10), int(fg_copy.height * (i+1)/10))
        fg_copy_resized = fg_copy.resize(size, resample=Image.BICUBIC)
        fg_copy_resized.putalpha(int((i+1)*255/10))
        result = bg.copy()
        result.alpha_composite(fg_copy_resized)
        result_images.append(result)

    # Save the resulting images as a GIF animation
    result_images[0].save('zoom_effect.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)
       
#if __name__ == "__main__":
bg_file = "gifs/background.jpg"
fg_file = "gifs/focus.jpg"
zoom_effect(bg_file, fg_file)


!ls gifs

from PIL import Image, ImageSequence
def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file)
    print(bg.size[0])
    fg = Image.open(fg_file)
    fg = fg.resize((bg.size), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width/2), int(fg_copy.height/2)))
    fg_copy.putalpha(0)
    result_images = []
    for i in range(10):
        size = (int(fg_copy.width * (i+1)/10), int(fg_copy.height * (i+1)/10))
        print('size',size)
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/10))
        result = bg.copy()
        print(result.size)
        result.paste(fg_copy_resized, (bg.size[0]//2,bg.size[1]//2), fg_copy_resized)
        result.save(str(i)+".png")
        result_images.append(result)

    # Save the resulting images as a GIF animation
    result_images[0].save('zoom_effect.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)
        
#if __name__ == "__main__":
bg_file = "/home/jack/Downloads/00de3/lexica/e12768a8-747e-4fd0-98d4-6f094f2962b3.jpeg"
fg_file = "/home/jack/Desktop/dockercommands/gypsy-princess11.jpg"
zoom_effect(bg_file, fg_file)






from PIL import Image

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file)
    fg = Image.open(fg_file)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width/2), int(fg_copy.height/2)))
    fg_copy.putalpha(0)
    result_images = []
    for i in range(10):
        size = (int(fg_copy.width * (i+1)/10), int(fg_copy.height * (i+1)/10))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/10))
        result = bg.copy()
        result.paste(fg_copy_resized, (0, 0), fg_copy_resized)
        result_images.append(result)
    for i in range(len(result_images)):
        result_images[i].save("result" + str(i) + ".jpg")

if __name__ == "__main__":
    zoom_effect("background.jpg", "focus.jpg")


from processing_py import *
from random import choice, shuffle, uniform, gauss

brush = {"x":None, "y":None, "px":None, "py":None}

colors = [
	(112, 112,  74, 150), # green
	(245, 198, 110, 150), # yellow
	(242, 229, 194, 150), # cream
	(115, 106,  97, 150), # light grey
	(215,  90,  34, 150), # orange
	(235,  61,   0, 150)] # red-orange
shuffle(colors)

def setup():
  size(400,400)
  base = colors.pop()
  background(base[0], base[1], base[2])

def draw():
  global brush
  if frameCount == 40:
    brush = {"x": mouseX, "y": mouseY, "px": mouseX-1, "py": mouseY-1}
  elif frameCount > 40:
    brush["x"] += (mouseX - brush["x"]) / 12
    brush["y"] += (mouseY - brush["y"]) / 12
    drizzle()
    brush["px"] = brush["x"]
    brush["py"] = brush["y"]
  
def drizzle():
  d = dist(brush["px"], brush["py"], brush["x"], brush["y"])
  s = min(15, 1+30/(d+0.0001))
  strokeWeight(s)
  stroke(0)
  line(brush["px"], brush["py"], brush["x"], brush["y"])
  stroke(255)
  line(width-brush["px"], height-brush["py"], width-brush["x"], height-brush["y"])

def stipple(bx, by, c):
  noStroke()
  fill(c)
  for i in range(2):
    d = uniform(3, 12)
    ellipse(bx + uniform(-30,30), by+uniform(-30,30), d, d)

def splatter(bx, by):
  rgb = choice(colors) 
  bx += uniform(-15,15)
  by += uniform(-15,15)
  mx = 5*(mouseX-pmouseX)
  my = 5*(mouseY-pmouseY)
  for i in range(80):
    x = bx + mx*gauss(0,0.25)
    y = by + my*gauss(0,0.25)
    d = dist(bx, by, x, y)
    s = min(10, 150/(d+0.00001))
    alpha = 255 - s*5
    noStroke()
    c = color(rgb[0], rgb[1], rgb[2], alpha)
    fill(c)
    ellipse(x,y,s,s)

def mouseMoved():
  if frameCount % 7 == 0:
    stipple(mouseX, mouseY, 0)
    stipple(width-mouseX, height-mouseY, 255)
    splatter(mouseX, mouseY)
    splatter(width-mouseX, height-mouseY)

setup()

from processing_py import *

app = App(600,400) # create window: width, height
app.background(255,0,0) # set background:  red, green, blue
app.redraw() # refresh the window

#app.exit() # close the window


from processing_py import *

app = App(600,400) # create window: width, height
app.background(0,0,0) # set background:  red, green, blue
app.fill(255,255,0) # set color for objects: red, green, blue
app.rect(100,100,200,100) # draw a rectangle: x0, y0, size_x, size_y
app.fill(0,0,255) # set color for objects: red, green, blue
app.ellipse(300,200,50,50) # draw a circle: center_x, center_y, size_x, size_y
app.redraw() # refresh the window


from processing_py import *
app = App(600,400) # create window: width, height

while(True):
   app.background(0,0,0) # set background:  red, green, blue
   app.fill(255,255,0) # set color for objects: red, green, blue
   app.ellipse(app.mouseX,app.mouseY,50,50) # draw a circle: center_x, center_y, size_x, size_y
   app.redraw() # refresh the window

https://jupyter-rfb.readthedocs.io/en/latest/guide.html#installation

from datetime import datetime

# datetime object containing current date and time
now = datetime.now()
 
print("now =", now)

# dd/mm/YY H:M:S
dt_string = now.strftime("%d/%m/%Y %H:%M:%S")
print("date and time =", dt_string)	


from datetime import datetime
from datetime import date

today = date.today()
print("Today's date:", today)

dir(datetime)



!ls *.png

%%writefile test2.png
data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAIACAYAAAAczR65AAAgAElEQVR4Xly9SZNt23qeNbOu9t7n3ApZFVbY90pAmHAPOv4pkvgvNG1kaNEiggZNHEEQQYMIoGMiANshCRsb5GsJCwmLq3ule4qdO6u1MpP3ed5vrNx27pMnVzHnmGN8dTW+cfQ3f/s/fr3fvtzuHx+2l+PjbTs62l63/PHftr28vvDRth2

def cvt_2_base64(file_name):
    with open(file_name , "rb") as image_file :
        data = base64.b64encode(image_file.read())
    return data.decode('utf-8')

file_name= "2color-20x20.png"
cvt_2_base64(file_name)

my_str = 'hello '

my_bytes = b'James Doe'
print(my_bytes)
result = my_str.encode('utf-8') + my_bytes

print(result)  # 👉️ b'hello James Doe'

import struct
var = struct.pack('hhl', 5, 10, 15)
print(var)
print(struct.unpack('hhl', var))

import struct
# ctypes is imported to create a string buffer
import ctypes

# As shown in previous example
size = struct.calcsize('hhl')
print(size)

# Buffer 'buff' is created from ctypes
buff = ctypes.create_string_buffer(size)

# struct.pack_into() packs data into buff and it doesn't return any value
# struct.unpack_from() unpacks data from buff, returns a tuple of values
print(struct.pack_into('hhl', buff, 0, 5, 10, 15))
print(struct.unpack_from('hhl', buff, 0))

@reboot /usr/local/bin/screen

# Exploration in PNG
#
# Just poking through the PNG file structure.
#
# Found weird output when examining MacOS screenshots. They contain an
# "iDOT" chunk, the purpose of which I couldn't determine. This chunk type
# is not part of the PNG standard, and seems to break some programs'
# ability to process these files, according to complaints I'm finding on
# Google. See sample output below. If no one's already done it, maybe there
# is some value in writing e script to strip out these invalid iDOT chunks.

import sys, struct

filename = "2color-20x20.png"
fh = open(filename, 'rb')

chunksize = 1

# The first eight bits on a PNG are always these:
first_eight = [137,80,78,71,13,10,26,10]
valid_count = 0;
i = 0
while i < 8:
    first_byte = fh.read(chunksize)
    val = struct.unpack('B',first_byte)[0]
    if first_eight[i] == val:
        valid_count += 1
    else:
        print ("ERROR: Invalid PNG signature")
        exit(-1)
    i += 1;

if valid_count == 8:
    print ("Valid PNG signature")

# Process the image header chunk to output some useful file info.
# I didn't make special functions for any other chunk type
def process_ihdr():
    # The ! is necessary because PNGs being Portable "NETWORK" Graphics use
    # network byte ordering (which is big-endian). 
    width = struct.unpack('!I', fh.read(4))[0]
    print ("width: ", width)
    height = struct.unpack('!I', fh.read(4))[0]
    print ("height: ", height)
    bit_depth = struct.unpack('B', fh.read(1))[0]
    colour_type = struct.unpack('B', fh.read(1))[0]
    compression_method = struct.unpack('B', fh.read(1))[0]
    filter_method = struct.unpack('B', fh.read(1))[0]
    interlace_method = struct.unpack('B', fh.read(1))[0]
    print ("bit_depth: ", bit_depth)
    print ("colour_type: ", colour_type)
    print ("compression_method: ", compression_method)
    print ("filter_method: ", filter_method)
    print ("interlace_method: ", interlace_method)
    fh.read(4)

# Process all the chunks and print a summary of each. 
idats_found = 0
idats_bytes = 0
while (True):
    chlen = fh.read(4)
    #chlen.encode('utf-8')
    print(chlen)

    if chlen == "":
        break

    chunk_length = struct.unpack('!I',chlen)[0]

    chunk_type = ""
    for i in range(4):
        #chunk_type.encode('utf-8') 
        #chunk_type += (struct.unpack('c',fh.read(1))[0])
        #chunk_type += (struct.unpack('c'))#,fh.read(1))[0])
    # Instead of printing the same report for each IDAT chunk, just
    # print a summary at the end, to reduce flooding your console
    if (chunk_type != "IDAT"):
        print("It is IDAT")
        print ("=" * 20)
        print ("chunk_length: " + str(chunk_length))    
        print ("chunk_type: ", chunk_type)
    else:
        idats_found += 1
        idats_bytes += chunk_length

    if (chunk_type == "IHDR"):
        process_ihdr()
    else:
        data = fh.read(chunk_length)
        crc = fh.read(4)

print ("=" * 20)
print ("Found a total of " + str(idats_found) + " IDAT chunks consuming " + str(idats_bytes) + " bytes.")

# Close the file
fh.close()


import base64
msg = bytes("SGVsbG8gd29ybGQh", encoding='utf-8')
decoded = base64.b64decode(msg)
print(decoded.decode('utf-8'))

import base64
msg = "Hello world!"
encoded = base64.b64encode(bytes(msg, encoding='utf-8'))
print(encoded.decode('utf-8'))

#with open('2color-20x20.png', 'r', encoding='utf-8') as f:
with open('2color-20x20.png', 'r', encoding= 'ISO-8859-1') as f:    
    # ✅ get list of all lines
    lines = f.read()
    print(lines)
       

# For both Python 2.7 and Python 3.x
import base64
with open("imageToSave-junk.png", "wb") as f:
    f.write(base64.decodebytes(Data.encode()))

with open('test2.png', 'r', encoding='utf-8') as f:
    # ✅ get list of all lines
    lines = f.readlines()
    for line in lines:
        Data =line[22:]
        

import base64
with open("/home/jack/Downloads/image.png", "r") as fh:
    lines = fh.readlines()
    for line in lines:
        Data =line[22:]
        image_data = base64.decodebytes(Data.encode())

# For both Python 2.7 and Python 3.x
import base64
with open("imageToSave.png", "wb") as f:
    f.write(base64.decodebytes(Data.encode()))

# For both Python 2.7 and Python 3.x
import base64
with open("imageToSave2.png", "wb") as f:
    f.write(image_data)

from PIL import Image
im = Image.open("imageToSave2.png")
im

from PIL import Image
im = Image.open("imageToSave.png")
print (im.size)
im

!pwd

DATA = base64.decodebytes(Data.encode())

from PIL import Image
import io

image_data = Data # byte values of the image
image = Image.open(io.BytesIO(DATA))
image.show()

https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes

Modes

The mode of an image is a string which defines the type and depth of a pixel in the image. Each pixel uses the full range of the bit depth. So a 1-bit pixel has a range of 0-1, an 8-bit pixel has a range of 0-255 and so on. The current release supports the following standard modes:

        1 (1-bit pixels, black and white, stored with one pixel per byte)

        L (8-bit pixels, black and white)

        P (8-bit pixels, mapped to any other mode using a color palette)

        RGB (3x8-bit pixels, true color)

        RGBA (4x8-bit pixels, true color with transparency mask)

        CMYK (4x8-bit pixels, color separation)

        YCbCr (3x8-bit pixels, color video format)

            Note that this refers to the JPEG, and not the ITU-R BT.2020, standard

        LAB (3x8-bit pixels, the L*a*b color space)

        HSV (3x8-bit pixels, Hue, Saturation, Value color space)

        I (32-bit signed integer pixels)

        F (32-bit floating point pixels)


Since strings because Unicode in Python 3, I think you'll need to use Image.frombytes() to do the same thing. – 
martineau
Apr 29 at 12:11
Note that you can convert a Python 3 string into bytes via my_string.encode('latin1')

PIL.Image.frombytes(mode, size, data, decoder_name='raw', *args)

img = Image.frombytes('RGB', (1024, 512), Data, 'RGB', 'F;16')

img = Image.frombytes('RGB', (1024, 512), Data, 'raw', 'F;16')

img = Image.fromstring('L', (1024, 512), Data, 'raw', 'F;16')

rawData = open("foo.raw" 'rb').read()
imgSize = (h,w)
img = Image.fromstring('L', imgSize, rawData, 'raw', 'F;16')
img.save("foo.png")

from PIL import Image
from PIL.PngImagePlugin import PngInfo

targetImage = Image.open("pathToImage.png")

metadata = PngInfo()
metadata.add_text("MyNewString", "A string")
metadata.add_text("MyNewInt", str(1234))

targetImage.save("NewPath.png", pnginfo=metadata)
targetImage = Image.open("NewPath.png")

print(targetImage.text)

from PIL import Image, ImageTk
import PIL
from sys import argv
import sys
from tkinter import *
import cv2
import numpy as np 

window = Tk(className="Clickable")
data = []
image = PIL.Image.open("mouse-sizing-n-cropping-files/jungle700.jpg")
canvas = Canvas(window, width=image.size[0], height=image.size[1])
canvas.pack()
image_tk = ImageTk.PhotoImage(image)
canvas.create_image(image.size[0]//2, image.size[1]//2, image=image_tk)

def callback(event):
    #print "[",event.x,"],["+event.y,"]"
    #Data= ("[",event.x,",",event.y,"],",)
    Data= [event.x,event.y]
    data.append(Data)
    print ("[",event.x,",",event.y,"],",)

canvas.bind("<Button-1>", callback)
mainloop()

poly = np.array(data, np.int32)

# Read images
dst = cv2.imread("mouse-sizing-n-cropping-files/soil600.jpg")
src = cv2.imread("mouse-sizing-n-cropping-files/jungle700.jpg")
src_mask = np.zeros(src.shape, src.dtype)
#poly = np.array([ [1,100], [1,50], [151,63], [254,37], [298,90], [272,134], [43,122] ], np.int32)
cv2.fillPoly(src_mask, [poly], (255, 255, 255))
# This is where the CENTER where thr 'poly' will be placed
center = (250,275)
# Clone seamlessly.
output = cv2.seamlessClone(src, dst, src_mask, center, cv2.NORMAL_CLONE)
# Write result
cv2.imwrite("01-cloning-xxz1.jpg", output);
im = PIL.Image.open("01-cloning-xxz1.jpg")
im

import tkinter as tk
Name="fds"
window = tk.Tk(className = Name)
def change_className():
    global Name
    Name = "dsfsd"
    window.title(Name)

#bool=True
button=tk.Button(window,command=change_className)
button.pack()
window.mainloop()

import tkinter as tk

class App(tk.Frame):
    def __init__(self, master=None):
        super().__init__(master)
        self.pack()

# create the application
myapp = App()

#
# here are method calls to the window manager class
#
myapp.master.title("My Do-Nothing Application")
myapp.master.minsize(600, 400)
myapp.master.maxsize(1000, 800)

# start the program
myapp.mainloop()



from PIL import Image, ImageTk
from sys import argv
from tkinter import *
import tkinter
from PIL import Image
window = tkinter.Tk(className="bla")

image = Image.open("mouse-sizing-n-cropping-files/jungle700.jpg")
canvas = tkinter.Canvas(window, width=image.size[0], height=image.size[1])
canvas.pack()
image_tk = ImageTk.PhotoImage(image)

data = (image.size[0]//2, image.size[1]//2, image==image_tk)
print (data)
#canvas.create_image(image.size[0]//2, image.size[1]//2, image==image_tk)

from PIL import Image, ImageTk
from sys import argv
from tkinter import *
import tkinter
from PIL import Image
window = tkinter.Tk(className="bla")

image = Image.open("mouse-sizing-n-cropping-files/jungle700.jpg")
canvas = tkinter.Canvas(window, width=image.size[0], height=image.size[1])
canvas.pack()
image_tk = ImageTk.PhotoImage(image)
canvas.create_image(image.size[0]//2, image.size[1]//2, image=image_tk)

def callback(event):
    #print "[",event.x,"],["+event.y,"]"
    print ("[",event.x,",",event.y,"],",)

canvas.bind("<Button-1>", callback)
tkinter.mainloop()



/home/jack/miniconda3/envs/bakup-clonebase/lib/python3.9/site-packages/PyQt6/Qt6/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/bakup-clonebase/lib/python3.9/site-packages/PySide2-5.15.2.1-py3.9-linux-x86_64.egg/PySide2/Qt/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/bakup-clonebase/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/cloned_base/lib/python3.9/site-packages/PyQt5/Qt5/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/cloned_base/lib/python3.9/site-packages/PyQt6/Qt6/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/cloned_base/lib/python3.9/site-packages/PySide2/Qt/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/cloned_base/lib/python3.9/site-packages/PySide6/Qt/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/cloned_base/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/cloned_py31/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/py31/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/py36/lib/python3.6/site-packages/PySide2/Qt/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/py37/lib/python3.7/site-packages/PyQt5_Qt5-5.15.2-py3.7-linux-x86_64.egg/PyQt5/Qt5/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/py37/lib/python3.7/site-packages/PyQt6/Qt6/plugins/platforms/libqvnc.so
/home/jack/miniconda3/envs/py37/plugins/platforms/libqvnc.so
/home/jack/miniconda3/lib/python3.9/site-packages/PySide2/Qt/plugins/platforms/libqvnc.so
/home/jack/miniconda3/pkgs/qt-5.12.9-hda022c4_4/plugins/platforms/libqvnc.so
/home/jack/miniconda3/pkgs/qt-main-5.15.2-h327a75a_7/plugins/platforms/libqvnc.so
/home/jack/miniconda3/plugins/platforms/libqvnc.so
/usr/lib/x86_64-linux-gnu/qt5/plugins/platforms/libqvnc.so


import NodeGraphQt
graph = NodeGraphQt.NodeGraph()

import NodeGraphQt

# create a node graph controller
graph = NodeGraphQt.NodeGraph()

# create two nodes
node1 = graph.create_node("NodeGraphQt.Nodes.Math.Add")
node2 = graph.create_node("NodeGraphQt.Nodes.Math.Multiply")

# connect the nodes by drag-and-drop
node1.output(0).connect_to(node2.input(0))

# show the node graph widget
graph.widget.show()


import os
import signal

from Qt import QtCore, QtWidgets

from NodeGraphQt import (
    NodeGraph,
    PropertiesBinWidget,
    NodesTreeWidget,
    NodesPaletteWidget
)
from nodes import basic_nodes, custom_ports_node, group_node, widget_nodes

#!/usr/bin/python
# -*- coding: utf-8 -*-
import os
import signal

from Qt import QtCore, QtWidgets

from NodeGraphQt import (
    NodeGraph,
    PropertiesBinWidget,
    NodesTreeWidget,
    NodesPaletteWidget
)

# import example nodes from the "example_nodes" package
from nodes import basic_nodes, custom_ports_node, group_node, widget_nodes

if __name__ == '__main__':

    # handle SIGINT to make the app terminate on CTRL+C
    signal.signal(signal.SIGINT, signal.SIG_DFL)

    QtCore.QCoreApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling)

    app = QtWidgets.QApplication([])

    # create graph controller.
    graph = NodeGraph()

    # set up context menu for the node graph.
    graph.set_context_menu_from_file('../examples/hotkeys/hotkeys.json')

    # registered example nodes.
    graph.register_nodes([
        basic_nodes.BasicNodeA,
        basic_nodes.BasicNodeB,
        custom_ports_node.CustomPortsNode,
        group_node.MyGroupNode,
        widget_nodes.DropdownMenuNode,
        widget_nodes.TextInputNode,
        widget_nodes.CheckboxNode
    ])

    # show the node graph widget.
    graph_widget = graph.widget
    graph_widget.resize(1100, 800)
    graph_widget.show()

    # create node with custom text color and disable it.
    n_basic_a = graph.create_node(
        'nodes.basic.BasicNodeA', text_color='#feab20')
    n_basic_a.set_disabled(True)

    # create node and set a custom icon.
    n_basic_b = graph.create_node(
        'nodes.basic.BasicNodeB', name='custom icon')
    this_path = os.path.dirname(os.path.abspath(__file__))
    icon = os.path.join(this_path, 'examples', 'star.png')
    n_basic_b.set_icon(icon)

    # create node with the custom port shapes.
    n_custom_ports = graph.create_node(
        'nodes.custom.ports.CustomPortsNode', name='custom ports')

    # create node with the embedded QLineEdit widget.
    n_text_input = graph.create_node(
        'nodes.widget.TextInputNode', name='text node', color='#0a1e20')

    # create node with the embedded QCheckBox widgets.
    n_checkbox = graph.create_node(
        'nodes.widget.CheckboxNode', name='checkbox node')

    # create node with the QComboBox widget.
    n_combo_menu = graph.create_node(
        'nodes.widget.DropdownMenuNode', name='combobox node')

    # create group node.
    n_group = graph.create_node('nodes.group.MyGroupNode')

    # make node connections.

    # (connect nodes using the .set_output method)
    n_text_input.set_output(0, n_custom_ports.input(0))
    n_text_input.set_output(0, n_checkbox.input(0))
    n_text_input.set_output(0, n_combo_menu.input(0))
    # (connect nodes using the .set_input method)
    n_group.set_input(0, n_custom_ports.output(1))
    n_basic_b.set_input(2, n_checkbox.output(0))
    n_basic_b.set_input(2, n_combo_menu.output(1))
    # (connect nodes using the .connect_to method from the port object)
    port = n_basic_a.input(0)
    port.connect_to(n_basic_b.output(0))

    # auto layout nodes.
    graph.auto_layout_nodes()

    # crate a backdrop node and wrap it around
    # "custom port node" and "group node".
    n_backdrop = graph.create_node('Backdrop')
    n_backdrop.wrap_nodes([n_custom_ports, n_combo_menu])

    # fit nodes to the viewer.
    graph.clear_selection()
    graph.fit_to_selection()

    # Custom builtin widgets from NodeGraphQt
    # ---------------------------------------

    # create a node properties bin widget.
    properties_bin = PropertiesBinWidget(node_graph=graph)
    properties_bin.setWindowFlags(QtCore.Qt.Tool)

    # example show the node properties bin widget when a node is double clicked.
    def display_properties_bin(node):
        if not properties_bin.isVisible():
            properties_bin.show()

    # wire function to "node_double_clicked" signal.
    graph.node_double_clicked.connect(display_properties_bin)

    # create a nodes tree widget.
    nodes_tree = NodesTreeWidget(node_graph=graph)
    nodes_tree.set_category_label('nodeGraphQt.nodes', 'Builtin Nodes')
    nodes_tree.set_category_label('nodes.custom.ports', 'Custom Port Nodes')
    nodes_tree.set_category_label('nodes.widget', 'Widget Nodes')
    nodes_tree.set_category_label('nodes.basic', 'Basic Nodes')
    nodes_tree.set_category_label('nodes.group', 'Group Nodes')
    # nodes_tree.show()

    # create a node palette widget.
    nodes_palette = NodesPaletteWidget(node_graph=graph)
    nodes_palette.set_category_label('nodeGraphQt.nodes', 'Builtin Nodes')
    nodes_palette.set_category_label('nodes.custom.ports', 'Custom Port Nodes')
    nodes_palette.set_category_label('nodes.widget', 'Widget Nodes')
    nodes_palette.set_category_label('nodes.basic', 'Basic Nodes')
    nodes_palette.set_category_label('nodes.group', 'Group Nodes')
    # nodes_palette.show()

    app.exec_()




from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
generator("The release of ", max_length=50, num_return_sequences=10)


generator("I will go crazy for an ", max_length=50, num_return_sequences=10)

generator("Would love to try ", max_length=50, num_return_sequences=10)

import sys
import os
PATH = "/home/jack/Downloads/"
path = os.listdir(PATH)
for i in path:
        print(PATH+i)


from generated_text import gentext
from random import randint
print(len (gentext()))

from generated_text import gentext
from random import randint
end = len (gentext())-1
ID = randint(0,end)
text = str(gentext()[ID])
text = text.replace("\\'t", "'t").replace("{", "").replace("}", "").replace("\\n", " ").replace("\"", "")
STR = text.replace("\\'s","'s'")
print (text)





from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
generator("The release of ", max_length=50, num_return_sequences=10)


generator("What was so bad about the ", max_length=50, num_return_sequences=10)

generator("The release of ", max_length=50, num_return_sequences=10
[{'generated_text': "The release of \xa0The World's Dumbest People is the first in a trilogy from David Silverman and David Fincher. You may have heard of this book, called\xa0 The Man Who Sold Your Life, or, as it would come"},
 {'generated_text': "The release of iphone/iPad apps was a big success, with many of them doing well so far. But many customers were not impressed that so many apps weren't working for them. It is now common for an Android phone not to"},
 {'generated_text': 'The release of ichthyosis, a syndrome associated with anemia, has occurred earlier in women and increases in certain risk factors for certain cancers. In some cases, anemia has led to kidney or liver failure; in others, the cause has'},
 {'generated_text': 'The release of \xa0Oscar nominee Amy Poehler, a role he had previously worked out with, helped launch the new era of musical theater in the country. The move to comedy shows has also led to a surge in ticket sales, a move'},
 {'generated_text': 'The release of \xa0The Black Queen \xa0on\xa0 March 23rd, 2013 was one of the most\xa0emotional \xa0scenes ever written. Even during the last months, the release of \xa0The Black Queen \xa0on\xa0 March'},
 {'generated_text': 'The release of vernacular speech will give people the possibility to express their ideas freely and get help for learning.'},
 {'generated_text': 'The release of \xa0A Clockwork Orange has stirred a lot of debate. I mean, what am I going to do?? I mean, what am I going to do with all those hours of thinking? I mean you are now thinking like an old'},
 {'generated_text': 'The release of _____ would mean that the public is no longer immune from being exposed to the threat of mass public suicide, and it could result in serious consequences for public safety, like the loss of lives, and may even lead to further criminal and'},
 {'generated_text': "The release of 々个非来》 is a bold and powerful moment. I can't believe I've read an English book where this happened. Even I wasn't able to read a few paragraphs right now. It"},
 {'generated_text': "The release of ??? was reported by both Chinese and Vietnamese media outlets, with both groups claiming that it was being directed from Beijing.\n\nBut the Chinese government has denied that it directed these groups to commit acts of terror like last weekend's Boston"}]

generator("I will go crazy for an ", max_length=50, num_return_sequences=10)

generator("Would love to try ", max_length=50, num_return_sequences=10)

import sys
import os
PATH = "/home/jack/Downloads/"
path = os.listdir(PATH)
for i in path:
        print(PATH+i)


from generated_text import gentext
from random import randint
print(len (gentext()))

from generated_text import gentext
from random import randint
end = len (gentext())-1
ID = randint(0,end)
text = str(gentext()[ID])
text = text.replace("\\'t", "'t").replace("{", "").replace("}", "").replace("\\n", " ").replace("\"", "")
STR = text.replace("\\'s","'s'")
print (text)





http://www.frontiernet.net/~eugene.ressler/

import vsketch


class SchotterSketch(vsketch.SketchClass):
    columns = vsketch.Param(12)
    rows = vsketch.Param(22)
    fuzziness = vsketch.Param(1.0)

    def draw(self, vsk: vsketch.Vsketch) -> None:
        vsk.size("a4", landscape=False)
        vsk.scale("cm")

        for j in range(self.rows):
            with vsk.pushMatrix():
                for i in range(self.columns):
                    with vsk.pushMatrix():
                        vsk.rotate(self.fuzziness * 0.03 * vsk.random(-j, j))
                        vsk.translate(
                            self.fuzziness * 0.01 * vsk.randomGaussian() * j,
                            self.fuzziness * 0.01 * vsk.randomGaussian() * j,
                        )
                        vsk.rect(0, 0, 1, 1)
                    vsk.translate(1, 0)
            vsk.translate(0, 1)

    def finalize(self, vsk: vsketch.Vsketch) -> None:
        vsk.vpype("linemerge linesimplify reloop linesort")


if __name__ == "__main__":
    SchotterSketch.display()


import numpy as np
np.__version__
'1.22.4'

import vsketch


class SchotterSketch(vsketch.SketchClass):
    columns = vsketch.Param(40)
    rows = vsketch.Param(40)
    fuzziness = vsketch.Param(1.0)

    def draw(self, vsk: vsketch.Vsketch) -> None:
        vsk.size("a4", landscape=False)
        vsk.scale("cm")
        
        for j in range(self.rows):
            with vsk.pushMatrix():
                for i in range(self.columns):
                    with vsk.pushMatrix():
                        vsk.rotate(self.fuzziness * 0.03 * vsk.random(-j, j))
                        vsk.translate(
                            self.fuzziness * 0.01 * vsk.randomGaussian() * j,
                            self.fuzziness * 0.01 * vsk.randomGaussian() * j,
                        )
                        vsk.rect(0, 0, 1, 1)
                    vsk.translate(1, 0)
            vsk.translate(0, 1)

    def finalize(self, vsk: vsketch.Vsketch) -> None:
        vsk.vpype("linemerge linesimplify reloop linesort")


if __name__ == "__main__":
    SchotterSketch.display()        

vsk.line(0, 0, 10, 20)
vsk.rect(10, 10, 5, 8)
vsk.circle(2, 2, radius=3)
vsk.triangle(0, 0, 1, 1, 0, 1)

#!/home/jack/miniconda3/envs/cloned_base/bin/python

"""
@author: The Absolute Tinkerer
"""

import os
import math
import numpy as np
from time import sleep
from PyQt5.QtGui import QColor


def QColor_HSV(h, s, v, a=255):
    """
    Hue        : > -1 [wraps between 0-360]
    Saturation : 0-255
    Value      : 0-255
    Alpha      : 0-255
    """
    color = QColor()
    color.setHsv(*[int(e) for e in [h, s, v, a]])
    return color


def save(p, fname='image', folder='Images', extension='jpg', quality=100, overwrite=True):
    if not os.path.exists(folder):
        os.mkdir(folder)

    # The image name
    imageFile = f'{folder}/{fname}.{extension}'

    # Do not overwrite the image if it exists already
    if os.path.exists(imageFile):
        assert overwrite, 'File exists and overwrite is set to False!'

    # fileName, format, quality [0 through 100]
    p.saveImage(imageFile, imageFile[-3:], quality)


def Perlin2D(width, height, n_x, n_y, clampHorizontal=False, clampVertical=False):
    """
    Constructor

    Optimizations were gained from studying:
    https://github.com/pvigier/perlin-numpy/blob/master/perlin_numpy/perlin2d.py

    Parameters:
    -----------
    width : int
        The width of the canvas
    height : int
        The height of the canvas
    n_x : int
        The number of x tiles; must correspond to an integer x-edge length
    n_y : int
        The number of y tiles; must correspond to an integer y-edge length
    clampHorizontal : boolean
        Imagine the Perlin Noise on a sheet of paper - form a cylinder with
        the horizontal edges. If True, cylinder will be continuous noise
    clampVertical : boolean
        Imagine the Perlin Noise on a sheet of paper - form a cylinder with
        the vertical edges. If True, cylinder will be continuous noise

    Returns:
    --------
    <value> : numpy array
        noise values for array[width, height] between -1 and 1
    """
    # First ensure even number of n_x and n_y divide into the width and height,
    # respectively
    msg = 'n_x and n_y must evenly divide into width and height, respectively'
    assert width % n_x == 0 and height % n_y == 0, msg

    # We start off by defining our interpolation function
    def fade(t):
        return t * t * t * (t * (t * 6 - 15) + 10)

    # Next, we generate the gradients that we are using for each corner point
    # of the grid
    angles = 2 * np.pi * np.random.rand(n_x + 1, n_y + 1)
    r = math.sqrt(2)  # The radius of the unit circle
    gradients = np.dstack((r * np.cos(angles), r * np.sin(angles)))

    # Now, if the user has chosen to clamp at all, set the first and last row/
    # column equal to one another
    if clampHorizontal:
        gradients[-1, :] = gradients[0, :]
    if clampVertical:
        gradients[:, -1] = gradients[:, 0]

    # Now that gradient vectors are complete, we need to create the normalized
    # distance from each point to its starting grid point. In other words, this
    # is the normalized distance from the grid tile's origin based upon the
    # grid tile's width and height
    delta = (n_x / width, n_y / height)
    grid = np.mgrid[0:n_x:delta[0], 0:n_y:delta[1]].transpose(1, 2, 0) % 1

    # At this point, we need to compute the dot products for each corner of the
    # grid. To do this, we first need proper-dimensioned gradient vectors - do
    # this now. A computation for number of points per tile is needed as well
    px, py = int(width / n_x), int(height / n_y)
    gradients = gradients.repeat(px, 0).repeat(py, 1)
    g00 = gradients[:-px, :-py]
    g10 = gradients[px:, :-py]
    g01 = gradients[:-px, py:]
    g11 = gradients[px:, py:]

    # Compute dot products for each corner
    d00 = np.sum(g00 * grid, 2)
    d10 = np.sum(g10 * np.dstack((grid[:, :, 0] - 1, grid[:, :, 1])), 2)
    d01 = np.sum(g01 * np.dstack((grid[:, :, 0], grid[:, :, 1] - 1)), 2)
    d11 = np.sum(g11 * np.dstack((grid[:, :, 0] - 1, grid[:, :, 1] - 1)), 2)

    # We're doing improved perlin noise, so we use a fade function to compute
    # the x and y fractions used in the linear interpolation computation
    # t is the faded grid
    # u is the faded dot product between the top corners
    # v is the faded dot product between the bottom corners
    # _x and _y are the fractional (0-1) location of x, y in the tile
    t = fade(grid)
    u = d00 + t[:, :, 0] * (d10 - d00)
    v = d01 + t[:, :, 0] * (d11 - d01)

    # Now perform the second dimension's linear interpolation to return value
    return u + t[:, :, 1] * (v - u)

"""
@author: The Absolute Tinkerer
"""

import os
import math
import time
import random

import numpy as np

from PIL import Image

from PyQt5.QtGui import QColor, QPen, QPixmap
from PyQt5.QtCore import QPointF, QRect

import Painter
from utils import QColor_HSV, save, Perlin2D
def is_even(i):
    return i % 4000 == 0

def draw_white_noise(width, height, fname):
    assert not os.path.exists(fname), 'File already exists!'

    # Create a matrix of random values between zero and one
    pixels = np.random.random(size=(height, width))

    # Now modify the random values to be 0-255 (pixel color range)
    pixels = 255*pixels

    # The function to write the array of pixels to an image requires integers, not float values
    pixels = pixels.astype(np.uint8)

    # We choose to make random values grayscale, so each RGB element is identical. This code adds the third dimension
    # to our pixels array
    pixels = pixels[:, :, np.newaxis]

    # We need to repeat each value to finalize the pixels arrays in the grayscale space
    pixels = np.repeat(pixels, 3, axis=2)

    # Now create the image from an array of pixels
    im = Image.fromarray(pixels)

    # Save the image to file
    im.save(fname)


def draw_perlin(nx, ny, width, height, fname):
    assert not os.path.exists(fname), 'File already exists'

    # Initialize Perlin Noise
    noise = (Perlin2D(width, height, nx, ny) + 1)/2

    # Convert to pixels
    pixels = 255 * noise
    pixels = pixels.astype(np.uint8)
    pixels = pixels[:, :, np.newaxis]
    pixels = np.repeat(pixels, 3, axis=2)

    # Create and save the image from pixels
    im = Image.fromarray(pixels)
    im.save(fname)

    return noise


def draw_vectors(nx, ny, width, height, seed=random.randint(0, 100000000), flow_length=100, n_vectors=50):
    p_path = f'{seed}_1_perlin_noise.jpg'
    v_path = f'{seed}_2_vectors'
    f_path = f'{seed}_3_flow_field'

    # Ensure we don't overwrite paths
    assert not os.path.exists(p_path), 'Perlin Noise image already exists!'
    assert not os.path.exists(v_path), 'Vectors image already exists!'
    assert not os.path.exists(f_path), 'Flow field image already exists!'

    # Set the random seed for repeatability
    np.random.seed(seed)

    # Create the Perlin Noise image
    noise = draw_perlin(nx, ny, width, height, p_path)

    # Initialize the Painter object for drawing
    p = Painter.Painter(width, height)
    p.setRenderHint(p.Antialiasing)  # allow smooth drawing

    def draw_arrow(p, x_i, y_i, length=100, angle=0):
        # Compute the second points and draw the arrow body
        x_f = x_i + length*math.cos(math.radians(angle))
        y_f = y_i - length*math.sin(math.radians(angle))
        p.drawLine(x_i, y_i, x_f, y_f)

        # Compute the arrow head second points
        a_angle1, a_angle2 = math.radians(angle-30), math.radians(angle+30)
        x1 = x_f - (length/10)*math.cos(a_angle1)
        y1 = y_f + (length/10)*math.sin(a_angle1)
        x2 = x_f - (length/10)*math.cos(a_angle2)
        y2 = y_f + (length/10)*math.sin(a_angle2)
        p.drawLine(x_f, y_f, x1, y1)
        p.drawLine(x_f, y_f, x2, y2)

    # Load the Perlin Noise image and draw it with the Painter
    p.drawPixmap(QRect(0, 0, width, height), QPixmap(p_path))

    # Now we're drawing red arrows for vectors, so set the pen color to red
    p.setPen(QColor(255, 0, 0))

    # We need arrow locations, so create a grid of n_vectors x n_vectors, excluding the image border
    _nx, _ny = n_vectors, n_vectors
    dx, dy = width / (_nx + 1), height / (_ny + 1)
    x_points = [dx + i*dx for i in range(_nx)]
    y_points = [dy + i*dy for i in range(_ny)]

    # Draw the arrows
    for x in x_points:
        for y in y_points:
            angle = 360*noise[int(x), int(y)]
            draw_arrow(p, x, y, length=min(dx, dy), angle=angle)

    # Save the vector image
    save(p, fname=v_path, folder='.')

    # Now draw the flow field. Start by initializing a new Painter
    p = Painter.Painter(width, height)
    p.setRenderHint(p.Antialiasing)  # allow smooth drawing
    p.setPen(QColor(0, 0, 0))  # pen color set to black

    # Step size between points
    STEP_SIZE = 0.001 * max(width, height)

    # Draw the flow field
    for x in x_points:
        for y in y_points:
            # The starting position
            x_s, y_s = x, y
            # The current line length tracking variable
            c_len = 0
            while c_len < flow_length:
                # angle between 0 and 2*pi
                angle = 2 * noise[int(x_s), int(y_s)] * math.pi

                # Compute the new point
                x_f = x_s + STEP_SIZE * math.cos(angle)
                y_f = y_s - STEP_SIZE * math.sin(angle)

                # Draw the line
                p.drawLine(QPointF(x_s, y_s), QPointF(x_f, y_f))

                # Update the line length
                c_len += math.sqrt((x_f - x_s) ** 2 + (y_f - y_s) ** 2)

                # Break from the loop if the new point is outside our image bounds
                # or if we've exceeded the line length; otherwise update the point
                if x_f < 0 or x_f >= width or y_f < 0 or y_f >= height or c_len > flow_length:
                    break
                else:
                    x_s, y_s = x_f, y_f
    save(p, fname=f_path, folder='.')


def draw_flow_field(width, height, seed=random.randint(0, 100000000)):
    # Set the random seed for repeatability
    np.random.seed(seed)

    # These are color hues
    colors = [200, 140, 70, 340, 280]
    for i, mod in enumerate(colors):
        print('Starting Image %s/%s' % (i + 1, len(colors)))
        p = Painter.Painter(width, height)

        # Allow smooth drawing
        p.setRenderHint(p.Antialiasing)

        # Draw the background color
        p.fillRect(0, 0, width, height, QColor(0, 0, 0))

        # Set the pen color
        p.setPen(QPen(QColor(150, 150, 225, 5), 2))

        num = 1
        for j in range(num):
            print('Creating Noise... (%s/%s)' % (j + 1, num))
            p_noise = Perlin2D(width, height, 2, 2)
            print('Noise Generated! (%s/%s)' % (j + 1, num))

            MAX_LENGTH = 2 * width
            STEP_SIZE = 0.001 * max(width, height)
            NUM = int(width * height / 1000)
            POINTS = [(random.randint(0, width - 1), random.randint(0, height - 1)) for i in range(NUM)]

            for k, (x_s, y_s) in enumerate(POINTS):
                print(f'{100 * (k + 1) / len(POINTS):.1f}'.rjust(5) + '% Complete', end='\r')

                # The current line length tracking variable
                c_len = 0

                # Actually draw the flow field
                while c_len < MAX_LENGTH:
                    # Set the pen color for this segment
                    sat = 200 * (MAX_LENGTH - c_len) / MAX_LENGTH
                    hue = (mod + 130 * (height - y_s) / height) % 360
                    p.setPen(QPen(QColor_HSV(hue, sat, 255, 20), 2))

                    # angle between -pi and pi
                    angle = p_noise[int(x_s), int(y_s)] * math.pi

                    # Compute the new point
                    x_f = x_s + STEP_SIZE * math.cos(angle)
                    y_f = y_s + STEP_SIZE * math.sin(angle)

                    # Draw the line
                    p.drawLine(QPointF(x_s, y_s), QPointF(x_f, y_f))

                    # Update the line length
                    c_len += math.sqrt((x_f - x_s) ** 2 + (y_f - y_s) ** 2)

                    # Break from the loop if the new point is outside our image bounds
                    # or if we've exceeded the line length; otherwise update the point
                    if x_f < 0 or x_f >= width or y_f < 0 or y_f >= height or c_len > MAX_LENGTH:
                        break
                    else:
                        x_s, y_s = x_f, y_f

            save(p, fname=f'image_{mod}_{num}_{seed}', folder='.', overwrite=True)


def draw_perlin_rounding(width, height, fname, seed=random.randint(0, 100000000)):
    # Ensure we don't overwrite paths
    cnts = 0
    assert not os.path.exists(fname), 'Image already exists!'

    # Set the random seed for repeatability
    np.random.seed(seed)
    cnt=0
    # Initialize a new Painter
    p = Painter.Painter(width, height)
    p.setRenderHint(p.Antialiasing)

    # Draw the background color
    p.fillRect(0, 0, width, height, QColor(0, 0, 0))
    #p.fillRect(0, 0, width, height, QColor(100,100, 0))
    #p.setPen(QColor(255, 0, 0, 127))
    p.setPen(QPen(QColor(150, 150, 225, 5), 2))
    # Set the pen color
    #pc = 250 - cnt
    #if pc <10:pc=250
    #p.setPen(QColor(200, 200, pc))

    print('Creating Noise...', end='', flush=True)
    noise = Perlin2D(width, height, 1, 1)
    print('Done!')

    # The maximum line length and step size
    MAX_LENGTH = 1000
    STEP_SIZE = 0.001 * max(width, height)

    # Compute a grid 200x200 points, centered in the screen
    dx, dy = width / (200 + 1), height / (200 + 1)
    POINTS = [[(i+1)*dx, (j+1)*dy] for i in range(200) for j in range(200)]

    for i, (x_s, y_s) in enumerate(POINTS):
        print(f'{100 * (i + 1) / len(POINTS):.1f}'.rjust(5) + '% Complete', end='\r')

        # The current line length tracking variable
        c_len = 0
        while c_len < MAX_LENGTH:
            # angle between -pi and pi
            angle = math.pi*noise[int(x_s), int(y_s)]

            # Round the angle to pi/4 increments
            angle = round(angle / (math.pi / 4)) * (math.pi / 4)

            # Compute the new point
            x_f = x_s + STEP_SIZE * math.cos(angle)
            y_f = y_s + STEP_SIZE * math.sin(angle)

            # Draw the line
            p.drawLine(x_s, y_s, x_f, y_f)
            '''pc = 200 - cnts
            if pc <10:pc=200
                
            pa = 200 - cnts
            if pa <10:pa=200 
            
            
            pb = 0 + cnts
            if pb >200:pb=0 
            cnts=cnts+1 
            pc = 200 - cnts
            if pc <10:pc=200
                
            pa = 200 - cnts
            if pa <10:pa=200 
            
            
            pb = 0 + cnts
            if pb >200:pb=0 
            cnts=cnts+1 '''
            pa=100
            pb=100
            pc=0
            print(pa, pb, pc)    
            p.setPen(QColor(pa, pb, pc))
 
            if is_even(cnt):
                    scnt = str(cnt)
                    #save(p, fname=f'{fname}_{scnt}_{seed}', folder='.')
                    save(p, fname=f'{fname}{scnt}', folder='.')
                    print(i,":",end=" ")
                    #sleep(1)
            
            #scnt = str(cnt)
            #save(p, fname=f'{fname}_{scnt}_{seed}', folder='.')
            #print(f'{fname}_{scnt}_{seed}')
            cnt = cnt+1   
            # Update the line length
            c_len += math.sqrt((x_f - x_s) ** 2 + (y_f - y_s) ** 2)

            # Break from the loop if the new point is outside our image bounds
            # or if we've exceeded the line length; otherwise update the point
            if (x_f < 0 or x_f >= width or y_f < 0 or y_f >= height or
                    c_len > MAX_LENGTH):
                break
            else:
                x_s, y_s = x_f, y_f
                


class Body:
    def __init__(self, x, y, vx, vy):
        self._position = np.array([x, y], dtype=np.float64)
        self._velocity = np.array([vx, vy], dtype=np.float64)

    @property
    def position(self):
        return self._position

    @property
    def velocity(self):
        return self._velocity

    def update(self, dt):
        # update the body position
        self._position = self._position + dt*self._velocity


class ExpandingCircleRandom:
    def __init__(self, radius, num_bodies, center=(0, 0), v_limits=(-2, 2)):
        self._bodies = [Body(center[0] + radius*math.cos(i*2*math.pi/num_bodies),
                             center[1] + radius*math.sin(i*2*math.pi/num_bodies),
                             v_limits[0]+(v_limits[1]-v_limits[0])*random.random(),
                             v_limits[0]+(v_limits[1]-v_limits[0])*random.random()) for i in range(num_bodies)]

    def draw(self, dt, Painter):
        # Connect the dots between each body
        for i in range(len(self._bodies)):
            # Handle the wrapping case
            if i == len(self._bodies) - 1:
                p1 = QPointF(*self._bodies[i].position)
                p2 = QPointF(*self._bodies[0].position)
            else:
                p1 = QPointF(*self._bodies[i].position)
                p2 = QPointF(*self._bodies[i+1].position)
            Painter.drawLine(p1, p2)

        # Update the position of each body
        for i in range(len(self._bodies)):
            self._bodies[i].update(dt)


class ExpandingCircleNoise:
    def __init__(self, radius, num_bodies, noise, center=(0, 0), v_max=2):
        self._bodies = [Body(center[0] + radius*math.cos(i*2*math.pi/num_bodies),
                             center[1] + radius*math.sin(i*2*math.pi/num_bodies),
                             0, 0) for i in range(num_bodies)]
        self._v_max = v_max
        self._noise = noise

    def draw(self, dt, Painter):
        # Connect the dots between each body
        for i in range(len(self._bodies)):
            # Handle the wrapping case
            if i == len(self._bodies) - 1:
                p1 = QPointF(*self._bodies[i].position)
                p2 = QPointF(*self._bodies[0].position)
            else:
                p1 = QPointF(*self._bodies[i].position)
                p2 = QPointF(*self._bodies[i + 1].position)
            Painter.drawLine(p1, p2)

            # Try to update the velocity for each body. If we can't its because the point is beyond the noise
            # field we've created, so at that point, just maintain velocity.
            try:
                a = math.pi*self._noise[int(p1.x()), int(p1.y())]
                v = np.array([self._v_max*math.cos(a), self._v_max*math.sin(a)])
                self._bodies[i]._velocity = v
            except IndexError:
                pass

        # Update the position of each body
        for i in range(len(self._bodies)):
            self._bodies[i].update(dt)


def draw_delta_body(width, height, seed=random.randint(0, 100000000), mode='noise'):
    assert mode in ['noise', 'random'], 'Mode must either be "noise" or "random"'

    # Set the random seed for repeatability
    np.random.seed(seed)
    random.seed(seed)

    # Initialize the Painter
    p = Painter.Painter(width, height)
    p.setRenderHint(p.Antialiasing)  # Allow smooth drawing

    # Draw the background color
    p.fillRect(0, 0, width, height, QColor(0, 0, 0))

    # Set the pen color
    p.setPen(QPen(QColor(220, 220, 220, 5), 1))

    # Initialize the expanding circle centered in the canvas
    if mode == 'random':
        circle = ExpandingCircleRandom(width/8, 100, center=(width/2, height/2), v_limits=(-2, 2))
    elif mode == 'noise':
        noise = Perlin2D(width, height, 5, 5)
        circle = ExpandingCircleNoise(width/6, 200, noise, center=(width/4, height/2), v_max=5)
    else:
        circle = None

    # Initialize the delta time we're applying to each update
    dt = 0.3

    iterations = 1000
    for i in range(iterations):
        circle.draw(dt, p)

    save(p, fname=f'delta_{mode}_{seed}', folder='.', overwrite=True)

    

width = 1000
height = 1000
fname = "XXXX/0"

draw_perlin_rounding(width, height, fname, seed=random.randint(0, 100000000))

!rm -r XXXX

!mkdir XXXX

from PIL import Image
im = Image.open("XXXX/test.png_14233798.jpg")
im

def is_even(i):
    return i % 100 == 0

for i in range(0,1000):
    if is_even(i):
        print(i)

p.drawLine(x_s, y_s, x_f, y_f)
pc = 200 - cnts
if pc <10:pc=200;pa = 200 - cnts
if pa <10:pa=200;pb = 0 + cnts
if pb >200:pb=0;cnts=cnts+1           
print(pa, pb, pc)    

import butterflow
dir(butterflow)

!./SLOWmo

!ls butterflow

%%writefile butterflow/__init__.py
#!/usr/bin/env python2

import os
import sys
import itertools





#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
import gettext
STR = gettext.gettext()
print (STR)
import cv2
from PIL import Image
randomframes = []
images=[]
count = 0
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    print(filename)
    return nim
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/test001.mp4"
filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", img1)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    #STR = randTXT()
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["wood-blur-frame.png","frames.png","lined-frame.png","black-blur-frame.png", "white-blur-frame.png","beige-blur-frame.png","frame-lite.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]

    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

!ls /home/jack/Downloads



import os
from time import sleep
import urllib, json
import urllib.request
import requests
import sys
import subprocess
from Completed import track_download
import sqlite3
import watchVID
database = 'LBRYDATA.db'
conn = sqlite3.connect(database)
sql = '''create table if not exists LBRY(
KEYWORDS TEXT, DATA TEXT, unique (DATA));'''
conn.execute(sql) # shortcut for conn.cursor().execute(sql)
conn.commit()
c=conn.cursor()
search=input("SEARCH : ")
url = "https://lighthouse.lbry.com/search?s="+search
response = urllib.request.urlopen(url)
data = json.loads(response.read())
cnt=0
LIST =[]
for line in data:
    cnt=cnt+1
    line = str(line)
    line =line.replace("u'","");line =line.replace("{claimId': ","")
    line =line.replace("', name':","");line =line.replace("}","")
    line =line.replace(",","");line =line.replace("'","")
    #line =line[:-1]
    #print (line)
    line=line.split(" ")
    print (cnt,line[3]+"#"+line[1])
    LIST.append(line[3]+"#"+line[1])
    content=line[3]+"#"+line[1]

!lbrynet get rental-income-new-service-item-new#25be251a20286034b65362f57d84e3e84bfb47f4


!ls -rant /home/jack/Downloads

!ffmpeg -i /home/jack/Downloads/javascript-tutorial-sorting.mp4 -vf scale=-1:360 -strict -2 -y /home/jack/Downloads/javascript-tutorial-sortingS.mp4

!ls /home/jack/Desktop/LBRY-toolbox/Downloads

!ls /home/jack/Desktop/LBRY-toolbox/Downloads/nature-sounds-1-hour-river.mp4

!cp vid.html /var/www/lbry-toolbox.com/public/videos/index.html

!cp vid.html

 %%writefile vid.html
<htm>
<video width="640" height="360" controls>
<source src="nature-sounds-1-hour-river.mp4" type="video/mp4">
Your browser does not support the video tag.
</video> 

from IPython.display import Video

Video("Downloads/nature-sounds-1-hour-river.mp4")

from IPython.display import HTML

HTML("""
    <video alt="test" controls>
        <source src="/home/jack/Downloads/javascript-tutorial-sortingS.mp4" type="video/mp4">
    </video>
""")

!ls /home/jack/Downloads/rental-income-new-service-item.mp4

import os
from time import sleep
import urllib, json
import urllib.request
import requests
import sys
import subprocess
from Completed import track_download
import sqlite3
import watchVID
database = 'LBRYDATA.db'
conn = sqlite3.connect(database)
sql = '''create table if not exists LBRY(
KEYWORDS TEXT, DATA TEXT, unique (DATA));'''
conn.execute(sql) # shortcut for conn.cursor().execute(sql)
conn.commit()
c=conn.cursor()
search=input("SEARCH : ")
url = "https://lighthouse.lbry.com/search?s="+search
response = urllib.request.urlopen(url)
data = json.loads(response.read())
cnt=0
LIST =[]
for line in data:
    cnt=cnt+1
    line = str(line)
    line =line.replace("u'","");line =line.replace("{claimId': ","")
    line =line.replace("', name':","");line =line.replace("}","")
    line =line.replace(",","");line =line.replace("'","")
    #line =line[:-1]
    #print (line)
    line=line.split(" ")
    print (cnt,line[3]+"#"+line[1])
    LIST.append(line[3]+"#"+line[1])
    content=line[3]+"#"+line[1]
    try:
        c.execute("INSERT INTO LBRY VALUES (?,?)", (search, content))
    except:
        pass
conn.commit()
conn.close()
choice =int(input("Enter the number you wish to download: Type 99 to exit "))
#if choice==99:raise SystemExit("Stop right there!")
if choice==99:raise SystemExit("Stopped by 99 !")
print ("You will be downloading:",(LIST[choice-1]))

requests.post("http://localhost:5279", json={"method": "get", "params": {"uri": LIST[choice-1] }}).json()
print ("Monitoring filesize increase of claim id "+LIST[choice-1][:-30])
'''
bashCommand = "lbrynet get "+LIST[choice-1]
print bashCommand
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate()
print "Monitoring filesize increase of claim id "+LIST[choice-1][:-30]
'''
track_download()

#uncomment below to auto-open vlc to view the downloaded video
#try:
#    watchVID.WATCH()
#except:
#    sys.exit()


!ls



from PIL import Image
# Creating a image1 object
image1 = Image.open("image1.jpg").convert("RGBA")
# Creating a image2 object
image2 = Image.open("image2.jpg").convert("RGBA")
image2 = image2.resize(image1.size)
count = 0
for i in range(0,11):
   I = float(i*.1)
   count = count +1
   # As Alpha value is 0.0, Image1 would be returned
   image3 = Image.blend(image1,image2,I)
   print(I)
   image3.save(str(count)+"output1.png")
#image3.show()
# As Alpha value is 0.5, Blend of both would be returned
image4 = Image.blend(image1,image2,0.5)
image4.save("output2.png")
image4.show()

from PIL import Image
# creating image object
img10 = Image.open("image2.jpg").convert("RGBA")
# creating image2 object having alpha
img2 = Image.open("image1.jpg").convert("RGBA")
img2 = img2.resize(img10.size)
# using alpha_composite
image4 = Image.blend(image1,image2,0.5)
image4.show()

!pwd

color = (0, 0, 0,3)
formatted_color = tuple(f"{c:04}".replace(" ", "0") for c in color)
print(formatted_color)


 opacity = 150
 r, g, b, a = 0, 0, 0, opacity
color = (r, g, b, a)
formatted_color = tuple(f"{c:04}".replace(" ", "0") for c in color)
r,g,b,a = formatted_color
R,G,B,A = int(r),int(g),int(b),int(a)
print(R,G,B,A)

img10



import os.path
import logging
import torch
from python_utils.logger import Logged
import python_utils.logger
import python_utils
from utils import utils_logger
from utils import utils_image as util
from utils import utils_model
from models.network_rrdbnet import RRDBNet as net

!ls models

import sys
sys.path.append('/home/jack/Desktop/BSRGAN')
sys.path
#['', ..., '/home/sergey']
sys.path.remove('/home/jack/hidden')
#sys.path

!find . -name '*.py' | grep get_image_paths

!find . -name '*.py' | grep "import os"

from utils import *

from utils import utils_image

util.get_image_paths

import os.path
import logging
import torch
import sys
import os
sys.path.append('/home/jack/Desktop/BSRGAN')
#from utils import utils_image
import utils_image
#sys.path.remove('/home/jack/hidden')
#sys.path
from python_utils.logger import Logged
import python_utils.logger
#import python_utils
#from utils import utils_logger
#from utils import utils_image as util
# from utils import utils_model
from models.network_rrdbnet import RRDBNet as net


"""
Spyder (Python 3.6-3.7)
PyTorch 1.4.0-1.8.1
Windows 10 or Linux
Kai Zhang (cskaizhang@gmail.com)
github: https://github.com/cszn/BSRGAN
        https://github.com/cszn/KAIR
If you have any question, please feel free to contact with me.
Kai Zhang (e-mail: cskaizhang@gmail.com)
by Kai Zhang ( March/2020 --> March/2021 --> )
This work was previously submitted to CVPR2021.

# --------------------------------------------
@inproceedings{zhang2021designing,
  title={Designing a Practical Degradation Model for Deep Blind Image Super-Resolution},
  author={Zhang, Kai and Liang, Jingyun and Van Gool, Luc and Timofte, Radu},
  booktitle={arxiv},
  year={2021}
}
# --------------------------------------------

"""


def main():

    #utils_logger.logger_info('blind_sr_log', log_path='blind_sr_log.log')
    Logged.info('blind_sr_log', log_path='blind_sr_log.log')
    #python_utils.logger('blind_sr_log', log_path='blind_sr_log.log')
    #logger = logging.getLogger('blind_sr_log')

#    print(torch.__version__)               # pytorch version
#    print(torch.version.cuda)              # cuda version
#    print(torch.backends.cudnn.version())  # cudnn version

    testsets = 'testsets'       # fixed, set path of testsets
    testset_Ls = ['RealSRSet']  # ['RealSRSet','DPED']

    #model_names = ['RRDB','ESRGAN','FSSR_DPED','FSSR_JPEG','RealSR_DPED','RealSR_JPEG']
    model_names = ['BSRGANx2']    # 'BSRGANx2' for scale factor 2



    save_results = True
    sf = 4
    device = torch.device('cpu')# if torch.cuda.is_available() else 'cpu')

    for model_name in model_names:
        if model_name in ['BSRGANx2']:
            sf = 2
        model_path = os.path.join('model_zoo', model_name+'.pth')          # set model path
        Logged.logger.info('{:>16s} : {:s}'.format('Model Name', model_name))

        # torch.cuda.set_device(0)      # set GPU ID
        #Logged.logger.info('{:>16s} : {:<d}'.format('GPU ID', torch.cuda.current_device()))
        #torch.cuda.empty_cache()

        # --------------------------------
        # define network and load model
        # --------------------------------
        model = net(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, sf=sf)  # define network

#            model_old = torch.load(model_path)
#            state_dict = model.state_dict()
#            for ((key, param),(key2, param2)) in zip(model_old.items(), state_dict.items()):
#                state_dict[key2] = param
#            model.load_state_dict(state_dict, strict=True)

        model.load_state_dict(torch.load(model_path), strict=True)
        model.eval()
        for k, v in model.named_parameters():
            v.requires_grad = False
        model = model.to(device)
        torch.cuda.empty_cache()

        for testset_L in testset_Ls:

            L_path = os.path.join(testsets, testset_L)
            #E_path = os.path.join(testsets, testset_L+'_'+model_name)
            E_path = os.path.join(testsets, testset_L+'_results_x'+str(sf))
            try:
                os.makedirs(E_path)
            except FileExistsError:
                # directory already exists
                pass
 
            Logged.logger.info('{:>16s} : {:s}'.format('Input Path', L_path))
            Logged.logger.info('{:>16s} : {:s}'.format('Output Path', E_path))
            idx = 0

            for img in utils_image.get_image_paths(L_path):

                # --------------------------------
                # (1) img_L
                # --------------------------------
                idx += 1
                img_name, ext = os.path.splitext(os.path.basename(img))
                Logged.logger.info('{:->4d} --> {:<s} --> x{:<d}--> {:<s}'.format(idx, model_name, sf, img_name+ext))

                img_L = utils_image.imread_uint(img, n_channels=3)
                img_L = utils_image.uint2tensor4(img_L)
                img_L = img_L.to(device)

                # --------------------------------
                # (2) inference
                # --------------------------------
                img_E = model(img_L)

                # --------------------------------
                # (3) img_E
                # --------------------------------
                img_E = utils_image.tensor2uint(img_E)
                if save_results:
                    utils_image.imsave(img_E, os.path.join(E_path, img_name+'_'+model_name+'.png'))


if __name__ == '__main__':

    main()


!cat /home/jack/Desktop/BSRGAN/blind_sr_log.log

import os
E_path = "Testing"
os.mkdir(E_path)


model_names = ['RRDB','ESRGAN','FSSR_DPED','FSSR_JPEG','RealSR_DPED','RealSR_JPEG']
model_names = ['BSRGAN','BSRGANx2']    # 'BSRGANx2' for scale factor 2

for model_name in model_names:
    if model_name in ['BSRGANx2']:
        sf = 2
        model_path = os.path.join('model_zoo', model_name+'.pth')          # set model path
        Logged.logger.info('{:>16s} : {:s}'.format('Model Name', model_name))
print (model_path)

import sys
sys.path.append('/home/jack/Desktop/BSRGAN')
sys.path
#['', ..., '/home/sergey']
sys.path.remove('/home/jack/hidden')
sys.path


#utils_logger.logger_info('blind_sr_log', log_path='blind_sr_log.log')
Logged.info('blind_sr_log', log_path='blind_sr_log.log')
#python_utils.logger('blind_sr_log', log_path='blind_sr_log.log')
#logger = logging.getLogger('blind_sr_log')

FORMAT = '%(asctime)s %(clientip)-15s %(user)-8s %(message)s'
logging.basicConfig(format=FORMAT)
d = {'clientip': '192.168.0.1', 'user': 'fbloggs'}
logger = logging.getLogger('tcpserver')
logger.warning('Protocol problem: %s', 'connection reset', extra=d)

Logged.info('blind_sr_log', log_path='blind_sr_log.log')

from python_utils import logger
import logging
import functools

Logged.info('blind_sr_log', log_path='blind_sr_log.log')

logger = logging.getLogger()
#Logged.info('blind_sr_log', log_path='blind_sr_log.log')
dir(logger)

#%%writefile ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/python_utils/logger.py
import abc
import functools
import logging

__all__ = ['Logged']

import typing


class LoggerBase(abc.ABC):
    '''Class which automatically adds logging utilities to your class when
    interiting. Expects `logger` to be a logging.Logger or compatible instance.

    Adds easy access to debug, info, warning, error, exception and log methods

    >>> class MyClass(LoggerBase):
    ...     logger = logging.getLogger(__name__)
    ...
    ...     def __init__(self):
    ...         Logged.__init__(self)

    >>> my_class = MyClass()
    >>> my_class.debug('debug')
    >>> my_class.info('info')
    >>> my_class.warning('warning')
    >>> my_class.error('error')
    >>> my_class.exception('exception')
    >>> my_class.log(0, 'log')
    '''

    # Being a tad lazy here and not creating a Protocol.
    # The actual classes define the correct type anyway
    logger: typing.Any

    @classmethod
    def __get_name(cls, *name_parts: str) -> str:
        return '.'.join(n.strip() for n in name_parts if n.strip())

    @classmethod
    @functools.wraps(logging.debug)
    def debug(cls, msg: str, *args: typing.Any, **kwargs: typing.Any):
        cls.logger.debug(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.info)
    def info(cls, msg: str, *args: typing.Any, **kwargs: typing.Any):
        cls.logger.info(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.warning)
    def warning(cls, msg: str, *args: typing.Any, **kwargs: typing.Any):
        cls.logger.warning(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.error)
    def error(cls, msg: str, *args: typing.Any, **kwargs: typing.Any):
        cls.logger.error(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.exception)
    def exception(cls, msg: str, *args: typing.Any, **kwargs: typing.Any):
        cls.logger.exception(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.log)
    def log(cls, lvl: int, msg: str, *args: typing.Any, **kwargs: typing.Any):
        cls.logger.log(lvl, msg, *args, **kwargs)


class Logged(LoggerBase):
    '''Class which automatically adds a named logger to your class when
    interiting

    Adds easy access to debug, info, warning, error, exception and log methods

    >>> class MyClass(Logged):
    ...     def __init__(self):
    ...         Logged.__init__(self)

    >>> my_class = MyClass()
    >>> my_class.debug('debug')
    >>> my_class.info('info')
    >>> my_class.warning('warning')
    >>> my_class.error('error')
    >>> my_class.exception('exception')
    >>> my_class.log(0, 'log')

    >>> my_class._Logged__get_name('spam')
    'spam'
    '''

    logger: logging.Logger  # pragma: no cover
    #jack    
    #logger = logging.getLogger()
    logger.info('info')
    
    
    @classmethod
    def __get_name(cls, *name_parts: str) -> str:
        return LoggerBase._LoggerBase__get_name(*name_parts)  # type: ignore

    def __new__(cls, *args, **kwargs):
        cls.logger = logging.getLogger(
            cls.__get_name(cls.__module__, cls.__name__)
        )
        return super(Logged, cls).__new__(cls)


from python_utils import logger as logger


dir(Logged.info)

dir(Logged)

dir(python_utils.logger)

# Example 1

from python_utils.logger import Logged

class MyClass(Logged):
    def __init__(self):
        Logged.__init__(self)


my_class = MyClass()
my_class.error('error')

!locate /utils/__init__.py | grep cloned

python_utils package
https://python-utils.readthedocs.io/en/latest/python_utils.html#
https://python-utils.readthedocs.io/en/latest/python_utils.html#module-python_utils.logger
    

from python_utils.logger import *
Logged

class MyClass(Logged):
     def __init__(self):
        Logged.__init__(self)
my_class = MyClass()
my_class.debug('debug')
my_class.info('info')
my_class.warning('warning')
my_class.error('error')
my_class.exception('exception')
my_class.log(0, 'log')

sys.path.append('./lib/python3.7/site-packages')

#python_utils.logger.Logged
import logging
import functools

__all__ = ['Logged']


#[docs]
class Logged(object):
    '''Class which automatically adds a named logger to your class when
    interiting

    Adds easy access to debug, info, warning, error, exception and log methods

    >>> class MyClass(Logged):
    ...     def __init__(self):
    ...         Logged.__init__(self)
    >>> my_class = MyClass()
    >>> my_class.debug('debug')
    >>> my_class.info('info')
    >>> my_class.warning('warning')
    >>> my_class.error('error')
    >>> my_class.exception('exception')
    >>> my_class.log(0, 'log')
    '''
    def __new__(cls, *args, **kwargs):
        cls.logger = logging.getLogger(
            cls.__get_name(cls.__module__, cls.__name__))
        return super(Logged, cls).__new__(cls)

    @classmethod
    def __get_name(cls, *name_parts):
        return '.'.join(n.strip() for n in name_parts if n.strip())

    @classmethod
    @functools.wraps(logging.debug)
    def debug(cls, msg, *args, **kwargs):
        cls.logger.debug(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.info)
    def info(cls, msg, *args, **kwargs):
        cls.logger.info(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.warning)
    def warning(cls, msg, *args, **kwargs):
        cls.logger.warning(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.error)
    def error(cls, msg, *args, **kwargs):
        cls.logger.error(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.exception)
    def exception(cls, msg, *args, **kwargs):
        cls.logger.exception(msg, *args, **kwargs)

    @classmethod
    @functools.wraps(logging.log)
    def log(cls, lvl, msg, *args, **kwargs):
        cls.logger.log(lvl, msg, *args, **kwargs)

import _functools
functools.__file__

# %load /home/jack/miniconda3/envs/cloned_base/lib/python3.9/functools.py
"""functools.py - Tools for working with functions and callable objects
"""
# Python module wrapper for _functools C module
# to allow utilities written in Python to be added
# to the functools module.
# Written by Nick Coghlan <ncoghlan at gmail.com>,
# Raymond Hettinger <python at rcn.com>,
# and Łukasz Langa <lukasz at langa.pl>.
#   Copyright (C) 2006-2013 Python Software Foundation.
# See C source code for _functools credits/copyright

__all__ = ['update_wrapper', 'wraps', 'WRAPPER_ASSIGNMENTS', 'WRAPPER_UPDATES',
           'total_ordering', 'cache', 'cmp_to_key', 'lru_cache', 'reduce',
           'partial', 'partialmethod', 'singledispatch', 'singledispatchmethod',
           'cached_property']

from abc import get_cache_token
from collections import namedtuple
# import types, weakref  # Deferred to single_dispatch()
from reprlib import recursive_repr
from _thread import RLock
from types import GenericAlias


################################################################################
### update_wrapper() and wraps() decorator
################################################################################

# update_wrapper() and wraps() are tools to help write
# wrapper functions that can handle naive introspection

WRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__qualname__', '__doc__',
                       '__annotations__')
WRAPPER_UPDATES = ('__dict__',)
def update_wrapper(wrapper,
                   wrapped,
                   assigned = WRAPPER_ASSIGNMENTS,
                   updated = WRAPPER_UPDATES):
    """Update a wrapper function to look like the wrapped function

       wrapper is the function to be updated
       wrapped is the original function
       assigned is a tuple naming the attributes assigned directly
       from the wrapped function to the wrapper function (defaults to
       functools.WRAPPER_ASSIGNMENTS)
       updated is a tuple naming the attributes of the wrapper that
       are updated with the corresponding attribute from the wrapped
       function (defaults to functools.WRAPPER_UPDATES)
    """
    for attr in assigned:
        try:
            value = getattr(wrapped, attr)
        except AttributeError:
            pass
        else:
            setattr(wrapper, attr, value)
    for attr in updated:
        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))
    # Issue #17482: set __wrapped__ last so we don't inadvertently copy it
    # from the wrapped function when updating __dict__
    wrapper.__wrapped__ = wrapped
    # Return the wrapper so this can be used as a decorator via partial()
    return wrapper

def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)


################################################################################
### total_ordering class decorator
################################################################################

# The total ordering functions all invoke the root magic method directly
# rather than using the corresponding operator.  This avoids possible
# infinite recursion that could occur when the operator dispatch logic
# detects a NotImplemented result and then calls a reflected method.

def _gt_from_lt(self, other, NotImplemented=NotImplemented):
    'Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).'
    op_result = type(self).__lt__(self, other)
    if op_result is NotImplemented:
        return op_result
    return not op_result and self != other

def _le_from_lt(self, other, NotImplemented=NotImplemented):
    'Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).'
    op_result = type(self).__lt__(self, other)
    if op_result is NotImplemented:
        return op_result
    return op_result or self == other

def _ge_from_lt(self, other, NotImplemented=NotImplemented):
    'Return a >= b.  Computed by @total_ordering from (not a < b).'
    op_result = type(self).__lt__(self, other)
    if op_result is NotImplemented:
        return op_result
    return not op_result

def _ge_from_le(self, other, NotImplemented=NotImplemented):
    'Return a >= b.  Computed by @total_ordering from (not a <= b) or (a == b).'
    op_result = type(self).__le__(self, other)
    if op_result is NotImplemented:
        return op_result
    return not op_result or self == other

def _lt_from_le(self, other, NotImplemented=NotImplemented):
    'Return a < b.  Computed by @total_ordering from (a <= b) and (a != b).'
    op_result = type(self).__le__(self, other)
    if op_result is NotImplemented:
        return op_result
    return op_result and self != other

def _gt_from_le(self, other, NotImplemented=NotImplemented):
    'Return a > b.  Computed by @total_ordering from (not a <= b).'
    op_result = type(self).__le__(self, other)
    if op_result is NotImplemented:
        return op_result
    return not op_result

def _lt_from_gt(self, other, NotImplemented=NotImplemented):
    'Return a < b.  Computed by @total_ordering from (not a > b) and (a != b).'
    op_result = type(self).__gt__(self, other)
    if op_result is NotImplemented:
        return op_result
    return not op_result and self != other

def _ge_from_gt(self, other, NotImplemented=NotImplemented):
    'Return a >= b.  Computed by @total_ordering from (a > b) or (a == b).'
    op_result = type(self).__gt__(self, other)
    if op_result is NotImplemented:
        return op_result
    return op_result or self == other

def _le_from_gt(self, other, NotImplemented=NotImplemented):
    'Return a <= b.  Computed by @total_ordering from (not a > b).'
    op_result = type(self).__gt__(self, other)
    if op_result is NotImplemented:
        return op_result
    return not op_result

def _le_from_ge(self, other, NotImplemented=NotImplemented):
    'Return a <= b.  Computed by @total_ordering from (not a >= b) or (a == b).'
    op_result = type(self).__ge__(self, other)
    if op_result is NotImplemented:
        return op_result
    return not op_result or self == other

def _gt_from_ge(self, other, NotImplemented=NotImplemented):
    'Return a > b.  Computed by @total_ordering from (a >= b) and (a != b).'
    op_result = type(self).__ge__(self, other)
    if op_result is NotImplemented:
        return op_result
    return op_result and self != other

def _lt_from_ge(self, other, NotImplemented=NotImplemented):
    'Return a < b.  Computed by @total_ordering from (not a >= b).'
    op_result = type(self).__ge__(self, other)
    if op_result is NotImplemented:
        return op_result
    return not op_result

_convert = {
    '__lt__': [('__gt__', _gt_from_lt),
               ('__le__', _le_from_lt),
               ('__ge__', _ge_from_lt)],
    '__le__': [('__ge__', _ge_from_le),
               ('__lt__', _lt_from_le),
               ('__gt__', _gt_from_le)],
    '__gt__': [('__lt__', _lt_from_gt),
               ('__ge__', _ge_from_gt),
               ('__le__', _le_from_gt)],
    '__ge__': [('__le__', _le_from_ge),
               ('__gt__', _gt_from_ge),
               ('__lt__', _lt_from_ge)]
}

def total_ordering(cls):
    """Class decorator that fills in missing ordering methods"""
    # Find user-defined comparisons (not those inherited from object).
    roots = {op for op in _convert if getattr(cls, op, None) is not getattr(object, op, None)}
    if not roots:
        raise ValueError('must define at least one ordering operation: < > <= >=')
    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__
    for opname, opfunc in _convert[root]:
        if opname not in roots:
            opfunc.__name__ = opname
            setattr(cls, opname, opfunc)
    return cls


################################################################################
### cmp_to_key() function converter
################################################################################

def cmp_to_key(mycmp):
    """Convert a cmp= function into a key= function"""
    class K(object):
        __slots__ = ['obj']
        def __init__(self, obj):
            self.obj = obj
        def __lt__(self, other):
            return mycmp(self.obj, other.obj) < 0
        def __gt__(self, other):
            return mycmp(self.obj, other.obj) > 0
        def __eq__(self, other):
            return mycmp(self.obj, other.obj) == 0
        def __le__(self, other):
            return mycmp(self.obj, other.obj) <= 0
        def __ge__(self, other):
            return mycmp(self.obj, other.obj) >= 0
        __hash__ = None
    return K

try:
    from _functools import cmp_to_key
except ImportError:
    pass


################################################################################
### reduce() sequence to a single item
################################################################################

_initial_missing = object()

def reduce(function, sequence, initial=_initial_missing):
    """
    reduce(function, sequence[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence,
    from left to right, so as to reduce the sequence to a single value.
    For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the sequence in the calculation, and serves as a default when the
    sequence is empty.
    """

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError("reduce() of empty sequence with no initial value") from None
    else:
        value = initial

    for element in it:
        value = function(value, element)

    return value

try:
    from _functools import reduce
except ImportError:
    pass


################################################################################
### partial() argument application
################################################################################

# Purely functional, no descriptor behaviour
class partial:
    """New function with partial application of the given arguments
    and keywords.
    """

    __slots__ = "func", "args", "keywords", "__dict__", "__weakref__"

    def __new__(cls, func, /, *args, **keywords):
        if not callable(func):
            raise TypeError("the first argument must be callable")

        if hasattr(func, "func"):
            args = func.args + args
            keywords = {**func.keywords, **keywords}
            func = func.func

        self = super(partial, cls).__new__(cls)

        self.func = func
        self.args = args
        self.keywords = keywords
        return self

    def __call__(self, /, *args, **keywords):
        keywords = {**self.keywords, **keywords}
        return self.func(*self.args, *args, **keywords)

    @recursive_repr()
    def __repr__(self):
        qualname = type(self).__qualname__
        args = [repr(self.func)]
        args.extend(repr(x) for x in self.args)
        args.extend(f"{k}={v!r}" for (k, v) in self.keywords.items())
        if type(self).__module__ == "functools":
            return f"functools.{qualname}({', '.join(args)})"
        return f"{qualname}({', '.join(args)})"

    def __reduce__(self):
        return type(self), (self.func,), (self.func, self.args,
               self.keywords or None, self.__dict__ or None)

    def __setstate__(self, state):
        if not isinstance(state, tuple):
            raise TypeError("argument to __setstate__ must be a tuple")
        if len(state) != 4:
            raise TypeError(f"expected 4 items in state, got {len(state)}")
        func, args, kwds, namespace = state
        if (not callable(func) or not isinstance(args, tuple) or
           (kwds is not None and not isinstance(kwds, dict)) or
           (namespace is not None and not isinstance(namespace, dict))):
            raise TypeError("invalid partial state")

        args = tuple(args) # just in case it's a subclass
        if kwds is None:
            kwds = {}
        elif type(kwds) is not dict: # XXX does it need to be *exactly* dict?
            kwds = dict(kwds)
        if namespace is None:
            namespace = {}

        self.__dict__ = namespace
        self.func = func
        self.args = args
        self.keywords = kwds

try:
    from _functools import partial
except ImportError:
    pass

# Descriptor version
class partialmethod(object):
    """Method descriptor with partial application of the given arguments
    and keywords.

    Supports wrapping existing descriptors and handles non-descriptor
    callables as instance methods.
    """

    def __init__(self, func, /, *args, **keywords):
        if not callable(func) and not hasattr(func, "__get__"):
            raise TypeError("{!r} is not callable or a descriptor"
                                 .format(func))

        # func could be a descriptor like classmethod which isn't callable,
        # so we can't inherit from partial (it verifies func is callable)
        if isinstance(func, partialmethod):
            # flattening is mandatory in order to place cls/self before all
            # other arguments
            # it's also more efficient since only one function will be called
            self.func = func.func
            self.args = func.args + args
            self.keywords = {**func.keywords, **keywords}
        else:
            self.func = func
            self.args = args
            self.keywords = keywords

    def __repr__(self):
        args = ", ".join(map(repr, self.args))
        keywords = ", ".join("{}={!r}".format(k, v)
                                 for k, v in self.keywords.items())
        format_string = "{module}.{cls}({func}, {args}, {keywords})"
        return format_string.format(module=self.__class__.__module__,
                                    cls=self.__class__.__qualname__,
                                    func=self.func,
                                    args=args,
                                    keywords=keywords)

    def _make_unbound_method(self):
        def _method(cls_or_self, /, *args, **keywords):
            keywords = {**self.keywords, **keywords}
            return self.func(cls_or_self, *self.args, *args, **keywords)
        _method.__isabstractmethod__ = self.__isabstractmethod__
        _method._partialmethod = self
        return _method

    def __get__(self, obj, cls=None):
        get = getattr(self.func, "__get__", None)
        result = None
        if get is not None:
            new_func = get(obj, cls)
            if new_func is not self.func:
                # Assume __get__ returning something new indicates the
                # creation of an appropriate callable
                result = partial(new_func, *self.args, **self.keywords)
                try:
                    result.__self__ = new_func.__self__
                except AttributeError:
                    pass
        if result is None:
            # If the underlying descriptor didn't do anything, treat this
            # like an instance method
            result = self._make_unbound_method().__get__(obj, cls)
        return result

    @property
    def __isabstractmethod__(self):
        return getattr(self.func, "__isabstractmethod__", False)

    __class_getitem__ = classmethod(GenericAlias)


# Helper functions

def _unwrap_partial(func):
    while isinstance(func, partial):
        func = func.func
    return func

################################################################################
### LRU Cache function decorator
################################################################################

_CacheInfo = namedtuple("CacheInfo", ["hits", "misses", "maxsize", "currsize"])

class _HashedSeq(list):
    """ This class guarantees that hash() will be called no more than once
        per element.  This is important because the lru_cache() will hash
        the key multiple times on a cache miss.

    """

    __slots__ = 'hashvalue'

    def __init__(self, tup, hash=hash):
        self[:] = tup
        self.hashvalue = hash(tup)

    def __hash__(self):
        return self.hashvalue

def _make_key(args, kwds, typed,
             kwd_mark = (object(),),
             fasttypes = {int, str},
             tuple=tuple, type=type, len=len):
    """Make a cache key from optionally typed positional and keyword arguments

    The key is constructed in a way that is flat as possible rather than
    as a nested structure that would take more memory.

    If there is only a single argument and its data type is known to cache
    its hash value, then that argument is returned without a wrapper.  This
    saves space and improves lookup speed.

    """
    # All of code below relies on kwds preserving the order input by the user.
    # Formerly, we sorted() the kwds before looping.  The new way is *much*
    # faster; however, it means that f(x=1, y=2) will now be treated as a
    # distinct call from f(y=2, x=1) which will be cached separately.
    key = args
    if kwds:
        key += kwd_mark
        for item in kwds.items():
            key += item
    if typed:
        key += tuple(type(v) for v in args)
        if kwds:
            key += tuple(type(v) for v in kwds.values())
    elif len(key) == 1 and type(key[0]) in fasttypes:
        return key[0]
    return _HashedSeq(key)

def lru_cache(maxsize=128, typed=False):
    """Least-recently-used cache decorator.

    If *maxsize* is set to None, the LRU features are disabled and the cache
    can grow without bound.

    If *typed* is True, arguments of different types will be cached separately.
    For example, f(3.0) and f(3) will be treated as distinct calls with
    distinct results.

    Arguments to the cached function must be hashable.

    View the cache statistics named tuple (hits, misses, maxsize, currsize)
    with f.cache_info().  Clear the cache and statistics with f.cache_clear().
    Access the underlying function with f.__wrapped__.

    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)

    """

    # Users should only access the lru_cache through its public API:
    #       cache_info, cache_clear, and f.__wrapped__
    # The internals of the lru_cache are encapsulated for thread safety and
    # to allow the implementation to change (including a possible C version).

    if isinstance(maxsize, int):
        # Negative maxsize is treated as 0
        if maxsize < 0:
            maxsize = 0
    elif callable(maxsize) and isinstance(typed, bool):
        # The user_function was passed in directly via the maxsize argument
        user_function, maxsize = maxsize, 128
        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)
        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}
        return update_wrapper(wrapper, user_function)
    elif maxsize is not None:
        raise TypeError(
            'Expected first argument to be an integer, a callable, or None')

    def decorating_function(user_function):
        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)
        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}
        return update_wrapper(wrapper, user_function)

    return decorating_function

def _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo):
    # Constants shared by all lru cache instances:
    sentinel = object()          # unique object used to signal cache misses
    make_key = _make_key         # build a key from the function arguments
    PREV, NEXT, KEY, RESULT = 0, 1, 2, 3   # names for the link fields

    cache = {}
    hits = misses = 0
    full = False
    cache_get = cache.get    # bound method to lookup a key or return None
    cache_len = cache.__len__  # get cache size without calling len()
    lock = RLock()           # because linkedlist updates aren't threadsafe
    root = []                # root of the circular doubly linked list
    root[:] = [root, root, None, None]     # initialize by pointing to self

    if maxsize == 0:

        def wrapper(*args, **kwds):
            # No caching -- just a statistics update
            nonlocal misses
            misses += 1
            result = user_function(*args, **kwds)
            return result

    elif maxsize is None:

        def wrapper(*args, **kwds):
            # Simple caching without ordering or size limit
            nonlocal hits, misses
            key = make_key(args, kwds, typed)
            result = cache_get(key, sentinel)
            if result is not sentinel:
                hits += 1
                return result
            misses += 1
            result = user_function(*args, **kwds)
            cache[key] = result
            return result

    else:

        def wrapper(*args, **kwds):
            # Size limited caching that tracks accesses by recency
            nonlocal root, hits, misses, full
            key = make_key(args, kwds, typed)
            with lock:
                link = cache_get(key)
                if link is not None:
                    # Move the link to the front of the circular queue
                    link_prev, link_next, _key, result = link
                    link_prev[NEXT] = link_next
                    link_next[PREV] = link_prev
                    last = root[PREV]
                    last[NEXT] = root[PREV] = link
                    link[PREV] = last
                    link[NEXT] = root
                    hits += 1
                    return result
                misses += 1
            result = user_function(*args, **kwds)
            with lock:
                if key in cache:
                    # Getting here means that this same key was added to the
                    # cache while the lock was released.  Since the link
                    # update is already done, we need only return the
                    # computed result and update the count of misses.
                    pass
                elif full:
                    # Use the old root to store the new key and result.
                    oldroot = root
                    oldroot[KEY] = key
                    oldroot[RESULT] = result
                    # Empty the oldest link and make it the new root.
                    # Keep a reference to the old key and old result to
                    # prevent their ref counts from going to zero during the
                    # update. That will prevent potentially arbitrary object
                    # clean-up code (i.e. __del__) from running while we're
                    # still adjusting the links.
                    root = oldroot[NEXT]
                    oldkey = root[KEY]
                    oldresult = root[RESULT]
                    root[KEY] = root[RESULT] = None
                    # Now update the cache dictionary.
                    del cache[oldkey]
                    # Save the potentially reentrant cache[key] assignment
                    # for last, after the root and links have been put in
                    # a consistent state.
                    cache[key] = oldroot
                else:
                    # Put result in a new link at the front of the queue.
                    last = root[PREV]
                    link = [last, root, key, result]
                    last[NEXT] = root[PREV] = cache[key] = link
                    # Use the cache_len bound method instead of the len() function
                    # which could potentially be wrapped in an lru_cache itself.
                    full = (cache_len() >= maxsize)
            return result

    def cache_info():
        """Report cache statistics"""
        with lock:
            return _CacheInfo(hits, misses, maxsize, cache_len())

    def cache_clear():
        """Clear the cache and cache statistics"""
        nonlocal hits, misses, full
        with lock:
            cache.clear()
            root[:] = [root, root, None, None]
            hits = misses = 0
            full = False

    wrapper.cache_info = cache_info
    wrapper.cache_clear = cache_clear
    return wrapper

try:
    from _functools import _lru_cache_wrapper
except ImportError:
    pass


################################################################################
### cache -- simplified access to the infinity cache
################################################################################

def cache(user_function, /):
    'Simple lightweight unbounded cache.  Sometimes called "memoize".'
    return lru_cache(maxsize=None)(user_function)


################################################################################
### singledispatch() - single-dispatch generic function decorator
################################################################################

def _c3_merge(sequences):
    """Merges MROs in *sequences* to a single MRO using the C3 algorithm.

    Adapted from https://www.python.org/download/releases/2.3/mro/.

    """
    result = []
    while True:
        sequences = [s for s in sequences if s]   # purge empty sequences
        if not sequences:
            return result
        for s1 in sequences:   # find merge candidates among seq heads
            candidate = s1[0]
            for s2 in sequences:
                if candidate in s2[1:]:
                    candidate = None
                    break      # reject the current head, it appears later
            else:
                break
        if candidate is None:
            raise RuntimeError("Inconsistent hierarchy")
        result.append(candidate)
        # remove the chosen candidate
        for seq in sequences:
            if seq[0] == candidate:
                del seq[0]

def _c3_mro(cls, abcs=None):
    """Computes the method resolution order using extended C3 linearization.

    If no *abcs* are given, the algorithm works exactly like the built-in C3
    linearization used for method resolution.

    If given, *abcs* is a list of abstract base classes that should be inserted
    into the resulting MRO. Unrelated ABCs are ignored and don't end up in the
    result. The algorithm inserts ABCs where their functionality is introduced,
    i.e. issubclass(cls, abc) returns True for the class itself but returns
    False for all its direct base classes. Implicit ABCs for a given class
    (either registered or inferred from the presence of a special method like
    __len__) are inserted directly after the last ABC explicitly listed in the
    MRO of said class. If two implicit ABCs end up next to each other in the
    resulting MRO, their ordering depends on the order of types in *abcs*.

    """
    for i, base in enumerate(reversed(cls.__bases__)):
        if hasattr(base, '__abstractmethods__'):
            boundary = len(cls.__bases__) - i
            break   # Bases up to the last explicit ABC are considered first.
    else:
        boundary = 0
    abcs = list(abcs) if abcs else []
    explicit_bases = list(cls.__bases__[:boundary])
    abstract_bases = []
    other_bases = list(cls.__bases__[boundary:])
    for base in abcs:
        if issubclass(cls, base) and not any(
                issubclass(b, base) for b in cls.__bases__
            ):
            # If *cls* is the class that introduces behaviour described by
            # an ABC *base*, insert said ABC to its MRO.
            abstract_bases.append(base)
    for base in abstract_bases:
        abcs.remove(base)
    explicit_c3_mros = [_c3_mro(base, abcs=abcs) for base in explicit_bases]
    abstract_c3_mros = [_c3_mro(base, abcs=abcs) for base in abstract_bases]
    other_c3_mros = [_c3_mro(base, abcs=abcs) for base in other_bases]
    return _c3_merge(
        [[cls]] +
        explicit_c3_mros + abstract_c3_mros + other_c3_mros +
        [explicit_bases] + [abstract_bases] + [other_bases]
    )

def _compose_mro(cls, types):
    """Calculates the method resolution order for a given class *cls*.

    Includes relevant abstract base classes (with their respective bases) from
    the *types* iterable. Uses a modified C3 linearization algorithm.

    """
    bases = set(cls.__mro__)
    # Remove entries which are already present in the __mro__ or unrelated.
    def is_related(typ):
        return (typ not in bases and hasattr(typ, '__mro__')
                                 and not isinstance(typ, GenericAlias)
                                 and issubclass(cls, typ))
    types = [n for n in types if is_related(n)]
    # Remove entries which are strict bases of other entries (they will end up
    # in the MRO anyway.
    def is_strict_base(typ):
        for other in types:
            if typ != other and typ in other.__mro__:
                return True
        return False
    types = [n for n in types if not is_strict_base(n)]
    # Subclasses of the ABCs in *types* which are also implemented by
    # *cls* can be used to stabilize ABC ordering.
    type_set = set(types)
    mro = []
    for typ in types:
        found = []
        for sub in typ.__subclasses__():
            if sub not in bases and issubclass(cls, sub):
                found.append([s for s in sub.__mro__ if s in type_set])
        if not found:
            mro.append(typ)
            continue
        # Favor subclasses with the biggest number of useful bases
        found.sort(key=len, reverse=True)
        for sub in found:
            for subcls in sub:
                if subcls not in mro:
                    mro.append(subcls)
    return _c3_mro(cls, abcs=mro)

def _find_impl(cls, registry):
    """Returns the best matching implementation from *registry* for type *cls*.

    Where there is no registered implementation for a specific type, its method
    resolution order is used to find a more generic implementation.

    Note: if *registry* does not contain an implementation for the base
    *object* type, this function may return None.

    """
    mro = _compose_mro(cls, registry.keys())
    match = None
    for t in mro:
        if match is not None:
            # If *match* is an implicit ABC but there is another unrelated,
            # equally matching implicit ABC, refuse the temptation to guess.
            if (t in registry and t not in cls.__mro__
                              and match not in cls.__mro__
                              and not issubclass(match, t)):
                raise RuntimeError("Ambiguous dispatch: {} or {}".format(
                    match, t))
            break
        if t in registry:
            match = t
    return registry.get(match)

def singledispatch(func):
    """Single-dispatch generic function decorator.

    Transforms a function into a generic function, which can have different
    behaviours depending upon the type of its first argument. The decorated
    function acts as the default implementation, and additional
    implementations can be registered using the register() attribute of the
    generic function.
    """
    # There are many programs that use functools without singledispatch, so we
    # trade-off making singledispatch marginally slower for the benefit of
    # making start-up of such applications slightly faster.
    import types, weakref

    registry = {}
    dispatch_cache = weakref.WeakKeyDictionary()
    cache_token = None

    def dispatch(cls):
        """generic_func.dispatch(cls) -> <function implementation>

        Runs the dispatch algorithm to return the best available implementation
        for the given *cls* registered on *generic_func*.

        """
        nonlocal cache_token
        if cache_token is not None:
            current_token = get_cache_token()
            if cache_token != current_token:
                dispatch_cache.clear()
                cache_token = current_token
        try:
            impl = dispatch_cache[cls]
        except KeyError:
            try:
                impl = registry[cls]
            except KeyError:
                impl = _find_impl(cls, registry)
            dispatch_cache[cls] = impl
        return impl

    def _is_valid_dispatch_type(cls):
        return isinstance(cls, type) and not isinstance(cls, GenericAlias)

    def register(cls, func=None):
        """generic_func.register(cls, func) -> func

        Registers a new implementation for the given *cls* on a *generic_func*.

        """
        nonlocal cache_token
        if _is_valid_dispatch_type(cls):
            if func is None:
                return lambda f: register(cls, f)
        else:
            if func is not None:
                raise TypeError(
                    f"Invalid first argument to `register()`. "
                    f"{cls!r} is not a class."
                )
            ann = getattr(cls, '__annotations__', {})
            if not ann:
                raise TypeError(
                    f"Invalid first argument to `register()`: {cls!r}. "
                    f"Use either `@register(some_class)` or plain `@register` "
                    f"on an annotated function."
                )
            func = cls

            # only import typing if annotation parsing is necessary
            from typing import get_type_hints
            argname, cls = next(iter(get_type_hints(func).items()))
            if not _is_valid_dispatch_type(cls):
                raise TypeError(
                    f"Invalid annotation for {argname!r}. "
                    f"{cls!r} is not a class."
                )

        registry[cls] = func
        if cache_token is None and hasattr(cls, '__abstractmethods__'):
            cache_token = get_cache_token()
        dispatch_cache.clear()
        return func

    def wrapper(*args, **kw):
        if not args:
            raise TypeError(f'{funcname} requires at least '
                            '1 positional argument')

        return dispatch(args[0].__class__)(*args, **kw)

    funcname = getattr(func, '__name__', 'singledispatch function')
    registry[object] = func
    wrapper.register = register
    wrapper.dispatch = dispatch
    wrapper.registry = types.MappingProxyType(registry)
    wrapper._clear_cache = dispatch_cache.clear
    update_wrapper(wrapper, func)
    return wrapper


# Descriptor version
class singledispatchmethod:
    """Single-dispatch generic method descriptor.

    Supports wrapping existing descriptors and handles non-descriptor
    callables as instance methods.
    """

    def __init__(self, func):
        if not callable(func) and not hasattr(func, "__get__"):
            raise TypeError(f"{func!r} is not callable or a descriptor")

        self.dispatcher = singledispatch(func)
        self.func = func

        # bpo-45678: special-casing for classmethod/staticmethod in Python <=3.9,
        # as functools.update_wrapper doesn't work properly in singledispatchmethod.__get__
        # if it is applied to an unbound classmethod/staticmethod
        if isinstance(func, (staticmethod, classmethod)):
            self._wrapped_func = func.__func__
        else:
            self._wrapped_func = func
    def register(self, cls, method=None):
        """generic_method.register(cls, func) -> func

        Registers a new implementation for the given *cls* on a *generic_method*.
        """
        # bpo-39679: in Python <= 3.9, classmethods and staticmethods don't
        # inherit __annotations__ of the wrapped function (fixed in 3.10+ as
        # a side-effect of bpo-43682) but we need that for annotation-derived
        # singledispatches. So we add that just-in-time here.
        if isinstance(cls, (staticmethod, classmethod)):
            cls.__annotations__ = getattr(cls.__func__, '__annotations__', {})
        return self.dispatcher.register(cls, func=method)

    def __get__(self, obj, cls=None):
        def _method(*args, **kwargs):
            method = self.dispatcher.dispatch(args[0].__class__)
            return method.__get__(obj, cls)(*args, **kwargs)

        _method.__isabstractmethod__ = self.__isabstractmethod__
        _method.register = self.register
        update_wrapper(_method, self._wrapped_func)
        return _method

    @property
    def __isabstractmethod__(self):
        return getattr(self.func, '__isabstractmethod__', False)


################################################################################
### cached_property() - computed once per instance, cached as attribute
################################################################################

_NOT_FOUND = object()


class cached_property:
    def __init__(self, func):
        self.func = func
        self.attrname = None
        self.__doc__ = func.__doc__
        self.lock = RLock()

    def __set_name__(self, owner, name):
        if self.attrname is None:
            self.attrname = name
        elif name != self.attrname:
            raise TypeError(
                "Cannot assign the same cached_property to two different names "
                f"({self.attrname!r} and {name!r})."
            )

    def __get__(self, instance, owner=None):
        if instance is None:
            return self
        if self.attrname is None:
            raise TypeError(
                "Cannot use cached_property instance without calling __set_name__ on it.")
        try:
            cache = instance.__dict__
        except AttributeError:  # not all objects have __dict__ (e.g. class defines slots)
            msg = (
                f"No '__dict__' attribute on {type(instance).__name__!r} "
                f"instance to cache {self.attrname!r} property."
            )
            raise TypeError(msg) from None
        val = cache.get(self.attrname, _NOT_FOUND)
        if val is _NOT_FOUND:
            with self.lock:
                # check if another thread filled cache while we awaited lock
                val = cache.get(self.attrname, _NOT_FOUND)
                if val is _NOT_FOUND:
                    val = self.func(instance)
                    try:
                        cache[self.attrname] = val
                    except TypeError:
                        msg = (
                            f"The '__dict__' attribute on {type(instance).__name__!r} instance "
                            f"does not support item assignment for caching {self.attrname!r} property."
                        )
                        raise TypeError(msg) from None
        return val

    __class_getitem__ = classmethod(GenericAlias)


import functools
dir(functools)

import functools


import functools as fn
functs = [fn]

All = [dir(functs.append)]
All[0]

from functools import partial
def power(a, b):
    return a**b
 
# partial functions
pow2 = partial(power, b=2)
pow4 = partial(power, b=4)
power_of_5 = partial(power, 5)
 
print("power(2, 3): ",power(2, 3))
print("pow2(4): ",pow2(4))
print("pow4(3): ",pow4(3))
print("power_of_5(2): ",(power_of_5(2)))
 
print('Function used in partial function pow2: ', pow2.func)
print('Default keywords for pow2: ', pow2.keywords)
print('Default arguments for power_of_5: ', power_of_5.args)

%%writefile utils_image.py
import os
import math
import random
import numpy as np
import torch
import cv2
from torchvision.utils import make_grid
from datetime import datetime
# import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"


'''
# --------------------------------------------
# Kai Zhang (github: https://github.com/cszn)
# 03/Mar/2019
# --------------------------------------------
# https://github.com/twhui/SRGAN-pyTorch
# https://github.com/xinntao/BasicSR
# --------------------------------------------
'''


IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tif']


def is_image_file(filename):
    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)


def get_timestamp():
    return datetime.now().strftime('%y%m%d-%H%M%S')


def imshow(x, title=None, cbar=False, figsize=None):
    plt.figure(figsize=figsize)
    plt.imshow(np.squeeze(x), interpolation='nearest', cmap='gray')
    if title:
        plt.title(title)
    if cbar:
        plt.colorbar()
    plt.show()


def surf(Z, cmap='rainbow', figsize=None):
    plt.figure(figsize=figsize)
    ax3 = plt.axes(projection='3d')

    w, h = Z.shape[:2]
    xx = np.arange(0,w,1)
    yy = np.arange(0,h,1)
    X, Y = np.meshgrid(xx, yy)
    ax3.plot_surface(X,Y,Z,cmap=cmap)
    #ax3.contour(X,Y,Z, zdim='z',offset=-2，cmap=cmap)
    plt.show()


'''
# --------------------------------------------
# get image pathes
# --------------------------------------------
'''


def get_image_paths(dataroot):
    paths = None  # return None if dataroot is None
    if dataroot is not None:
        paths = sorted(_get_paths_from_images(dataroot))
    return paths


def _get_paths_from_images(path):
    assert os.path.isdir(path), '{:s} is not a valid directory'.format(path)
    images = []
    for dirpath, _, fnames in sorted(os.walk(path)):
        for fname in sorted(fnames):
            if is_image_file(fname):
                img_path = os.path.join(dirpath, fname)
                images.append(img_path)
    assert images, '{:s} has no valid image file'.format(path)
    return images


'''
# --------------------------------------------
# split large images into small images 
# --------------------------------------------
'''


def patches_from_image(img, p_size=512, p_overlap=64, p_max=800):
    w, h = img.shape[:2]
    patches = []
    if w > p_max and h > p_max:
        w1 = list(np.arange(0, w-p_size, p_size-p_overlap, dtype=np.int))
        h1 = list(np.arange(0, h-p_size, p_size-p_overlap, dtype=np.int))
        w1.append(w-p_size)
        h1.append(h-p_size)
#        print(w1)
#        print(h1)
        for i in w1:
            for j in h1:
                patches.append(img[i:i+p_size, j:j+p_size,:])
    else:
        patches.append(img)

    return patches


def imssave(imgs, img_path):
    """
    imgs: list, N images of size WxHxC
    """
    img_name, ext = os.path.splitext(os.path.basename(img_path))

    for i, img in enumerate(imgs):
        if img.ndim == 3:
            img = img[:, :, [2, 1, 0]]
        new_path = os.path.join(os.path.dirname(img_path), img_name+str('_s{:04d}'.format(i))+'.png')
        cv2.imwrite(new_path, img)


def split_imageset(original_dataroot, taget_dataroot, n_channels=3, p_size=800, p_overlap=96, p_max=1000):
    """
    split the large images from original_dataroot into small overlapped images with size (p_size)x(p_size), 
    and save them into taget_dataroot; only the images with larger size than (p_max)x(p_max)
    will be splitted.

    Args:
        original_dataroot:
        taget_dataroot:
        p_size: size of small images
        p_overlap: patch size in training is a good choice
        p_max: images with smaller size than (p_max)x(p_max) keep unchanged.
    """
    paths = get_image_paths(original_dataroot)
    for img_path in paths:
        # img_name, ext = os.path.splitext(os.path.basename(img_path))
        img = imread_uint(img_path, n_channels=n_channels)
        patches = patches_from_image(img, p_size, p_overlap, p_max)
        imssave(patches, os.path.join(taget_dataroot,os.path.basename(img_path)))
        #if original_dataroot == taget_dataroot:
        #del img_path

'''
# --------------------------------------------
# makedir
# --------------------------------------------
'''


def mkdir(path):
    if not os.path.exists(path):
        os.makedirs(path)


def mkdirs(paths):
    if isinstance(paths, str):
        mkdir(paths)
    else:
        for path in paths:
            mkdir(path)


def mkdir_and_rename(path):
    if os.path.exists(path):
        new_name = path + '_archived_' + get_timestamp()
        print('Path already exists. Rename it to [{:s}]'.format(new_name))
        os.rename(path, new_name)
    os.makedirs(path)


'''
# --------------------------------------------
# read image from path
# opencv is fast, but read BGR numpy image
# --------------------------------------------
'''


# --------------------------------------------
# get uint8 image of size HxWxn_channles (RGB)
# --------------------------------------------
def imread_uint(path, n_channels=3):
    #  input: path
    # output: HxWx3(RGB or GGG), or HxWx1 (G)
    if n_channels == 1:
        img = cv2.imread(path, 0)  # cv2.IMREAD_GRAYSCALE
        img = np.expand_dims(img, axis=2)  # HxWx1
    elif n_channels == 3:
        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # BGR or G
        if img.ndim == 2:
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # GGG
        else:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB
    return img


# --------------------------------------------
# matlab's imwrite
# --------------------------------------------
def imsave(img, img_path):
    img = np.squeeze(img)
    if img.ndim == 3:
        img = img[:, :, [2, 1, 0]]
    cv2.imwrite(img_path, img)

def imwrite(img, img_path):
    img = np.squeeze(img)
    if img.ndim == 3:
        img = img[:, :, [2, 1, 0]]
    cv2.imwrite(img_path, img)



# --------------------------------------------
# get single image of size HxWxn_channles (BGR)
# --------------------------------------------
def read_img(path):
    # read image by cv2
    # return: Numpy float32, HWC, BGR, [0,1]
    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # cv2.IMREAD_GRAYSCALE
    img = img.astype(np.float32) / 255.
    if img.ndim == 2:
        img = np.expand_dims(img, axis=2)
    # some images have 4 channels
    if img.shape[2] > 3:
        img = img[:, :, :3]
    return img


'''
# --------------------------------------------
# image format conversion
# --------------------------------------------
# numpy(single) <--->  numpy(unit)
# numpy(single) <--->  tensor
# numpy(unit)   <--->  tensor
# --------------------------------------------
'''


# --------------------------------------------
# numpy(single) [0, 1] <--->  numpy(unit)
# --------------------------------------------


def uint2single(img):

    return np.float32(img/255.)


def single2uint(img):

    return np.uint8((img.clip(0, 1)*255.).round())


def uint162single(img):

    return np.float32(img/65535.)


def single2uint16(img):

    return np.uint16((img.clip(0, 1)*65535.).round())


# --------------------------------------------
# numpy(unit) (HxWxC or HxW) <--->  tensor
# --------------------------------------------


# convert uint to 4-dimensional torch tensor
def uint2tensor4(img):
    if img.ndim == 2:
        img = np.expand_dims(img, axis=2)
    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.).unsqueeze(0)


# convert uint to 3-dimensional torch tensor
def uint2tensor3(img):
    if img.ndim == 2:
        img = np.expand_dims(img, axis=2)
    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.)


# convert 2/3/4-dimensional torch tensor to uint
def tensor2uint(img):
    img = img.data.squeeze().float().clamp_(0, 1).cpu().numpy()
    if img.ndim == 3:
        img = np.transpose(img, (1, 2, 0))
    return np.uint8((img*255.0).round())


# --------------------------------------------
# numpy(single) (HxWxC) <--->  tensor
# --------------------------------------------


# convert single (HxWxC) to 3-dimensional torch tensor
def single2tensor3(img):
    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float()


# convert single (HxWxC) to 4-dimensional torch tensor
def single2tensor4(img):
    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().unsqueeze(0)


# convert torch tensor to single
def tensor2single(img):
    img = img.data.squeeze().float().cpu().numpy()
    if img.ndim == 3:
        img = np.transpose(img, (1, 2, 0))

    return img

# convert torch tensor to single
def tensor2single3(img):
    img = img.data.squeeze().float().cpu().numpy()
    if img.ndim == 3:
        img = np.transpose(img, (1, 2, 0))
    elif img.ndim == 2:
        img = np.expand_dims(img, axis=2)
    return img


def single2tensor5(img):
    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float().unsqueeze(0)


def single32tensor5(img):
    return torch.from_numpy(np.ascontiguousarray(img)).float().unsqueeze(0).unsqueeze(0)


def single42tensor4(img):
    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float()


# from skimage.io import imread, imsave
def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):
    '''
    Converts a torch Tensor into an image Numpy array of BGR channel order
    Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order
    Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)
    '''
    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # squeeze first, then clamp
    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]
    n_dim = tensor.dim()
    if n_dim == 4:
        n_img = len(tensor)
        img_np = make_grid(tensor, nrow=int(math.sqrt(n_img)), normalize=False).numpy()
        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR
    elif n_dim == 3:
        img_np = tensor.numpy()
        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR
    elif n_dim == 2:
        img_np = tensor.numpy()
    else:
        raise TypeError(
            'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}'.format(n_dim))
    if out_type == np.uint8:
        img_np = (img_np * 255.0).round()
        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.
    return img_np.astype(out_type)


'''
# --------------------------------------------
# Augmentation, flipe and/or rotate
# --------------------------------------------
# The following two are enough.
# (1) augmet_img: numpy image of WxHxC or WxH
# (2) augment_img_tensor4: tensor image 1xCxWxH
# --------------------------------------------
'''


def augment_img(img, mode=0):
    '''Kai Zhang (github: https://github.com/cszn)
    '''
    if mode == 0:
        return img
    elif mode == 1:
        return np.flipud(np.rot90(img))
    elif mode == 2:
        return np.flipud(img)
    elif mode == 3:
        return np.rot90(img, k=3)
    elif mode == 4:
        return np.flipud(np.rot90(img, k=2))
    elif mode == 5:
        return np.rot90(img)
    elif mode == 6:
        return np.rot90(img, k=2)
    elif mode == 7:
        return np.flipud(np.rot90(img, k=3))


def augment_img_tensor4(img, mode=0):
    '''Kai Zhang (github: https://github.com/cszn)
    '''
    if mode == 0:
        return img
    elif mode == 1:
        return img.rot90(1, [2, 3]).flip([2])
    elif mode == 2:
        return img.flip([2])
    elif mode == 3:
        return img.rot90(3, [2, 3])
    elif mode == 4:
        return img.rot90(2, [2, 3]).flip([2])
    elif mode == 5:
        return img.rot90(1, [2, 3])
    elif mode == 6:
        return img.rot90(2, [2, 3])
    elif mode == 7:
        return img.rot90(3, [2, 3]).flip([2])


def augment_img_tensor(img, mode=0):
    '''Kai Zhang (github: https://github.com/cszn)
    '''
    img_size = img.size()
    img_np = img.data.cpu().numpy()
    if len(img_size) == 3:
        img_np = np.transpose(img_np, (1, 2, 0))
    elif len(img_size) == 4:
        img_np = np.transpose(img_np, (2, 3, 1, 0))
    img_np = augment_img(img_np, mode=mode)
    img_tensor = torch.from_numpy(np.ascontiguousarray(img_np))
    if len(img_size) == 3:
        img_tensor = img_tensor.permute(2, 0, 1)
    elif len(img_size) == 4:
        img_tensor = img_tensor.permute(3, 2, 0, 1)

    return img_tensor.type_as(img)


def augment_img_np3(img, mode=0):
    if mode == 0:
        return img
    elif mode == 1:
        return img.transpose(1, 0, 2)
    elif mode == 2:
        return img[::-1, :, :]
    elif mode == 3:
        img = img[::-1, :, :]
        img = img.transpose(1, 0, 2)
        return img
    elif mode == 4:
        return img[:, ::-1, :]
    elif mode == 5:
        img = img[:, ::-1, :]
        img = img.transpose(1, 0, 2)
        return img
    elif mode == 6:
        img = img[:, ::-1, :]
        img = img[::-1, :, :]
        return img
    elif mode == 7:
        img = img[:, ::-1, :]
        img = img[::-1, :, :]
        img = img.transpose(1, 0, 2)
        return img


def augment_imgs(img_list, hflip=True, rot=True):
    # horizontal flip OR rotate
    hflip = hflip and random.random() < 0.5
    vflip = rot and random.random() < 0.5
    rot90 = rot and random.random() < 0.5

    def _augment(img):
        if hflip:
            img = img[:, ::-1, :]
        if vflip:
            img = img[::-1, :, :]
        if rot90:
            img = img.transpose(1, 0, 2)
        return img

    return [_augment(img) for img in img_list]


'''
# --------------------------------------------
# modcrop and shave
# --------------------------------------------
'''


def modcrop(img_in, scale):
    # img_in: Numpy, HWC or HW
    img = np.copy(img_in)
    if img.ndim == 2:
        H, W = img.shape
        H_r, W_r = H % scale, W % scale
        img = img[:H - H_r, :W - W_r]
    elif img.ndim == 3:
        H, W, C = img.shape
        H_r, W_r = H % scale, W % scale
        img = img[:H - H_r, :W - W_r, :]
    else:
        raise ValueError('Wrong img ndim: [{:d}].'.format(img.ndim))
    return img


def shave(img_in, border=0):
    # img_in: Numpy, HWC or HW
    img = np.copy(img_in)
    h, w = img.shape[:2]
    img = img[border:h-border, border:w-border]
    return img


'''
# --------------------------------------------
# image processing process on numpy image
# channel_convert(in_c, tar_type, img_list):
# rgb2ycbcr(img, only_y=True):
# bgr2ycbcr(img, only_y=True):
# ycbcr2rgb(img):
# --------------------------------------------
'''


def rgb2ycbcr(img, only_y=True):
    '''same as matlab rgb2ycbcr
    only_y: only return Y channel
    Input:
        uint8, [0, 255]
        float, [0, 1]
    '''
    in_img_type = img.dtype
    img.astype(np.float32)
    if in_img_type != np.uint8:
        img *= 255.
    # convert
    if only_y:
        rlt = np.dot(img, [65.481, 128.553, 24.966]) / 255.0 + 16.0
    else:
        rlt = np.matmul(img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],
                              [24.966, 112.0, -18.214]]) / 255.0 + [16, 128, 128]
    if in_img_type == np.uint8:
        rlt = rlt.round()
    else:
        rlt /= 255.
    return rlt.astype(in_img_type)


def ycbcr2rgb(img):
    '''same as matlab ycbcr2rgb
    Input:
        uint8, [0, 255]
        float, [0, 1]
    '''
    in_img_type = img.dtype
    img.astype(np.float32)
    if in_img_type != np.uint8:
        img *= 255.
    # convert
    rlt = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],
                          [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]
    if in_img_type == np.uint8:
        rlt = rlt.round()
    else:
        rlt /= 255.
    return rlt.astype(in_img_type)


def bgr2ycbcr(img, only_y=True):
    '''bgr version of rgb2ycbcr
    only_y: only return Y channel
    Input:
        uint8, [0, 255]
        float, [0, 1]
    '''
    in_img_type = img.dtype
    img.astype(np.float32)
    if in_img_type != np.uint8:
        img *= 255.
    # convert
    if only_y:
        rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0
    else:
        rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],
                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]
    if in_img_type == np.uint8:
        rlt = rlt.round()
    else:
        rlt /= 255.
    return rlt.astype(in_img_type)


def channel_convert(in_c, tar_type, img_list):
    # conversion among BGR, gray and y
    if in_c == 3 and tar_type == 'gray':  # BGR to gray
        gray_list = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in img_list]
        return [np.expand_dims(img, axis=2) for img in gray_list]
    elif in_c == 3 and tar_type == 'y':  # BGR to y
        y_list = [bgr2ycbcr(img, only_y=True) for img in img_list]
        return [np.expand_dims(img, axis=2) for img in y_list]
    elif in_c == 1 and tar_type == 'RGB':  # gray/y to BGR
        return [cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for img in img_list]
    else:
        return img_list


'''
# --------------------------------------------
# metric, PSNR and SSIM
# --------------------------------------------
'''


# --------------------------------------------
# PSNR
# --------------------------------------------
def calculate_psnr(img1, img2, border=0):
    # img1 and img2 have range [0, 255]
    #img1 = img1.squeeze()
    #img2 = img2.squeeze()
    if not img1.shape == img2.shape:
        raise ValueError('Input images must have the same dimensions.')
    h, w = img1.shape[:2]
    img1 = img1[border:h-border, border:w-border]
    img2 = img2[border:h-border, border:w-border]

    img1 = img1.astype(np.float64)
    img2 = img2.astype(np.float64)
    mse = np.mean((img1 - img2)**2)
    if mse == 0:
        return float('inf')
    return 20 * math.log10(255.0 / math.sqrt(mse))


# --------------------------------------------
# SSIM
# --------------------------------------------
def calculate_ssim(img1, img2, border=0):
    '''calculate SSIM
    the same outputs as MATLAB's
    img1, img2: [0, 255]
    '''
    #img1 = img1.squeeze()
    #img2 = img2.squeeze()
    if not img1.shape == img2.shape:
        raise ValueError('Input images must have the same dimensions.')
    h, w = img1.shape[:2]
    img1 = img1[border:h-border, border:w-border]
    img2 = img2[border:h-border, border:w-border]

    if img1.ndim == 2:
        return ssim(img1, img2)
    elif img1.ndim == 3:
        if img1.shape[2] == 3:
            ssims = []
            for i in range(3):
                ssims.append(ssim(img1[:,:,i], img2[:,:,i]))
            return np.array(ssims).mean()
        elif img1.shape[2] == 1:
            return ssim(np.squeeze(img1), np.squeeze(img2))
    else:
        raise ValueError('Wrong input image dimensions.')


def ssim(img1, img2):
    C1 = (0.01 * 255)**2
    C2 = (0.03 * 255)**2

    img1 = img1.astype(np.float64)
    img2 = img2.astype(np.float64)
    kernel = cv2.getGaussianKernel(11, 1.5)
    window = np.outer(kernel, kernel.transpose())

    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid
    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]
    mu1_sq = mu1**2
    mu2_sq = mu2**2
    mu1_mu2 = mu1 * mu2
    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq
    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq
    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *
                                                            (sigma1_sq + sigma2_sq + C2))
    return ssim_map.mean()


'''
# --------------------------------------------
# matlab's bicubic imresize (numpy and torch) [0, 1]
# --------------------------------------------
'''


# matlab 'imresize' function, now only support 'bicubic'
def cubic(x):
    absx = torch.abs(x)
    absx2 = absx**2
    absx3 = absx**3
    return (1.5*absx3 - 2.5*absx2 + 1) * ((absx <= 1).type_as(absx)) + \
        (-0.5*absx3 + 2.5*absx2 - 4*absx + 2) * (((absx > 1)*(absx <= 2)).type_as(absx))


def calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):
    if (scale < 1) and (antialiasing):
        # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width
        kernel_width = kernel_width / scale

    # Output-space coordinates
    x = torch.linspace(1, out_length, out_length)

    # Input-space coordinates. Calculate the inverse mapping such that 0.5
    # in output space maps to 0.5 in input space, and 0.5+scale in output
    # space maps to 1.5 in input space.
    u = x / scale + 0.5 * (1 - 1 / scale)

    # What is the left-most pixel that can be involved in the computation?
    left = torch.floor(u - kernel_width / 2)

    # What is the maximum number of pixels that can be involved in the
    # computation?  Note: it's OK to use an extra pixel here; if the
    # corresponding weights are all zero, it will be eliminated at the end
    # of this function.
    P = math.ceil(kernel_width) + 2

    # The indices of the input pixels involved in computing the k-th output
    # pixel are in row k of the indices matrix.
    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(
        1, P).expand(out_length, P)

    # The weights used to compute the k-th output pixel are in row k of the
    # weights matrix.
    distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices
    # apply cubic kernel
    if (scale < 1) and (antialiasing):
        weights = scale * cubic(distance_to_center * scale)
    else:
        weights = cubic(distance_to_center)
    # Normalize the weights matrix so that each row sums to 1.
    weights_sum = torch.sum(weights, 1).view(out_length, 1)
    weights = weights / weights_sum.expand(out_length, P)

    # If a column in weights is all zero, get rid of it. only consider the first and last column.
    weights_zero_tmp = torch.sum((weights == 0), 0)
    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):
        indices = indices.narrow(1, 1, P - 2)
        weights = weights.narrow(1, 1, P - 2)
    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):
        indices = indices.narrow(1, 0, P - 2)
        weights = weights.narrow(1, 0, P - 2)
    weights = weights.contiguous()
    indices = indices.contiguous()
    sym_len_s = -indices.min() + 1
    sym_len_e = indices.max() - in_length
    indices = indices + sym_len_s - 1
    return weights, indices, int(sym_len_s), int(sym_len_e)


# --------------------------------------------
# imresize for tensor image [0, 1]
# --------------------------------------------
def imresize(img, scale, antialiasing=True):
    # Now the scale should be the same for H and W
    # input: img: pytorch tensor, CHW or HW [0,1]
    # output: CHW or HW [0,1] w/o round
    need_squeeze = True if img.dim() == 2 else False
    if need_squeeze:
        img.unsqueeze_(0)
    in_C, in_H, in_W = img.size()
    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)
    kernel_width = 4
    kernel = 'cubic'

    # Return the desired dimension order for performing the resize.  The
    # strategy is to perform the resize first along the dimension with the
    # smallest scale factor.
    # Now we do not support this.

    # get weights and indices
    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(
        in_H, out_H, scale, kernel, kernel_width, antialiasing)
    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(
        in_W, out_W, scale, kernel, kernel_width, antialiasing)
    # process H dimension
    # symmetric copying
    img_aug = torch.FloatTensor(in_C, in_H + sym_len_Hs + sym_len_He, in_W)
    img_aug.narrow(1, sym_len_Hs, in_H).copy_(img)

    sym_patch = img[:, :sym_len_Hs, :]
    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(1, inv_idx)
    img_aug.narrow(1, 0, sym_len_Hs).copy_(sym_patch_inv)

    sym_patch = img[:, -sym_len_He:, :]
    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(1, inv_idx)
    img_aug.narrow(1, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)

    out_1 = torch.FloatTensor(in_C, out_H, in_W)
    kernel_width = weights_H.size(1)
    for i in range(out_H):
        idx = int(indices_H[i][0])
        for j in range(out_C):
            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])

    # process W dimension
    # symmetric copying
    out_1_aug = torch.FloatTensor(in_C, out_H, in_W + sym_len_Ws + sym_len_We)
    out_1_aug.narrow(2, sym_len_Ws, in_W).copy_(out_1)

    sym_patch = out_1[:, :, :sym_len_Ws]
    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(2, inv_idx)
    out_1_aug.narrow(2, 0, sym_len_Ws).copy_(sym_patch_inv)

    sym_patch = out_1[:, :, -sym_len_We:]
    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(2, inv_idx)
    out_1_aug.narrow(2, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)

    out_2 = torch.FloatTensor(in_C, out_H, out_W)
    kernel_width = weights_W.size(1)
    for i in range(out_W):
        idx = int(indices_W[i][0])
        for j in range(out_C):
            out_2[j, :, i] = out_1_aug[j, :, idx:idx + kernel_width].mv(weights_W[i])
    if need_squeeze:
        out_2.squeeze_()
    return out_2


# --------------------------------------------
# imresize for numpy image [0, 1]
# --------------------------------------------
def imresize_np(img, scale, antialiasing=True):
    # Now the scale should be the same for H and W
    # input: img: Numpy, HWC or HW [0,1]
    # output: HWC or HW [0,1] w/o round
    img = torch.from_numpy(img)
    need_squeeze = True if img.dim() == 2 else False
    if need_squeeze:
        img.unsqueeze_(2)

    in_H, in_W, in_C = img.size()
    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)
    kernel_width = 4
    kernel = 'cubic'

    # Return the desired dimension order for performing the resize.  The
    # strategy is to perform the resize first along the dimension with the
    # smallest scale factor.
    # Now we do not support this.

    # get weights and indices
    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(
        in_H, out_H, scale, kernel, kernel_width, antialiasing)
    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(
        in_W, out_W, scale, kernel, kernel_width, antialiasing)
    # process H dimension
    # symmetric copying
    img_aug = torch.FloatTensor(in_H + sym_len_Hs + sym_len_He, in_W, in_C)
    img_aug.narrow(0, sym_len_Hs, in_H).copy_(img)

    sym_patch = img[:sym_len_Hs, :, :]
    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(0, inv_idx)
    img_aug.narrow(0, 0, sym_len_Hs).copy_(sym_patch_inv)

    sym_patch = img[-sym_len_He:, :, :]
    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(0, inv_idx)
    img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)

    out_1 = torch.FloatTensor(out_H, in_W, in_C)
    kernel_width = weights_H.size(1)
    for i in range(out_H):
        idx = int(indices_H[i][0])
        for j in range(out_C):
            out_1[i, :, j] = img_aug[idx:idx + kernel_width, :, j].transpose(0, 1).mv(weights_H[i])

    # process W dimension
    # symmetric copying
    out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)
    out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)

    sym_patch = out_1[:, :sym_len_Ws, :]
    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(1, inv_idx)
    out_1_aug.narrow(1, 0, sym_len_Ws).copy_(sym_patch_inv)

    sym_patch = out_1[:, -sym_len_We:, :]
    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(1, inv_idx)
    out_1_aug.narrow(1, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)

    out_2 = torch.FloatTensor(out_H, out_W, in_C)
    kernel_width = weights_W.size(1)
    for i in range(out_W):
        idx = int(indices_W[i][0])
        for j in range(out_C):
            out_2[:, i, j] = out_1_aug[:, idx:idx + kernel_width, j].mv(weights_W[i])
    if need_squeeze:
        out_2.squeeze_()

    return out_2.numpy()


if __name__ == '__main__':
    print('---')
#    img = imread_uint('test.bmp', 3)
#    img = uint2single(img)
#    img_bicubic = imresize_np(img, 1/4)



    
    
    
    
    



%% writefile train2.py
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import argparse
import sresnet
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.manual_seed(100)
torch.cuda.manual_seed(100)

parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')
parser.add_argument('--depth', default=18, type=int)
parser.add_argument('--class_num', default=100, type=int)
parser.add_argument('--epoch', default=200, type=int)
parser.add_argument('--lambda_KD', default=0.5, type=float)
args = parser.parse_args()
print(args)


def CrossEntropy(outputs, targets):
    log_softmax_outputs = F.log_softmax(outputs/3.0, dim=1)
    softmax_targets = F.softmax(targets/3.0, dim=1)
    return -(log_softmax_outputs * softmax_targets).sum(dim=1).mean()


BATCH_SIZE = 128
LR = 0.1

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
trainset, testset = None, None
if args.class_num == 100:
    print("dataset: CIFAR100")
    trainset = torchvision.datasets.CIFAR100(
        root='/home/jack/data',
        train=True,
        download=False,
        transform=transform_train
    )
    testset = torchvision.datasets.CIFAR100(
        root='/home/jack/data',
        train=False,
        download=False,
        transform=transform_test
    )
if args.class_num == 10:
    print("dataset: CIFAR10")
    trainset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root='./data',
        train=False,
        download=True,
        transform=transform_test
    )
trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=4
)
testloader = torch.utils.data.DataLoader(
    testset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=4
)

net = None
if args.depth == 18:
    net = sresnet.resnet18(num_classes=args.class_num, align="CONV")
    print("using resnet 18")
if args.depth == 50:
    net = sresnet.resnet50(num_classes=args.class_num, align="CONV")
    print("using resnet 50")
if args.depth == 101:
    net = sresnet.resnet101(num_classes=args.class_num, align="CONV")
    print("using resnet 101")
if args.depth == 152:
    net = sresnet.resnet152(num_classes=args.class_num, align="CONV")
    print("using resnet 152")

net.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=LR, weight_decay=5e-4, momentum=0.9)

if __name__ == "__main__":
    best_acc = 0
    print("Start Training")  # 定义遍历数据集的次数
    with open("acc.txt", "w") as f:
        with open("log.txt", "w")as f2:
            for epoch in range(args.epoch):
                correct4, correct3, correct2, correct1, correct0 = 0, 0, 0, 0, 0
                predicted4, predicted3, predicted2, predicted1, predicted0 = 0, 0, 0, 0, 0
                if epoch in [75, 130, 180]:
                    for param_group in optimizer.param_groups:
                        param_group['lr'] /= 10
                net.train()
                sum_loss = 0.0
                correct = 0.0
                total = 0.0
                for i, data in enumerate(trainloader, 0):
                    length = len(trainloader)
                    inputs, labels = data
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs, feature_loss = net(inputs)

                    ensemble = sum(outputs[:-1])/len(outputs)
                    ensemble.detach_()
                    ensemble.requires_grad = False

                    #   compute loss
                    loss = torch.FloatTensor([0.]).to(device)

                    #   for deepest classifier
                    loss += criterion(outputs[0], labels)

                    #   for soft & hard target
                    teacher_output = outputs[0].detach()
                    teacher_output.requires_grad = False

                    for index in range(1, len(outputs)):
                        loss += CrossEntropy(outputs[index], teacher_output) * args.lambda_KD * 9
                        loss += criterion(outputs[index], labels) * (1 - args.lambda_KD)

                    #   for faeture align loss
                    if args.lambda_KD != 0:
                        loss += feature_loss * 5e-7

                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    total += float(labels.size(0))
                    sum_loss += loss.item()

                    _0, predicted0 = torch.max(outputs[0].data, 1)
                    _1, predicted1 = torch.max(outputs[1].data, 1)
                    _2, predicted2 = torch.max(outputs[2].data, 1)
                    _3, predicted3 = torch.max(outputs[3].data, 1)
                    _4, predicted4 = torch.max(ensemble.data, 1)

                    correct0 += float(predicted0.eq(labels.data).cpu().sum())
                    correct1 += float(predicted1.eq(labels.data).cpu().sum())
                    correct2 += float(predicted2.eq(labels.data).cpu().sum())
                    correct3 += float(predicted3.eq(labels.data).cpu().sum())
                    correct4 += float(predicted4.eq(labels.data).cpu().sum())

                    print('[epoch:%d, iter:%d] Loss: %.03f | Acc: 4/4: %.2f%% 3/4: %.2f%% 2/4: %.2f%%  1/4: %.2f%%'
                          ' Ensemble: %.2f%%' % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1),
                                                  100 * correct0 / total, 100 * correct1 / total,
                                                  100 * correct2 / total, 100 * correct3 / total,
                                                  100 * correct4 / total))

                print("Waiting Test!")
                with torch.no_grad():
                    correct4, correct3, correct2, correct1, correct0 = 0, 0, 0, 0, 0
                    predicted4, predicted3, predicted2, predicted1, predicted0 = 0, 0, 0, 0, 0
                    correct = 0.0
                    total = 0.0
                    for data in testloader:
                        net.eval()
                        images, labels = data
                        images, labels = images.to(device), labels.to(device)
                        outputs, feature_loss = net(images)
                        ensemble = sum(outputs) / len(outputs)
                        _0, predicted0 = torch.max(outputs[0].data, 1)
                        _1, predicted1 = torch.max(outputs[1].data, 1)
                        _2, predicted2 = torch.max(outputs[2].data, 1)
                        _3, predicted3 = torch.max(outputs[3].data, 1)
                        _4, predicted4 = torch.max(ensemble.data, 1)

                        correct0 += float(predicted0.eq(labels.data).cpu().sum())
                        correct1 += float(predicted1.eq(labels.data).cpu().sum())
                        correct2 += float(predicted2.eq(labels.data).cpu().sum())
                        correct3 += float(predicted3.eq(labels.data).cpu().sum())
                        correct4 += float(predicted4.eq(labels.data).cpu().sum())
                        total += float(labels.size(0))

                    print('Test Set AccuracyAcc: 4/4: %.4f%% 3/4: %.4f%% 2/4: %.4f%%  1/4: %.4f%%'
                          ' Ensemble: %.4f%%' % (100 * correct0 / total, 100 * correct1 / total,
                                                 100 * correct2 / total, 100 * correct3 / total,
                                                 100 * correct4 / total))
                    if correct0/total > best_acc:
                        torch.save(net.state_dict(), "./4att/bestmodel.pth")
                        print("model saved")
                        best_acc = correct0/total

            print("Training Finished, TotalEPOCH=%d" % args.epoch)





!python train2.py




'''
Implementation of Compositional Pattern Producing Networks in Tensorflow

https://en.wikipedia.org/wiki/Compositional_pattern-producing_network

@hardmaru, 2016

'''

import numpy as np
import tensorflow as tf
from ops import *

class CPPN():
  def __init__(self, batch_size=1, z_dim = 32, c_dim = 1, scale = 8.0, net_size = 32):
    """

    Args:
    z_dim: how many dimensions of the latent space vector (R^z_dim)
    c_dim: 1 for mono, 3 for rgb.  dimension for output space.  you can modify code to do HSV rather than RGB.
    net_size: number of nodes for each fully connected layer of cppn
    scale: the bigger, the more zoomed out the picture becomes

    """

    self.batch_size = batch_size
    self.net_size = net_size
    x_dim = 256
    y_dim = 256
    self.x_dim = x_dim
    self.y_dim = y_dim
    self.scale = scale
    self.c_dim = c_dim
    self.z_dim = z_dim

    # tf Graph batch of image (batch_size, height, width, depth)
    self.batch = tf.placeholder(tf.float32, [batch_size, x_dim, y_dim, c_dim])

    n_points = x_dim * y_dim
    self.n_points = n_points

    self.x_vec, self.y_vec, self.r_vec = self._coordinates(x_dim, y_dim, scale)

    # latent vector
    self.z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim])
    # inputs to cppn, like coordinates and radius from centre
    self.x = tf.placeholder(tf.float32, [self.batch_size, None, 1])
    self.y = tf.placeholder(tf.float32, [self.batch_size, None, 1])
    self.r = tf.placeholder(tf.float32, [self.batch_size, None, 1])

    # builds the generator network
    self.G = self.generator(x_dim = self.x_dim, y_dim = self.y_dim)

    self.init()

  def init(self):

    # Initializing the tensor flow variables
    init = tf.global_variables_initializer()
    # Launch the session
    self.sess = tf.Session()
    self.sess.run(init)

  def reinit(self):
    init = tf.initialize_variables(tf.trainable_variables())
    self.sess.run(init)

  def _coordinates(self, x_dim = 32, y_dim = 32, scale = 1.0):
    '''
    calculates and returns a vector of x and y coordintes, and corresponding radius from the centre of image.
    '''
    n_points = x_dim * y_dim
    x_range = scale*(np.arange(x_dim)-(x_dim-1)/2.0)/(x_dim-1)/0.5
    y_range = scale*(np.arange(y_dim)-(y_dim-1)/2.0)/(y_dim-1)/0.5
    x_mat = np.matmul(np.ones((y_dim, 1)), x_range.reshape((1, x_dim)))
    y_mat = np.matmul(y_range.reshape((y_dim, 1)), np.ones((1, x_dim)))
    r_mat = np.sqrt(x_mat*x_mat + y_mat*y_mat)
    x_mat = np.tile(x_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)
    y_mat = np.tile(y_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)
    r_mat = np.tile(r_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)
    return x_mat, y_mat, r_mat

  def generator(self, x_dim, y_dim, reuse = False):

    if reuse:
        tf.get_variable_scope().reuse_variables()

    net_size = self.net_size
    n_points = x_dim * y_dim

    # note that latent vector z is scaled to self.scale factor.
    z_scaled = tf.reshape(self.z, [self.batch_size, 1, self.z_dim]) * \
                    tf.ones([n_points, 1], dtype=tf.float32) * self.scale
    z_unroll = tf.reshape(z_scaled, [self.batch_size*n_points, self.z_dim])
    x_unroll = tf.reshape(self.x, [self.batch_size*n_points, 1])
    y_unroll = tf.reshape(self.y, [self.batch_size*n_points, 1])
    r_unroll = tf.reshape(self.r, [self.batch_size*n_points, 1])

    U = fully_connected(z_unroll, net_size, 'g_0_z') + \
        fully_connected(x_unroll, net_size, 'g_0_x', with_bias = False) + \
        fully_connected(y_unroll, net_size, 'g_0_y', with_bias = False) + \
        fully_connected(r_unroll, net_size, 'g_0_r', with_bias = False)


    '''
    Below are a bunch of examples of different CPPN configurations.
    Feel free to comment out and experiment!
    '''

    ###
    ### Example: 3 layers of tanh() layers, with net_size = 32 activations/layer
    ###
    #'''
    H = tf.nn.tanh(U)
    for i in range(3):
      H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_'+str(i)))
    output = tf.sigmoid(fully_connected(H, self.c_dim, 'g_final'))
    #'''

    ###
    ### Similar to example above, but instead the output is
    ### a weird function rather than just the sigmoid
    '''
    H = tf.nn.tanh(U)
    for i in range(3):
      H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_'+str(i)))
    output = tf.sqrt(1.0-tf.abs(tf.tanh(fully_connected(H, self.c_dim, 'g_final'))))
    '''

    ###
    ### Example: mixing softplus and tanh layers, with net_size = 32 activations/layer
    ###
    '''
    H = tf.nn.tanh(U)
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_1'))
    H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_2'))
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_2'))
    H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_2'))
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_2'))
    output = tf.sigmoid(fully_connected(H, self.c_dim, 'g_final'))
    '''

    ###
    ### Example: mixing sinusoids, tanh and multiple softplus layers
    ###
    '''
    H = tf.nn.tanh(U)
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_1'))
    H = tf.nn.tanh(fully_connected(H, net_size, 'g_tanh_2'))
    H = tf.nn.softplus(fully_connected(H, net_size, 'g_softplus_2'))
    output = 0.5 * tf.sin(fully_connected(H, self.c_dim, 'g_final')) + 0.5
    '''

    ###
    ### Example: residual network of 4 tanh() layers
    ###
    '''
    H = tf.nn.tanh(U)
    for i in range(3):
      H = H+tf.nn.tanh(fully_connected(H, net_size, g_tanh_'+str(i)))
    output = tf.sigmoid(fully_connected(H, self.c_dim, 'g_final'))
    '''

    '''
    The final hidden later is pass thru a fully connected sigmoid later, so outputs -> (0, 1)
    Also, the output has a dimention of c_dim, so can be monotone or RGB
    '''
    result = tf.reshape(output, [self.batch_size, y_dim, x_dim, self.c_dim])

    return result

  def generate(self, z=None, x_dim = 26, y_dim = 26, scale = 8.0):
    """ Generate data by sampling from latent space.

    If z is not None, data for this point in latent space is
    generated. Otherwise, z is drawn from prior in latent
    space.
    """
    if z is None:
        z = np.random.uniform(-1.0, 1.0, size=(self.batch_size, self.z_dim)).astype(np.float32)
    # Note: This maps to mean of distribution, we could alternatively
    # sample from Gaussian distribution

    G = self.generator(x_dim = x_dim, y_dim = y_dim, reuse = True)
    x_vec, y_vec, r_vec = self._coordinates(x_dim, y_dim, scale = scale)
    image = self.sess.run(G, feed_dict={self.z: z, self.x: x_vec, self.y: y_vec, self.r: r_vec})
    return image

  def close(self):
    self.sess.close()


generate()

'''
Implementation of Compositional Pattern Producing Networks in Tensorflow

https://en.wikipedia.org/wiki/Compositional_pattern-producing_network

@hardmaru, 2016

Sampler Class

This file is meant to be run inside an IPython session, as it is meant
to be used interacively for experimentation.

It shouldn't be that hard to take bits of this code into a normal
command line environment though if you want to use outside of IPython.

usage:

%run -i sampler.py

sampler = Sampler(z_dim = 4, c_dim = 1, scale = 8.0, net_size = 32)

'''
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

import numpy as np
#import tensorflow as tf
import math
import random
import PIL
from PIL import Image
import pylab
from model import CPPN
import matplotlib.pyplot as plt
import images2gif
from images2gif import writeGif

#mgc = get_ipython().magic
%matplotlib inline
pylab.rcParams['figure.figsize'] = (10.0, 10.0)

class Sampler():
    def __init__(self, z_dim = 8, c_dim = 1, scale = 10.0, net_size = 32):
        self.cppn = CPPN(z_dim = z_dim, c_dim = c_dim, scale = scale, net_size = net_size)
        self.z = self.generate_z() # saves most recent z here, in case we find a nice image and want the z-vec
    def reinit(self):
        self.cppn.reinit()
    def generate_z(self):
        z = np.random.uniform(-1.0, 1.0, size=(1, self.cppn.z_dim)).astype(np.float32)
        return z
    def generate(self, z=None, x_dim=1080, y_dim=1060, scale = 10.0):
        if z is None:
            z = self.generate_z()
        else:
            z = np.reshape(z, (1, self.cppn.z_dim))
            self.z = z
            return self.cppn.generate(z, x_dim, y_dim, scale)[0]
    def show_image(self, image_data):
        '''
        image_data is a tensor, in [height width depth]
        image_data is NOT the PIL.Image class
        '''
        plt.subplot(1, 1, 1)
        y_dim = image_data.shape[0]
        x_dim = image_data.shape[1]
        c_dim = self.cppn.c_dim
        if c_dim > 1:
            plt.imshow(image_data, interpolation='nearest')
        else:
            plt.imshow(image_data.reshape(y_dim, x_dim), cmap='Greys', interpolation='nearest')
            plt.axis('off')
            plt.show()
    def save_png(self, image_data, filename):
        img_data = np.array(1-image_data)
        y_dim = image_data.shape[0]
        x_dim = image_data.shape[1]
        c_dim = self.cppn.c_dim
        if c_dim > 1:
            img_data = np.array(img_data.reshape((y_dim, x_dim, c_dim))*255.0, dtype=np.uint8)
        else:
            img_data = np.array(img_data.reshape((y_dim, x_dim))*255.0, dtype=np.uint8)
        im = Image.fromarray(img_data)
        im.save(filename)
    def to_image(self, image_data):
        # convert to PIL.Image format from np array (0, 1)
        img_data = np.array(1-image_data)
        y_dim = image_data.shape[0]
        x_dim = image_data.shape[1]
        c_dim = self.cppn.c_dim
        if c_dim > 1:
            img_data = np.array(img_data.reshape((y_dim, x_dim, c_dim))*255.0, dtype=np.uint8)
        else:
            img_data = np.array(img_data.reshape((y_dim, x_dim))*255.0, dtype=np.uint8)
            im = Image.fromarray(img_data)
        return im
    def save_anim_gif(self, z1, z2, filename, n_frame = 10, duration1 = 0.5, \
                    duration2 = 1.0, duration = 0.1, x_dim = 512, y_dim = 512, scale = 10.0, reverse = True):
        '''
        this saves an animated gif from two latent states z1 and z2
        n_frame: number of states in between z1 and z2 morphing effect, exclusive of z1 and z2
        duration1, duration2, control how long z1 and z2 are shown.  duration controls frame speed, in seconds
        '''
        delta_z = (z2-z1) / (n_frame+1)
        total_frames = n_frame + 2
        images = []
        for i in range(total_frames):
            z = z1 + delta_z*float(i)
            images.append(self.to_image(self.generate(z, x_dim, y_dim, scale)))
            print ("processing image ", i)
            durations = [duration1]+[duration]*n_frame+[duration2]
        if reverse == True: # go backwards in time back to the first state
            revImages = list(images)
            revImages.reverse()
            revImages = revImages[1:]
            images = images+revImages
            durations = durations + [duration]*n_frame + [duration1]
            print ("writing gif file...")
            writeGif(filename, images, duration = durations)


Sampler()

Instead of tf.placeholder(shape=[None, 2], dtype=tf.float32) 
tf.compat.v1.placeholder(shape=[None, 2],
 dtype=tf.float32) if you don't want to disable v2 completely.

#replace import tensorflow as tf by following
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

#Source:stackoverflow.com
#1
#module 'tensorflow' has no attribute 'placeholder' tf 2.0
#Python By Marc Tolkmitt on May 14 2022 ThankComment

#change the <tf.placeholder> as <tf.compat.v1.placeholder>

#such as

x = tf.placeholder(shape = [None, image_pixels], dtype = tf.float32)
#change as

#x = tf.compat.v1.placeholder(shape = [None, image_pixels], dtype = tf.float32)
#but there would be another problem about runtime error with eager execution add <tf.compat.v1.disable_eager_execution()> after import part

import tensorflow as tf
tf.compat.v1.disable_eager_execution()