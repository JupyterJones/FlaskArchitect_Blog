-txt = twitter.search(q=HashTag)

for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list





import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
import Key
from random import randint
import FileLen
Max = FileLen.filelen("ToUse.txt")
num = randint(0, Max)
with open('ToUse.txt') as f:
    for i, STR in enumerate(f, 1):
        if i == num:
            break

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)
#PATH = '/home/jack/Desktop/3DFRACT/Mandelbulb3Dv199/cloud.jpg'
#PATH = '/home/jack/Desktop/3DFRACT/Mandelbulb3Dv199/post-002.jpg'
#PATH = '/home/jack/Desktop/3DFRACT/Mandelbulb3Dv199/post-068.jpg'
#PATH = '/home/jack/Desktop/text_stuff/instagram/post-054.jpg'
PATH = '/home/jack/Desktop/text_stuff/instagram/post-056.jpg'
#PATH = '/home/jack/Desktop/text_stuff/junk/post-color3.png'

STR ="#C++imagery #python I enjoy pallet swapping most of all #imageprocessing"
photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


twitter.search(q='python')

from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
#url = u'https://twitter.com/search?q='
HashTag = raw_input("HashTag  : ") or "CNN"
#url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
url = u'https://twitter.com/search?q='+ HashTag +'&src=typd'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


from textblob import TextBlob
from nltk.corpus import wordnet
import io
tweetfile = 'phrases.txt'
filein = open(tweetfile, 'w')
filein.close() 

with io.open("hashtag.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 

blob = TextBlob(essays)
for np in blob.noun_phrases:
    filein = open(tweetfile, 'a')
    np = '\n'+ u''.join((np)).encode('utf-8').strip()
    np=np.replace("#","");np=np.replace("//","")
    np=np.replace("... ‚Äù","");np=np.replace("... ","")
    np=np.replace(".. ","")
    np = np.lstrip()
    if len(np) < 16 and len(np) >3:
            np = (np+"\n")
            filein.write(np)
            print np
            filein.close() 

from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
HashTag = (random_line('phrases.txt'))
url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
#HashTag = (random_line('nodupsnohash.txt'))
#HashTag = (random_line('clean.txt'))
HashTag = "CNN"
url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


def findhash():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('hashonly.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                hashed = hashed.replace("#","\n#")
                outfile.write(hashed)


findhash()

def removeblank():
    with open('hashonly.txt') as infile, open('hashonlyNoBlank.txt', 'w') as outfile:
        for line in infile:
            if not line.strip(): continue  # skip the empty line
            outfile.write(line)  # non-empty
            
removeblank() 

%%writefile /home/jack/anaconda2/lib/python2.7/site-packages/Txmanip/RemoveBlank.py

def removeblank(origFile, saveAS ):
    with open(origFile) as infile, open(saveAS, 'w') as outfile:
        for line in infile:
            if not line.strip(): continue  # skip the empty line
            outfile.write(line)  # non-empty
            
 

def remove_duplicates(infile):
    tempstore = set()
    with open('hashonlyNoBlankNoDups.txt', 'w+') as out:
        for line in open(infile):
            if line not in tempstore:
                if len(line) < 16 and len(line) >4:
                    out.write(line)
                    tempstore.add(line)

remove_duplicates('hashonlyNoBlank.txt')

%%writefile /home/jack/anaconda2/lib/python2.7/site-packages/Txmanip/RemoveDuplicate.py
def removeduplicate(infile,outfile):
    tempstore = set()
    with open(outfile, 'w+') as out:
        for line in open(infile):
            if line not in tempstore:
                if len(line) > 4:
                    out.write(line)
                    tempstore.add(line)



def findhash():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('hashonly.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                hashed = hashed.replace("#","\n#")
                outfile.write(hashed)


findhash()

def removeblank():
    with open('hashonly.txt') as infile, open('hashonlyNoBlank.txt', 'w') as outfile:
        for line in infile:
            if not line.strip(): continue  # skip the empty line
            outfile.write(line)  # non-empty
            
removeblank() 

def remove_duplicates(infile):
    tempstore = set()
    with open('hashonlyNoBlankNoDups.txt', 'w+') as out:
        for line in open(infile):
            if line not in tempstore:
                if len(line) < 16 and len(line) >4:
                    out.write(line)
                    tempstore.add(line)

remove_duplicates('hashonlyNoBlank.txt')

def findusers():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('users.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("@") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                hashed = hashed.replace("'","")#;hashed = hashed.replace("@\n ","")
                hashed = hashed.replace("@","\n@")
                outfile.write(hashed)
findusers()
def removeblank():
    with open('users.txt') as infile, open('usersNoBlank.txt', 'w') as outfile:
        for line in infile:
            if not line.strip(): continue  # skip the empty line
            outfile.write(line)  # non-empty
removeblank()

def remove_duplicates(infile):
    tempstore = set()
    with open('usersNoDups.txt', 'w+') as out:
        for line in open(infile):
            if line not in tempstore:
                if len(line) < 16 and len(line) >4:
                    out.write(line)
                    tempstore.add(line)

remove_duplicates('usersNoBlank.txt')


def removeHash(infile):
    with open('clean.txt', 'w+') as out:
        for line in open(infile):
            line = line.replace("#","")
            out.write(line)

removeHash('hashonlyNoBlankNoDups.txt')

# %load clean.txt
Solar
EnergyStorage
netmetering
consumption"
Chinas"
soybeans
tobacco
QuitSmoking!
AskArvi
HealthTips
social
MEASummit2017
Bitcoin
energy
Industry
Growth
cities
develop
streetcar
decrease
traffic
congestion
increase
economic
activity
downtown
Sydney
BackTheBlue
Togo
Trump
CFCU19
ElvisHarte
TeamDOLL
none
NONE
deranged
filthy
Sane
clean
currently
Progressive
None
Zero
Nilch
Nada
Meme
threadless
The
ArtistShops
FAIL
MAKE
None.Its
Each
Day
Always
Other
Appreci88ion
NASCARMPD
Winnie
pooh
gluten
Honey
none.
know
Love
All
Trust
Few
Wrong
NoNe
Redskins
Fuck
Respect
Liberal
Bastards
America
DHS
ICE
AwanBrothers
ImranAwan
FBI
Wednesday
MAGA
POTUS
pick6
sofly
shadeson
dhs
ANTIFA
RIGHT_WING
OSG
Obama
Russia
DOJ
CIA
NSA
TheResistance
Equifax
news
JONESACT
PuertoRico
DHS:
JonesAct
ALLCargo
Ports
Aid
Hur...
Blockchain
Funding
infosec
cybersecurity
CISO
privacy
CIO
CSO
CO18
SEN18R
MAN
WHEATGRASS
COCONUT
DIE
PAINFUL
DEATH!!
NOTFUNNY!!!
Man
Grokitman
Iot
smarter
comic
book
superhero!
reading
learning
history
science
Grok
man
Scania
DAF
Nowguesswho
Sabeans
believe
especially
gotitcoming.
Recompense
gym
hairy
muscles
men
workout
workingout
chest
handsome
blueeyes
question
semantics
grok
grokitman
grokit
Blessed
oklahoma
selfie
biceps
lockerroom
cute
wrestler
guy
Knee
Persident
SOB
MOM
man-flu
big-baby
6pack
sixpack
tummy
toned
georgia
wrestle
wrestling
speedos
beefy
weightlifter
Gay
tbt
boy
beard
veteran
Fancy
Spending
Weekend
Himself
exsas?
oneofthebest
muscle
daddy
contest
contestants
pool
bodybuilder
bodybuilding
muscleman
pumpedup
delts
cop
police
officer
policeman
captain
strong
Microbial
Spoilage
PCR
PCR\xc2\xad
PCR!
HumpDay
Medicare
IMXEMS
MaxPain
Genetic
Molecular
ONEOF1ignore
Biotech
Biotechnology
DNA
pcr
visonmixer
consumers
BAX
Salmonella
ESICM2017
lives2017
Vienna
mcm
diagnostics
PCR!!!
EndTB
Swaziland
research
biology
lifescience
biotech
laboratory
curiosity
ourstuart
cancer
cells
RNAextraction
cellculture
rna
capitalism
environment.
consumption
World
Energy
Consumption
OECD
Freedom.
PlanetEarth.
war.
consumption.
7DeadlySins
IRS
Tax
WriteOffs
Deductions
FairTax!
awareness
local
smallholder
farmers
agriculture
land
green
materialism.
Indian
GDP
EOTYatSSE
reduce
bill
Multinational
Corps
Nutrition
Food
Manufactured
Logistics
mrx
Direct
Insurance
Carrier
Revenues
Global
Demand
pricesetting
tax
alcohol
Logos
tomgalle
earth
McDonalds
Spanish
kerosene
consumption:
maximum
climatechange
environment
ecology
globalwarming
population
lifestyle
Workstation
workstation.
VMware
vExpert
Fusion
setup
dreamsetup
workstation
battlestation
workspace
pcgaming
deskspace
desksetup
gaming
game
workhard
buongiorno
yql
vmware
fusion
newreleases
pcgamer
gamer
gamingpc
computer
gamingsetup
electric
desks
health
performance
Autodesk
Maya
SPEC
benchmark
PISDMathChat
startup
desk
WorksForMe
movies
design
uidesign
dribbble
behance
office
effects
movie
Steps
Run
KaliLinux
Howto
Linux
Tactig
photography
Market
Profit
Packaged
Frozen
Wholesalers
FreeAccess:
Alternative
food
Waste
Collection
business
cloud
technology?
CyberSecurity
Analytics
Security
Compliance
adobe
security
Unix/
chrome
edge
safari
ITSecurity
Crypto
CSOCartoon
CISSP
ITsec
infosectruth
NSASymposium
cryptohistory
fintech
Insurtech
education
cio
ciso
cso
First
Female
Cybersecurity
Tech
Regulation
datasec
corpgov
gdpr
eugdpr
finserv
Boston
InfoSec
cto
CyberConnect
Podcast-
GDPR
governance
regtech
microfocus
encryption
threatintel
ehr
ghc17
excited
opportunity
gearing
thanks
ABI
GOOGLE
folding
Galaxy
festiveseason
weddings
shows
socialising
parties
mittiofkutch
treasures
positive
negative
investment
BikeServicing
Calella
Gearing
Brakes
Bolts
HotelMontRosa
timewalking
Wow
HAAS.
AM100
Automotive
News
Commission
European
MyBTCcoinThe
HTC
big
announcement
tomorrow
Google
acquisition
mfg
gears
engineering
machinetools
automotive
shoot
pic
potrait
photo
agchef
marriage
styleignore
marketing
LOST
Lost
frenchbulldog
missing
lost
stolen
bpd
Lost&Found
love
lost.
thiem
ParsonCross
Sheffield
Lostdog
ScanMe
Missing
puppy
BorderCollie
Ramsgate
Kent
Chipped
Silent
auction
Helping
Charities
Fundraising
Auctions
Mask
silent
Garba
against
noise
pollution
surat
MYFM
world
Rohingya
thieves
History
headphones
awarness
Hugging
matter
hero
ParkShinHye
ChoiHeeJung
Chiswick
Sanctuary
Silent..
knowledge
angel
birthday
birthdayboy
youngjeezy
september
yeg
sunset
yegwx
skywatcher
September
adventure
funwalk
Saturday
wedotourism
tourismmonth
barefoot
dirtyfeet
autumn
harvest
backtoschool
joinus
GameModeon
emojis
indiedev
gamedev
cadencequiz
inflation
Spain
co2018
classsong
SEPTEMBER
28th
september28
2009
SanSalvador.
youthforpeace
devoted
children
Tupperware
ToaPayoh
TaxReform
CNN
CNNTownHall
sanjuan
puertorico
cnn
BoycottNFL
FoxNews
NFL
WeThePeople
TakeAKnee
Kaepernick
MSNBC
cnn"
Trumps"
newday
dotard
PUERTORICO
Fox
BREAKING:Lt
HuffPost
CNN:
bbc
sky
BBC
Idlib
Syria.
FAKE
CONSERVATIVE
SKY
NEWS
TORY
BREXIT
Biafra
NigelFarage:
Remainers
Leave
LabourParty
ANTISEMITIC
Labour
JeremyCorbyn
JEWS!
TheresaMay
ITV
UKIP
LABOUR
TimesNow
Rakhine.
corruption
EU!
Retweet:
Petition:
USA
TRAITOR
PhilipHammond
Political
COWARD
government
SELLOUT!
Florence
Brexit
breaking
bbcnews
LGBT
golf
democrat
war
dnc
usa
afghanistan
msnbc
nbc
abc
cbs
pbs
Flag
Anthem
Country.
Country
OneNation
DoorMat
TrumpsArmy
Military
LEOs
Classless
disgrace
Democrat
TakesAKnee
Veterans
americanflag
Texas
boycottNFL
takeastand
ServiceK9
Jinky
OnDuty
Family
Red
White
Blue
BlueFamily
nfl
kneeling
American
flag
protest
majority
america
disgrace.
potus.
StandUp
MyPresident
BoycottTheNFL
ANTHEM
FLOTUS
MichelleObama
FLAG
MILITARY
POLICE
Scotland
travel
Scottish
Edinburgh
Boycott
Packers
country
Flag..
anthem.
TrickleDown
morningjoe
bobcostas
GrahamCassidy
SaveACA
boundaries
NewDay
Regan
myth
mocofb
other
other.
each
alisonmoyet
voice
othertour
iOS11
OTHER
BEHAVIOR
YOUR
pets
dogs
cats
birds
hamsters
FollowsMe
Litecoin
VideoGame
Manga
Information
And
Sitting
Choir
Rihminds
Addition
Things
Thought
Rihlationship
KamelaHarris
Malutto
Negro
Malutto"
dont"
choose
Prospect
mine.
Prophetic
Warrior
Prayer
Circle
Leader"
System
Most
Own
Agenda
Identified
Time
Identify
Other...
NPm
Mirror
him
philosophers
playdate
play
lethimout
iFunny
meme
memes
memethis
memeit
Mickeynnials
Alsace
Woups
Sorry
TweetClash
spending
trump
regime
Swamp
TaxMoney
TrumpSwamp
surveys
premises
Save
today
budget
inequality.
consult
spending.
tcot
money
CapeTown
jozi
birthday.
October!
findom
greedy
Saving
Expenses
DailyHabits
Robotics
jobs
sales
economy
Startup
growth
Pruitt
EPAs"
13years
Lost.
Friends
LakeEola
MegWhite...
WhiteStripes
Florida
Coincidence
IthinkNot
karma
coincidence
Wonderful
KylieJenner
Kuwtk
weird
nyc
gymlife
coincidence?
ThinkNot
Coincidence?
publicpower
ithinknot
Destiny
Chance
ExamineFate
Humour
ASMSG
BYNR
IARTG
CoIncidence
national
awards
India
Current
Affair
Today.
Who
NTS
Question
KaShMiR
Affairs
important
CURRENT
AFFAIRS
RD28917
current
affairs
basic
quiz
IBPS
RRB
Clerk
VSS365
BigData
MLaaS
Tech-savvy
human
Jobs
Autonomous
Watson
Cognitive
IoT
Industry40
IIoT
DataScience
DataScientist
futurism
robotics
deepmind
technology
DeepLearning
DLUPC
HPC
tech
Cloud
Video
Bot
SMM
IoT?
SmartCity
Health
3DPrinting
AISummit
SEO
deeplearning
searchengines
healthcare
robots
robot
bot
Mpgvip
AI?
Fintech
defstar5
4IR
Deeplearning
EntBullets
3wordQuote
mpgvip
Entrepreneur
Businesstips
ss11
ioT
mlm
LessPay
DataMining
IOT
IOE
blockchain
RBC
net
Websites
banking
NMCHI17
Algorithms
AI:
abdsc
Applications
Bigdata
SaaS
DataViz
IaaS
PaaS
content
random
Apple
ARKit
iOS
Keywords
disruption
retail
innovation
cstore
kirin970
AI.
Ethiopia
todays
abayfm
Todays
Mental
ImpeachTrump
Fakenews
VeryFakenews!
Trump:"Maybe
SNL
MSM
VOTE!
FOX
TeamUSAon
PeoplePower
retweet

!rm tweets.db

import sqlite3
import time
import sys
import base64
conn = sqlite3.connect('tweets.db')
c = conn.cursor()
c.execute('''CREATE VIRTUAL TABLE twitter
             USING FTS3 (id,twittertext)''')


def findhash():
    id = 0
    with open('hashtag.txt', 'r') as textin:
          for line in textin.readlines(): 
                id = id+1
                #hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = line.split()
                #hashed = str(hashed)
                #hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                #hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                #hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                #hashed = hashed.replace("#","\n#")
                time.sleep(1)
                conn = sqlite3.connect('tweets.db')
                conn.text_factory = str
                c = conn.cursor()
                c.execute("INSERT INTO twitter VALUES (?,?)", (id, line)) 
                conn.commit()
                conn.close()                        
                print line

                
                
                
                
                
                

findhash()

def findhash():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('hashonly.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                hashed = hashed.replace("#","\n#")
                outfile.write(hashed)


findhash()

def removeblank():
    with open('hashonly.txt') as infile, open('hashonlyNoBlank.txt', 'w') as outfile:
        for line in infile:
            if not line.strip(): continue  # skip the empty line
            outfile.write(line)  # non-empty
            
removeblank() 

def remove_duplicates(infile):
    tempstore = set()
    with open('hashonlyNoBlankNoDups.txt', 'w+') as out:
        for line in open(infile):
            if line not in tempstore:
                if len(line) < 16 and len(line) >4:
                    out.write(line)
                    tempstore.add(line)

remove_duplicates('hashonlyNoBlank.txt')

def removeHash(infile):
    with open('clean2.txt', 'w+') as out:
        for line in open(infile):
            line = line.replace("#","");line = line.replace('"','')
            out.write(line)

removeHash('hashonlyNoBlankNoDups.txt')



def findhash():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('temp.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                hashed = hashed.replace("#","\n#")
                outfile.write(hashed)
    outfile.close()
    with open('temp.txt', 'r') as infile:
        with open('temp2.txt', 'w') as outfile:
             for line in infile:
                if not line.strip(): continue  # skip the empty line
                outfile.write(line)  # non-empty
    outfile.close()            
    with open('cleanwords.txt', 'w') as out:
        for line in open('temp2.txt', 'r'):
            line = line.replace("#","")
            out.write(line)                
                
                
findhash() 

def findhash():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('temp.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                hashed = hashed.replace("#","\n#")
                outfile.write(hashed)
    textin.close
    outfile.close()            
    with open('temp2.txt', 'w+') as out:
        for line in open('temp.txt', 'r'):
            line = line.replace("#","")
            out.write(line)                
    outfile.close()
    with open('temp2.txt', 'r') as infile:
        with open('cleanout.txt', 'w') as outfile:
             for line in infile:
                if not line.strip(): continue  # skip the empty line
                outfile.write(line)  # non-empty                
                
findhash() 





!ls /usr/share/dict/

text = open("/usr/share/dict/british-english",'r').readlines()

for lines in text:
    if lines.find("word") == 0:
        print "word is in the dictionary"

text = open("/usr/share/dict/british-english",'r').readlines()
with io.open("clean2.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
for lines in text:
    if lines.find(essays) == 0:
        print "word is in the dictionary"

text = open("/usr/share/dict/british-english",'r').readlines()
badtext = ''.join(c for c in map(chr, range(256)) if not c.isalnum())

def stripNonAlphaNum(text):
    import re
    return re.compile(r'\W+', re.UNICODE).split(text)

with open("clean2.txt", "r") as my_file:
    words = my_file.read()
    #words = stripNonAlphaNum(words)
    for lines in text:
        if lines.find(words) != 0:
            print words    


text = open("/usr/share/dict/british-english",'r').readlines()
badtext = ''.join(c for c in map(chr, range(256)) if not c.isalnum())

def stripNonAlphaNum(text):
    import re
    return re.compile(r'\W+', re.UNICODE).split(text)

with io.open("clean2.txt", "r", encoding="utf-8") as my_file:
    words = my_file.read()
    words = stripNonAlphaNum(words)
    words = str(words)
    for lines in text:
        if lines.find(words) == 0:
            print words

text = open("/usr/share/dict/british-english",'r').readlines()
badtext = ''.join(c for c in map(chr, range(256)) if not c.isalnum())
with io.open("clean2.txt", "r", encoding="utf-8") as my_file:
    words = my_file.read()
    word = words.translate(None, badtext)
for lines in text:
    if lines.find(words) == 0:
        print "word is in the dictionary"

badtext = ''.join(c for c in map(chr, range(173)) if not c.isalnum())
print badtext

word = words.translate(None, badtext)

from textblob import TextBlob
from nltk.corpus import wordnet

tweetfile = 'realwords.txt'
filein = open(tweetfile, 'w')
filein.close()  
with io.open("clean2.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
    if not wordnet.synsets(essays): 
        #Not an English Word
        print essays,"- Is not a word"
    else:
        #English Word    
        filein = open(tweetfile, 'a')
        filein.write(essays)
        filein.close()    

        print essays


from textblob import TextBlob
tweetfile = 'realwords.txt'
filein = open(tweetfile, 'w')
filein.close()  
with io.open("clean2.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read()
    if wordnet.synsets(essays):
        print essays


import time
def findhash():
    with open('hashonly.txt', 'r') as textin:
        tempstore = set()
        with open('hashonlyS.txt', 'w') as outfile:
            for line in textin.readlines():
                line = line.replace("##","#")
                line = line.replace("#","\n#")
                
                if len(line) < 16 and len(line) >3: 
                    outfile.write(line)
findhash()

def findhash():
    with open('phrases.txt', 'r') as textin:
        tempstore = set()
        with open('hashonly.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                outfile.write(hashed)


findhash()

with open('hashonly.txt') as infile, open('hashonlyNoBlank.txt', 'w') as outfile:
    for line in infile:
        if not line.strip(): continue  # skip the empty line
        outfile.write(line)  # non-empty

with open('hashonlyS.txt') as infile, open('NShashonlyS.txt', 'w') as outfile:
    for line in infile:
        if not line.strip(): continue  # skip the empty line
        outfile.write(line)  # non-empty

from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
HashTag = (random_line('hashonly.txt'))
HashTag = HashTag.replace("#", "")
print HashTag

textin= open('hashonly.txt', 'r')
lines = textin.read().splitlines()
time.sleep(1)
print lines





from textblob import TextBlob
from nltk.corpus import wordnet
tweetfile = 'phrases.txt'
filein = open(tweetfile, 'w')
filein.close()  
with io.open("hashtag.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 

blob = TextBlob(essays)
try:
    for np in blob.noun_phrases:
        filein = open(tweetfile, 'a')
        np = '\n'+ u''.join((np)).encode('utf-8').strip()
        np=np.replace("#","");np=np.replace("//","")
        np=np.replace("... ‚Äù","");np=np.replace("... ","")
        np=np.replace(".. ","")
        np = np.lstrip()
        if len(np) < 16 and len(np) >3:
            np = (np+"\n")
            np = str(np)
    if wordnet.synsets(np):
        filein.write(np)
        filein.close()    

               
        # block raising an exception
except:
       pass # doing nothing on exception        
print np





from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
HashTag = (random_line('hashonly.txt'))
HashTag = HashTag.replace("#", "")
url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
HashTag = (random_line('hashonly.txt'))
HashTag = HashTag.replace("#", "")
#url = u'https://twitter.com/search?q='+ HashTag +'&src=typd&lang=en'
#https://twitter.com/search?q=%23cnn&src=typd&lang=en
#url = u'https://twitter.com/search?q='+ HashTag +'&src=typd&lang=en'
url = u'https://twitter.com/'+ HashTag +'?lang=en'
#url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')
tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]


from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
HashTag = (random_line('hashonly.txt'))
print HashTag

from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
HashTag = (random_line('hashonly.txt'))
url = u'https://twitter.com/'+ HashTag +'?lang=en'
#url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
HashTag = (random_line('hashonly.txt'))
url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


def random_line(fname):
    lines = open(fname).readlines()
text = random_line('hashonly.txt') 
print text

def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)
#url = u'https://twitter.com/search?q='
HashTag = (random_line('hashtag-nouns.txt'))
print HashTag

textin= open('hashtag.txt', 'r')
lines = textin.read().splitlines()
time.sleep(1)
print lines,

def findhash():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('hashonly.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("#","\n#")
                if len(hashed) < 36 and len(hashed) >3:
                    hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                    hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                    hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                    hashed = hashed.replace('"','')
                    outfile.write(hashed)


findhash()


def linesonly():
    with open('hashonly.txt') as infile, open('hashonlyNoBlank.txt', 'w') as outfile:
        for line in infile:
            if not line.strip(): continue  # skip the empty line
            outfile.write(line)  # non-empty
linesonly()            

def findhash():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('hashonly.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("#","\n#")
                if len(hashed) < 36 and len(hashed) >3:
                    hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                    hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                    hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                    hashed = hashed.replace('"','')
                    outfile.write(hashed)


findhash()