-import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
search = raw_input("ROWID :")
for row in c.execute('SELECT rowid, file, content, description FROM ipynb WHERE rowid = ?',(search,)):
    print row[2]
c.close()
conn.close()

import os
import os.path
title = "sept8_IPNB.list"
f= open(title,"w");f.close()
count=0
for dirpath, dirnames, filenames in os.walk("/home/"):
    filenames = [f for f in filenames if not f[0] == '.']
    dirnames[:] = [d for d in dirnames if not d[0] == '.']
    for filename in [f for f in filenames if f.endswith(".ipynb")]:
        count=count+1
        Path = os.path.join(dirpath, filename)
        with open(title, 'a') as outfile:
            path = Path+"\n"
            outfile.write(path)

import os
import os.path
title = "JupyterNotebook-Graphics.list"
f= open(title,"w");f.close()
count=0
for dirpath, dirnames, filenames in os.walk("/home/jack/Desktop/dockercommands/"):
    filenames = [f for f in filenames if not f[0] == '.']
    dirnames[:] = [d for d in dirnames if not d[0] == '.']
    for filename in [f for f in filenames if f.endswith(".db")]:
        count=count+1
        Path = os.path.join(dirpath, filename)
        with open(title, 'a') as outfile:
            path = Path+"\n"
            outfile.write(path)

!sqlite3 /home/jack/nov19b.db "pragma integrity_check;"

import sqlite3
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory=str
c = conn.cursor()
c.execute("SELECT COUNT(*) from ipynb")
(number_of_rows,)=c.fetchone()
print (number_of_rows,) 

!ls /home/jack/*.db

!rm /home/jack/Desktop/PROCESSING/Processing/nov19b.db

file = "/home/jack/Desktop/GRAPHICS/nov19b.db"
import os.path, time
print("last modified: %s" % time.ctime(os.path.getmtime(file)))
print("created: %s" % time.ctime(os.path.getctime(file)))

!ls /home/jack/nov19b.db

!locate nov19b.db

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
count=1
search = raw_input("Title :")
# ONLY the DATA table can use MATCH
for row in c.execute('SELECT rowid, file, content, description FROM ipynb WHERE content MATCH ?',(search,)):
    print row[0],row[1],row[3],"\n----------------------"
    count=count+1
    if count>25:break
c.close()
conn.close()

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
count=1
search = raw_input("Title :")
for row in c.execute('SELECT rowid, file, content, description FROM ipynb WHERE content MATCH ?',(search,)):
    print row[0],row[1],row[3],"\n----------------------"
    count=count+1
    if count>10:break
c.close()
conn.close()


        

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
count=1
search = raw_input("ROWID :")
for row in c.execute('SELECT rowid, file, content, description FROM ipynb WHERE rowid = ?',(search,)):
    print row[0],row[1],row[3],"\n----------------------"
    count=count+1
    if count>10:break
c.close()
conn.close()


        

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"

conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
search = raw_input("FileName :")
f= open(search,"w");f.close()
with open(search, 'a') as outfile:
    for row in c.execute('SELECT content FROM ipynb WHERE file MATCH ?', (search,)):
        row0 = row[0]
        #row0 =str(row0,"\n")
        outfile.write(row0)
        print row0
        
conn.close()
f.close()

        

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"

conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
search = raw_input("ROWID :")
f= open(search,"w");f.close()
with open('newtest.ipynb', 'a') as outfile:
    for row in c.execute('SELECT content FROM ipynb WHERE rowid = ?', (search,)):
        row0 = row[0]
        #row0 =str(row0,"\n")
        outfile.write(row0)
        
conn.close()
f.close()

        

1753

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"
f= open('.ipynb',"w")
f.close()
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
search = raw_input("ROWID :")
with open('.ipynb', 'a') as outfile:
        for row in c.execute('SELECT content FROM ipynb WHERE rowid = ?', (search,)):
            #for row in c.execute('SELECT * FROM ipynb WHERE rowid = 100 LIMIT 10 OFFSET 0'):
            row0 = row[0]
            #row0 =str(row0,"\n")
            outfile.write(row0)

conn.close()
f.close()

        

import json
import sys
from time import sleep
import sqlite3
import csv
#database = "junk.db"
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
start =raw_input("ROWID-Start :")
end =raw_input("ROWID-End :")
#for row in c.execute('SELECT rowid, * FROM ipynb WHERE rowid = ?', (search,)):
#for row in c.execute('SELECT rowid, * FROM ipynb WHERE rowid > 40 and rowid < 100'):    
for row in c.execute('SELECT rowid, * FROM ipynb WHERE rowid > ? and rowid < ?',(start, end)):    
     
        print row[0],row[1],"\n\n",row[3]
conn.close()
f.close()

        

import sqlite3
import sys
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory = str
c = conn.cursor()
count=0;req=200
id1 = raw_input("Starting ID : ")
id2 = raw_input("How Many Rows : ")
# id1 is start id2 is how many lines
for row in c.execute('SELECT rowid, * from ipynb LIMIT ? OFFSET ?',  (id2, id1)):
    count=count+1
    #print row[0],row[1],"\n",row[2]
    print row[0]
    if count > req:
        conn.close()
        sys.exit()

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
ROWID = 
c.execute("DELETE FROM ipynb WHERE rowid = ?", (ROWID,))
conn.commit()
conn.close()
        

import sqlite3
database = "test.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c = conn.cursor()
for row in c.execute('SELECT rowid, file, description FROM ipynb WHERE file = "Index" '):
    print row[0],row[1],row[2]       

c.close()
conn.close()

import re
title = "ipynb.list"
titles = open(title,"r")
for title in titles.readlines():
            filename = os.path.basename(title)
            description = filename
            description = description.replace("-", " ")
            description = description.replace("_", " ")
            description = description.replace(".ipynb", " ")
            description = re.sub("([a-z])([A-Z])","\g<1> \g<2>",description)
            print title, filename, description,"\n\n\n"

import json
import sys
from time import sleep
import sqlite3
import csv
#database = "FTS4_IPYNB_indexed.db"
database = "oct1_IPNB.list.db"
conn = sqlite3.connect(database)
conn.text_factory=str
c = conn.cursor()
c.execute("SELECT COUNT(*) from ipynb")
print c.fetchone()

#print number_of_rows    

import json
import sys
from time import sleep
import sqlite3
import csv
#database = "junk.db"
database = "test.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
count = 1
search = "Index"
for row in c.execute('SELECT rowid, * FROM ipynb WHERE content = ?', (search,)):
        print count,"\n", row[0],row[1],row[3],"\n-----------------"
        count=count+1
        if count>10:break
conn.close()
f.close()

        

!rm test.db

import sqlite3
database = "/home/jack/postit.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS3(title, line);
""")
conn.commit()
conn.close()

import base64
from time import sleep
title = "ALL-WORKNG-SQLITE_and_MYSQL.ipynb"
import sqlite3
conn = sqlite3.connect('oct1_IPNB.list.db')
conn.text_factory=str 
c = conn.cursor()
with open(title, 'r') as f:
    lines = f.readlines()
    for line in lines:
        c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
        conn.commit()
        #print encodedlistvalue

conn.close()
f.close()

import os
PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/"
for file in os.listdir(PATH):
    if file.endswith(".ipynb"):
        print(os.path.join(PATH, file))

import base64
from time import sleep
import os
import sqlite3
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/"
PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/postit/"
database = "/home/jack/postit.db"
for file in os.listdir(PATH):
    if file.endswith(".ipynb"):
        print(os.path.join(PATH, file))
        title2 = PATH+file
        conn = sqlite3.connect(database)
        conn.text_factory=str 
        c = conn.cursor()
        with open(title2, 'r') as f:
            lines = f.readlines()
            for line in lines:
                c.execute("INSERT INTO ipynb VALUES (?,?)", (file, line)) 
                conn.commit()

        conn.close()
        f.close()
        conn = sqlite3.connect(database)
        conn.text_factory=str 
        c = conn.cursor()
        line = file
        #line ="Good-mouse-sizing-and-cropping.ipynb"
        title = "index"
        c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
        conn.commit()
        conn.close()
        f.close()

from time import sleep
database = "/home/jack/postit.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS4(title, line);
""")
conn.commit()
conn.close()

title2 = PATH+title
import sqlite3
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
with open(title2, 'r') as f:
    lines = f.readlines()
    for line in lines:
        c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
        conn.commit()

    line = title
    title = "index"
    c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
    conn.commit()


conn.close()
f.close()

!rm /home/jack/Desktop/text_stuff/postit.db

!rm /home/jack/Desktop/text_stuff/postit.db

import base64
from time import sleep
import os
title = "Title-and-Sign-titlenpost.py.ipynb"
import sqlite3
conn = sqlite3.connect('newipynb.db')
conn.text_factory=str 
c = conn.cursor()
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS3(title, line);
""")

#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/"
PATH = "/home/jack/Desktop/text_stuff/"
for file in os.listdir(PATH):
    if file.endswith(".ipynb"):
        print(os.path.join(PATH, file))
        title2 = PATH+file
        with open(title2, 'r') as f:
            lines = f.readlines()
            for line in lines:
                c.execute("INSERT INTO ipynb VALUES (?,?)", (file, line)) 
                conn.commit()
                #print encodedlistvalue
            conn = sqlite3.connect('newipynb.db')
            conn.text_factory=str 
            c = conn.cursor()

            line = file
            #line ="Good-mouse-sizing-and-cropping.ipynb"
            title = "index"
            c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
            conn.commit()
                    #print encodedlistvalue

conn.close()
f.close()

!rm newtest.db

file = 'deep-dream-generator.ipynb'
PATH = "/home/jack/python3-starter/notebooks/"
filename = PATH+file
filein = PATH
filein = filein.replace("/home/jack/", "")
filein = filein.replace("/", "_")
filein = filein+file
description = filein
description = description.replace("_", " ")
description = description.replace("-", " ")
description = description.replace("/", " ")
description = description+"\n"+PATH+file+"\n"+file
print filein, "\n\n", description

file = 'SOme.txt'
PATH = "/home/jack/python3-starter/notebooks/"

description = PATH
description = description.replace("_", " ")
description = description.replace("-", " ")
description = description.replace("/", " ")
filename = PATH+file
description = description+(file)
#filein = "python3-starter-notebooks_"+file
print description


!rm FTS4_notebooks.db

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
search = raw_input("Title :")
for row in c.execute('SELECT rowid, * FROM ipynb WHERE file MATCH ?',(search,)):
    print row[0],row[1]

c.close()
conn.close()


        

import json
import sys
from time import sleep
import sqlite3
import csv
database = "test.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
search = raw_input("ROWID :")
for row in c.execute('SELECT content, * FROM ipynb WHERE rowid = ?',(search,)):
    print row[0],row[1]

c.close()
conn.close()


        

database = "newtest.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
with open("Output.ipynb", "wb") as output_file:
    c.execute("SELECT content FROM ipynb WHERE rowid = '1753'")
    ablob = c.fetchone()
    output_file.write(ablob[0])

c.close()
conn.close()

import sqlite3
database = "newtest.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS3(file, content);
""")
conn.commit()
conn.close()
conn = sqlite3.connect(database)
c = conn.cursor()
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/"
#PATH = "/home/jack/Desktop/text_stuff/"
#PATH = "/home/jack/Desktop/imagebot/"
#PATH = "/home/jack/Desktop/Snippet_Warehouse/"
#PATH = "/home/jack/Desktop/gitjupyter/"
#PATH = "/home/jack/Desktop/jack_watch/"
#PATH = "/home/jack/Desktop/jack_watch/nltk/"
#PATH = "/home/jack/Desktop/jack_watch/Python-Lectures/"
#PATH = "/home/jack/Desktop/jack_watch/jupyter_examples-master/"
#PATH = "/home/jack/Desktop/Books/numerical-python-book-code/"
#PATH = "/home/jack/Desktop/Books/pydata-book/"
#PATH = "/home/jack/Desktop/Ruby/"
#PATH = "/home/jack/Desktop/alice/ChatterBot/"
#PATH = "/home/jack/Desktop/deep-dream-generator/LOCAL-notebooks/"
#PATH = "/home/jack/Desktop/numpy-array-filters/"
#PATH = "/home/jack/Desktop/pycode/"
#PATH = "/home/jack/Desktop/pycode/vpython2/TrigonometryBot/"
#PATH = "/home/jack/Desktop/temp/args_csv_Twython_ImageBot/"
PATH = "/home/jack/python3-starter/notebooks/"
for file in os.listdir(PATH):
    if file.endswith(".ipynb"):
        
        filename = PATH+file
        #filein = "deep-dream-generator_"+file
        #filein = "text_stuff_"+file
        filein = "python3-starter-notebooks_"+file
        with open(filename, "rb") as input_file:
            ablob = input_file.read()
            #content = [sqlite3.Binary(ablob)]
            content  = sqlite3.Binary(ablob)
            c.execute("INSERT INTO ipynb (file, content) VALUES(?, ?)", (filein, content))
            print os.path.join(PATH, file, filein)
            conn.commit()
        

c.close()
conn.close()
    
conn = sqlite3.connect(database) 
conn.text_factory=str 
c = conn.cursor()
#for row in c.execute('SELECT rowid, file FROM ipynb'):
#    print row[0],row[1]       

c.close()
conn.close()

import sqlite3
database = "newtest.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c = conn.cursor()
for row in c.execute('SELECT rowid, file FROM ipynb WHERE rowid = '):
    print row[0],row[1]       

c.close()
conn.close()

import sqlite3
database = "newtest.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS3(file, content);
""")
conn.commit()
conn.close()
conn = sqlite3.connect(database)
c = conn.cursor()
file = "jupyter-to-database-FTS4.ipynb"
with open(filename, "rb") as input_file:
    ablob = input_file.read()
    #content = [sqlite3.Binary(ablob)]
    content  = sqlite3.Binary(ablob)
    c.execute("INSERT INTO ipynb (file, content) VALUES(?, ?)", (file, content))
    conn.commit()

with open("Output.ipynb", "wb") as output_file:
    c.execute("SELECT content FROM ipynb WHERE file = 'jupyter-to-database-FTS4.ipynb'")
    ablob = c.fetchone()
    output_file.write(ablob[0])

c.close()
conn.close()

import sqlite3
database = "newtest.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS3(file, content);
""")
conn.commit()
conn.close()
conn = sqlite3.connect(database)
c = conn.cursor()
filename = "jupyter-to-database-FTS4.ipynb"
with open(filename, "rb") as input_file:
    ablob = input_file.read()
    c.execute("INSERT INTO ipynb (file, content) VALUES(?, ?)", (filename, [sqlite3.Binary(ablob)]))
    conn.commit()

with open("Output.ipynb", "wb") as output_file:
    #c.execute("SELECT file FROM file WHERE id = 0")
    c.execute('SELECT content FROM ipynb WHERE file MATCH ?',(file,))   
    ablob = cursor.fetchone()
    output_file.write(ablob[0])

cursor.close()
conn.close()

import base64
from time import sleep
import os
import sqlite3
database = "/home/jack/Desktop/text_stuff/postit.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS3(file, content);
""")
conn.commit()
conn.close()
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/"
PATH = "/home/jack/Desktop/text_stuff/"
for file in os.listdir(PATH):
    if file.endswith(".ipynb"):
        print(os.path.join(PATH, file))
        title2 = PATH+file
        conn = sqlite3.connect(database)
        conn.text_factory=str 
        c = conn.cursor()
        #with open(title2, 'r') as f:
        keyword = file
        #content=open(title2, 'r').read().decode("utf-8", "ignore")
        content=open(title2, 'r').read()   
        c.execute("INSERT INTO ipynb VALUES (?,?)", (file,content))
        #c.execute("INSERT INTO ipynb VALUES (?,?)", (file, content)) 
        conn.commit()

        conn.close()
        f.close()
        conn = sqlite3.connect(database)
        conn.text_factory=str 
        c = conn.cursor()
        line = file
        #line ="Good-mouse-sizing-and-cropping.ipynb"
        content = "Index"
        c.execute("INSERT INTO ipynb VALUES (?,?)", (file, content)) 
        conn.commit()
        conn.close()
        f.close()
        
conn = sqlite3.connect(database) 
conn.text_factory=str 
c = conn.cursor()
for row in c.execute('SELECT rowid, file FROM ipynb'):
#for row in c.execute('SELECT line FROM ipynb WHERE file MATCH ?',(file,)):    
    sleep(.5)
    #row0 = base64.b64decode(row[0])
    #print row0, row[1]
    print row[0],row[1]   

import base64
from time import sleep
import sqlite3
database = "/home/jack/Desktop/text_stuff/postit.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
line ="ALL-WORKNG-SQLITE_and_MYSQL.ipynb"
#line ="Good-mouse-sizing-and-cropping.ipynb"
title = "index"
c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
conn.commit()
        #print encodedlistvalue

conn.close()
f.close()



import json
import sys
from time import sleep
import sqlite3
database = "/home/jack/Desktop/text_stuff/postit2.db"
#database = '/home/jack/Desktop/text_stuff/notebooks.db'
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
#for row in c.execute('SELECT title, line  FROM ipynb'):
search = "jupyter-to-database-FTS4.ipynb"
#with open('jupyter-to-database-FTS4-2.ipynb', 'a') as outfile:
for row in c.execute('SELECT line FROM ipynb WHERE title MATCH ?',(search,)):    
    
    
    sleep(.5)
    #row0 = base64.b64decode(row[0])
    #print row0, row[1]
    print row[1]

import base64
from time import sleep
import sqlite3
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory=str 
c = conn.cursor()
line ="ALL-WORKNG-SQLITE_and_MYSQL.ipynb"
#line ="Good-mouse-sizing-and-cropping.ipynb"
title = "index"
c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
conn.commit()
        #print encodedlistvalue

conn.close()
f.close()

import base64
from time import sleep
import sqlite3
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory=str 
c = conn.cursor()
line ="""
astral.py.ipynb
Astronomy-II.ipynb
Astronomy-I.ipynb 
DEEP-DREAM-ONLY.ipynb
earth-satellites.ipynb
jupyter-to-database-FTS4.ipynb
Image_Pack.ipynb    
Snippet_Warehouse-MySQL-SQLite-code-storage-search.ipynb    
WikiPedia.ipynb
Creation-of-collection.db.ipynb    
Palett_Swap.ipynb    
Parsing_and_Storing_RSS_newsfeeds_in_SQLite.ipynb
Creating-C++-Image-blender.ipynb
Creating-C++-Image-viewer.ipynb
Text_Generation-BEST.ipynb
Words-and-Sentences.ipynb
Bible_Search_in_SQL.ipynb
"""
title = "index"
c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
conn.commit()
        #print encodedlistvalue

conn.close()
f.close()

import json
import sys
from time import sleep
import sqlite3
import csv

f= open('newtest.ipynb',"w")
f.close()
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory=str 
c = conn.cursor()
#search = raw_input("Title :")
search = "Matplotlib.ipynb"
with open('newtest.ipynb', 'a') as outfile:
    for row in c.execute('SELECT line FROM ipynb WHERE title MATCH ?',\
                         (search,)):
        row0 = row[0]
        #row0 =str(row0,"\n")
        outfile.write(row0)
        
conn.close()
f.close()

        

import json
import sys
from time import sleep
import sqlite3
import csv

f= open('jupyter-to-database-FTS4-2.ipynb',"w")
f.close()
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory=str 
c = conn.cursor()
#search = raw_input("Title :")
search = 13
with open('jupyter-to-database-FTS4-2.ipynb', 'a') as outfile:
    c.execute('SELECT rowid, * FROM ipynb WHERE rowid = ?',(search,))
    print row[0],row[1]

conn.close()
f.close()

        

import json
import sys
from time import sleep
import sqlite3
import csv
title = "index.txt"
f= open(title,"w")
f.close()
database = "/home/jack/Desktop/text_stuff/postit.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
with open(title, 'a') as outfile:
    for row in c.execute('SELECT rowid, title, line, keyword FROM ipynb WHERE title MATCH ?', ("Index",)):
        row0 =row[0], row[3]
        row0 =str(row0)
        row0 = row0+"\n"
        outfile.write(row0)
conn.close()
f.close()

        

import json
import sys
from time import sleep
import sqlite3
import csv
database = "/home/jack/Desktop/text_stuff/postit.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
#with open(title, 'a') as outfile:
#content=open(filename, 'r').read().decode("utf-8", "ignore")    
for row in c.execute('SELECT rowid, title, line FROM ipynb WHERE title = ?', ("Index",)):
    row0 = row[0],row[2]
    sleep(1)
    print row0
conn.close()

import json
import sys
from time import sleep
import sqlite3
import csv
database = "/home/jack/Desktop/text_stuff/postit.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
#with open(title, 'a') as outfile:
#content=open(filename, 'r').read().decode("utf-8", "ignore")    
c.execute('SELECT rowid, title, line, keyword FROM ipynb WHERE title = ?', ("jupyter-to-database-FTS4.ipynb",))
print row[0:]
conn.close()

import json
import sys
from time import sleep
import sqlite3
import csv
database = "/home/jack/Desktop/text_stuff/postit.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
#with open(title, 'a') as outfile:
#content=open(filename, 'r').read().decode("utf-8", "ignore")    
for row in c.execute('SELECT rowid, title, line FROM ipynb'):
    row0 = row[0],row[1]
    sleep(1)
    print row0
conn.close()

import json
import sys
from time import sleep
import sqlite3
import csv
database = "/home/jack/Desktop/text_stuff/postit.db"
conn = sqlite3.connect(database)

conn.text_factory=str 
c = conn.cursor()
#search = "FTS3_Database.ipynb"
search = raw_input("SEARCH : ")
with open(title, 'a') as outfile:
    for row in c.execute('SELECT rowid, title, keyword FROM ipynb WHERE rowid = ?', (search,)):
        print row[0],row[1],row[2]
        

import json
import sys
from time import sleep
import sqlite3
import csv
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory=str 
c = conn.cursor()
#search = "FTS3_Database.ipynb"
search = raw_input("SEARCH : ")
with open(title, 'a') as outfile:
    for row in c.execute('SELECT rowid, title, line FROM ipynb WHERE line MATCH ?', (search,)):
        print row[0],row[1],"\n",row[2]
        

import json
import sys
from time import sleep
import sqlite3
import csv
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory=str 
c = conn.cursor()
#c.execute("DELETE title, line FROM ipynb where rowid MATCH '362113' "):
#c.execute("delete from ipynb where rowid=362113;"):
c.execute("DELETE FROM ipynb WHERE title = ?", ('Good-mouse-sizing-and-cropping.ipynb',))
conn.commit()
conn.close()
        

import sqlite3
import sys
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
id1 = raw_input("Starting ID : ")
id2 = raw_input("How Many Rows : ")
# id1 is start id2 is how many lines
for row in c.execute('SELECT rowid, * from ipynb LIMIT ? OFFSET ?',  (id2, id1)):
    count=count+1
    #print row[0],row[1],"\n",row[2]
    print row[2]
    if count > req:
        conn.close()
        sys.exit()

import json
import sys
from time import sleep
import sqlite3
import csv
data
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory=str 
c = conn.cursor()
search = "earth-satellites.ipynb"
with open(title, 'a') as outfile:
    for row in c.execute("SELECT rowid, file FROM ipynb where title MATCH 'jupyter-to-database-FTS4.ipynb' "):
        row0 = row[1]
        sleep(1)
        print row[0],row[1]
        



import sqlite3
import sys
conn = sqlite3.connect('/home/jack/Desktop/text_stuff/notebooks.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=20
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, title, line FROM ipynb WHERE title MATCH ?', (search,)):    
    count=count+1
    print row[0],row[1],"\n",row[2]
    if count > req:
        conn.close()
        sys.exit()

import sqlite3
import sys
database = "test.db"
conn = sqlite3.connect(database)
conn.text_factory = str
c = conn.cursor()
count=0;req=50
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, file, description FROM ipynb WHERE file MATCH ?', (search,)):    
    count=count+1
    print row[0],"\n",row[1],"\n",row[2]
    if count > req:
        conn.close()
        sys.exit()

!sqlite3 unique-End2.db "pragma integrity_check;"

!locate snippet.db

import sqlite3
import sys
database = "/home/jack/snippet.db""
conn = sqlite3.connect(database)
ID = raw_input("Delete ID Number : ")
c = conn.cursor()
c.execute('DELETE FROM snippet WHERE id=?',(ID))
conn.commit()
conn.close()

Search : remove duplicate
ID :  78 
%%writefile HashCheck.py
"" USEAGE:
(insert hash)

for example to get hash of a filename:
def md5(Path):
    hash_md5 = hashlib.md5()
    with open(Path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

Hash = md5(title)

import HashCheck
hasht = "jsudsfhndsfjunjsd"
HashCheck.hashfill(Hash)
---------------
import HashCheck
HashCheck.hashcheck()

import HashCheck
HashCheck.killmem()
""
import sqlite3
import sys
import os

def md5(Path):
    hash_md5 = hashlib.md5()
    with open(Path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

!ls ~/hubiC/Databases

import sqlite3
import sys
database = "/home/jack/hubiC/Databases/snippet.db"
conn = sqlite3.connect(database)
conn.text_factory = str
c = conn.cursor()
count=0;req=200
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, * FROM snippet WHERE snippet MATCH ?', (search,)):    
    count=count+1
    print "ID : ",(row)[0],(row)[2]," -- KEYWORDS",(row)[3],"\n"
    if count > req:
        conn.close()
        sys.exit()

import sqlite3
import sys
from time import sleep
database = "/home/jack/snippet.db"
conn = sqlite3.connect(database)
conn.text_factory = str
c = conn.cursor()
count = 0
f = open("All-Snippets.txt", "w");f.close()
f = open("All-Snippets.txt", "a")
#search = raw_input("Search : ")
search = 'sqlite'
for row in c.execute('SELECT rowid, * FROM snippet'):    
    count=count+1
    count = str(count)
    iD = row[0]
    iD = str(iD)
    line = row[2]
    desc = (row)[3]
    j = "Line No. :",count,"\nROWID :",iD,"\n",line,"\nDescription :",desc,"\n\n-------new entry -------"
    line = "".join(j)
    #line = (count+(row)[0]+(row)[2]+" -- KEYWORDS"*(row)[3]+"\n")
    sleep(1)
    f.write(line)
    print line
    count = int(count)
    
conn.close()
Print "Search is Completed"
sys.exit()

test_input = []
keep = set(mapping[data] for data in test_input)

inpuT = raw_input("Test : ").split("/")
keep.update(inpuT)

print keep

import sqlite3
import sys
from time import sleep

def md5(line):
    hash_md5 = hashlib.md5()
    for chunk in iter(lambda: line(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()




database = "/home/jack/snippet.db"
conn = sqlite3.connect(database)
conn.text_factory = str
c = conn.cursor()
count = 0
f = open("this-Snippets.txt", "w");f.close()
f = open("this-Snippets.txt", "a")
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, * FROM snippet WHERE snippet MATCH ?', (search,)):  
    count=count+1
    count = str(count)
    iD = row[0]
    iD = str(iD)
    line = row[2]
    desc = (row)[3]
    j = "Line No. :",count,"\nROWID :",iD,"\n",line,"\nDescription :",desc,"\n\n-------new entry -------"
    line = "".join(j)
    sleep(1)
    f.write(line)
    print line
    count = int(count)
    
conn.close()
print "Results Complete"
sys.exit()

!ls ~/Desktop/text_stuff

import os
import timeit
import sqlite3
def txsearch():
    
    #database = "FTS4_IPYNB.db"
    database = "PythonCode.db"
    conn = sqlite3.connect(database)
    c = conn.cursor()
    conn.text_factory=str 
    c.execute("""
    CREATE VIRTUAL TABLE IF NOT EXISTS PYTHON 
    USING FTS4(PATH, fname, line);
    """)
    conn.commit()
    
    # Ask to enter string to search
    Sstring = raw_input("Search Phrase")
    for fname in os.listdir('/home/jack/Desktop/text_stuff'):
       # Apply file type filter 
       path = "/home/jack/Desktop/text_stuff/" 
       if fname.endswith(".txt"):
            # Open file for reading
            PATH = path+fname
            fo = open(PATH)
            # Read the first line from the file
            line = fo.readline()
            # Initialize counter for line number
            line_no = 1
            # Loop until EOF
            while line != '' :
                    index = line.find(Sstring)
                    c.execute("INSERT INTO PYTHON VALUES (?,?,?)", (PATH, fname, line))
                    conn.commit()
                    if ( index != -1) :
                        # Set some parameters no lines longer than 240 characters 
                        # or less than search phrase +30 characters 
                        if len(line)< 240 and len(line)> len(Sstring)+20 :
                            #print(fname, "[", line_no, ",", index, "] ", line)
                            #print fname,line[1:-8],"  "
                            print fname,line_no,line
                    # Read next line
                    line = fo.readline()  
                    # Increment line counter
                    line_no += 1
            # Close the files
            fo.close()
    conn.close()                
            
txsearch()            

import sqlite3
from time import sleep
database = "PythonCode.db"
conn = sqlite3.connect(database)
c = conn.cursor()
search = raw_input("SEARCH : ")
#for row in c.execute("SELECT rowid,* from PYTHON WHERE line MATCH ?", (search,)):
for row in c.execute("SELECT rowid,* from PYTHON "):   
    sleep(1)                 
    print row[0],row[1],row[2]

conn.close()

import os
import timeit
def txsearch():
    # Ask to enter string to search
    Sstring = raw_input("Search Phrase  : ")
    for fname in os.listdir('/home/jack/Desktop/text_stuff'):
       # Apply file type filter 
       path = "/home/jack/Desktop/text_stuff/" 
       if fname.endswith(".txt"):
            # Open file for reading
            fo = open(path+fname)
            # Read the first line from the file
            line = fo.readline()
            # Initialize counter for line number
            line_no = 1
            # Loop until EOF
            while line != '' :
                    index = line.find(Sstring)
                    if ( index != -1) :
                        # Set some parameters no lines longer than 240 characters 
                        # or less than search phrase +30 characters 
                        if len(line)< 240 and len(line)> len(Sstring)+20 :
                            #print(fname, "[", line_no, ",", index, "] ", line)
                            #print fname,line[1:-8],"  "
                            print fname,line_no,line
                    # Read next line
                    line = fo.readline()  
                    # Increment line counter
                    line_no += 1
            # Close the files
            fo.close()
            
txsearch()            

!mkdir Databases



import sqlite3
import sys
database = "/home/TEMP/Databases/SNIPPETS.db"
conn = sqlite3.connect(database)
conn.text_factory = str
c = conn.cursor()
count=0;req=200
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, * FROM snippets WHERE snippets MATCH ?', (search,)):    
    count=count+1
    print "ID : ",(row)[0],(row)[1]," -- KEYWORDS",(row)[2],"\n"
    if count > req:
        conn.close()
        sys.exit()

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"

conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
search = raw_input("ID :")
filename = search+"DB.ipynb"
f= open(filename,"w");f.close()
SEARCH = search+1
for row in c.execute('SELECT content FROM ipynb WHERE rowid == ?', (SEARCH,)):
        print row[1]
with open(filename, 'a') as outfile:
    for row in c.execute('SELECT content FROM ipynb WHERE rowid == ?', (search,)):
        row0 = row[1]
        #row0 =str(row0,"\n")
        #outfile.write(row0)
        print row0
        
conn.close()
f.close()

  # 427      

import json
import sys
from time import sleep
import sqlite3
import csv
database = "oct1_IPNB.list.db"

conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
search = raw_input("ID :")
#filename = search+"DB.ipynb"
#f= open(filename,"w");f.close()
SEARCH = int(search)+1
for row in c.execute('SELECT content FROM ipynb WHERE rowid == ?', (SEARCH,)):
        filename = row[0]
with open(filename, 'w') as outfile:
    for row in c.execute('SELECT content FROM ipynb WHERE rowid == ?', (search,)):
        row0 = row[0]
        #row0 =str(row0,"\n")
        outfile.write(row0)
        print row0
print filename        
conn.close()
f.close()


  # 427      



import ipywidgets
label = ipywidgets.Label('Hello World')
label

# it should also handle custom msg'es
label.send({'msg': 'Hello'})

import ipywidgets
label = ipywidgets.Label('Hello World')
display(label)

# it should also handle custom msg'es
label.send({'msg': 'Hello'})

import ipywidgets
label = ipywidgets.Label('Hello World')
label

# it should also handle custom msg'es
label.send({'msg': 'Hello'})

!pip install tf-nightly --upgrade

import numpy as np
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.LSTM(20),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

# Load MNIST dataset.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)

# Change this to True if you want to test the flow rapidly.
# Train with a small dataset and only 1 epoch. The model will work poorly
# but this provides a fast way to test if the conversion works end to end.
_FAST_TRAINING = False
_EPOCHS = 5
if _FAST_TRAINING:
  _EPOCHS = 1
  _TRAINING_DATA_COUNT = 1000
  x_train = x_train[:_TRAINING_DATA_COUNT]
  y_train = y_train[:_TRAINING_DATA_COUNT]

model.fit(x_train, y_train, epochs=_EPOCHS)
model.evaluate(x_test, y_test, verbose=0)


converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Run the model with TensorFlow to get expected results.
expected = model.predict(x_test[0:1])

# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]["index"], x_test[0:1, :, :])
interpreter.invoke()
result = interpreter.get_tensor(output_details[0]["index"])

# Assert if the result of TFLite model is consistent with the TF model.
np.testing.assert_almost_equal(expected, result)
print("Done. The result of TensorFlow matches the result of TensorFlow Lite.")



!pip install tf-nightly

import numpy as np
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.LSTM(20, time_major=False, return_sequences=True),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

# Load MNIST dataset.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)

# Change this to True if you want to test the flow rapidly.
# Train with a small dataset and only 1 epoch. The model will work poorly
# but this provides a fast way to test if the conversion works end to end.
_FAST_TRAINING = False
_EPOCHS = 5
if _FAST_TRAINING:
  _EPOCHS = 1
  _TRAINING_DATA_COUNT = 1000
  x_train = x_train[:_TRAINING_DATA_COUNT]
  y_train = y_train[:_TRAINING_DATA_COUNT]

model.fit(x_train, y_train, epochs=_EPOCHS)
model.evaluate(x_test, y_test, verbose=0)

run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 28
INPUT_SIZE = 28
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = "keras_lstm"
model.save(MODEL_DIR, save_format="tf", signatures=concrete_func)

converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()

# Run the model with TensorFlow to get expected results.
TEST_CASES = 10

# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

for i in range(TEST_CASES):
  expected = model.predict(x_test[i:i+1])
  interpreter.set_tensor(input_details[0]["index"], x_test[i:i+1, :, :])
  interpreter.invoke()
  result = interpreter.get_tensor(output_details[0]["index"])

  # Assert if the result of TFLite model is consistent with the TF model.
  np.testing.assert_almost_equal(expected, result, decimal=5)
  print("Done. The result of TensorFlow matches the result of TensorFlow Lite.")

  # Please note: TfLite fused Lstm kernel is stateful, so we need to reset
  # the states.
  # Clean up internal states.
  interpreter.reset_all_variables()

!ls /mnt/HDD500/

import os
import shutil

# Function to get two sample images from directories with over 10 images
def get_sample_images(folder_path, samples_folder):
    # Create samples directory if it doesn't exist
    if not os.path.exists(samples_folder):
        os.makedirs(samples_folder)

    # Traverse directory
    for root, dirs, files in os.walk(folder_path):
        image_files = [file for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]
        if len(image_files) > 10:
            # Copy two sample images
            sample_images = image_files[:2]
            for image_file in sample_images:
                src_path = os.path.join(root, image_file)
                # Construct new filename
                new_filename = os.path.join(samples_folder, '_' + root.replace('/', '_') + '_' + image_file)
                # Copy file with new filename
                shutil.copy(src_path, new_filename)
                print(f"Copied {src_path} to {new_filename}")


# Specify the folder path to search for images
#folder_path = '/home/jack'
#folder_path = '/home/jack/Desktop/HDD500/'
folder_path = '/mnt/HDD500/'

# Specify the folder path to save sample images
samples_folder = 'SAMPLES'

# Call the function to get sample images from directories with over 10 images
get_sample_images(folder_path, samples_folder)
folder_path = '/home/jack'
get_sample_images(folder_path, samples_folder)

import os
import shutil

# Function to get two sample images from directories with over 10 images
def get_sample_images(folder_path, samples_folder):
    # Create samples directory if it doesn't exist
    if not os.path.exists(samples_folder):
        os.makedirs(samples_folder)

    # Traverse directory
    for root, dirs, files in os.walk(folder_path):
        image_files = [file for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]
        if len(image_files) > 10:
            # Copy two sample images
            sample_images = image_files[:2]
            for image_file in sample_images:
                src_path = os.path.join(root, image_file)
                dst_path = os.path.join(samples_folder, image_file.replace('/', '_'))
                shutil.copy(src_path, dst_path)
                print(f"Copied {src_path} to {dst_path}")

# Specify the folder path to search for images
#folder_path = '/home/jack'
#folder_path = '/home/jack/Desktop/HDD500/'
folder_path = '/mnt/HDD500/'

# Specify the folder path to save sample images
samples_folder = 'SAMPLES'

# Call the function to get sample images from directories with over 10 images
get_sample_images(folder_path, samples_folder)


import os
import datetime

# Function to find images created in the last two days
def find_images_last_two_days(folder_path):
    # Get current date
    current_date = datetime.datetime.now()

    # Calculate two days ago
    two_days_ago = current_date - datetime.timedelta(days=2)

    # List to store image files
    image_files = []

    # Traverse directory
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            # Check if the file is an image file and created within the last two days
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')) and \
                    os.path.getctime(file_path) > two_days_ago.timestamp():
                image_files.append(file_path)
                print(len(image_files),end='.')

    return image_files

# Specify the folder path where you want to search for images
#folder_path = '/home/jack'
#folder_path = '/home/jack/Desktop/HDD500/'
folder_path = '/mnt/HDD500/'

# Call the function to find images created in the last two days
images_last_two_days = find_images_last_two_days(folder_path)

# Specify the output file path
output_file_path = 'mnt_image_files_last_two_days.txt'

# Write the image file paths to the output file
with open(output_file_path, 'w') as file:
    for image_file in images_last_two_days:
        file.write(image_file + '\n')

print(f"Results saved to {output_file_path}")


import os
import datetime

# Function to find images created in the last two days
def find_images_last_two_days(folder_path):
    # Get current date
    current_date = datetime.datetime.now()

    # Calculate two days ago
    two_days_ago = current_date - datetime.timedelta(days=2)

    # List to store image files
    image_files = []

    # Traverse directory
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            # Check if the file is an image file and created within the last two days
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')) and \
                    os.path.getctime(file_path) > two_days_ago.timestamp():
                image_files.append(file_path)

    return image_files

# Specify the folder path where you want to search for images
folder_path = '/home/jack'

# Call the function to find images created in the last two days
images_last_two_days = find_images_last_two_days(folder_path)

# Print the list of image files
print("Images created in the last two days:")
for image_file in images_last_two_days:
    print(image_file)



/home/jack/Desktop/SCRIPTS/krea_orig/vokoscreenNG-2024-02-12_09-46-55.mkv



vid = sorted(glob.glob(Videos), key=lambda x: os.path.getmtime(x), reverse=True)


import glob
import os
#Videos="/home/jack/Desktop/HDD500/Videos/*.mkv"
#Videos="/home/jack/Desktop/SCRIPTS/krea_orig/*.mkv"
Videos="/home/jack/Desktop/SCRIPTS/krea_orig/*.mkv"
#Videos = "/path/to/videos/*.mp4"  # Replace with your actual path and file extension

# Get a list of video file paths
vid = sorted(glob.glob(Videos), key=lambda x: os.path.getctime(x),reverse=True)
VIDEOS=[]
# Print the sorted list
for video in vid:
    VIDEOS.append(video)
    #print(video)
print(VIDEOS[0])

import string
def remove_punctuation(DIR):
    return DIR.translate(str.maketrans('', '', string.punctuation))

Input_Video=VIDEOS[0]
import subprocess

def play_video_with_vlc(video_path):
    try:
        # Use subprocess to launch VLC player with the video file
        subprocess.run(['vlc', video_path])
    except Exception as e:
        print(f"Error: {e}")

# Replace '/path/to/your/video.mp4' with the actual path to your video file
video_path = VIDEOS[0]
#video_path = "/home/jack/Desktop/SCRIPTS/krea_orig/vokoscreenNG-2024-01-13_13-03-53.mkv"
play_video_with_vlc(video_path)

DIR=Input_Video.split("/")[-1]
new_dir=remove_punctuation(DIR)
print(new_dir)

print(VIDEOS[0][:-5])



import subprocess
import logging

# Configure logging
logging.basicConfig(filename='ffmpeg_remove_duplicates.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def remove_duplicate_frames(input_video, output_video):
    try:
        # Run ffmpeg command to remove duplicate frames
        ffmpeg_command = [
            'ffmpeg','-hide_banner',
            '-i', input_video,
            '-vf', 'mpdecimate,setpts=N/FRAME_RATE/TB',
            '-c:a', 'copy',
            '-y',
            output_video
        ]

        # Log the ffmpeg command
        logging.info(f'Running ffmpeg command: {" ".join(ffmpeg_command)}')

        # Execute the ffmpeg command
        subprocess.run(ffmpeg_command, check=True)

        logging.info('Duplicate frames removed successfully.')

    except subprocess.CalledProcessError as e:
    
        logging.error(f'Error occurred: {e}')
        raise
if __name__ == "__main__":
    #input_video_path = '/home/jack/Videos/vokoscreenNG-2023-12-20_13-20-32.mkv'
    input_video_path = VIDEOS[0]
    #output_video_path = VIDEOS[0][:-5]+"No_dups.mp4"
    input_video_path=video_path
    output_video_path=video_path[:-5]+"No_dups.mp4"
    print(output_video_path)

    # Call the function to remove duplicate frames
    remove_duplicate_frames(input_video_path, output_video_path)


!vlc {output_video_path}



#!ls /home/jack/Desktop/SCRIPTS/krea_orig/

!ls {output_video_path}

out_vid=VIDEOS[0][:-5]+"shorts.mp4"
print(out_vid)

out_vid=output_video_path[:-5]+"shorts.mp4"
print(out_vid)


!ffmpeg -i {output_video_path} -y -t 15 {out_vid}

!vlc {out_vid}


import os
import cv2
from datetime import datetime

def log_message(log_file_path, level, message):
    """
    Write a log message to the specified log file.
    
    Args:
    - log_file_path (str): Path to the log file.
    - level (str): Log level (e.g., INFO, ERROR).
    - message (str): Log message to be written.
    """
    # Get the directory of the log file
    log_directory = os.path.dirname(log_file_path)
    
    # Create the log directory if it doesn't exist
    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    # Get the current timestamp
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # Format the log message
    formatted_message = f"{timestamp} - {level} - {message}\n"

    # Write the message to the log file
    with open(log_file_path, 'a') as log_file:
        log_file.write(formatted_message)

def extract_frames(video_file, output_directory, log_file):
    """
    Extract frames from a video file and save them as JPG images.
    
    Args:
    - video_file (str): Path to the input video file.
    - output_directory (str): Directory to save the extracted frames.
    - log_file (str): Path to the log file.
    """
    try:
        # Open the video file
        cap = cv2.VideoCapture(video_file)

        # Create the output directory if it doesn't exist
        if not os.path.exists(output_directory):
            os.makedirs(output_directory)

        # Extract and save frames
        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame_path = os.path.join(output_directory, f"frame_{frame_count}.jpg")
            cv2.imwrite(frame_path, frame)
            frame_count += 1
        cap.release()

        log_message(log_file, 'INFO', f"Successfully extracted {frame_count} frames from {video_file}")
    except Exception as e:
        log_message(log_file, 'ERROR', f"Error extracting frames from {video_file}: {str(e)}")

def main():
    # Specify the paths
    video_file = "vokoscreenNG-2024-02-12_09-46-5No_dupshorts.mp4"  # Change this to your input video file
    output_directory = "KREA_output_images/"  # Change this to the desired output folder
    log_file = 'extract_frames.log'
    
    # Call the extract_frames function
    extract_frames(video_file, output_directory, log_file)

if __name__ == "__main__":
    main()


import os
import cv2
from datetime import datetime

def log_message(log_file_path, level, message):
    log_directory = os.path.dirname(log_file_path)
    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    formatted_message = f"{timestamp} - {level} - {message}\n"

    with open(log_file_path, 'a') as log_file:
        log_file.write(formatted_message)

def extract_frames(video_file, output_directory, log_file):
    try:
        cap = cv2.VideoCapture(video_file)

        if not os.path.exists(output_directory):
            os.makedirs(output_directory)

        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame_path = os.path.join(output_directory, f"frame_{frame_count}.jpg")
            cv2.imwrite(frame_path, frame)
            frame_count += 1
        cap.release()

        log_message(log_file, 'INFO', f"Successfully extracted {frame_count} frames from {video_file}")
    except Exception as e:
        log_message(log_file, 'ERROR', f"Error extracting frames from {video_file}: {str(e)}")

def main():
    video_file = "vokoscreenNG-2024-02-12_09-46-5No_dupshorts.mp4"  
    # Change this to your input video file
    output_directory = "KREA_output_images/"  # Change this to the desired output folder

    log_file = 'extract_frames.log'
    extract_frames(video_file, output_directory, log_file)

if __name__ == "__main__":
    main()


import logging

# Configure logging
logging.basicConfig(filename='example.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Example usage of logging
def main():
    logging.debug('This is a debug message')
    logging.info('This is an info message')
    logging.warning('This is a warning message')
    logging.error('This is an error message')
    logging.critical('This is a critical message')

if __name__ == "__main__":
    main()


import os
import moviepy.editor as mp
import logging

# Get the current directory
current_dir = os.getcwd()
log_file = os.path.join(current_dir, "conversion.log")

# Configure logging to save messages to both console and a log file
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(message)s',
                    handlers=[logging.FileHandler(log_file),
                              logging.StreamHandler()])

def convert_video_to_images(video_path, output_folder):
    """
    Convert an MP4 video to JPG images.

    Parameters:
        video_path (str): Path to the input MP4 video file.
        output_folder (str): Path to the output folder where JPG images will be saved.
    """
    try:
        # Load the video clip
        clip = mp.VideoFileClip(video_path)
        
        # Print debug information
        logging.info(f"Video duration: {clip.duration} seconds")
        logging.info(f"Total frames: {clip.reader.nframes}")

        # Create the output folder if it doesn't exist
        os.makedirs(output_folder, exist_ok=True)

        # Iterate over each frame and save it as a JPG image
        for i, frame in enumerate(clip.iter_frames()):
            output_path = os.path.join(output_folder, f"frame_{i}.jpg")
            logging.info(f"Saving frame {i} as {output_path}")
            mp.imsave(output_path, frame)

        logging.info("Conversion completed successfully.")

    except Exception as e:
        logging.error(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    video_path = "vokoscreenNG-2024-02-12_09-46-5No_dupshorts.mp4"  
    # Change this to your input video file
    output_folder = "KREA_output_images/"  # Change this to the desired output folder

    convert_video_to_images(video_path, output_folder)


!pwd

import os
from datetime import datetime

def log_message(log_file_path, level, message):
    """
    Write a log message to the specified log file.
    
    Args:
    - log_file_path (str): Path to the log file.
    - level (str): Log level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL).
    - message (str): Log message to be written.
    """
    # Create the directory if it doesn't exist
    log_directory = os.path.dirname(log_file_path)
    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    # Get the current timestamp
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # Format the log message
    formatted_message = f"{timestamp} - {level} - {message}\n"

    # Write the message to the log file
    with open(log_file_path, 'a') as log_file:
        log_file.write(formatted_message)

# Example usage of the logging function
def main():
    log_file_path = '/home/jack/Desktop/EXP_notebooks/example.log'
    log_message(log_file_path, 'DEBUG', 'This is a debug message')
    log_message(log_file_path, 'INFO', 'This is an info message')
    log_message(log_file_path, 'WARNING', 'This is a warning message')
    log_message(log_file_path, 'ERROR', 'This is an error message')
    log_message(log_file_path, 'CRITICAL', 'This is a critical message')

if __name__ == "__main__":
    main()


!cat example.log

!cat conversion.log

import os
import moviepy.editor as mp
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def convert_video_to_images(video_path, output_folder):
    """
    Convert an MP4 video to JPG images.

    Parameters:
        video_path (str): Path to the input MP4 video file.
        output_folder (str): Path to the output folder where JPG images will be saved.
    """
    try:
        # Load the video clip
        clip = mp.VideoFileClip(video_path)
        
        # Create the output folder if it doesn't exist
        os.makedirs(output_folder, exist_ok=True)

        # Iterate over each frame and save it as a JPG image
        for i, frame in enumerate(clip.iter_frames()):
            output_path = os.path.join(output_folder, f"frame_{i}.jpg")
            print(output_path)
            mp.imsave(output_path, frame)
            logging.info(f"Saved frame {i} as {output_path}")

        logging.info("Conversion completed successfully.")

    except Exception as e:
        logging.error(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    video_path = "/home/jack/Desktop/SCRIPTS/krea_orig/vokoscreenNG-2024-02-12_09-46-5No_dupshorts.mp4"  # Change this to your input video file
    output_folder = "KREA_output_images/"  # Change this to the desired output folder

    convert_video_to_images(video_path, output_folder)


!ls KREA_output_images/

import subprocess
import logging

# Configure logging
logging.basicConfig(filename='ffmpeg_slow_down.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def slow_down_video(input_video, output_video, slowdown_factor):
    try:
        # Run ffmpeg command to slow down the video by a specified factor without audio
        ffmpeg_command = [
            'ffmpeg','-ss','1',
            '-i', input_video,
            '-vf', f'setpts={slowdown_factor}*PTS',
            '-an', '-y', # Remove audio
            output_video
        ]

        # Log the ffmpeg command
        logging.info(f'Running ffmpeg command: {" ".join(ffmpeg_command)}')

        # Execute the ffmpeg command
        subprocess.run(ffmpeg_command, check=True)

        logging.info(f'Video slowed down by {slowdown_factor} times without audio successfully.')

    except subprocess.CalledProcessError as e:
        logging.error(f'Error occurred: {e}')
        raise

if __name__ == "__main__":
    #input_video_path = '/home/jack/Desktop/SCRIPTS/all-in-one_no_dupesS.mp4'
    input_video_path = out_vid
    #output_video_path = '/home/jack/Desktop/SCRIPTS/all-in-one_no_dupesS_10_kdenlive_ready.mp4'
    output_video_path = out_vid[:-5]+'_slowed_5.mp4'
    slowdown_factor = 8.0  # Adjust this factor as needed

    # Call the function to slow down the video
    slow_down_video(input_video_path, output_video_path, slowdown_factor)
!vlc {output_video_path}

print(output_video_path)

!vlc {output_video_path}

!add_frame {output_video_path}

!vlc temp/final_output3.mp4



import os
from moviepy.video.io.VideoFileClip import VideoFileClip
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def split_video(input_path, output_dir, num_parts=6):
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Open the video file
    video_clip = VideoFileClip(input_path)

    # Get the duration of the video
    total_duration = video_clip.duration

    # Calculate the duration for each part
    part_duration = total_duration / num_parts

    logger.info(f"Splitting {input_path} into {num_parts} equal parts.")

    # Loop through each part and write it to a new file
    for i in range(num_parts):
        start_time = i * part_duration
        end_time = (i + 1) * part_duration

        # Create a subclip
        subclip = video_clip.subclip(start_time, end_time)

        # Output file path
        output_path = os.path.join(output_dir, f"{i + 1}sound.mp4")

        # Write subclip to file
        subclip.write_videofile(output_path, codec="libx264", audio_codec="aac")

        logger.info(f"Part {i + 1} saved to {output_path}")

    # Close the video clip
    video_clip.close()

if __name__ == "__main__":
    # Input video file path
    input_video_path = "/home/jack/Videos/edited_original_sound.mp4"

    # Output directory for split videos
    output_directory = "/home/jack/Videos"

    # Number of parts to split the video into
    num_parts = 6

    # Call the split_video function
    split_video(input_video_path, output_directory, num_parts)


!mkdir mixamo

import cv2

vidcap = cv2.VideoCapture('mixamo.mp4')
success, image = vidcap.read()
count = 1

while success:
    # Save frame as image with zero-padded filename
    cv2.imwrite("mixamo/{:05d}.jpg".format(count), image)
    success, image = vidcap.read()
    count += 1



ffmpeg -i output2a.mp4 -vf setpts=4*PTS -an -y output_video.mp4

print(len(VIDEOS))

vid=VIDEOS[0]

!ls /mnt/HDD500/EXPER/static/assets/Title_Image02.png



!locate /MUSIC/

!ls /mnt/HDD500/collections/Music/*.mp3

%%writefile /usr/local/bin/add_frame
#!/home/jack/miniconda3/envs/cloned_base/bin/python
from moviepy.video.compositing.transitions import slide_in
from moviepy.video.fx import all
from moviepy.editor import *
import glob
import random
from PIL import Image
import cv2
import os
import uuid
import shutil
from sys import argv

def add_title_image(video_path, hex_color = "#A52A2A"):
    hex_color=random.choice(["#A52A2A","#ad1f1f","#16765c","#7a4111","#9b1050","#8e215d","#2656ca"])
    # Define the directory path
    directory_path = "temp"
    # Check if the directory exists
    if not os.path.exists(directory_path):
        # If not, create it
        os.makedirs(directory_path)
        print(f"Directory '{directory_path}' created.")
    else:
        print(f"Directory '{directory_path}' already exists.") 
    # Load the video file and title image
    video_clip = VideoFileClip(video_path)
    print(video_clip.size)
    width, height = video_clip.size
    title_image_path = "/mnt/HDD500/EXPER/static/assets/Title_Image02.png"
    # Set the desired size of the padded video (e.g., video width + padding, video height + padding)
    padded_size = (width + 90, height + 90)

    # Calculate the position for centering the video within the larger frame
    x_position = (padded_size[0] - video_clip.size[0]) / 2
    y_position = (padded_size[1] - video_clip.size[1]) / 2
    #hex_color = "#09723c"
    # Remove the '#' and split the hex code into R, G, and B components
    r = int(hex_color[1:3], 16)
    g = int(hex_color[3:5], 16)
    b = int(hex_color[5:7], 16)

    # Create an RGB tuple
    rgb_tuple = (r, g, b)

    # Create a blue ColorClip as the background
    blue_background = ColorClip(padded_size, color=rgb_tuple)

    # Add the video clip on top of the red background
    padded_video_clip = CompositeVideoClip([blue_background, video_clip.set_position((x_position, y_position))])
    padded_video_clip = padded_video_clip.set_duration(video_clip.duration)
    #title_image_path = "/home/jack/Desktop/EXPER/static/assets/Title_Image02.png"
    # Load the title image
    title_image = ImageClip(title_image_path)

    # Set the duration of the title image
    title_duration = video_clip.duration
    title_image = title_image.set_duration(title_duration)

    print(video_clip.size)
    # Position the title image at the center and resize it to fit the video dimensions
    #title_image = title_image.set_position(("left", "top"))
    title_image = title_image.set_position((0, -5))
    #video_clip.size = (620,620)
    title_image = title_image.resize(padded_video_clip.size)

    # Position the title image at the center and resize it to fit the video dimensions
    #title_image = title_image.set_position(("center", "center")).resize(video_clip.size)

    # Create a composite video clip with the title image overlay
    composite_clip = CompositeVideoClip([padded_video_clip, title_image])
    # Limit the length to 58 seconds
    composite_clip = composite_clip.subclip(0, 32)
    # Load a random background music
    mp3_files = glob.glob("/mnt/HDD500/FlaskAppArchitect_Flask_App_Creator/static/MUSIC/*.mp3")
    random.shuffle(mp3_files)

    # Now choose a random MP3 file from the shuffled list
    mp_music = random.choice(mp3_files)

    # Load the background music without setting duration
    music_clip = AudioFileClip(mp_music)
    music_clip = music_clip.subclip(0, 32)
    fade_duration = 1.0
    music_clip = music_clip.audio_fadein(fade_duration).audio_fadeout(fade_duration)
    # Set the audio of the composite clip to the background music
    composite_clip = composite_clip.set_audio(music_clip)
    output_path = 'temp/final_output3.mp4'
    # Export the final video with the background music
    composite_clip.write_videofile(output_path)
    uid = str(uuid.uuid4())  # Generate a unique ID using uuid
    mp4_file =  f"/home/jack/Desktop/HDD500/collections/vids/Ready_Post_{uid}.mp4"
    shutil.copyfile(output_path, mp4_file)     
    print(mp4_file)
    VIDEO = output_path
    return VIDEO
if __name__=="__main__":
    #video_path = "/mnt/HDD500/EXPER/static/assets/Title_Image02.png"
    video_path = argv[1]
    add_title_image(video_path, hex_color = "#A52A2A")    


/home/jack/Desktop/SCRIPTS/krea_orig/vokoscreenNG-2024-01-14_22-30-05.mkv

print(VIDEOS[0])



import cv2
import ipywidgets as widgets
from IPython.display import display, clear_output

def play_video(video_path):
    # Open the video file
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("Error: Could not open the video file.")
        return

    # Create an output widget for displaying video frames
    out = widgets.Output()
    display(out)

    while True:
        # Read a frame from the video
        ret, frame = cap.read()

        # Check if the video has ended
        if not ret:
            break

        # Display the frame in the output widget
        with out:
            clear_output(wait=True)
            _, img = cv2.imencode('.jpeg', cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            display(widgets.Image(value=img.tobytes(), format='jpeg'))

        try:
            # Attempt to capture keyboard input
            key = cv2.waitKey(25) & 0xFF

            # Exit the loop when the user presses 'q'
            if key == ord('q'):
                break
        except KeyboardInterrupt:
            # Handle KeyboardInterrupt (e.g., stop button in Jupyter)
            break

    # Release the video capture object
    cap.release()

# Replace '/path/to/your/video.mp4' with the actual path to your video file
video_path = VIDEOS[0]
play_video(video_path)


import cv2
import ipywidgets as widgets
from IPython.display import display, clear_output

def play_video(video_path):
    # Open the video file
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("Error: Could not open the video file.")
        return

    # Create an output widget for displaying video frames
    out = widgets.Output()
    display(out)

    while True:
        # Read a frame from the video
        ret, frame = cap.read()

        # Check if the video has ended
        if not ret:
            break

        # Display the frame in the output widget
        with out:
            clear_output(wait=True)
            cv2.imshow('Video Player', frame)
            display(frame)

        try:
            # Attempt to capture keyboard input
            key = cv2.waitKey(25) & 0xFF

            # Exit the loop when the user presses 'q'
            if key == ord('q'):
                break
        except KeyboardInterrupt:
            # Handle KeyboardInterrupt (e.g., stop button in Jupyter)
            break

    # Release the video capture object
    cap.release()

# Replace '/path/to/your/video.mp4' with the actual path to your video file
video_path = VIDEOS[0]
play_video(video_path)


import cv2

def play_video(video_path):
    # Open the video file
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("Error: Could not open the video file.")
        return

    while True:
        # Read a frame from the video
        ret, frame = cap.read()

        # Check if the video has ended
        if not ret:
            break

        # Display the frame
        cv2.imshow('Video Player', frame)

       
    # Release the video capture object and close the window
    cap.release()
    cv2.destroyAllWindows()

# Replace '/path/to/your/video.mp4' with the actual path to your video file
video_path = VIDEOS[1]
play_video(video_path)


!ls /home/jack/Desktop/SCRIPTS/krea_orig/vokoscreenNG-2024-01-13_13-03-53.mkv


/home/jack/Videos/vokoscreenNG-2023-12-19_20-01-14.mkv
/home/jack/Videos/edited_original.mp4
/home/jack/Videos/vokoscreenNG-2023-12-20_13-20-32.mkv

#%%writefile claim_search
#!/usr/bin/python2
import os
from time import sleep
import urllib
import simplejson as json
import requests
import sys
import subprocess
from Completed import track_download
import sqlite3
import watchVID
database = '5279.db'
conn = sqlite3.connect(database)
# text_factory deals with the non-asci characters
conn.text_factory = str
sql = '''create table if not exists LBRY(
description TEXT, url TEXT, unique (url));'''
conn.execute(sql) # shortcut for conn.cursor().execute(sql)
conn.commit()
c=conn.cursor()
search=raw_input("SEARCH : ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": search,  "page_size":75}}).json()
LINES = (json.dumps(data, indent=2 * ' '))
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    DES = []
    URL = []
    line= line.replace('\\n',' ')
    if "permanent_url" in line and "@" not in line:
        cnt= cnt+1
        Url = line.lstrip()
        URL.append(Url) 
        #print Url
        #print Url[25:-4]
        TEXT=Url[25:-4]+":::"
        #print TEXT
    if "description" in line:
        Des=line.lstrip()
        if len(Des)<5:Des="----------------No Description Given--"
        DES.append(Des)
        #print Des
        TEXT = str(TEXT)+(Des[16:-2])
        #print TEXT        
        #print Des[16:-2]
        TEXT = TEXT.split(":::")
        try:
            print TEXT[0]+"------"+TEXT[1]
            c.execute("INSERT INTO LBRY VALUES (?,?)", (TEXT[0],TEXT[1]))
        except:
            pass
conn.commit()
conn.close()

%%writefile watchVID.py
import subprocess
import glob
import os
def WATCH():
    # detox: remove all spaces from filenames Linux: apt install detox
    #PYTHON3 use: subprocess.run(["detox", "/home/jack/Downloads"])
    THIS_dir = os.getcwd()
    subprocess.call(["detox", THIS_dir])
    list_of_files = glob.glob(THIS_dir+"/*.mp4") # * means all if need specific format then *.csv
    src = max(list_of_files, key=os.path.getctime)
    bashCommand ="vlc --preferred-resolution 240 "+src 
    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
    output, error = process.communicate() 
    return 



#%%writefile search_data 
#!/usr/bin/python2
import sqlite3
database = '/home/jack/Desktop/LBRY-toolbox/5279.db'
conn = sqlite3.connect(database)
conn.text_factory = str
cnt = 0
Search = raw_input("SEARCH: ")
c = conn.cursor()
for row in c.execute("select * from LBRY"):
    cnt=cnt+1
    if Search in row[0] or Search in row[1]:
        print "Download Link: ",row[0]
        print row[1]
        print "-------------------------------"
print cnt    

requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"claim_ids": [], "channel": "@channel", "channel_ids": [], "not_channel_ids": [], "has_channel_signature": false, "valid_channel_signature": false, "invalid_channel_signature": false, "is_controlling": false, "stream_types": [], "media_types": [], "any_tags": [], "all_tags": [], "not_tags": [], "any_languages": [], "all_languages": [], "not_languages": [], "any_locations": [], "all_locations": [], "not_locations": [], "order_by": []}}).json()

    name    optionalstr
    claim name (normalized)
    text    optionalstr    full text search
    claim_id    optionalstr    full or partial claim id
    claim_ids    optionallist
    list of full claim ids
    txid    optionalstr
    transaction id    nout    optionalstr
    position in the transaction
    channel    optionalstr
    claims signed by this channel (argument is a URL which automatically gets resolved), see --channel_ids if you need to filter by multiple channels at the same time, includes claims with invalid signatures, use in conjunction with --valid_channel_signature
    channel_ids   optionallist  claims signed by any of these channels (arguments must be claim ids of the channels), includes claims with invalid signatures, implies --has_channel_signature, use in conjunction with --valid_channel_signature
    not_channel_ids   optionallist  exclude claims signed by any of these channels (arguments must be claim ids of the channels)
    has_channel_signature  optionalbool
    claims with a channel signature (valid or invalid)
    valid_channel_signature
    optionalbool
    claims with a valid channel signature or no signature, use in conjunction with --has_channel_signature to only get claims with valid signatures
    invalid_channel_signature
    optionalbool
    claims with invalid channel signature or no signature, use in conjunction with --has_channel_signature to only get claims with invalid signatures
    is_controlling
    optionalbool
    winning claims of their respective name
    public_key_id
    optionalstr
    only return channels having this public key id, this is the same key as used in the wallet file to map channel certificate private keys: {'public_key_id': 'private key'}
    height              optionalint    last updated block height (supports equality constraints)
    timestamp           optionalint    last updated timestamp (supports equality constraints)
    creation_height     optionalint    created at block height (supports equality constraints)
    creation_timestamp  optionalint
    created at timestamp (supports equality constraints)
    activation_height   optionalint     height at which claim starts competing for name (supports equality constraints)
    expiration_height   optionalint    height at which claim will expire (supports equality constraints)
    release_time        optionalint    limit to claims self-described as having been released to the public on or after this UTC timestamp, when claim does not provide a release time the publish time is used instead (supports equality constraints)
    amount              optionalint    limit by claim value (supports equality constraints)
    support_amount      optionalint    limit by supports and tips received (supports equality constraints)
    effective_amount    optionalint    limit by total value (initial claim value plus all tips and supports received), this amount is blank until claim has reached activation height (supports equality constraints)
    trending_group      optionalint    group numbers 1 through 4 representing the trending groups of the content: 4 means content is trending globally and independently, 3 means content is not trending globally but is trending independently (locally), 2 means it is trending globally but not independently and 1 means it's not trending globally or locally (supports equality constraints)
    trending_mixed      optionalint    trending amount taken from the global or local value depending on the trending group: 4 - global value, 3 - local value, 2 - global value, 1 - local value (supports equality constraints)
    trending_local      optionalint    trending value calculated relative only to the individual contents past history (supports equality constraints)
    trending_global     optionalint    trending value calculated relative to all trending content globally (supports equality constraints)
    reposted_claim_id   optionalstr    all reposts of the specified original claim id
    reposted            optionalint    claims reposted this many times (supports equality constraints)
    claim_type          optionalstr    filter by 'channel', 'stream' or 'unknown'
    stream_types        optionallist   filter by 'video', 'image', 'document', etc
    media_types         optionallist   filter by 'video/mp4', 'image/png', etc
    fee_currency        optionalstring specify fee currency: LBC, BTC, USD
    fee_amount          optionaldecimal    content download fee (supports equality constraints)
    any_tags            optionallist    find claims containing any of the tags
    all_tags            optionallist    find claims containing every tag
    not_tags            optionallist    find claims not containing any of these tags
    any_languages       optionallist    find claims containing any of the languages
    all_languages       optionallist    find claims containing every language
    not_languages       optionallist    find claims not containing any of these languages
    any_locations       optionallist    find claims containing any of the locations
    all_locations       optionallist    find claims containing every location
    not_locations       optionallist    find claims not containing any of these locations
    page                optionalint     page to return during paginating
    page_size           optionalint     number of items on page during pagination
    order_by            optionallist    field to order by, default is descending order, to do an ascending order prepend ^ to the field name, eg. '^amount' available fields: 'name', 'height', 'release_time', 'publish_time', 'amount', 'effective_amount', 'support_amount', 'trending_group', 'trending_mixed', 'trending_local', 'trending_global', 'activation_height'
    no_totals  optionalbool
    do not calculate the total number of pages and items in result set (significant performance boost)
    wallet_id optionalstr 

import urllib
import simplejson as json
import requests
import sys
SEARCH = raw_input("Search: ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": SEARCH,  "page_size":50}}).json()

print data

DATA =str(data)
Data =DATA.split("{")
for line in Data:
     print line

print json.dumps(data)

print(json.dumps(data, sort_keys=True, indent=2 * ' '))

import urllib
import simplejson as json
import requests
import sys
search =raw_input("Search Term: ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": search,  "page_size":25}}).json()
LINES = (json.dumps(data, indent=2 * ' '))
LIST =[]
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    line= line.replace('\\n',' ')
    if "media_type" in line:
        print line.lstrip()
        LIST.append(line)
    if "permanent_url" in line and "@" not in line:
        cnt= cnt+1
        print cnt,line.lstrip()
        LIST.append(line)

!lbrynet get testing-file-download-functionality#f4e9cd3bc78b481ca62e06f1ae1a29b602255292 --download_directory=/home/jack/Desktop/JupyterNotebooks-languages

import urllib
import simplejson as json
import requests
import sys
search =raw_input("Search Term: ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": search, "page_size":60}}).json()
LINES = (json.dumps(data, indent=2 * ' '))
LIST =[]
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    line= line.replace('\\n',' ')
    if "description" in line:
        print line.lstrip()
        LIST.append(line)
    if "permanent_url" in line and "@" not in line:
        cnt= cnt+1
        print cnt,line.lstrip()
        LIST.append(line)

LINES = (json.dumps(data, indent=2 * ' '))
LIST =[]
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    line= line.replace('\\n',' ')
    if "description" in line:
        print line
        LIST.append(line)
    if "permanent_url" in line:
        cnt= cnt+1
        print cnt,line
        LIST.append(line)

for lines in LIST:
    if 'permanent_url' in lines:
        print lines

LINES = (json.dumps(data, indent=2 * ' '))
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    line= line.replace('\\n',' ')
    if "description" in line:
        print line
    if "permanent_url" in line:
        cnt= cnt+1
        print cnt, line,"\n"
        print '----------------------'

print(json.dumps(data, indent=2 * ' '))

text ='''(1, '"permanent_url": "lbry://Intro_to_Python_Python_Basics_Part_1#4c1de7f251ad4bea5f9c5e09d3ab17918a2514ac",')'''
print text[30:-4]

!rm /home/jack/Desktop/LBRY-toolbox/5279.db

# %load LBRY_DLOAD
#!/usr/bin/python2
import os
from time import sleep
import urllib
import simplejson as json
import requests
import sys
import subprocess
from Completedpy2 import track_download
import sqlite3
import watchVID
database = '/home/jack/Desktop/LBRY-toolbox/5279.db'
conn = sqlite3.connect(database)
conn.text_factory = str
sql = '''create table if not exists LBRY(
description TEXT, url TEXT, unique (url));'''
conn.execute(sql) # shortcut for conn.cursor().execute(sql)
conn.commit()
c=conn.cursor()
search=raw_input("SEARCH : ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": search,  "page_size":50}}).json()
LINES = (json.dumps(data, indent=2 * ' '))
DES = ""
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    DES = ""
    URL = ""
    line= line.replace('\\n',' ')
    if "description" in line:
        print line.lstrip()
        DES.append(line)
    if "permanent_url" in line:
        cnt= cnt+1
        print cnt,line.lstrip()
        URL.append(line)     
    
    
    
    try:
        c.execute("INSERT INTO LBRY VALUES (?,?)", (search, content))
    except:
        pass
conn.commit()
conn.close()
choice =int(raw_input("Enter the number you wish to download: Type 99 to exit "))
#if choice==99:raise SystemExit("Stop right there!")
if choice==99:raise SystemExit("Stopped by 99 !")    
print "You will be downloading:",(LIST[choice-1])

requests.post("http://localhost:5279", json={"method": "get", "params": {"uri": LIST[choice-1] }}).json()
print "Monitoring filesize increase of claim id "+LIST[choice-1][:-30]
'''
This is an alternative using " lbrynet "
bashCommand = "lbrynet get "+LIST[choice-1]+" --download_directory=/home/jack/Desktop/LBRY-cli/VIDEOS"
print bashCommand
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate()
print "Monitoring filesize increase of claim id "+LIST[choice-1][:-30]
'''
track_download()

try:
    watchVID.WATCH()
except:
    sys.exit()




import os
HOME = os.environ['HOME']
print HOME

#%%writefile blobsize
#!/usr/bin/python2
import subprocess
import os
HOME = os.environ['HOME']

path = HOME+'/.local/share/lbry/lbrynet/blobfiles'
size = subprocess.check_output(['du','-sh' ,path]).split()[0].decode('utf-8')
print("Directory size: " + size)

# makes blobsize executeable
!chmod +x blobsize

import os
HOME = os.environ['HOME']
for r, d, f in os.walk(HOME+'/.local/share/lbry/lbrynet/blobfiles'):
    size = sum(os.path.getsize(os.path.join(r,n)) for n in f) / 1048576
    print "{} is {}{}".format(r, size, " Meg")

#!/home/jack/miniconda3/bin/python
# Script to get the size of the LBRY blobfiles directory
import subprocess
from os.path import expanduser
# Get HOME
HOME = expanduser("~")
# To the HOME add /.local/share/lbry/lbrynet/blobfiles
path = HOME+'/.local/share/lbry/lbrynet/blobfiles'
#get the directory size 
size = subprocess.check_output(['du','-sh' ,path]).split()[0].decode('utf-8')
print('Directory-size: lbrynet/blobfiles ' + size)



#%%writefile claim_search
#!/usr/bin/python2
import os
from time import sleep
import urllib
import simplejson as json
import requests
import sys
import subprocess
from Completed import track_download
import sqlite3
import watchVID
database = '5279.db'
conn = sqlite3.connect(database)
# text_factory deals with the non-asci characters
conn.text_factory = str
sql = '''create table if not exists LBRY(
description TEXT, url TEXT, unique (url));'''
conn.execute(sql) # shortcut for conn.cursor().execute(sql)
conn.commit()
c=conn.cursor()
search=raw_input("SEARCH : ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": search,  "page_size":75}}).json()
LINES = (json.dumps(data, indent=2 * ' '))
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    DES = []
    URL = []
    line= line.replace('\\n',' ')
    if "permanent_url" in line and "@" not in line:
        cnt= cnt+1
        Url = line.lstrip()
        URL.append(Url) 
        #print Url
        #print Url[25:-4]
        TEXT=Url[25:-4]+":::"
        #print TEXT
    if "description" in line:
        Des=line.lstrip()
        if len(Des)<5:Des="----------------No Description Given--"
        DES.append(Des)
        #print Des
        TEXT = str(TEXT)+(Des[16:-2])
        #print TEXT        
        #print Des[16:-2]
        TEXT = TEXT.split(":::")
        try:
            print TEXT[0]+"------"+TEXT[1]
            c.execute("INSERT INTO LBRY VALUES (?,?)", (TEXT[0],TEXT[1]))
        except:
            pass
conn.commit()
conn.close()

%%writefile watchVID.py
import subprocess
import glob
import os
def WATCH():
    # detox: remove all spaces from filenames Linux: apt install detox
    #PYTHON3 use: subprocess.run(["detox", "/home/jack/Downloads"])
    THIS_dir = os.getcwd()
    subprocess.call(["detox", THIS_dir])
    list_of_files = glob.glob(THIS_dir+"/*.mp4") # * means all if need specific format then *.csv
    src = max(list_of_files, key=os.path.getctime)
    bashCommand ="vlc --preferred-resolution 240 "+src 
    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
    output, error = process.communicate() 
    return 



#%%writefile search_data 
#!/usr/bin/python2
import sqlite3
database = '/home/jack/Desktop/LBRY-toolbox/5279.db'
conn = sqlite3.connect(database)
conn.text_factory = str
cnt = 0
Search = raw_input("SEARCH: ")
c = conn.cursor()
for row in c.execute("select * from LBRY"):
    cnt=cnt+1
    if Search in row[0] or Search in row[1]:
        print "Download Link: ",row[0]
        print row[1]
        print "-------------------------------"
print cnt    

requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"claim_ids": [], "channel": "@channel", "channel_ids": [], "not_channel_ids": [], "has_channel_signature": false, "valid_channel_signature": false, "invalid_channel_signature": false, "is_controlling": false, "stream_types": [], "media_types": [], "any_tags": [], "all_tags": [], "not_tags": [], "any_languages": [], "all_languages": [], "not_languages": [], "any_locations": [], "all_locations": [], "not_locations": [], "order_by": []}}).json()

    name    optionalstr
    claim name (normalized)
    text    optionalstr    full text search
    claim_id    optionalstr    full or partial claim id
    claim_ids    optionallist
    list of full claim ids
    txid    optionalstr
    transaction id    nout    optionalstr
    position in the transaction
    channel    optionalstr
    claims signed by this channel (argument is a URL which automatically gets resolved), see --channel_ids if you need to filter by multiple channels at the same time, includes claims with invalid signatures, use in conjunction with --valid_channel_signature
    channel_ids   optionallist  claims signed by any of these channels (arguments must be claim ids of the channels), includes claims with invalid signatures, implies --has_channel_signature, use in conjunction with --valid_channel_signature
    not_channel_ids   optionallist  exclude claims signed by any of these channels (arguments must be claim ids of the channels)
    has_channel_signature  optionalbool
    claims with a channel signature (valid or invalid)
    valid_channel_signature
    optionalbool
    claims with a valid channel signature or no signature, use in conjunction with --has_channel_signature to only get claims with valid signatures
    invalid_channel_signature
    optionalbool
    claims with invalid channel signature or no signature, use in conjunction with --has_channel_signature to only get claims with invalid signatures
    is_controlling
    optionalbool
    winning claims of their respective name
    public_key_id
    optionalstr
    only return channels having this public key id, this is the same key as used in the wallet file to map channel certificate private keys: {'public_key_id': 'private key'}
    height              optionalint    last updated block height (supports equality constraints)
    timestamp           optionalint    last updated timestamp (supports equality constraints)
    creation_height     optionalint    created at block height (supports equality constraints)
    creation_timestamp  optionalint
    created at timestamp (supports equality constraints)
    activation_height   optionalint     height at which claim starts competing for name (supports equality constraints)
    expiration_height   optionalint    height at which claim will expire (supports equality constraints)
    release_time        optionalint    limit to claims self-described as having been released to the public on or after this UTC timestamp, when claim does not provide a release time the publish time is used instead (supports equality constraints)
    amount              optionalint    limit by claim value (supports equality constraints)
    support_amount      optionalint    limit by supports and tips received (supports equality constraints)
    effective_amount    optionalint    limit by total value (initial claim value plus all tips and supports received), this amount is blank until claim has reached activation height (supports equality constraints)
    trending_group      optionalint    group numbers 1 through 4 representing the trending groups of the content: 4 means content is trending globally and independently, 3 means content is not trending globally but is trending independently (locally), 2 means it is trending globally but not independently and 1 means it's not trending globally or locally (supports equality constraints)
    trending_mixed      optionalint    trending amount taken from the global or local value depending on the trending group: 4 - global value, 3 - local value, 2 - global value, 1 - local value (supports equality constraints)
    trending_local      optionalint    trending value calculated relative only to the individual contents past history (supports equality constraints)
    trending_global     optionalint    trending value calculated relative to all trending content globally (supports equality constraints)
    reposted_claim_id   optionalstr    all reposts of the specified original claim id
    reposted            optionalint    claims reposted this many times (supports equality constraints)
    claim_type          optionalstr    filter by 'channel', 'stream' or 'unknown'
    stream_types        optionallist   filter by 'video', 'image', 'document', etc
    media_types         optionallist   filter by 'video/mp4', 'image/png', etc
    fee_currency        optionalstring specify fee currency: LBC, BTC, USD
    fee_amount          optionaldecimal    content download fee (supports equality constraints)
    any_tags            optionallist    find claims containing any of the tags
    all_tags            optionallist    find claims containing every tag
    not_tags            optionallist    find claims not containing any of these tags
    any_languages       optionallist    find claims containing any of the languages
    all_languages       optionallist    find claims containing every language
    not_languages       optionallist    find claims not containing any of these languages
    any_locations       optionallist    find claims containing any of the locations
    all_locations       optionallist    find claims containing every location
    not_locations       optionallist    find claims not containing any of these locations
    page                optionalint     page to return during paginating
    page_size           optionalint     number of items on page during pagination
    order_by            optionallist    field to order by, default is descending order, to do an ascending order prepend ^ to the field name, eg. '^amount' available fields: 'name', 'height', 'release_time', 'publish_time', 'amount', 'effective_amount', 'support_amount', 'trending_group', 'trending_mixed', 'trending_local', 'trending_global', 'activation_height'
    no_totals  optionalbool
    do not calculate the total number of pages and items in result set (significant performance boost)
    wallet_id optionalstr 

import urllib
import simplejson as json
import requests
import sys
SEARCH = raw_input("Search: ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": SEARCH,  "page_size":50}}).json()

print data

DATA =str(data)
Data =DATA.split("{")
for line in Data:
     print line

print json.dumps(data)

print(json.dumps(data, sort_keys=True, indent=2 * ' '))

import urllib
import simplejson as json
import requests
import sys
search =raw_input("Search Term: ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": search,  "page_size":25}}).json()
LINES = (json.dumps(data, indent=2 * ' '))
LIST =[]
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    line= line.replace('\\n',' ')
    if "media_type" in line:
        print line.lstrip()
        LIST.append(line)
    if "permanent_url" in line and "@" not in line:
        cnt= cnt+1
        print cnt,line.lstrip()
        LIST.append(line)

!lbrynet get testing-file-download-functionality#f4e9cd3bc78b481ca62e06f1ae1a29b602255292 --download_directory=/home/jack/Desktop/JupyterNotebooks-languages

import urllib
import simplejson as json
import requests
import sys
search =raw_input("Search Term: ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": search, "page_size":60}}).json()
LINES = (json.dumps(data, indent=2 * ' '))
LIST =[]
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    line= line.replace('\\n',' ')
    if "description" in line:
        print line.lstrip()
        LIST.append(line)
    if "permanent_url" in line and "@" not in line:
        cnt= cnt+1
        print cnt,line.lstrip()
        LIST.append(line)

LINES = (json.dumps(data, indent=2 * ' '))
LIST =[]
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    line= line.replace('\\n',' ')
    if "description" in line:
        print line
        LIST.append(line)
    if "permanent_url" in line:
        cnt= cnt+1
        print cnt,line
        LIST.append(line)

for lines in LIST:
    if 'permanent_url' in lines:
        print lines

LINES = (json.dumps(data, indent=2 * ' '))
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    line= line.replace('\\n',' ')
    if "description" in line:
        print line
    if "permanent_url" in line:
        cnt= cnt+1
        print cnt, line,"\n"
        print '----------------------'

print(json.dumps(data, indent=2 * ' '))

text ='''(1, '"permanent_url": "lbry://Intro_to_Python_Python_Basics_Part_1#4c1de7f251ad4bea5f9c5e09d3ab17918a2514ac",')'''
print text[30:-4]

!rm /home/jack/Desktop/LBRY-toolbox/5279.db

# %load LBRY_DLOAD
#!/usr/bin/python2
import os
from time import sleep
import urllib
import simplejson as json
import requests
import sys
import subprocess
from Completedpy2 import track_download
import sqlite3
import watchVID
database = '/home/jack/Desktop/LBRY-toolbox/5279.db'
conn = sqlite3.connect(database)
conn.text_factory = str
sql = '''create table if not exists LBRY(
description TEXT, url TEXT, unique (url));'''
conn.execute(sql) # shortcut for conn.cursor().execute(sql)
conn.commit()
c=conn.cursor()
search=raw_input("SEARCH : ")
data = requests.post("http://localhost:5279", json={"method": "claim_search", "params": {"text": search,  "page_size":50}}).json()
LINES = (json.dumps(data, indent=2 * ' '))
DES = ""
#print LINES
Lin =str(LINES)
L = Lin.split("\n")
cnt = 0
for line in L:
    DES = ""
    URL = ""
    line= line.replace('\\n',' ')
    if "description" in line:
        print line.lstrip()
        DES.append(line)
    if "permanent_url" in line:
        cnt= cnt+1
        print cnt,line.lstrip()
        URL.append(line)     
    
    
    
    try:
        c.execute("INSERT INTO LBRY VALUES (?,?)", (search, content))
    except:
        pass
conn.commit()
conn.close()
choice =int(raw_input("Enter the number you wish to download: Type 99 to exit "))
#if choice==99:raise SystemExit("Stop right there!")
if choice==99:raise SystemExit("Stopped by 99 !")    
print "You will be downloading:",(LIST[choice-1])

requests.post("http://localhost:5279", json={"method": "get", "params": {"uri": LIST[choice-1] }}).json()
print "Monitoring filesize increase of claim id "+LIST[choice-1][:-30]
'''
This is an alternative using " lbrynet "
bashCommand = "lbrynet get "+LIST[choice-1]+" --download_directory=/home/jack/Desktop/LBRY-cli/VIDEOS"
print bashCommand
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate()
print "Monitoring filesize increase of claim id "+LIST[choice-1][:-30]
'''
track_download()

try:
    watchVID.WATCH()
except:
    sys.exit()




import os
HOME = os.environ['HOME']
print HOME

#%%writefile blobsize
#!/usr/bin/python2
import subprocess
import os
HOME = os.environ['HOME']

path = HOME+'/.local/share/lbry/lbrynet/blobfiles'
size = subprocess.check_output(['du','-sh' ,path]).split()[0].decode('utf-8')
print("Directory size: " + size)

# makes blobsize executeable
!chmod +x blobsize

import os
HOME = os.environ['HOME']
for r, d, f in os.walk(HOME+'/.local/share/lbry/lbrynet/blobfiles'):
    size = sum(os.path.getsize(os.path.join(r,n)) for n in f) / 1048576
    print "{} is {}{}".format(r, size, " Meg")

#!/home/jack/miniconda3/bin/python
# Script to get the size of the LBRY blobfiles directory
import subprocess
from os.path import expanduser
# Get HOME
HOME = expanduser("~")
# To the HOME add /.local/share/lbry/lbrynet/blobfiles
path = HOME+'/.local/share/lbry/lbrynet/blobfiles'
#get the directory size 
size = subprocess.check_output(['du','-sh' ,path]).split()[0].decode('utf-8')
print('Directory-size: lbrynet/blobfiles ' + size)

!locate notebook.json



import onnx
import os


# Load the ONNX model
onnx_model = onnx.load(os.path.join('resources', 'single_relu.onnx'))
print(onnx_model)

!locate *.ipynb >>ALL-IPYNB.list

!ls

files = open("ALL-IPYNB.list").readlines()
for file in files:
    file = file.replace("\n","")
    print(file)
    

from pathlib import Path
import shutil
files = open("ALL-IPYNB.list").readlines()
for file in files:
    file = file.replace("\n","")
    print(file)
    #shutil.copy2(os.path.join(src,fname), trg)
    shutil.copy2(file, ".")
    #shutil.copy2(os.path.join(src,fname), trg)

files = open("ALL-IPYNB.list").readlines()
SEARCH = input("Search for a term: ")
for file in files:
    file = file.replace("\n","")
    if SEARCH in file:
        print(file)

!ls

import os
filelist=[]
files = os.listdir(".")
for file in files:
    filelist.append(file)


print(len(filelist))

SEARCH =input("Search for Term in a notebook:")
for filename in filelist:
    if "checkpoints" not in filename:
        count=0
        data = open(filename).readlines()
        for line in data:
            if SEARCH in line:
                count=count+1
                print( count,filename)



!cat logs/api.log

#import logging
# Create and configure logger
#logging.basicConfig(filename="newfile.log",
format='%(asctime)s %(message)s',
#filemode='a')
# Creating an object
#api_logger = logging.getLogger()
# Setting the threshold of logger to DEBUG
#api_logger.setLevel(logging.DEBUG)
# Test messages
api_logger.debug('Harmless debug Message')
api_logger.info('Just an information')
api_logger.warning('Its a Warning')
api_logger.error('Did you try to divide by zero')
api_logger.critical('Internet is down')

quote_h  = "0200-000000-000000-c0f7769ec2-5500000200000-00000-0000-00b7f73ac6fc0100"
api_logger.info(quote_h)

import ctypes
example_string = "Hello world!"
address = id(example_string)
api_logger.info(address)
#string_from_address = ctypes.cast(address).value
#print(string_from_address)

import os
basedir = os.getcwd()
logger.info(basedir)

import codecs
quote_h  = "0200000000000000c0f7769ec2550000020000000000000000b7f73ac6fc0100"
api_logger.debug('Harmless debug Message')
api_logger.info('Just an information')
api_logger.warning('Its a Warning')
api_logger.error('Did you try to divide by zero')
api_logger.critical('Internet is down')
api_logger.info("new error",quote_h)
quote_a  = codecs.decode(quote_h, 'hex').decode("ASCII")

quote    = quote_a.replace(';', '\n- ')
print(quote)

!rm /home/jack/hidden/logger_settings.py

!cat logs/api.log

from ctypes import string_at
from sys import getsizeof

a = 0x7f31baf7b700 
print(string_at(id(a),getsizeof(a)).hex())


quote_h  = "4d7920736f667477617265206e657665722068617320627567732e204974206a75737420646576656c6f70732072616e646f6d2066656174757265732e3b416e6f6e796d6f7573"
quote_a  = codecs.decode(quote_h, 'hex').decode("ASCII")
quote    = quote_a.replace(';', '\n- ')
print(quote)

%%writefile /home/jack/hidden/logger_settings.py
import sys
import os
import logging
from logging.config import dictConfig
import os


basedir = os.getcwd()
directory = "logs"
if not os.path.exists(directory):
    os.mkdir(os.path.join(basedir, directory))



logging_config = dict(
    version=1,
    formatters={
        'verbose': {
            'format': ("[%(asctime)s] %(levelname)s "
                       "[%(name)s:%(lineno)s] %(message)s"),
            'datefmt': "%d/%b/%Y %H:%M:%S",
        },
        'simple': {
            'format': '%(levelname)s %(message)s',
        },
    },
    handlers={
        'api-logger': {'class': 'logging.handlers.RotatingFileHandler',
                           'formatter': 'verbose',
                           'level': logging.DEBUG,
                           'filename': basedir+'/logs/api.log',
                           'maxBytes': 52428800,
                           'backupCount': 7},
        'batch-process-logger': {'class': 'logging.handlers.RotatingFileHandler',
                             'formatter': 'verbose',
                             'filename': basedir+'/logs/batch.log',
                             'maxBytes': 52428800,
                             'backupCount': 7},
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'DEBUG',
            'formatter': 'simple',
            'stream': sys.stdout,
        },
    },
    loggers={
        'api_logger': {
            'handlers': ['api-logger', 'console'],
            'level': logging.DEBUG
        },
        'batch_process_logger': {
            'handlers': ['batch-process-logger', 'console'],
            'level': logging.DEBUG
        }
    }
)

dictConfig(logging_config)

api_logger = logging.getLogger('api_logger')
batch_process_logger = logging.getLogger('batch_process_logger')


import ctypes
example_string = "Hello world!"
address = id(example_string)
#value = ctypes.c_int.from_address(address)
#value = ctypes.cast(address, id )
#print (value)
print(address)

from logger_settings import api_logger
api_logger.info('more')
api_logger.debug('Harmless debug Message')
api_logger.info('Just an information')
api_logger.warning('Its a Warning')
api_logger.error('Did you try to divide by zero')
api_logger.critical('Internet is down')

!cat logs/api.log

    Logger.info(msg) : This will log a message with level INFO on this logger.
    Logger.warning(msg) : This will log a message with a level WARNING on this logger.
    Logger.error(msg) : This will log a message with level ERROR on this logger.
    Logger.critical(msg) : This will log a message with level CRITICAL on this logger.
    Logger.log(lvl,msg) : This will Logs a message with integer level lvl on this logger.
    Logger.exception(msg) : This will log a message with level ERROR on this logger.
    Logger.setLevel(lvl) : This function sets the threshold of this logger to lvl. This means that all the messages below this level will be ignored.
    Logger.addFilter(filt) : This adds a specific filter filt into this logger.
    Logger.removeFilter(filt) : This removes a specific filter filt into this logger.
    Logger.filter(record) : This method applies the logger’s filter to the record provided and returns True if the record is to be processed. Else, it will return False.
    Logger.addHandler(hdlr) : This adds a specific handler hdlr to this logger.
    Logger.removeHandler(hdlr) : This removes a specific handler hdlr into this logger.
    Logger.hasHandlers() : This checks if the logger has any handler configured or not. 

   #utils_logger.logger_info('blind_sr_log', log_path='blind_sr_log.log')
    Logged.info('blind_sr_log', log_path='blind_sr_log.log')
    #python_utils.logger('blind_sr_log', log_path='blind_sr_log.log')
    #logger = logging.getLogger('blind_sr_log')

import sys
sys.path.append("/media/jack/HDD500/")
from model_zoo.ModelList import MODEL
model_name = MODEL()[3]



api_logger.info('{:>16s} : {:s}'.format('Model Name', model_name))

model_names = ['RRDB','ESRGAN','FSSR_DPED','FSSR_JPEG','RealSR_DPED','RealSR_JPEG']
#model_names = ['BSRGAN','BSRGANx2']    # 'BSRGANx2' for scale factor 2
for model_name in model_names:
    if model_name in ['BSRGANx2']:
        sf = 2
    model_path = os.path.join('/media/jack/HDD500/model_zoo', model_name+'.pth')          # set model path
    api_logger.info('{:>16s} : {:s}'.format('Model Name', model_name))
    print (model_path)

import sys
sys.path.append('/home/jack/Desktop/BSRGAN')
sys.path
#['', ..., '/home/sergey']
sys.path.remove('/home/jack/hidden')
#sys.path

FORMAT = '%(asctime)s %(clientip)-15s %(user)-8s %(message)s'
api_logger.info(FORMAT)
d = {'clientip': '192.168.0.1', 'user': 'fbloggs'}
api_logger.debug('tcpserver')
api_logger.warning('Protocol problem: %s', 'connection reset', extra=d)

# simple logging example
#import logging

#level = logging.DEBUG
#logging_format = "[%(levelname)s] %(asctime)s - %(message)s"
#logging.basicConfig(level = level, format=logging_format)

def print_vs_logging():
    api_logger.debug("What is the value of this variable")
    api_logger.info("Just FYI")
    api_logger.error("We found the error")

print_vs_logging()

api_logger.info("-------------------------------\n")

!cat logs/api.log

import os.path
import logging
import torch
from python_utils.logger import Logged
import python_utils.logger
import python_utils
from utils import utils_logger
from utils import utils_image as util
from utils import utils_model
from models.network_rrdbnet import RRDBNet as net
import sys
sys.path.append("/media/jack/HDD500/")
from model_zoo.ModelList import MODEL
data = MODEL()[3]
print(data)
MODELlist = []
model_names = ['RRDB','ESRGAN','FSSR_DPED','FSSR_JPEG','RealSR_DPED','RealSR_JPEG','BSRGANx2']
cnt = -1
for model_ in MODEL():
    cnt = cnt +1
    for model_name in model_names:
        if model_name in model_:
            print(model_name,"MODEL("+str(cnt)+")", MODEL()[cnt])
            MODELlist.append(MODEL()[cnt])

import os.path
import logging
import torch
import sys
import os
sys.path.append('/home/jack/Desktop/BSRGAN')
#from utils import utils_image
import utils_image
#sys.path.remove('/home/jack/hidden')
#sys.path
from python_utils.logger import Logged
import python_utils.logger
#import python_utils
#from utils import utils_logger
#from utils import utils_image as util
# from utils import utils_model
from models.network_rrdbnet import RRDBNet as net


"""
Spyder (Python 3.6-3.7)
PyTorch 1.4.0-1.8.1
Windows 10 or Linux
Kai Zhang (cskaizhang@gmail.com)
github: https://github.com/cszn/BSRGAN
        https://github.com/cszn/KAIR
If you have any question, please feel free to contact with me.
Kai Zhang (e-mail: cskaizhang@gmail.com)
by Kai Zhang ( March/2020 --> March/2021 --> )
This work was previously submitted to CVPR2021.

# --------------------------------------------
@inproceedings{zhang2021designing,
  title={Designing a Practical Degradation Model for Deep Blind Image Super-Resolution},
  author={Zhang, Kai and Liang, Jingyun and Van Gool, Luc and Timofte, Radu},
  booktitle={arxiv},
  year={2021}
}
# --------------------------------------------

"""


def main():

    #utils_logger.logger_info('blind_sr_log', log_path='blind_sr_log.log')
    #Logged.info('blind_sr_log', log_path='blind_sr_log.log')
    #python_utils.logger('blind_sr_log', log_path='blind_sr_log.log')
    #logger = logging.getLogger('blind_sr_log')

#    print(torch.__version__)               # pytorch version
#    print(torch.version.cuda)              # cuda version
#    print(torch.backends.cudnn.version())  # cudnn version

    testsets = 'testsets'       # fixed, set path of testsets
    testset_Ls = ['RealSRSet']  # ['RealSRSet','DPED']

    #model_names = ['RRDB','ESRGAN','FSSR_DPED','FSSR_JPEG','RealSR_DPED','RealSR_JPEG','BSRGANx2']
    #model_names = ['BSRGANx2']    # 'BSRGANx2' for scale factor 2



    save_results = True
    sf = 4
    device = torch.device('cpu')# if torch.cuda.is_available() else 'cpu')

    for model_path in MODELlist:
        if model_path == MODEL()[3]:
            sf = 2
        else:
            sf=4
            print(model_path)
        #model_path = os.path.join('model_zoo', model_name+'.pth')          # set model path
        api_logger.info('{:>16s} : {:s}'.format('Model Name', model_name))

        # torch.cuda.set_device(0)      # set GPU ID
        #Logged.logger.info('{:>16s} : {:<d}'.format('GPU ID', torch.cuda.current_device()))
        #torch.cuda.empty_cache()

        # --------------------------------
        # define network and load model
        # --------------------------------
        model = net(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, sf=sf)  # define network

#            model_old = torch.load(model_path)
#            state_dict = model.state_dict()
#            for ((key, param),(key2, param2)) in zip(model_old.items(), state_dict.items()):
#                state_dict[key2] = param
#            model.load_state_dict(state_dict, strict=True)
        print(model_path)
        suf =model_path[29:][:6]
        print ("SUF: ",suf)
        model.load_state_dict(torch.load(model_path), strict=True)
        model.eval()
        for k, v in model.named_parameters():
            v.requires_grad = False
        model = model.to(device)
        torch.cuda.empty_cache()

        for testset_L in testset_Ls:

            L_path = os.path.join(testsets, testset_L)
            #E_path = os.path.join(testsets, testset_L+'_'+model_name)
            E_path = os.path.join(testsets, testset_L+'_results_x'+str(sf))
            try:
                os.makedirs(E_path)
            except FileExistsError:
                # directory already exists
                pass
 
            api_logger.info('{:>16s} : {:s}'.format('Input Path', L_path))
            api_logger.info('{:>16s} : {:s}'.format('Output Path', E_path))
            idx = 0

            for img in utils_image.get_image_paths(L_path):

                # --------------------------------
                # (1) img_L
                # --------------------------------
                idx += 1
                img_name, ext = os.path.splitext(os.path.basename(img))
                api_logger.info('{:->4d} --> {:<s} --> x{:<d}--> {:<s}'.format(idx, model_name, sf, img_name+ext))

                img_L = utils_image.imread_uint(img, n_channels=3)
                img_L = utils_image.uint2tensor4(img_L)
                img_L = img_L.to(device)

                # --------------------------------
                # (2) inference
                # --------------------------------
                img_E = model(img_L)

                # --------------------------------
                # (3) img_E
                # --------------------------------
                img_E = utils_image.tensor2uint(img_E)
                imagenumber = 0
                if save_results:
                    imagenumber =imagenumber +1
                    number = str(imagenumber)
                    utils_image.imsave(img_E, os.path.join(E_path, img_name+suf+'_'+number+'_.png'))
                    print(os.path.join(E_path, img_name+suf+'_'+number+'_.png'))


if __name__ == '__main__':

    main()


!cat newfile.log

import logging
from python_utils.logger import Logged
import python_utils.logger

!pwd

!ls logs

!cat logs/api.log

!cat logs/batch.log

from python_utils import logger
import logging
import functools

Logged.info('blind_sr_log', log_path='blind_sr_log.log')

Logged.info('blind_sr_log', log_path='blind_sr_log.log')

!cat blind_sr_log.log



import (
    "fmt"
    "os"
)

(func() {
    f, err := os.Create("many_cells.ipynb")
    if err != nil {
        fmt.Printf("Failed to open: %v\n", err)
    }
    defer f.Close()
    f.WriteString(`{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# many cells\n",
    "This notebook demonstrates lgo can handle thousands of cells though lgo builds and loads shared objects file on every executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import (\n",
    "    \"fmt\"\n",
    "    \"strings\"\n",
    "    \"time\"\n",
    ")\n",
    "\n",
    "interval := 64\n",
    "start := time.Now()\n",
    "var sum int64\n",
    "var msgs []string"
   ]
  },
`)
    for i := 1; i <= 1024; i++ {
        f.WriteString(fmt.Sprintf(`{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    n := %d\n",
    "    sum += int64(n)\n",
    "    if n %% interval == 0 {\n",
    "        end := time.Now()\n",
    "        msgs = append(\n",
    "            msgs,\n",
    "            fmt.Sprintf(\"cycle: [%%d, %%d], took %%v on average\", n - interval + 1, n, end.Sub(start)/time.Duration(interval)))\n",
    "        start = end\n",
    "    }\n",
    "}"
   ]
  },
`, i))   
    }
    f.WriteString(`{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524800\n"
     ]
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycle: [1, 64], took 684.526294ms on average\n",
      "cycle: [65, 128], took 672.778457ms on average\n",
      "cycle: [129, 192], took 681.855261ms on average\n",
      "cycle: [193, 256], took 674.783885ms on average\n",
      "cycle: [257, 320], took 686.477005ms on average\n",
      "cycle: [321, 384], took 683.289412ms on average\n",
      "cycle: [385, 448], took 687.293805ms on average\n",
      "cycle: [449, 512], took 693.450195ms on average\n",
      "cycle: [513, 576], took 672.261078ms on average\n",
      "cycle: [577, 640], took 682.784516ms on average\n",
      "cycle: [641, 704], took 691.448044ms on average\n",
      "cycle: [705, 768], took 671.875065ms on average\n",
      "cycle: [769, 832], took 673.786842ms on average\n",
      "cycle: [833, 896], took 681.162642ms on average\n",
      "cycle: [897, 960], took 672.475553ms on average\n",
      "cycle: [961, 1024], took 680.732248ms on average\n",
      "765\n",
      "<nil>\n"
     ]
    }
   ],
   "source": [
    "fmt.Println(strings.Join(msgs, \"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go (lgo)",
   "language": "go",
   "name": "lgo"
  },
  "language_info": {
   "file_extension": "",
   "mimetype": "",
   "name": "go",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
`)
})()

import (
    "fmt"
    "os"
)

(func() {
    f, err := os.Create("many_cells4.ipynb")
    if err != nil {
        fmt.Printf("Failed to open: %v\n", err)
    }
    defer f.Close()
    f.WriteString(`{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# many cells\n",
    "This notebook demonstrates lgo can handle thousands of cells though lgo builds and loads shared objects file on every executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import (\n",
    "    \"fmt\"\n",
    "    \"strings\"\n",
    "    \"time\"\n",
    ")\n",
    "\n",
    "interval := 64\n",
    "start := time.Now()\n",
    "var sum int64\n",
    "var msgs []string"
   ]
  },
`)
    for i := 1; i <= 1024; i++ {
        f.WriteString(fmt.Sprintf(`{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    n := %d\n",
    "    sum += int64(n)\n",
    "    if n %% interval == 0 {\n",
    "        end := time.Now()\n",
    "        msgs = append(\n",
    "            msgs,\n",
    "            fmt.Sprintf(\"cycle: [%%d, %%d], took %%v on average\", n - interval + 1, n, end.Sub(start)/time.Duration(interval)))\n",
    "        start = end\n",
    "    }\n",
    "}"
   ]
  },
`, i))   
    }
    f.WriteString(`{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524800\n"
     ]
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycle: [1, 64], took 684.526294ms on average\n",
      "cycle: [65, 128], took 672.778457ms on average\n",
      "cycle: [129, 192], took 681.855261ms on average\n",
      "cycle: [193, 256], took 674.783885ms on average\n",
      "cycle: [257, 320], took 686.477005ms on average\n",
      "cycle: [321, 384], took 683.289412ms on average\n",
      "cycle: [385, 448], took 687.293805ms on average\n",
      "cycle: [449, 512], took 693.450195ms on average\n",
      "cycle: [513, 576], took 672.261078ms on average\n",
      "cycle: [577, 640], took 682.784516ms on average\n",
      "cycle: [641, 704], took 691.448044ms on average\n",
      "cycle: [705, 768], took 671.875065ms on average\n",
      "cycle: [769, 832], took 673.786842ms on average\n",
      "cycle: [833, 896], took 681.162642ms on average\n",
      "cycle: [897, 960], took 672.475553ms on average\n",
      "cycle: [961, 1024], took 680.732248ms on average\n",
      "765\n",
      "<nil>\n"
     ]
    }
   ],
   "source": [
    "fmt.Println(strings.Join(msgs, \"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go (lgo)",
   "language": "go",
   "name": "lgo"
  },
  "language_info": {
   "file_extension": "",
   "mimetype": "",
   "name": "go",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
`)
})()



import onnx
from onnx import helper
from onnx import AttributeProto, TensorProto, GraphProto


# The protobuf definition can be found here:
# https://github.com/onnx/onnx/blob/main/onnx/onnx.proto


# Create one input (ValueInfoProto)
X = helper.make_tensor_value_info('X', TensorProto.FLOAT, [1, 2])

# Create second input (ValueInfoProto)
Pads = helper.make_tensor_value_info('Pads', TensorProto.INT64, [4])

# Create one output (ValueInfoProto)
Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1, 4])

# Create a node (NodeProto)
node_def = helper.make_node(
    'Pad', # node name
    ['X', 'Pads'], # inputs
    ['Y'], # outputs
    mode='constant', # Attributes
)

# Create the graph (GraphProto)
graph_def = helper.make_graph(
    [node_def],
    "test-model",
    [X, Pads],
    [Y],
    [helper.make_tensor('Pads', TensorProto.INT64, [4,], [0, 0, 1, 1,])],
)

# Create the model (ModelProto)
model_def = helper.make_model(graph_def,
                              producer_name='onnx-example')

print('The producer_name in model: {}\n'.format(model_def.producer_name))
print('The graph in model:\n{}'.format(model_def.graph))
onnx.checker.check_model(model_def)
print('The model is checked!')

from PIL import Image, ImageOps
import cv2
from cv2 import *

picture = Image.new( 'RGB', (5,5), "black")
x, y = picture.size
h = x*-1
v = y*-1
while (h<x):
    while (v <y):
        
        
        v=v+1
    else:    
        h=h+1
        v=0

from PIL import Image, ImageOps
import cv2
from cv2 import *

picture = Image.new( 'RGB', (5,5), "black")
# Get the size of the image
x, y = picture.size
h = x*-1
v = y*-1
while (h<x):
    while (v <y):
        
        
        v=v+1
    else:    
        h=h+1
        v=0
        


x=0
y=0

while (x<10):
    while (y <10):
        print x,y
        y=y+1
    else:    
        x=x+1
        y=0
        print x,y

import caffe

!sudo locate caffe/_caffe

sys.path.insert(0, "/usr/local/caffe/python/python/caffe")
import _caffe

import sys
sys.path.insert(0, "/usr/local/")
sys.path.insert(0, "/usr/local/caffe/python/python/caffe")
import caffe

from PIL import Image, ImageOps
import cv2
from cv2 import *

picture = Image.new( 'RGB', (5,5), "black")
# Get the size of the image
x, y = picture.size
h = x*-1
v = y*-1
while (h<x):
    while (v <y):
        
        
        v=v+1
    else:    
        h=h+1
        v=0
        


x=0
y=0

while (x<10):
    while (y <10):
        print x,y
        y=y+1
    else:    
        x=x+1
        y=0
        print x,y

!ls haarcascades/haarcascade_frontalface_alt.xml

https://www.graph.cool/pricing/

import sys
sys.path.insert(0, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import cv2

%%writefile testblend.py
# ________  EXPERIMENT

"""
This opens two random images from the associated directories. Both images are resized to 
640x640. This is done in order to use the blend function. To use blend, both images must 
be the same size. The first image has a randomly created transparent region, to add 
to the effects. The blend is also a random alpha so all images created do not have an 
identical blend. All files saved are 640x640 and saved by " date-filename.png "
The purpose of these files are to create images to be pasted on the base or back 
ground collage piece.
"""
def Blend():
    import PIL
    import time
    from PIL import Image
    import random, os
    from random import randint
    import random, os
    path = r"../../GRAPHICS/gmic/640x640/"
    random_filename = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
    ])

    img1 = path+"/"+random_filename
    im1=Image.open(img1)
    longer_side = max(im1.size)
    basewidth = longer_side
    img = Image.open(img1)
    wpercent = (basewidth / float(img.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = img.size[0] / 2
    half_the_height = img.size[1] / 2
    img4 = img.crop(
        (
            half_the_width - 320,
            half_the_height - 320,
            half_the_width + 320,
            half_the_height + 320
        )
    )
    img4.save("STUFF/"+"TEMP-img5.png")
    from PIL import ImageDraw
    im = Image.open("STUFF/"+"TEMP-img5.png")

    x1=(randint(1, 150))
    x2=(randint(1, 150))
    y1=(randint(151,320))
    y2=(randint(151, 320))
    transparent_area = (x1,x2,300,500)

    mask=Image.new('L', im.size, color=255)
    draw=ImageDraw.Draw(mask) 
    draw.rectangle(transparent_area, fill=0)
    im.putalpha(mask)
    im.save("STUFF/"+"TEMP-img4.png")
    path2 = r"../../GRAPHICS/gmic/640x640/"
    #path2 = r"bugs/advertisements1800/"
    random_filename2 = random.choice([
        y for y in os.listdir(path2)
        if os.path.isfile(os.path.join(path2, y))
    ])

    img1a = path2+"/"+random_filename2
    im1a=Image.open(img1a)
    basewidth = 640
    imga = Image.open(img1a)
    wpercent = (basewidth / float(imga.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    imga = imga.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = imga.size[0] / 2
    half_the_height = imga.size[1] / 2
    img4a = imga.crop(
        (
            half_the_width - 320,
            half_the_height - 320,
            half_the_width + 320,
            half_the_height + 320
        )
    )
    img4a.save("STUFF/"+"TEMP-img4a.png")
    img1 = "STUFF/"+"TEMP-img4.png"
    img2 = "STUFF/"+"TEMP-img4a.png"

    from PIL import Image
    #im1=Image.open(img1)
    #im1.size # (width,height) tuple
    #im2=Image.open(img2)
    #im2.size # (width,height) tuple
    background = Image.open(img1)
    overlay = Image.open(img2)

    #background = Image.open(img2)
    background = background.convert("RGBA")
    #overlay = Image.open(img2)
    overlay = overlay.convert("RGBA")
    Alpha=(randint(1, 9))*0.1
    new_img = Image.blend(background, overlay, Alpha)

    timename = time.strftime("%Y%m%d-%H%M%S")
    #filename = timename+".png"
    filename = timename
    #new_img.save(filename,"PNG")
    saveas = "ManRay_temp/"+filename+".png"
    new_img.save(saveas,"PNG")
    return saveas

from testblend import Blend
from time import sleep
count =0
while count <50:
    Blend()
    count=count+1
    sleep(1.5)
    print count,

#1- GOOD RANDOM resize and crop
import PIL
import time
from PIL import Image
import random, os
import random, os
count = 0
while (count < 5):

    path = r"photos"

    random_filename = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
    ])
    img1 = path+"/"+random_filename
    im1=Image.open(img1)
    basewidth = 1040
    img = Image.open(img1)
    wpercent = (basewidth / float(img.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = img.size[0] / 2
    half_the_height = img.size[1] / 2
    img4 = img.crop(
        (
            half_the_width - 520,
            half_the_height - 520,
            half_the_width + 520,
            half_the_height + 520
        )
    )
    img4.save("TEMP/img4.png")

    path2 = r"photos"
    #path2 = r"bugs/advertisements1800/"
    random_filename2 = random.choice([
        y for y in os.listdir(path2)
        if os.path.isfile(os.path.join(path2, y))
    ])

    img1a = path2+"/"+random_filename2
    im1a=Image.open(img1a)
    basewidth = 1040
    imga = Image.open(img1a)
    wpercent = (basewidth / float(imga.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    imga = imga.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = imga.size[0] / 2
    half_the_height = imga.size[1] / 2
    img4a = imga.crop(
        (
            half_the_width - 520,
            half_the_height - 520,
            half_the_width + 520,
            half_the_height + 520
        )
    )
    img4a.save("TEMP/img4a.png")
    img1 = "TEMP/img4.png"
    img2 = "TEMP/img4a.png"

    from PIL import Image
    #im1=Image.open(img1)
    #im1.size # (width,height) tuple
    #im2=Image.open(img2)
    #im2.size # (width,height) tuple
    background = Image.open(img1)
    overlay = Image.open(img2)

    #background = Image.open(img2)
    background = background.convert("RGBA")
    #overlay = Image.open(img2)
    overlay = overlay.convert("RGBA")

    new_img = Image.blend(background, overlay, 0.5)



    new_img = new_img.convert("RGBA")


    #new = PIL.Image.new("RGB", 1040, 1040, color=1)
    white = (255,255,255)
    img_base = Image.new("RGB", [1040,1040], white)



    offset = (0, 0)
    new_img2 = img_base.paste(new_img, offset)




    #new_img2 = Image.blend(new_img, base_img, 1)

    timename = time.strftime("%Y%m%d-%H%M%S")
    #filename = timename+".png"
    filename = timename
    #new_img.save(filename,"PNG")

    img_base.save("ManRay/"+filename+".png","PNG")

    count = count +1
    time.sleep(1.5)
    print count,

%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

from IPython.display import display, Image

from glob import glob

import PIL
images = [ PIL.Image.open(f) for f in glob('ManRay/*') ]

def img2array(im):
    if im.mode != 'RGB':
        im = im.convert(mode='RGB')
    return np.fromstring(im.tobytes(), dtype='uint8').reshape((im.size[1], im.size[0], 3))

np_images = [ img2array(im) for im in images ]

for img in np_images:
    plt.figure()
    plt.imshow(img)

#1- GOOD RANDOM resize and crop
#This is fine makes a 1040x1040 background
import PIL
import time
from PIL import Image
import random, os
import random, os
#count = 0
#while (count < 30):

    path = r"bugs/lupe/"

    random_filename = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
    ])
    img1 = path+"/"+random_filename
    im1=Image.open(img1)
    basewidth = 1040
    img = Image.open(img1)
    wpercent = (basewidth / float(img.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = img.size[0] / 2
    half_the_height = img.size[1] / 2
    img4 = img.crop(
        (
            half_the_width - 520,
            half_the_height - 520,
            half_the_width + 520,
            half_the_height + 520
        )
    )
    img4.save("TEMP-img4.png")


    path2 = r"cycle/"
    #path2 = r"bugs/advertisements1800/"
    random_filename2 = random.choice([
        y for y in os.listdir(path2)
        if os.path.isfile(os.path.join(path2, y))
    ])

    img1a = path2+"/"+random_filename2
    im1a=Image.open(img1a)
    basewidth = 1040
    imga = Image.open(img1a)
    wpercent = (basewidth / float(imga.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    imga = imga.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = imga.size[0] / 2
    half_the_height = imga.size[1] / 2
    img4a = imga.crop(
        (
            half_the_width - 520,
            half_the_height - 520,
            half_the_width + 520,
            half_the_height + 520
        )
    )
    img4a.save("TEMP-img4a.png")
    img1 = "TEMP-img4.png"
    img2 = "TEMP-img4a.png"

    from PIL import Image
    #im1=Image.open(img1)
    #im1.size # (width,height) tuple
    #im2=Image.open(img2)
    #im2.size # (width,height) tuple
    background = Image.open(img1)
    overlay = Image.open(img2)

    #background = Image.open(img2)
    background = background.convert("RGBA")
    #overlay = Image.open(img2)
    overlay = overlay.convert("RGBA")

    new_img = Image.blend(background, overlay, 0.5)



    new_img = new_img.convert("RGBA")


    #new = PIL.Image.new("RGB", 1040, 1040, color=1)
    white = (255,255,255)
    img_base = Image.new("RGB", [1040,1040], white)



    offset = (0, 0)
    new_img2 = img_base.paste(new_img, offset)




    #new_img2 = Image.blend(new_img, base_img, 1)

    timename = time.strftime("%Y%m%d-%H%M%S")
    #filename = timename+".png"
    filename = timename
    #new_img.save(filename,"PNG")

    img_base.save("cycle2/"+filename+".png","PNG")

    #count = count +1
    return img_base

# 2- GOOD RANDOM resize and crop  Sepia experiment
import PIL
from PIL import Image, ImageOps
import random, os
import time

import random, os

count = 0
while (count < 30):


    path = r"/home/jack/Desktop/deep-dream-generator/notebooks/test/"
    random_filename = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
    ])

    img1 = path+"/"+random_filename
    im1=Image.open(img1)
    basewidth = 1040
    img = Image.open(img1)
    wpercent = (basewidth / float(img.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = img.size[0] / 2
    half_the_height = img.size[1] / 2
    im = img.crop(
        (
            half_the_width - 520,
            half_the_height - 520,
            half_the_width + 520,
            half_the_height + 520
        )
    )




    # GOOD WORKING TURNS IMAGE SEPIA


    def make_linear_ramp(white):
        # putpalette expects [r,g,b,r,g,b,...]
        ramp = []
        r, g, b = white
        for i in range(255):
            ramp.extend((r*i/255, g*i/255, b*i/255))
        return ramp
    # make sepia ramp (tweak color as necessary)
    sepia = make_linear_ramp((255, 240, 192))

    #imgo=Image.open("newimage.png", "r")
    #im = Image.open(img1S, 'r')
    #img_w, img_h = imgo.size
    #background = Image.new('RGBA', (1440, 900), (255, 255, 255, 255))

    # convert to grayscale
    if im.mode != "L":
        im = im.convert("L")

    # optional: apply contrast enhancement here, e.g.
    im = ImageOps.autocontrast(im)

    # apply sepia palette
    im.putpalette(sepia)

    # convert back to RGB so we can save it as JPEG
    # (alternatively, save it in PNG or similar)
    #im = im.convert("RGB")

    #im.save("file.jpg")
    timename = time.strftime("%Y%m%d-%H%M%S")
    #filename = timename+".png"
    filename = timename
    #new_img.save(filename,"PNG")

    #background.save("STUFF/"+filename+".png","PNG")
    im.save("TEMP-img4.png","PNG")


    #img4.save("TEMP-img4.png")


    path2 = r"/home/jack/Desktop/deep-dream-generator/notebooks/test/"
    #path2 = r"bugs/deco/"
    random_filename2 = random.choice([
        y for y in os.listdir(path2)
        if os.path.isfile(os.path.join(path2, y))
    ])

    img1a = path2+"/"+random_filename2
    im1a=Image.open(img1a)
    basewidth = 1200
    imga = Image.open(img1a)
    wpercent = (basewidth / float(imga.size[0]))
    #hsize = int((float(img.size[1]) * float(wpercent)))
    hsize = basewidth
    imga = imga.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = imga.size[0] / 2
    half_the_height = imga.size[1] / 2
    img4a = imga.crop(
        (
            half_the_width - 520,
            half_the_height - 520,
            half_the_width + 520,
            half_the_height + 520
        )
    )
    img4a.save("TEMP-img4a.png")
    img1 = "TEMP-img4.png"
    img2 = "TEMP-img4a.png"

    from PIL import Image
    #im1=Image.open(img1)
    #im1.size # (width,height) tuple
    #im2=Image.open(img2)
    #im2.size # (width,height) tuple
    background = Image.open(img1)
    overlay = Image.open(img2)

    #background = Image.open(img2)
    background = background.convert("RGBA")
    #overlay = Image.open(img2)
    overlay = overlay.convert("RGBA")

    new_img = Image.blend(background, overlay, 0.5)



    timename = time.strftime("%Y%m%d-%H%M%S")
    #filename = timename+".png"
    filename = timename
    #new_img.save(filename,"PNG")

    new_img.save("/home/jack/Desktop/deep-dream-generator/notebooks/ManRay_temp/"+filename+".png","PNG")
return new_image
count = count +1

# 4- GooD -EXPERIMENT
import PIL
from PIL import Image
from PIL import Image
import random, os
from PIL import Image
import time
import random, os
count = 0
while (count < 30):



    path = r"/home/jack/Desktop/deep-dream-generator/notebooks/test"
    random_filename = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
    ])

    img1 = path+"/"+random_filename
    im1=Image.open(img1)

    basewidth = 1040
    img = Image.open(img1)
    wpercent = (basewidth / float(img.size[0]))
    #hsize = int((float(img.size[1]) * float(wpercent)))
    hsize = basewidth
    img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = img.size[0] / 2
    half_the_height = img.size[1] / 2
    img4 = img.crop(
        (
            half_the_width - 520,
            half_the_height - 520,
            half_the_width + 520,
            half_the_height + 520
        )
    )
    #img4.save("TEMP-img4.png")



    path2 = "/home/jack/Desktop/deep-dream-generator/notebooks/test"
    random_filename2 = random.choice([
        y for y in os.listdir(path2)
        if os.path.isfile(os.path.join(path2, y))
    ])

    img1a = path2+"/"+random_filename2
    im1a=Image.open(img1a)
    basewidth = 1040
    imga = Image.open(img1a)
    wpercent = (basewidth / float(imga.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    imga = imga.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    #half_the_width = imga.size[0] / 2
    #half_the_height = imga.size[1] / 2
    from random import randint
    x1=(randint(1, 520-count))
    x2=(randint(1, 520-count))
    x3=(randint(x1, 1039-count))
    x4=(randint(x2, 1039-count))

    img4a = imga.crop(
        (
            x1,
            x2,
            x3,
            x4,
        )
    )
    img4a.save("TEMP-img4a.png")
    img1 = "TEMP-img4.png"
    img2 = "TEMP-img4a.png"

    from PIL import Image
    #im1=Image.open(img1)
    #im1.size # (width,height) tuple
    #im2=Image.open(img2)
    #im2.size # (width,height) tuple
    background = Image.open(img1)
    overlay = Image.open(img2)

    #background = Image.open(img2)
    background = background.convert("RGBA")
    #overlay = Image.open(img2)
    #overlay = overlay.convert("RGBA")
    overlay = overlay.convert("RGBA")

    bg_w, bg_h = background.size
    ctr1=random.randint(50, 700)
    ctr2=random.randint(50, 700)

    offset = (ctr1-200, ctr2-200)

    #image=Image.open("star_blue.png")
    #Alpha=(randint(4, 9))*0.1
    bands=list(overlay.split())
    if len(bands)==4:
        bands[3]=bands[3].point(lambda x:x*1)
        new_image=Image.merge(overlay.mode,bands)

    #new_img = Image.blend(background, overlay, 0.3)
    background.paste(new_image, offset)

    timename = time.strftime("%Y%m%d-%H%M%S")
    #filename = timename+".png"
    filename = timename
    #new_img.save(filename,"PNG")
    #background.save("STUFF/experiment/TEMP-img4a.png","PNG")
    background.save("/home/jack/Desktop/deep-dream-generator/notebooks/ManRay_temp/XXXexperiment.png","PNG")
    time.sleep(1.5)    #
count=count+1

# 4- GooD -EXPERIMENT
#KEE
import PIL
from PIL import Image
from PIL import Image
import random, os
from PIL import Image
import time
import random, os
count = 0
while (count < 30):



    path = r"bugs/spiders/"
    random_filename = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
    ])

    img1 = path+"/"+random_filename
    im1=Image.open(img1)

    basewidth = 1040
    img = Image.open(img1)
    wpercent = (basewidth / float(img.size[0]))
    hsize = int((float(img.size[1]) * float(wpercent)))
    hsize = basewidth
    img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    half_the_width = img.size[0] / 2
    half_the_height = img.size[1] / 2
    img4 = img.crop(
        (
            half_the_width - 520,
            half_the_height - 520,
            half_the_width + 520,
            half_the_height + 520
        )
    )
    img4.save("TEMP-img4.png")



    path2 = "bugs/vintage-labels/"
    random_filename2 = random.choice([
        y for y in os.listdir(path2)
        if os.path.isfile(os.path.join(path2, y))
    ])

    img1a = path2+"/"+random_filename2
    im1a=Image.open(img1a)
    
    inter= 500-(count*3)
    side=(randint(1, 70))
    sidea=(randint(1, inter))
    sideb= side+sidea
    img4a = im1a.resize((sidea,sideb), PIL.Image.ANTIALIAS)
    
    
    
    
    
    
    
    
  
    #basewidth = x1
    #imga = Image.open(img1a)
    #wpercent = (basewidth / float(imga.size[0]))
    #hsize = int((float(img.size[1]) * float(wpercent)))
    #img4a = imga.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
    #half_the_width = imga.size[0] / 2
    #half_the_height = imga.size[1] / 2
    from random import randint
    x1=(randint(1, 520-count))
    
    img4a.save("TEMP-img4a.png")
    img1 = "TEMP-img4.png"
    img2 = "TEMP-img4a.png"

    from PIL import Image
    #im1=Image.open(img1)
    #im1.size # (width,height) tuple
    #im2=Image.open(img2)
    #im2.size # (width,height) tuple
    background = Image.open(img1)
    overlay = Image.open(img2)

    #background = Image.open(img2)
    background = background.convert("RGBA")
    #overlay = Image.open(img2)
    #overlay = overlay.convert("RGBA")
    overlay = overlay.convert("RGBA")

    bg_w, bg_h = background.size
    ctr1=random.randint(1, 900)
    ctr2=random.randint(1, 900)

    offset = (ctr1-200, ctr2-200)

    #image=Image.open("star_blue.png")
    #Alpha=(randint(4, 9))*0.1
    bands=list(overlay.split())
    if len(bands)==4:
        bands[3]=bands[3].point(lambda x:x*1)
        new_image=Image.merge(overlay.mode,bands)

    #new_img = Image.blend(background, overlay, 0.3)
    background.paste(new_image, offset)

    timename = time.strftime("%Y%m%d-%H%M%S")
    #filename = timename+".png"
    filename = timename
    #new_img.save(filename,"PNG")
    #background.save("STUFF/experiment/TEMP-img4a.png","PNG")
    background.save("/home/jack/Desktop/deep-dream-generator/notebooks/ManRay_temp/experiment9.png","PNG")
    time.sleep(1.5)    #
count=count+1

# 4- GooD -EXPERIMENT
import PIL
from PIL import Image
from PIL import Image
import random, os
from PIL import Image
import time
path = r"bugs/nouveau/"
import random, os
random_filename = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])

img1 = path+"/"+random_filename
im1=Image.open(img1)

basewidth = 1040
img = Image.open(img1)
wpercent = (basewidth / float(img.size[0]))
#hsize = int((float(img.size[1]) * float(wpercent)))
hsize = basewidth
img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
half_the_width = img.size[0] / 2
half_the_height = img.size[1] / 2
img4 = img.crop(
    (
        half_the_width - 520,
        half_the_height - 520,
        half_the_width + 520,
        half_the_height + 520
    )
)
img4.save("TEMP-img4.png")



path2 = r"bugs/advertisements/"
random_filename2 = random.choice([
    y for y in os.listdir(path2)
    if os.path.isfile(os.path.join(path2, y))
])

img1a = path2+"/"+random_filename2
im1a=Image.open(img1a)
basewidth = 1040
imga = Image.open(img1a)
wpercent = (basewidth / float(imga.size[0]))
hsize = int((float(img.size[1]) * float(wpercent)))
imga = imga.resize((basewidth, hsize), PIL.Image.ANTIALIAS)
#half_the_width = imga.size[0] / 2
#half_the_height = imga.size[1] / 2
from random import randint
x1=(randint(22, 520))
x2=(randint(22, 520))
x3=(randint(x1+50, 1039))
x4=(randint(x2+50, 1039))

img4a = imga.crop(
    (
        x1,
        x2,
        x3,
        x4,
    )
)
img4a.save("TEMP-img4a.png")
img1 = "TEMP-img4.png"
img2 = "TEMP-img4a.png"

from PIL import Image
#im1=Image.open(img1)
#im1.size # (width,height) tuple
#im2=Image.open(img2)
#im2.size # (width,height) tuple
background = Image.open(img1)
overlay = Image.open(img2)

#background = Image.open(img2)
background = background.convert("RGBA")
#overlay = Image.open(img2)
#overlay = overlay.convert("RGBA")
overlay = overlay.convert("RGBA")

bg_w, bg_h = background.size
ctr1=random.randint(1, 800)
ctr2=random.randint(1, 800)






offset = (ctr1, ctr2)


#image=Image.open("star_blue.png")
Alpha=(randint(1, 9))*0.1
bands=list(overlay.split())
if len(bands)==4:
    bands[3]=bands[3].point(lambda x:x*Alpha)
    new_image=Image.merge(overlay.mode,bands)