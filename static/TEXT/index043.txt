-
def draw_arrow(p, x_i, y_i, length=100, angle=0):
    # Compute the second points and draw the arrow body
    x_f = int(x_i + length*math.cos(math.radians(angle)))
    y_f = int(y_i - length*math.sin(math.radians(angle)))
    p.drawLine(x_i, y_i, x_f, y_f)
    # Compute the arrow head second points
    a_angle1, a_angle2 = math.radians(angle-30), math.radians(angle+30)
    x1 = int(x_f - (length/10)*math.cos(a_angle1))
    y1 = int(y_f + (length/10)*math.sin(a_angle1))
    x2 = int(x_f - (length/10)*math.cos(a_angle2))
    y2 = int(y_f + (length/10)*math.sin(a_angle2))
    p.drawLine(x_f, y_f, x1, y1)
    p.drawLine(x_f, y_f, x2, y2)
    
draw_arrow(p, x_i, y_i, length=100, angle=0) 
v_path = "new"
 # Save the vector image
save(p, fname=v_path, folder='XXXX/New/')

def draw_perlin(nx, ny, width, height, fname):
    assert not os.path.exists(fname), 'File already exists'
    nx = int(nx)
    ny = int(ny)
    # Initialize Perlin Noise
    noise = (Perlin2D(width, height, nx, ny) + 1)/2

    # Convert to pixels
    pixels = 255 * noise
    pixels = pixels.astype(np.uint8)
    pixels = pixels[:, :, np.newaxis]
    pixels = np.repeat(pixels, 3, axis=2)

    # Create and save the image from pixels
    im = Image.fromarray(pixels)
    im.save(fname)

    return noise

width= 1000
height=1000
for inc in range(2,1000,2):
    fname = "/home/jack/Desktop/R-Studio/newseries/test_"+str(inc)+".jpg"
    nx = inc
    ny = inc
    try:
        #draw_perlin(nx, ny, width, height, fname)
        draw_vectors(nx, ny, width, height, seed=random.randint(0, 100000000), flow_length=150, n_vectors=50)
    except:
        pass

!ls -rant newseries

!ffmpeg -i newseries/slowout360.mp4 -i newseries/crack360spc.png -filter_complex "[0:v][1:v] overlay=0:0" \
-c:a copy newseries/output.mp4


!ffmpeg -i newseries/output.60fps.mp4 -vf scale=360x360 -vcodec libx264 -crf 24 newseries/smoutoutput.mp4

!ffmpeg -i /home/jack/Documents/QmdjwggR6x7ex3p99GWZWQqut7RsPfu4QofvYcA8CPvWyQ.mp4 \
-vcodec libx264 -crf 24 -vf "minterpolate=fps=60:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1" \
  -y newseries/outputS.60fps.mp4

/home/jack/Documents/QmNfgkiiAazV14NkS5PGJHNMigDs4VQYEB3wz2DcyB6FDa.mp4

!ffmpeg -i /home/jack/Documents/QmbFosnt9NSPkPCDRZh22TC5FYcrHY4M6DFJu6hhmcjVL3.mp4 \
-crf 20 -filter_complex  "scale=360:260,minterpolate=fps=60:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1" \
  -y newseries/girl20.60fps.mp4

!ffmpeg -i /home/jack/Documents/QmNfgkiiAazV14NkS5PGJHNMigDs4VQYEB3wz2DcyB6FDa.mp4 \
-crf 30 -vf "minterpolate=fps=60:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1" \
  -y newseries/newput30.60fps.mp4

!ffmpeg -i newseries/vid360.mp4 -i newseries/crack360spc.png -filter_complex \
"[1:v]format=argb,geq=r='r(X,Y)':a='1.0*alpha(X,Y)'[zork]; \
   [0:v][zork]overlay" -vcodec libx264 -y newseries/outputvideo.mp4

width= 1000
height=1000
for inc in range(2,1000,2):
    fname = "/home/jack/Desktop/R-Studio/newseries/test_"+str(inc)+".jpg"
    nx = inc
    ny = inc
    try:
        draw_perlin(nx, ny, width, height, fname)
    except:
        pass

width= 1000
height=1000
draw_flow_field(width, height, seed=random.randint(0, 100000000))

width= 1000
height=1000
draw_delta_body(width, height, iterations = 2000,seed=random.randint(0, 100000000), mode='noise')

cnt=1
width= 1000
height=1000
fname = "XXXX/o"
print(fname)
draw_perlin_rounding(width, height, fname, seed=random.randint(0, 100000000))
#draw_delta_body(width, height, iterations = 1000,seed=random.randint(0, 100000000), mode='noise')

!ls XXXX

"""
@author: The Absolute Tinkerer
"""

import os
import math
import time
import random

import numpy as np

from PIL import Image

from PyQt5.QtGui import QColor, QPen, QPixmap
from PyQt5.QtCore import QPointF, QRect

import painter
from utils import QColor_HSV, save, Perlin2D


def draw_white_noise(width, height, fname):
    assert not os.path.exists(fname), 'File already exists!'

    # Create a matrix of random values between zero and one
    pixels = np.random.random(size=(height, width))

    # Now modify the random values to be 0-255 (pixel color range)
    pixels = 255*pixels

    # The function to write the array of pixels to an image requires integers, not float values
    pixels = pixels.astype(np.uint8)

    # We choose to make random values grayscale, so each RGB element is identical. This code adds the third dimension
    # to our pixels array
    pixels = pixels[:, :, np.newaxis]

    # We need to repeat each value to finalize the pixels arrays in the grayscale space
    pixels = np.repeat(pixels, 3, axis=2)

    # Now create the image from an array of pixels
    im = Image.fromarray(pixels)

    # Save the image to file
    im.save(fname)


def draw_perlin(nx, ny, width, height, fname):
    assert not os.path.exists(fname), 'File already exists'

    # Initialize Perlin Noise
    noise = (Perlin2D(width, height, nx, ny) + 1)/2

    # Convert to pixels
    pixels = 255 * noise
    pixels = pixels.astype(np.uint8)
    pixels = pixels[:, :, np.newaxis]
    pixels = np.repeat(pixels, 3, axis=2)

    # Create and save the image from pixels
    im = Image.fromarray(pixels)
    im.save(fname)

    return noise


def draw_vectors(nx, ny, width, height, seed=random.randint(0, 100000000), flow_length=100, n_vectors=50):
    p_path = f'{seed}_1_perlin_noise.jpg'
    v_path = f'{seed}_2_vectors'
    f_path = f'{seed}_3_flow_field'

    # Ensure we don't overwrite paths
    assert not os.path.exists(p_path), 'Perlin Noise image already exists!'
    assert not os.path.exists(v_path), 'Vectors image already exists!'
    assert not os.path.exists(f_path), 'Flow field image already exists!'

    # Set the random seed for repeatability
    np.random.seed(seed)

    # Create the Perlin Noise image
    noise = draw_perlin(nx, ny, width, height, p_path)

    # Initialize the painter object for drawing
    p = painter.Painter(width, height)
    p.setRenderHint(p.Antialiasing)  # allow smooth drawing

    def draw_arrow(p, x_i, y_i, length=100, angle=0):
        # Compute the second points and draw the arrow body
        x_f = x_i + length*math.cos(math.radians(angle))
        y_f = y_i - length*math.sin(math.radians(angle))
        p.drawLine(x_i, y_i, x_f, y_f)

        # Compute the arrow head second points
        a_angle1, a_angle2 = math.radians(angle-30), math.radians(angle+30)
        x1 = x_f - (length/10)*math.cos(a_angle1)
        y1 = y_f + (length/10)*math.sin(a_angle1)
        x2 = x_f - (length/10)*math.cos(a_angle2)
        y2 = y_f + (length/10)*math.sin(a_angle2)
        p.drawLine(x_f, y_f, x1, y1)
        p.drawLine(x_f, y_f, x2, y2)

    # Load the Perlin Noise image and draw it with the painter
    p.drawPixmap(QRect(0, 0, width, height), QPixmap(p_path))

    # Now we're drawing red arrows for vectors, so set the pen color to red
    p.setPen(QColor(255, 0, 0))

    # We need arrow locations, so create a grid of n_vectors x n_vectors, excluding the image border
    _nx, _ny = n_vectors, n_vectors
    dx, dy = width / (_nx + 1), height / (_ny + 1)
    x_points = [dx + i*dx for i in range(_nx)]
    y_points = [dy + i*dy for i in range(_ny)]

    # Draw the arrows
    for x in x_points:
        for y in y_points:
            angle = 360*noise[int(x), int(y)]
            draw_arrow(p, x, y, length=min(dx, dy), angle=angle)

    # Save the vector image
    save(p, fname=v_path, folder='.')

    # Now draw the flow field. Start by initializing a new painter
    p = painter.Painter(width, height)
    p.setRenderHint(p.Antialiasing)  # allow smooth drawing
    p.setPen(QColor(0, 0, 0))  # pen color set to black

    # Step size between points
    STEP_SIZE = 0.001 * max(width, height)

    # Draw the flow field
    for x in x_points:
        for y in y_points:
            # The starting position
            x_s, y_s = x, y
            # The current line length tracking variable
            c_len = 0
            while c_len < flow_length:
                # angle between 0 and 2*pi
                angle = 2 * noise[int(x_s), int(y_s)] * math.pi

                # Compute the new point
                x_f = x_s + STEP_SIZE * math.cos(angle)
                y_f = y_s - STEP_SIZE * math.sin(angle)

                # Draw the line
                p.drawLine(QPointF(x_s, y_s), QPointF(x_f, y_f))

                # Update the line length
                c_len += math.sqrt((x_f - x_s) ** 2 + (y_f - y_s) ** 2)

                # Break from the loop if the new point is outside our image bounds
                # or if we've exceeded the line length; otherwise update the point
                if x_f < 0 or x_f >= width or y_f < 0 or y_f >= height or c_len > flow_length:
                    break
                else:
                    x_s, y_s = x_f, y_f
    save(p, fname=f_path, folder='.')


def draw_flow_field(width, height, seed=random.randint(0, 100000000)):
    # Set the random seed for repeatability
    np.random.seed(seed)

    # These are color hues
    colors = [200, 140, 70, 340, 280]
    for i, mod in enumerate(colors):
        print('Starting Image %s/%s' % (i + 1, len(colors)))
        p = painter.Painter(width, height)

        # Allow smooth drawing
        p.setRenderHint(p.Antialiasing)

        # Draw the background color
        p.fillRect(0, 0, width, height, QColor(0, 0, 0))

        # Set the pen color
        p.setPen(QPen(QColor(150, 150, 225, 5), 2))

        num = 1
        for j in range(num):
            print('Creating Noise... (%s/%s)' % (j + 1, num))
            p_noise = Perlin2D(width, height, 2, 2)
            print('Noise Generated! (%s/%s)' % (j + 1, num))

            MAX_LENGTH = 2 * width
            STEP_SIZE = 0.001 * max(width, height)
            NUM = int(width * height / 1000)
            POINTS = [(random.randint(0, width - 1), random.randint(0, height - 1)) for i in range(NUM)]

            for k, (x_s, y_s) in enumerate(POINTS):
                print(f'{100 * (k + 1) / len(POINTS):.1f}'.rjust(5) + '% Complete', end='\r')

                # The current line length tracking variable
                c_len = 0

                # Actually draw the flow field
                while c_len < MAX_LENGTH:
                    # Set the pen color for this segment
                    sat = 200 * (MAX_LENGTH - c_len) / MAX_LENGTH
                    hue = (mod + 130 * (height - y_s) / height) % 360
                    p.setPen(QPen(QColor_HSV(hue, sat, 255, 20), 2))

                    # angle between -pi and pi
                    angle = p_noise[int(x_s), int(y_s)] * math.pi

                    # Compute the new point
                    x_f = x_s + STEP_SIZE * math.cos(angle)
                    y_f = y_s + STEP_SIZE * math.sin(angle)

                    # Draw the line
                    p.drawLine(QPointF(x_s, y_s), QPointF(x_f, y_f))

                    # Update the line length
                    c_len += math.sqrt((x_f - x_s) ** 2 + (y_f - y_s) ** 2)

                    # Break from the loop if the new point is outside our image bounds
                    # or if we've exceeded the line length; otherwise update the point
                    if x_f < 0 or x_f >= width or y_f < 0 or y_f >= height or c_len > MAX_LENGTH:
                        break
                    else:
                        x_s, y_s = x_f, y_f

            save(p, fname=f'image_{mod}_{num}_{seed}', folder='.', overwrite=True)


def draw_perlin_rounding(width, height, fname, seed=random.randint(0, 100000000)):
    # Ensure we don't overwrite paths
    assert not os.path.exists(fname), 'Image already exists!'

    # Set the random seed for repeatability
    np.random.seed(seed)

    # Initialize a new painter
    p = painter.Painter(width, height)
    p.setRenderHint(p.Antialiasing)

    # Draw the background color
    p.fillRect(0, 0, width, height, QColor(0, 0, 0))

    # Set the pen color
    p.setPen(QColor(200, 200, 200))

    print('Creating Noise...', end='', flush=True)
    noise = Perlin2D(width, height, 1, 1)
    print('Done!')

    # The maximum line length and step size
    MAX_LENGTH = 1000
    STEP_SIZE = 0.001 * max(width, height)

    # Compute a grid 200x200 points, centered in the screen
    dx, dy = width / (200 + 1), height / (200 + 1)
    POINTS = [[(i+1)*dx, (j+1)*dy] for i in range(200) for j in range(200)]

    for i, (x_s, y_s) in enumerate(POINTS):
        print(f'{100 * (i + 1) / len(POINTS):.1f}'.rjust(5) + '% Complete', end='\r')

        # The current line length tracking variable
        c_len = 0
        while c_len < MAX_LENGTH:
            # angle between -pi and pi
            angle = math.pi*noise[int(x_s), int(y_s)]

            # Round the angle to pi/4 increments
            angle = round(angle / (math.pi / 4)) * (math.pi / 4)

            # Compute the new point
            x_f = x_s + STEP_SIZE * math.cos(angle)
            y_f = y_s + STEP_SIZE * math.sin(angle)

            # Draw the line
            p.drawLine(x_s, y_s, x_f, y_f)

            # Update the line length
            c_len += math.sqrt((x_f - x_s) ** 2 + (y_f - y_s) ** 2)

            # Break from the loop if the new point is outside our image bounds
            # or if we've exceeded the line length; otherwise update the point
            if (x_f < 0 or x_f >= width or y_f < 0 or y_f >= height or
                    c_len > MAX_LENGTH):
                break
            else:
                x_s, y_s = x_f, y_f

    print('100% Complete!')
    save(p, fname=f'{fname}_{seed}', folder='.')


class Body:
    def __init__(self, x, y, vx, vy):
        self._position = np.array([x, y], dtype=np.float64)
        self._velocity = np.array([vx, vy], dtype=np.float64)

    @property
    def position(self):
        return self._position

    @property
    def velocity(self):
        return self._velocity

    def update(self, dt):
        # update the body position
        self._position = self._position + dt*self._velocity


class ExpandingCircleRandom:
    def __init__(self, radius, num_bodies, center=(0, 0), v_limits=(-2, 2)):
        self._bodies = [Body(center[0] + radius*math.cos(i*2*math.pi/num_bodies),
                             center[1] + radius*math.sin(i*2*math.pi/num_bodies),
                             v_limits[0]+(v_limits[1]-v_limits[0])*random.random(),
                             v_limits[0]+(v_limits[1]-v_limits[0])*random.random()) for i in range(num_bodies)]

    def draw(self, dt, painter):
        # Connect the dots between each body
        for i in range(len(self._bodies)):
            # Handle the wrapping case
            if i == len(self._bodies) - 1:
                p1 = QPointF(*self._bodies[i].position)
                p2 = QPointF(*self._bodies[0].position)
            else:
                p1 = QPointF(*self._bodies[i].position)
                p2 = QPointF(*self._bodies[i+1].position)
            painter.drawLine(p1, p2)

        # Update the position of each body
        for i in range(len(self._bodies)):
            self._bodies[i].update(dt)


class ExpandingCircleNoise:
    def __init__(self, radius, num_bodies, noise, center=(0, 0), v_max=2):
        self._bodies = [Body(center[0] + radius*math.cos(i*2*math.pi/num_bodies),
                             center[1] + radius*math.sin(i*2*math.pi/num_bodies),
                             0, 0) for i in range(num_bodies)]
        self._v_max = v_max
        self._noise = noise

    def draw(self, dt, painter):
        # Connect the dots between each body
        for i in range(len(self._bodies)):
            # Handle the wrapping case
            if i == len(self._bodies) - 1:
                p1 = QPointF(*self._bodies[i].position)
                p2 = QPointF(*self._bodies[0].position)
            else:
                p1 = QPointF(*self._bodies[i].position)
                p2 = QPointF(*self._bodies[i + 1].position)
            painter.drawLine(p1, p2)

            # Try to update the velocity for each body. If we can't its because the point is beyond the noise
            # field we've created, so at that point, just maintain velocity.
            try:
                a = math.pi*self._noise[int(p1.x()), int(p1.y())]
                v = np.array([self._v_max*math.cos(a), self._v_max*math.sin(a)])
                self._bodies[i]._velocity = v
            except IndexError:
                pass

        # Update the position of each body
        for i in range(len(self._bodies)):
            self._bodies[i].update(dt)


def draw_delta_body(width, height, seed=random.randint(0, 100000000), mode='noise'):
    assert mode in ['noise', 'random'], 'Mode must either be "noise" or "random"'

    # Set the random seed for repeatability
    np.random.seed(seed)
    random.seed(seed)

    # Initialize the painter
    p = painter.Painter(width, height)
    p.setRenderHint(p.Antialiasing)  # Allow smooth drawing

    # Draw the background color
    p.fillRect(0, 0, width, height, QColor(0, 0, 0))

    # Set the pen color
    p.setPen(QPen(QColor(220, 220, 220, 5), 1))

    # Initialize the expanding circle centered in the canvas
    if mode == 'random':
        circle = ExpandingCircleRandom(width/8, 100, center=(width/2, height/2), v_limits=(-2, 2))
    elif mode == 'noise':
        noise = Perlin2D(width, height, 5, 5)
        circle = ExpandingCircleNoise(width/6, 200, noise, center=(width/4, height/2), v_max=5)
    else:
        circle = None

    # Initialize the delta time we're applying to each update
    dt = 0.3

    iterations = 1000
    for i in range(iterations):
        circle.draw(dt, p)

    save(p, fname=f'delta_{mode}_{seed}', folder='.', overwrite=True)

"""Provide RGB color constants and a colors dictionary with
elements formatted: colors[colorname] = CONSTANT"""
from collections import namedtuple, OrderedDict
Color = namedtuple('RGB','red, green, blue')
colors = {} #dict of colors
class RGB(Color):
    def hex_format(self):
        '''Returns color in hex format'''
        return '#{:02X}{:02X}{:02X}'.format(self.red,self.green,self.blue)
#Color Contants
ALICEBLUE = RGB(240, 248, 255)
ANTIQUEWHITE = RGB(250, 235, 215)
ANTIQUEWHITE1 = RGB(255, 239, 219)
ANTIQUEWHITE2 = RGB(238, 223, 204)
ANTIQUEWHITE3 = RGB(205, 192, 176)
ANTIQUEWHITE4 = RGB(139, 131, 120)
AQUA = RGB(0, 255, 255)
AQUAMARINE1 = RGB(127, 255, 212)
AQUAMARINE2 = RGB(118, 238, 198)
AQUAMARINE3 = RGB(102, 205, 170)
AQUAMARINE4 = RGB(69, 139, 116)
AZURE1 = RGB(240, 255, 255)
AZURE2 = RGB(224, 238, 238)
AZURE3 = RGB(193, 205, 205)
AZURE4 = RGB(131, 139, 139)
BANANA = RGB(227, 207, 87)
BEIGE = RGB(245, 245, 220)
BISQUE1 = RGB(255, 228, 196)
BISQUE2 = RGB(238, 213, 183)
BISQUE3 = RGB(205, 183, 158)
BISQUE4 = RGB(139, 125, 107)
BLACK = RGB(0, 0, 0)
BLANCHEDALMOND = RGB(255, 235, 205)
BLUE = RGB(0, 0, 255)
BLUE2 = RGB(0, 0, 238)
BLUE3 = RGB(0, 0, 205)
BLUE4 = RGB(0, 0, 139)
BLUEVIOLET = RGB(138, 43, 226)
BRICK = RGB(156, 102, 31)
BROWN = RGB(165, 42, 42)
BROWN1 = RGB(255, 64, 64)
BROWN2 = RGB(238, 59, 59)
BROWN3 = RGB(205, 51, 51)
BROWN4 = RGB(139, 35, 35)
BURLYWOOD = RGB(222, 184, 135)
BURLYWOOD1 = RGB(255, 211, 155)
BURLYWOOD2 = RGB(238, 197, 145)
BURLYWOOD3 = RGB(205, 170, 125)
BURLYWOOD4 = RGB(139, 115, 85)
BURNTSIENNA = RGB(138, 54, 15)
BURNTUMBER = RGB(138, 51, 36)
CADETBLUE = RGB(95, 158, 160)
CADETBLUE1 = RGB(152, 245, 255)
CADETBLUE2 = RGB(142, 229, 238)
CADETBLUE3 = RGB(122, 197, 205)
CADETBLUE4 = RGB(83, 134, 139)
CADMIUMORANGE = RGB(255, 97, 3)
CADMIUMYELLOW = RGB(255, 153, 18)
CARROT = RGB(237, 145, 33)
CHARTREUSE1 = RGB(127, 255, 0)
CHARTREUSE2 = RGB(118, 238, 0)
CHARTREUSE3 = RGB(102, 205, 0)
CHARTREUSE4 = RGB(69, 139, 0)
CHOCOLATE = RGB(210, 105, 30)
CHOCOLATE1 = RGB(255, 127, 36)
CHOCOLATE2 = RGB(238, 118, 33)
CHOCOLATE3 = RGB(205, 102, 29)
CHOCOLATE4 = RGB(139, 69, 19)
COBALT = RGB(61, 89, 171)
COBALTGREEN = RGB(61, 145, 64)
COLDGREY = RGB(128, 138, 135)
CORAL = RGB(255, 127, 80)
CORAL1 = RGB(255, 114, 86)
CORAL2 = RGB(238, 106, 80)
CORAL3 = RGB(205, 91, 69)
CORAL4 = RGB(139, 62, 47)
CORNFLOWERBLUE = RGB(100, 149, 237)
CORNSILK1 = RGB(255, 248, 220)
CORNSILK2 = RGB(238, 232, 205)
CORNSILK3 = RGB(205, 200, 177)
CORNSILK4 = RGB(139, 136, 120)
CRIMSON = RGB(220, 20, 60)
CYAN2 = RGB(0, 238, 238)
CYAN3 = RGB(0, 205, 205)
CYAN4 = RGB(0, 139, 139)
DARKGOLDENROD = RGB(184, 134, 11)
DARKGOLDENROD1 = RGB(255, 185, 15)
DARKGOLDENROD2 = RGB(238, 173, 14)
DARKGOLDENROD3 = RGB(205, 149, 12)
DARKGOLDENROD4 = RGB(139, 101, 8)
DARKGRAY = RGB(169, 169, 169)
DARKGREEN = RGB(0, 100, 0)
DARKKHAKI = RGB(189, 183, 107)
DARKOLIVEGREEN = RGB(85, 107, 47)
DARKOLIVEGREEN1 = RGB(202, 255, 112)
DARKOLIVEGREEN2 = RGB(188, 238, 104)
DARKOLIVEGREEN3 = RGB(162, 205, 90)
DARKOLIVEGREEN4 = RGB(110, 139, 61)
DARKORANGE = RGB(255, 140, 0)
DARKORANGE1 = RGB(255, 127, 0)
DARKORANGE2 = RGB(238, 118, 0)
DARKORANGE3 = RGB(205, 102, 0)
DARKORANGE4 = RGB(139, 69, 0)
DARKORCHID = RGB(153, 50, 204)
DARKORCHID1 = RGB(191, 62, 255)
DARKORCHID2 = RGB(178, 58, 238)
DARKORCHID3 = RGB(154, 50, 205)
DARKORCHID4 = RGB(104, 34, 139)
DARKSALMON = RGB(233, 150, 122)
DARKSEAGREEN = RGB(143, 188, 143)
DARKSEAGREEN1 = RGB(193, 255, 193)
DARKSEAGREEN2 = RGB(180, 238, 180)
DARKSEAGREEN3 = RGB(155, 205, 155)
DARKSEAGREEN4 = RGB(105, 139, 105)
DARKSLATEBLUE = RGB(72, 61, 139)
DARKSLATEGRAY = RGB(47, 79, 79)
DARKSLATEGRAY1 = RGB(151, 255, 255)
DARKSLATEGRAY2 = RGB(141, 238, 238)
DARKSLATEGRAY3 = RGB(121, 205, 205)
DARKSLATEGRAY4 = RGB(82, 139, 139)
DARKTURQUOISE = RGB(0, 206, 209)
DARKVIOLET = RGB(148, 0, 211)
DEEPPINK1 = RGB(255, 20, 147)
DEEPPINK2 = RGB(238, 18, 137)
DEEPPINK3 = RGB(205, 16, 118)
DEEPPINK4 = RGB(139, 10, 80)
DEEPSKYBLUE1 = RGB(0, 191, 255)
DEEPSKYBLUE2 = RGB(0, 178, 238)
DEEPSKYBLUE3 = RGB(0, 154, 205)
DEEPSKYBLUE4 = RGB(0, 104, 139)
DIMGRAY = RGB(105, 105, 105)
DIMGRAY = RGB(105, 105, 105)
DODGERBLUE1 = RGB(30, 144, 255)
DODGERBLUE2 = RGB(28, 134, 238)
DODGERBLUE3 = RGB(24, 116, 205)
DODGERBLUE4 = RGB(16, 78, 139)
EGGSHELL = RGB(252, 230, 201)
EMERALDGREEN = RGB(0, 201, 87)
FIREBRICK = RGB(178, 34, 34)
FIREBRICK1 = RGB(255, 48, 48)
FIREBRICK2 = RGB(238, 44, 44)
FIREBRICK3 = RGB(205, 38, 38)
FIREBRICK4 = RGB(139, 26, 26)
FLESH = RGB(255, 125, 64)
FLORALWHITE = RGB(255, 250, 240)
FORESTGREEN = RGB(34, 139, 34)
GAINSBORO = RGB(220, 220, 220)
GHOSTWHITE = RGB(248, 248, 255)
GOLD1 = RGB(255, 215, 0)
GOLD2 = RGB(238, 201, 0)
GOLD3 = RGB(205, 173, 0)
GOLD4 = RGB(139, 117, 0)
GOLDENROD = RGB(218, 165, 32)
GOLDENROD1 = RGB(255, 193, 37)
GOLDENROD2 = RGB(238, 180, 34)
GOLDENROD3 = RGB(205, 155, 29)
GOLDENROD4 = RGB(139, 105, 20)
GRAY = RGB(128, 128, 128)
GRAY1 = RGB(3, 3, 3)
GRAY10 = RGB(26, 26, 26)
GRAY11 = RGB(28, 28, 28)
GRAY12 = RGB(31, 31, 31)
GRAY13 = RGB(33, 33, 33)
GRAY14 = RGB(36, 36, 36)
GRAY15 = RGB(38, 38, 38)
GRAY16 = RGB(41, 41, 41)
GRAY17 = RGB(43, 43, 43)
GRAY18 = RGB(46, 46, 46)
GRAY19 = RGB(48, 48, 48)
GRAY2 = RGB(5, 5, 5)
GRAY20 = RGB(51, 51, 51)
GRAY21 = RGB(54, 54, 54)
GRAY22 = RGB(56, 56, 56)
GRAY23 = RGB(59, 59, 59)
GRAY24 = RGB(61, 61, 61)
GRAY25 = RGB(64, 64, 64)
GRAY26 = RGB(66, 66, 66)
GRAY27 = RGB(69, 69, 69)
GRAY28 = RGB(71, 71, 71)
GRAY29 = RGB(74, 74, 74)
GRAY3 = RGB(8, 8, 8)
GRAY30 = RGB(77, 77, 77)
GRAY31 = RGB(79, 79, 79)
GRAY32 = RGB(82, 82, 82)
GRAY33 = RGB(84, 84, 84)
GRAY34 = RGB(87, 87, 87)
GRAY35 = RGB(89, 89, 89)
GRAY36 = RGB(92, 92, 92)
GRAY37 = RGB(94, 94, 94)
GRAY38 = RGB(97, 97, 97)
GRAY39 = RGB(99, 99, 99)
GRAY4 = RGB(10, 10, 10)
GRAY40 = RGB(102, 102, 102)
GRAY42 = RGB(107, 107, 107)
GRAY43 = RGB(110, 110, 110)
GRAY44 = RGB(112, 112, 112)
GRAY45 = RGB(115, 115, 115)
GRAY46 = RGB(117, 117, 117)
GRAY47 = RGB(120, 120, 120)
GRAY48 = RGB(122, 122, 122)
GRAY49 = RGB(125, 125, 125)
GRAY5 = RGB(13, 13, 13)
GRAY50 = RGB(127, 127, 127)
GRAY51 = RGB(130, 130, 130)
GRAY52 = RGB(133, 133, 133)
GRAY53 = RGB(135, 135, 135)
GRAY54 = RGB(138, 138, 138)
GRAY55 = RGB(140, 140, 140)
GRAY56 = RGB(143, 143, 143)
GRAY57 = RGB(145, 145, 145)
GRAY58 = RGB(148, 148, 148)
GRAY59 = RGB(150, 150, 150)
GRAY6 = RGB(15, 15, 15)
GRAY60 = RGB(153, 153, 153)
GRAY61 = RGB(156, 156, 156)
GRAY62 = RGB(158, 158, 158)
GRAY63 = RGB(161, 161, 161)
GRAY64 = RGB(163, 163, 163)
GRAY65 = RGB(166, 166, 166)
GRAY66 = RGB(168, 168, 168)
GRAY67 = RGB(171, 171, 171)
GRAY68 = RGB(173, 173, 173)
GRAY69 = RGB(176, 176, 176)
GRAY7 = RGB(18, 18, 18)
GRAY70 = RGB(179, 179, 179)
GRAY71 = RGB(181, 181, 181)
GRAY72 = RGB(184, 184, 184)
GRAY73 = RGB(186, 186, 186)
GRAY74 = RGB(189, 189, 189)
GRAY75 = RGB(191, 191, 191)
GRAY76 = RGB(194, 194, 194)
GRAY77 = RGB(196, 196, 196)
GRAY78 = RGB(199, 199, 199)
GRAY79 = RGB(201, 201, 201)
GRAY8 = RGB(20, 20, 20)
GRAY80 = RGB(204, 204, 204)
GRAY81 = RGB(207, 207, 207)
GRAY82 = RGB(209, 209, 209)
GRAY83 = RGB(212, 212, 212)
GRAY84 = RGB(214, 214, 214)
GRAY85 = RGB(217, 217, 217)
GRAY86 = RGB(219, 219, 219)
GRAY87 = RGB(222, 222, 222)
GRAY88 = RGB(224, 224, 224)
GRAY89 = RGB(227, 227, 227)
GRAY9 = RGB(23, 23, 23)
GRAY90 = RGB(229, 229, 229)
GRAY91 = RGB(232, 232, 232)
GRAY92 = RGB(235, 235, 235)
GRAY93 = RGB(237, 237, 237)
GRAY94 = RGB(240, 240, 240)
GRAY95 = RGB(242, 242, 242)
GRAY97 = RGB(247, 247, 247)
GRAY98 = RGB(250, 250, 250)
GRAY99 = RGB(252, 252, 252)
GREEN = RGB(0, 128, 0)
GREEN1 = RGB(0, 255, 0)
GREEN2 = RGB(0, 238, 0)
GREEN3 = RGB(0, 205, 0)
GREEN4 = RGB(0, 139, 0)
GREENYELLOW = RGB(173, 255, 47)
HONEYDEW1 = RGB(240, 255, 240)
HONEYDEW2 = RGB(224, 238, 224)
HONEYDEW3 = RGB(193, 205, 193)
HONEYDEW4 = RGB(131, 139, 131)
HOTPINK = RGB(255, 105, 180)
HOTPINK1 = RGB(255, 110, 180)
HOTPINK2 = RGB(238, 106, 167)
HOTPINK3 = RGB(205, 96, 144)
HOTPINK4 = RGB(139, 58, 98)
INDIANRED = RGB(176, 23, 31)
INDIANRED = RGB(205, 92, 92)
INDIANRED1 = RGB(255, 106, 106)
INDIANRED2 = RGB(238, 99, 99)
INDIANRED3 = RGB(205, 85, 85)
INDIANRED4 = RGB(139, 58, 58)
INDIGO = RGB(75, 0, 130)
IVORY1 = RGB(255, 255, 240)
IVORY2 = RGB(238, 238, 224)
IVORY3 = RGB(205, 205, 193)
IVORY4 = RGB(139, 139, 131)
IVORYBLACK = RGB(41, 36, 33)
KHAKI = RGB(240, 230, 140)
KHAKI1 = RGB(255, 246, 143)
KHAKI2 = RGB(238, 230, 133)
KHAKI3 = RGB(205, 198, 115)
KHAKI4 = RGB(139, 134, 78)
LAVENDER = RGB(230, 230, 250)
LAVENDERBLUSH1 = RGB(255, 240, 245)
LAVENDERBLUSH2 = RGB(238, 224, 229)
LAVENDERBLUSH3 = RGB(205, 193, 197)
LAVENDERBLUSH4 = RGB(139, 131, 134)
LAWNGREEN = RGB(124, 252, 0)
LEMONCHIFFON1 = RGB(255, 250, 205)
LEMONCHIFFON2 = RGB(238, 233, 191)
LEMONCHIFFON3 = RGB(205, 201, 165)
LEMONCHIFFON4 = RGB(139, 137, 112)
LIGHTBLUE = RGB(173, 216, 230)
LIGHTBLUE1 = RGB(191, 239, 255)
LIGHTBLUE2 = RGB(178, 223, 238)
LIGHTBLUE3 = RGB(154, 192, 205)
LIGHTBLUE4 = RGB(104, 131, 139)
LIGHTCORAL = RGB(240, 128, 128)
LIGHTCYAN1 = RGB(224, 255, 255)
LIGHTCYAN2 = RGB(209, 238, 238)
LIGHTCYAN3 = RGB(180, 205, 205)
LIGHTCYAN4 = RGB(122, 139, 139)
LIGHTGOLDENROD1 = RGB(255, 236, 139)
LIGHTGOLDENROD2 = RGB(238, 220, 130)
LIGHTGOLDENROD3 = RGB(205, 190, 112)
LIGHTGOLDENROD4 = RGB(139, 129, 76)
LIGHTGOLDENRODYELLOW = RGB(250, 250, 210)
LIGHTGREY = RGB(211, 211, 211)
LIGHTPINK = RGB(255, 182, 193)
LIGHTPINK1 = RGB(255, 174, 185)
LIGHTPINK2 = RGB(238, 162, 173)
LIGHTPINK3 = RGB(205, 140, 149)
LIGHTPINK4 = RGB(139, 95, 101)
LIGHTSALMON1 = RGB(255, 160, 122)
LIGHTSALMON2 = RGB(238, 149, 114)
LIGHTSALMON3 = RGB(205, 129, 98)
LIGHTSALMON4 = RGB(139, 87, 66)
LIGHTSEAGREEN = RGB(32, 178, 170)
LIGHTSKYBLUE = RGB(135, 206, 250)
LIGHTSKYBLUE1 = RGB(176, 226, 255)
LIGHTSKYBLUE2 = RGB(164, 211, 238)
LIGHTSKYBLUE3 = RGB(141, 182, 205)
LIGHTSKYBLUE4 = RGB(96, 123, 139)
LIGHTSLATEBLUE = RGB(132, 112, 255)
LIGHTSLATEGRAY = RGB(119, 136, 153)
LIGHTSTEELBLUE = RGB(176, 196, 222)
LIGHTSTEELBLUE1 = RGB(202, 225, 255)
LIGHTSTEELBLUE2 = RGB(188, 210, 238)
LIGHTSTEELBLUE3 = RGB(162, 181, 205)
LIGHTSTEELBLUE4 = RGB(110, 123, 139)
LIGHTYELLOW1 = RGB(255, 255, 224)
LIGHTYELLOW2 = RGB(238, 238, 209)
LIGHTYELLOW3 = RGB(205, 205, 180)
LIGHTYELLOW4 = RGB(139, 139, 122)
LIMEGREEN = RGB(50, 205, 50)
LINEN = RGB(250, 240, 230)
MAGENTA = RGB(255, 0, 255)
MAGENTA2 = RGB(238, 0, 238)
MAGENTA3 = RGB(205, 0, 205)
MAGENTA4 = RGB(139, 0, 139)
MANGANESEBLUE = RGB(3, 168, 158)
MAROON = RGB(128, 0, 0)
MAROON1 = RGB(255, 52, 179)
MAROON2 = RGB(238, 48, 167)
MAROON3 = RGB(205, 41, 144)
MAROON4 = RGB(139, 28, 98)
MEDIUMORCHID = RGB(186, 85, 211)
MEDIUMORCHID1 = RGB(224, 102, 255)
MEDIUMORCHID2 = RGB(209, 95, 238)
MEDIUMORCHID3 = RGB(180, 82, 205)
MEDIUMORCHID4 = RGB(122, 55, 139)
MEDIUMPURPLE = RGB(147, 112, 219)
MEDIUMPURPLE1 = RGB(171, 130, 255)
MEDIUMPURPLE2 = RGB(159, 121, 238)
MEDIUMPURPLE3 = RGB(137, 104, 205)
MEDIUMPURPLE4 = RGB(93, 71, 139)
MEDIUMSEAGREEN = RGB(60, 179, 113)
MEDIUMSLATEBLUE = RGB(123, 104, 238)
MEDIUMSPRINGGREEN = RGB(0, 250, 154)
MEDIUMTURQUOISE = RGB(72, 209, 204)
MEDIUMVIOLETRED = RGB(199, 21, 133)
MELON = RGB(227, 168, 105)
MIDNIGHTBLUE = RGB(25, 25, 112)
MINT = RGB(189, 252, 201)
MINTCREAM = RGB(245, 255, 250)
MISTYROSE1 = RGB(255, 228, 225)
MISTYROSE2 = RGB(238, 213, 210)
MISTYROSE3 = RGB(205, 183, 181)
MISTYROSE4 = RGB(139, 125, 123)
MOCCASIN = RGB(255, 228, 181)
NAVAJOWHITE1 = RGB(255, 222, 173)
NAVAJOWHITE2 = RGB(238, 207, 161)
NAVAJOWHITE3 = RGB(205, 179, 139)
NAVAJOWHITE4 = RGB(139, 121, 94)
NAVY = RGB(0, 0, 128)
OLDLACE = RGB(253, 245, 230)
OLIVE = RGB(128, 128, 0)
OLIVEDRAB = RGB(107, 142, 35)
OLIVEDRAB1 = RGB(192, 255, 62)
OLIVEDRAB2 = RGB(179, 238, 58)
OLIVEDRAB3 = RGB(154, 205, 50)
OLIVEDRAB4 = RGB(105, 139, 34)
ORANGE = RGB(255, 128, 0)
ORANGE1 = RGB(255, 165, 0)
ORANGE2 = RGB(238, 154, 0)
ORANGE3 = RGB(205, 133, 0)
ORANGE4 = RGB(139, 90, 0)
ORANGERED1 = RGB(255, 69, 0)
ORANGERED2 = RGB(238, 64, 0)
ORANGERED3 = RGB(205, 55, 0)
ORANGERED4 = RGB(139, 37, 0)
ORCHID = RGB(218, 112, 214)
ORCHID1 = RGB(255, 131, 250)
ORCHID2 = RGB(238, 122, 233)
ORCHID3 = RGB(205, 105, 201)
ORCHID4 = RGB(139, 71, 137)
PALEGOLDENROD = RGB(238, 232, 170)
PALEGREEN = RGB(152, 251, 152)
PALEGREEN1 = RGB(154, 255, 154)
PALEGREEN2 = RGB(144, 238, 144)
PALEGREEN3 = RGB(124, 205, 124)
PALEGREEN4 = RGB(84, 139, 84)
PALETURQUOISE1 = RGB(187, 255, 255)
PALETURQUOISE2 = RGB(174, 238, 238)
PALETURQUOISE3 = RGB(150, 205, 205)
PALETURQUOISE4 = RGB(102, 139, 139)
PALEVIOLETRED = RGB(219, 112, 147)
PALEVIOLETRED1 = RGB(255, 130, 171)
PALEVIOLETRED2 = RGB(238, 121, 159)
PALEVIOLETRED3 = RGB(205, 104, 137)
PALEVIOLETRED4 = RGB(139, 71, 93)
PAPAYAWHIP = RGB(255, 239, 213)
PEACHPUFF1 = RGB(255, 218, 185)
PEACHPUFF2 = RGB(238, 203, 173)
PEACHPUFF3 = RGB(205, 175, 149)
PEACHPUFF4 = RGB(139, 119, 101)
PEACOCK = RGB(51, 161, 201)
PINK = RGB(255, 192, 203)
PINK1 = RGB(255, 181, 197)
PINK2 = RGB(238, 169, 184)
PINK3 = RGB(205, 145, 158)
PINK4 = RGB(139, 99, 108)
PLUM = RGB(221, 160, 221)
PLUM1 = RGB(255, 187, 255)
PLUM2 = RGB(238, 174, 238)
PLUM3 = RGB(205, 150, 205)
PLUM4 = RGB(139, 102, 139)
POWDERBLUE = RGB(176, 224, 230)
PURPLE = RGB(128, 0, 128)
PURPLE1 = RGB(155, 48, 255)
PURPLE2 = RGB(145, 44, 238)
PURPLE3 = RGB(125, 38, 205)
PURPLE4 = RGB(85, 26, 139)
RASPBERRY = RGB(135, 38, 87)
RAWSIENNA = RGB(199, 97, 20)
RED1 = RGB(255, 0, 0)
RED2 = RGB(238, 0, 0)
RED3 = RGB(205, 0, 0)
RED4 = RGB(139, 0, 0)
ROSYBROWN = RGB(188, 143, 143)
ROSYBROWN1 = RGB(255, 193, 193)
ROSYBROWN2 = RGB(238, 180, 180)
ROSYBROWN3 = RGB(205, 155, 155)
ROSYBROWN4 = RGB(139, 105, 105)
ROYALBLUE = RGB(65, 105, 225)
ROYALBLUE1 = RGB(72, 118, 255)
ROYALBLUE2 = RGB(67, 110, 238)
ROYALBLUE3 = RGB(58, 95, 205)
ROYALBLUE4 = RGB(39, 64, 139)
SALMON = RGB(250, 128, 114)
SALMON1 = RGB(255, 140, 105)
SALMON2 = RGB(238, 130, 98)
SALMON3 = RGB(205, 112, 84)
SALMON4 = RGB(139, 76, 57)
SANDYBROWN = RGB(244, 164, 96)
SAPGREEN = RGB(48, 128, 20)
SEAGREEN1 = RGB(84, 255, 159)
SEAGREEN2 = RGB(78, 238, 148)
SEAGREEN3 = RGB(67, 205, 128)
SEAGREEN4 = RGB(46, 139, 87)
SEASHELL1 = RGB(255, 245, 238)
SEASHELL2 = RGB(238, 229, 222)
SEASHELL3 = RGB(205, 197, 191)
SEASHELL4 = RGB(139, 134, 130)
SEPIA = RGB(94, 38, 18)
SGIBEET = RGB(142, 56, 142)
SGIBRIGHTGRAY = RGB(197, 193, 170)
SGICHARTREUSE = RGB(113, 198, 113)
SGIDARKGRAY = RGB(85, 85, 85)
SGIGRAY12 = RGB(30, 30, 30)
SGIGRAY16 = RGB(40, 40, 40)
SGIGRAY32 = RGB(81, 81, 81)
SGIGRAY36 = RGB(91, 91, 91)
SGIGRAY52 = RGB(132, 132, 132)
SGIGRAY56 = RGB(142, 142, 142)
SGIGRAY72 = RGB(183, 183, 183)
SGIGRAY76 = RGB(193, 193, 193)
SGIGRAY92 = RGB(234, 234, 234)
SGIGRAY96 = RGB(244, 244, 244)
SGILIGHTBLUE = RGB(125, 158, 192)
SGILIGHTGRAY = RGB(170, 170, 170)
SGIOLIVEDRAB = RGB(142, 142, 56)
SGISALMON = RGB(198, 113, 113)
SGISLATEBLUE = RGB(113, 113, 198)
SGITEAL = RGB(56, 142, 142)
SIENNA = RGB(160, 82, 45)
SIENNA1 = RGB(255, 130, 71)
SIENNA2 = RGB(238, 121, 66)
SIENNA3 = RGB(205, 104, 57)
SIENNA4 = RGB(139, 71, 38)
SILVER = RGB(192, 192, 192)
SKYBLUE = RGB(135, 206, 235)
SKYBLUE1 = RGB(135, 206, 255)
SKYBLUE2 = RGB(126, 192, 238)
SKYBLUE3 = RGB(108, 166, 205)
SKYBLUE4 = RGB(74, 112, 139)
SLATEBLUE = RGB(106, 90, 205)
SLATEBLUE1 = RGB(131, 111, 255)
SLATEBLUE2 = RGB(122, 103, 238)
SLATEBLUE3 = RGB(105, 89, 205)
SLATEBLUE4 = RGB(71, 60, 139)
SLATEGRAY = RGB(112, 128, 144)
SLATEGRAY1 = RGB(198, 226, 255)
SLATEGRAY2 = RGB(185, 211, 238)
SLATEGRAY3 = RGB(159, 182, 205)
SLATEGRAY4 = RGB(108, 123, 139)
SNOW1 = RGB(255, 250, 250)
SNOW2 = RGB(238, 233, 233)
SNOW3 = RGB(205, 201, 201)
SNOW4 = RGB(139, 137, 137)
SPRINGGREEN = RGB(0, 255, 127)
SPRINGGREEN1 = RGB(0, 238, 118)
SPRINGGREEN2 = RGB(0, 205, 102)
SPRINGGREEN3 = RGB(0, 139, 69)
STEELBLUE = RGB(70, 130, 180)
STEELBLUE1 = RGB(99, 184, 255)
STEELBLUE2 = RGB(92, 172, 238)
STEELBLUE3 = RGB(79, 148, 205)
STEELBLUE4 = RGB(54, 100, 139)
TAN = RGB(210, 180, 140)
TAN1 = RGB(255, 165, 79)
TAN2 = RGB(238, 154, 73)
TAN3 = RGB(205, 133, 63)
TAN4 = RGB(139, 90, 43)
TEAL = RGB(0, 128, 128)
THISTLE = RGB(216, 191, 216)
THISTLE1 = RGB(255, 225, 255)
THISTLE2 = RGB(238, 210, 238)
THISTLE3 = RGB(205, 181, 205)
THISTLE4 = RGB(139, 123, 139)
TOMATO1 = RGB(255, 99, 71)
TOMATO2 = RGB(238, 92, 66)
TOMATO3 = RGB(205, 79, 57)
TOMATO4 = RGB(139, 54, 38)
TURQUOISE = RGB(64, 224, 208)
TURQUOISE1 = RGB(0, 245, 255)
TURQUOISE2 = RGB(0, 229, 238)
TURQUOISE3 = RGB(0, 197, 205)
TURQUOISE4 = RGB(0, 134, 139)
TURQUOISEBLUE = RGB(0, 199, 140)
VIOLET = RGB(238, 130, 238)
VIOLETRED = RGB(208, 32, 144)
VIOLETRED1 = RGB(255, 62, 150)
VIOLETRED2 = RGB(238, 58, 140)
VIOLETRED3 = RGB(205, 50, 120)
VIOLETRED4 = RGB(139, 34, 82)
WARMGREY = RGB(128, 128, 105)
WHEAT = RGB(245, 222, 179)
WHEAT1 = RGB(255, 231, 186)
WHEAT2 = RGB(238, 216, 174)
WHEAT3 = RGB(205, 186, 150)
WHEAT4 = RGB(139, 126, 102)
WHITE = RGB(255, 255, 255)
WHITESMOKE = RGB(245, 245, 245)
WHITESMOKE = RGB(245, 245, 245)
YELLOW1 = RGB(255, 255, 0)
YELLOW2 = RGB(238, 238, 0)
YELLOW3 = RGB(205, 205, 0)
YELLOW4 = RGB(139, 139, 0)
#Add colors to colors dictionary
colors['aliceblue'] = ALICEBLUE
colors['antiquewhite'] = ANTIQUEWHITE
colors['antiquewhite1'] = ANTIQUEWHITE1
colors['antiquewhite2'] = ANTIQUEWHITE2
colors['antiquewhite3'] = ANTIQUEWHITE3
colors['antiquewhite4'] = ANTIQUEWHITE4
colors['aqua'] = AQUA
colors['aquamarine1'] = AQUAMARINE1
colors['aquamarine2'] = AQUAMARINE2
colors['aquamarine3'] = AQUAMARINE3
colors['aquamarine4'] = AQUAMARINE4
colors['azure1'] = AZURE1
colors['azure2'] = AZURE2
colors['azure3'] = AZURE3
colors['azure4'] = AZURE4
colors['banana'] = BANANA
colors['beige'] = BEIGE
colors['bisque1'] = BISQUE1
colors['bisque2'] = BISQUE2
colors['bisque3'] = BISQUE3
colors['bisque4'] = BISQUE4
colors['black'] = BLACK
colors['blanchedalmond'] = BLANCHEDALMOND
colors['blue'] = BLUE
colors['blue2'] = BLUE2
colors['blue3'] = BLUE3
colors['blue4'] = BLUE4
colors['blueviolet'] = BLUEVIOLET
colors['brick'] = BRICK
colors['brown'] = BROWN
colors['brown1'] = BROWN1
colors['brown2'] = BROWN2
colors['brown3'] = BROWN3
colors['brown4'] = BROWN4
colors['burlywood'] = BURLYWOOD
colors['burlywood1'] = BURLYWOOD1
colors['burlywood2'] = BURLYWOOD2
colors['burlywood3'] = BURLYWOOD3
colors['burlywood4'] = BURLYWOOD4
colors['burntsienna'] = BURNTSIENNA
colors['burntumber'] = BURNTUMBER
colors['cadetblue'] = CADETBLUE
colors['cadetblue1'] = CADETBLUE1
colors['cadetblue2'] = CADETBLUE2
colors['cadetblue3'] = CADETBLUE3
colors['cadetblue4'] = CADETBLUE4
colors['cadmiumorange'] = CADMIUMORANGE
colors['cadmiumyellow'] = CADMIUMYELLOW
colors['carrot'] = CARROT
colors['chartreuse1'] = CHARTREUSE1
colors['chartreuse2'] = CHARTREUSE2
colors['chartreuse3'] = CHARTREUSE3
colors['chartreuse4'] = CHARTREUSE4
colors['chocolate'] = CHOCOLATE
colors['chocolate1'] = CHOCOLATE1
colors['chocolate2'] = CHOCOLATE2
colors['chocolate3'] = CHOCOLATE3
colors['chocolate4'] = CHOCOLATE4
colors['cobalt'] = COBALT
colors['cobaltgreen'] = COBALTGREEN
colors['coldgrey'] = COLDGREY
colors['coral'] = CORAL
colors['coral1'] = CORAL1
colors['coral2'] = CORAL2
colors['coral3'] = CORAL3
colors['coral4'] = CORAL4
colors['cornflowerblue'] = CORNFLOWERBLUE
colors['cornsilk1'] = CORNSILK1
colors['cornsilk2'] = CORNSILK2
colors['cornsilk3'] = CORNSILK3
colors['cornsilk4'] = CORNSILK4
colors['crimson'] = CRIMSON
colors['cyan2'] = CYAN2
colors['cyan3'] = CYAN3
colors['cyan4'] = CYAN4
colors['darkgoldenrod'] = DARKGOLDENROD
colors['darkgoldenrod1'] = DARKGOLDENROD1
colors['darkgoldenrod2'] = DARKGOLDENROD2
colors['darkgoldenrod3'] = DARKGOLDENROD3
colors['darkgoldenrod4'] = DARKGOLDENROD4
colors['darkgray'] = DARKGRAY
colors['darkgreen'] = DARKGREEN
colors['darkkhaki'] = DARKKHAKI
colors['darkolivegreen'] = DARKOLIVEGREEN
colors['darkolivegreen1'] = DARKOLIVEGREEN1
colors['darkolivegreen2'] = DARKOLIVEGREEN2
colors['darkolivegreen3'] = DARKOLIVEGREEN3
colors['darkolivegreen4'] = DARKOLIVEGREEN4
colors['darkorange'] = DARKORANGE
colors['darkorange1'] = DARKORANGE1
colors['darkorange2'] = DARKORANGE2
colors['darkorange3'] = DARKORANGE3
colors['darkorange4'] = DARKORANGE4
colors['darkorchid'] = DARKORCHID
colors['darkorchid1'] = DARKORCHID1
colors['darkorchid2'] = DARKORCHID2
colors['darkorchid3'] = DARKORCHID3
colors['darkorchid4'] = DARKORCHID4
colors['darksalmon'] = DARKSALMON
colors['darkseagreen'] = DARKSEAGREEN
colors['darkseagreen1'] = DARKSEAGREEN1
colors['darkseagreen2'] = DARKSEAGREEN2
colors['darkseagreen3'] = DARKSEAGREEN3
colors['darkseagreen4'] = DARKSEAGREEN4
colors['darkslateblue'] = DARKSLATEBLUE
colors['darkslategray'] = DARKSLATEGRAY
colors['darkslategray1'] = DARKSLATEGRAY1
colors['darkslategray2'] = DARKSLATEGRAY2
colors['darkslategray3'] = DARKSLATEGRAY3
colors['darkslategray4'] = DARKSLATEGRAY4
colors['darkturquoise'] = DARKTURQUOISE
colors['darkviolet'] = DARKVIOLET
colors['deeppink1'] = DEEPPINK1
colors['deeppink2'] = DEEPPINK2
colors['deeppink3'] = DEEPPINK3
colors['deeppink4'] = DEEPPINK4
colors['deepskyblue1'] = DEEPSKYBLUE1
colors['deepskyblue2'] = DEEPSKYBLUE2
colors['deepskyblue3'] = DEEPSKYBLUE3
colors['deepskyblue4'] = DEEPSKYBLUE4
colors['dimgray'] = DIMGRAY
colors['dimgray'] = DIMGRAY
colors['dodgerblue1'] = DODGERBLUE1
colors['dodgerblue2'] = DODGERBLUE2
colors['dodgerblue3'] = DODGERBLUE3
colors['dodgerblue4'] = DODGERBLUE4
colors['eggshell'] = EGGSHELL
colors['emeraldgreen'] = EMERALDGREEN
colors['firebrick'] = FIREBRICK
colors['firebrick1'] = FIREBRICK1
colors['firebrick2'] = FIREBRICK2
colors['firebrick3'] = FIREBRICK3
colors['firebrick4'] = FIREBRICK4
colors['flesh'] = FLESH
colors['floralwhite'] = FLORALWHITE
colors['forestgreen'] = FORESTGREEN
colors['gainsboro'] = GAINSBORO
colors['ghostwhite'] = GHOSTWHITE
colors['gold1'] = GOLD1
colors['gold2'] = GOLD2
colors['gold3'] = GOLD3
colors['gold4'] = GOLD4
colors['goldenrod'] = GOLDENROD
colors['goldenrod1'] = GOLDENROD1
colors['goldenrod2'] = GOLDENROD2
colors['goldenrod3'] = GOLDENROD3
colors['goldenrod4'] = GOLDENROD4
colors['gray'] = GRAY
colors['gray1'] = GRAY1
colors['gray10'] = GRAY10
colors['gray11'] = GRAY11
colors['gray12'] = GRAY12
colors['gray13'] = GRAY13
colors['gray14'] = GRAY14
colors['gray15'] = GRAY15
colors['gray16'] = GRAY16
colors['gray17'] = GRAY17
colors['gray18'] = GRAY18
colors['gray19'] = GRAY19
colors['gray2'] = GRAY2
colors['gray20'] = GRAY20
colors['gray21'] = GRAY21
colors['gray22'] = GRAY22
colors['gray23'] = GRAY23
colors['gray24'] = GRAY24
colors['gray25'] = GRAY25
colors['gray26'] = GRAY26
colors['gray27'] = GRAY27
colors['gray28'] = GRAY28
colors['gray29'] = GRAY29
colors['gray3'] = GRAY3
colors['gray30'] = GRAY30
colors['gray31'] = GRAY31
colors['gray32'] = GRAY32
colors['gray33'] = GRAY33
colors['gray34'] = GRAY34
colors['gray35'] = GRAY35
colors['gray36'] = GRAY36
colors['gray37'] = GRAY37
colors['gray38'] = GRAY38
colors['gray39'] = GRAY39
colors['gray4'] = GRAY4
colors['gray40'] = GRAY40
colors['gray42'] = GRAY42
colors['gray43'] = GRAY43
colors['gray44'] = GRAY44
colors['gray45'] = GRAY45
colors['gray46'] = GRAY46
colors['gray47'] = GRAY47
colors['gray48'] = GRAY48
colors['gray49'] = GRAY49
colors['gray5'] = GRAY5
colors['gray50'] = GRAY50
colors['gray51'] = GRAY51
colors['gray52'] = GRAY52
colors['gray53'] = GRAY53
colors['gray54'] = GRAY54
colors['gray55'] = GRAY55
colors['gray56'] = GRAY56
colors['gray57'] = GRAY57
colors['gray58'] = GRAY58
colors['gray59'] = GRAY59
colors['gray6'] = GRAY6
colors['gray60'] = GRAY60
colors['gray61'] = GRAY61
colors['gray62'] = GRAY62
colors['gray63'] = GRAY63
colors['gray64'] = GRAY64
colors['gray65'] = GRAY65
colors['gray66'] = GRAY66
colors['gray67'] = GRAY67
colors['gray68'] = GRAY68
colors['gray69'] = GRAY69
colors['gray7'] = GRAY7
colors['gray70'] = GRAY70
colors['gray71'] = GRAY71
colors['gray72'] = GRAY72
colors['gray73'] = GRAY73
colors['gray74'] = GRAY74
colors['gray75'] = GRAY75
colors['gray76'] = GRAY76
colors['gray77'] = GRAY77
colors['gray78'] = GRAY78
colors['gray79'] = GRAY79
colors['gray8'] = GRAY8
colors['gray80'] = GRAY80
colors['gray81'] = GRAY81
colors['gray82'] = GRAY82
colors['gray83'] = GRAY83
colors['gray84'] = GRAY84
colors['gray85'] = GRAY85
colors['gray86'] = GRAY86
colors['gray87'] = GRAY87
colors['gray88'] = GRAY88
colors['gray89'] = GRAY89
colors['gray9'] = GRAY9
colors['gray90'] = GRAY90
colors['gray91'] = GRAY91
colors['gray92'] = GRAY92
colors['gray93'] = GRAY93
colors['gray94'] = GRAY94
colors['gray95'] = GRAY95
colors['gray97'] = GRAY97
colors['gray98'] = GRAY98
colors['gray99'] = GRAY99
colors['green'] = GREEN
colors['green1'] = GREEN1
colors['green2'] = GREEN2
colors['green3'] = GREEN3
colors['green4'] = GREEN4
colors['greenyellow'] = GREENYELLOW
colors['honeydew1'] = HONEYDEW1
colors['honeydew2'] = HONEYDEW2
colors['honeydew3'] = HONEYDEW3
colors['honeydew4'] = HONEYDEW4
colors['hotpink'] = HOTPINK
colors['hotpink1'] = HOTPINK1
colors['hotpink2'] = HOTPINK2
colors['hotpink3'] = HOTPINK3
colors['hotpink4'] = HOTPINK4
colors['indianred'] = INDIANRED
colors['indianred'] = INDIANRED
colors['indianred1'] = INDIANRED1
colors['indianred2'] = INDIANRED2
colors['indianred3'] = INDIANRED3
colors['indianred4'] = INDIANRED4
colors['indigo'] = INDIGO
colors['ivory1'] = IVORY1
colors['ivory2'] = IVORY2
colors['ivory3'] = IVORY3
colors['ivory4'] = IVORY4
colors['ivoryblack'] = IVORYBLACK
colors['khaki'] = KHAKI
colors['khaki1'] = KHAKI1
colors['khaki2'] = KHAKI2
colors['khaki3'] = KHAKI3
colors['khaki4'] = KHAKI4
colors['lavender'] = LAVENDER
colors['lavenderblush1'] = LAVENDERBLUSH1
colors['lavenderblush2'] = LAVENDERBLUSH2
colors['lavenderblush3'] = LAVENDERBLUSH3
colors['lavenderblush4'] = LAVENDERBLUSH4
colors['lawngreen'] = LAWNGREEN
colors['lemonchiffon1'] = LEMONCHIFFON1
colors['lemonchiffon2'] = LEMONCHIFFON2
colors['lemonchiffon3'] = LEMONCHIFFON3
colors['lemonchiffon4'] = LEMONCHIFFON4
colors['lightblue'] = LIGHTBLUE
colors['lightblue1'] = LIGHTBLUE1
colors['lightblue2'] = LIGHTBLUE2
colors['lightblue3'] = LIGHTBLUE3
colors['lightblue4'] = LIGHTBLUE4
colors['lightcoral'] = LIGHTCORAL
colors['lightcyan1'] = LIGHTCYAN1
colors['lightcyan2'] = LIGHTCYAN2
colors['lightcyan3'] = LIGHTCYAN3
colors['lightcyan4'] = LIGHTCYAN4
colors['lightgoldenrod1'] = LIGHTGOLDENROD1
colors['lightgoldenrod2'] = LIGHTGOLDENROD2
colors['lightgoldenrod3'] = LIGHTGOLDENROD3
colors['lightgoldenrod4'] = LIGHTGOLDENROD4
colors['lightgoldenrodyellow'] = LIGHTGOLDENRODYELLOW
colors['lightgrey'] = LIGHTGREY
colors['lightpink'] = LIGHTPINK
colors['lightpink1'] = LIGHTPINK1
colors['lightpink2'] = LIGHTPINK2
colors['lightpink3'] = LIGHTPINK3
colors['lightpink4'] = LIGHTPINK4
colors['lightsalmon1'] = LIGHTSALMON1
colors['lightsalmon2'] = LIGHTSALMON2
colors['lightsalmon3'] = LIGHTSALMON3
colors['lightsalmon4'] = LIGHTSALMON4
colors['lightseagreen'] = LIGHTSEAGREEN
colors['lightskyblue'] = LIGHTSKYBLUE
colors['lightskyblue1'] = LIGHTSKYBLUE1
colors['lightskyblue2'] = LIGHTSKYBLUE2
colors['lightskyblue3'] = LIGHTSKYBLUE3
colors['lightskyblue4'] = LIGHTSKYBLUE4
colors['lightslateblue'] = LIGHTSLATEBLUE
colors['lightslategray'] = LIGHTSLATEGRAY
colors['lightsteelblue'] = LIGHTSTEELBLUE
colors['lightsteelblue1'] = LIGHTSTEELBLUE1
colors['lightsteelblue2'] = LIGHTSTEELBLUE2
colors['lightsteelblue3'] = LIGHTSTEELBLUE3
colors['lightsteelblue4'] = LIGHTSTEELBLUE4
colors['lightyellow1'] = LIGHTYELLOW1
colors['lightyellow2'] = LIGHTYELLOW2
colors['lightyellow3'] = LIGHTYELLOW3
colors['lightyellow4'] = LIGHTYELLOW4
colors['limegreen'] = LIMEGREEN
colors['linen'] = LINEN
colors['magenta'] = MAGENTA
colors['magenta2'] = MAGENTA2
colors['magenta3'] = MAGENTA3
colors['magenta4'] = MAGENTA4
colors['manganeseblue'] = MANGANESEBLUE
colors['maroon'] = MAROON
colors['maroon1'] = MAROON1
colors['maroon2'] = MAROON2
colors['maroon3'] = MAROON3
colors['maroon4'] = MAROON4
colors['mediumorchid'] = MEDIUMORCHID
colors['mediumorchid1'] = MEDIUMORCHID1
colors['mediumorchid2'] = MEDIUMORCHID2
colors['mediumorchid3'] = MEDIUMORCHID3
colors['mediumorchid4'] = MEDIUMORCHID4
colors['mediumpurple'] = MEDIUMPURPLE
colors['mediumpurple1'] = MEDIUMPURPLE1
colors['mediumpurple2'] = MEDIUMPURPLE2
colors['mediumpurple3'] = MEDIUMPURPLE3
colors['mediumpurple4'] = MEDIUMPURPLE4
colors['mediumseagreen'] = MEDIUMSEAGREEN
colors['mediumslateblue'] = MEDIUMSLATEBLUE
colors['mediumspringgreen'] = MEDIUMSPRINGGREEN
colors['mediumturquoise'] = MEDIUMTURQUOISE
colors['mediumvioletred'] = MEDIUMVIOLETRED
colors['melon'] = MELON
colors['midnightblue'] = MIDNIGHTBLUE
colors['mint'] = MINT
colors['mintcream'] = MINTCREAM
colors['mistyrose1'] = MISTYROSE1
colors['mistyrose2'] = MISTYROSE2
colors['mistyrose3'] = MISTYROSE3
colors['mistyrose4'] = MISTYROSE4
colors['moccasin'] = MOCCASIN
colors['navajowhite1'] = NAVAJOWHITE1
colors['navajowhite2'] = NAVAJOWHITE2
colors['navajowhite3'] = NAVAJOWHITE3
colors['navajowhite4'] = NAVAJOWHITE4
colors['navy'] = NAVY
colors['oldlace'] = OLDLACE
colors['olive'] = OLIVE
colors['olivedrab'] = OLIVEDRAB
colors['olivedrab1'] = OLIVEDRAB1
colors['olivedrab2'] = OLIVEDRAB2
colors['olivedrab3'] = OLIVEDRAB3
colors['olivedrab4'] = OLIVEDRAB4
colors['orange'] = ORANGE
colors['orange1'] = ORANGE1
colors['orange2'] = ORANGE2
colors['orange3'] = ORANGE3
colors['orange4'] = ORANGE4
colors['orangered1'] = ORANGERED1
colors['orangered2'] = ORANGERED2
colors['orangered3'] = ORANGERED3
colors['orangered4'] = ORANGERED4
colors['orchid'] = ORCHID
colors['orchid1'] = ORCHID1
colors['orchid2'] = ORCHID2
colors['orchid3'] = ORCHID3
colors['orchid4'] = ORCHID4
colors['palegoldenrod'] = PALEGOLDENROD
colors['palegreen'] = PALEGREEN
colors['palegreen1'] = PALEGREEN1
colors['palegreen2'] = PALEGREEN2
colors['palegreen3'] = PALEGREEN3
colors['palegreen4'] = PALEGREEN4
colors['paleturquoise1'] = PALETURQUOISE1
colors['paleturquoise2'] = PALETURQUOISE2
colors['paleturquoise3'] = PALETURQUOISE3
colors['paleturquoise4'] = PALETURQUOISE4
colors['palevioletred'] = PALEVIOLETRED
colors['palevioletred1'] = PALEVIOLETRED1
colors['palevioletred2'] = PALEVIOLETRED2
colors['palevioletred3'] = PALEVIOLETRED3
colors['palevioletred4'] = PALEVIOLETRED4
colors['papayawhip'] = PAPAYAWHIP
colors['peachpuff1'] = PEACHPUFF1
colors['peachpuff2'] = PEACHPUFF2
colors['peachpuff3'] = PEACHPUFF3
colors['peachpuff4'] = PEACHPUFF4
colors['peacock'] = PEACOCK
colors['pink'] = PINK
colors['pink1'] = PINK1
colors['pink2'] = PINK2
colors['pink3'] = PINK3
colors['pink4'] = PINK4
colors['plum'] = PLUM
colors['plum1'] = PLUM1
colors['plum2'] = PLUM2
colors['plum3'] = PLUM3
colors['plum4'] = PLUM4
colors['powderblue'] = POWDERBLUE
colors['purple'] = PURPLE
colors['purple1'] = PURPLE1
colors['purple2'] = PURPLE2
colors['purple3'] = PURPLE3
colors['purple4'] = PURPLE4
colors['raspberry'] = RASPBERRY
colors['rawsienna'] = RAWSIENNA
colors['red1'] = RED1
colors['red2'] = RED2
colors['red3'] = RED3
colors['red4'] = RED4
colors['rosybrown'] = ROSYBROWN
colors['rosybrown1'] = ROSYBROWN1
colors['rosybrown2'] = ROSYBROWN2
colors['rosybrown3'] = ROSYBROWN3
colors['rosybrown4'] = ROSYBROWN4
colors['royalblue'] = ROYALBLUE
colors['royalblue1'] = ROYALBLUE1
colors['royalblue2'] = ROYALBLUE2
colors['royalblue3'] = ROYALBLUE3
colors['royalblue4'] = ROYALBLUE4
colors['salmon'] = SALMON
colors['salmon1'] = SALMON1
colors['salmon2'] = SALMON2
colors['salmon3'] = SALMON3
colors['salmon4'] = SALMON4
colors['sandybrown'] = SANDYBROWN
colors['sapgreen'] = SAPGREEN
colors['seagreen1'] = SEAGREEN1
colors['seagreen2'] = SEAGREEN2
colors['seagreen3'] = SEAGREEN3
colors['seagreen4'] = SEAGREEN4
colors['seashell1'] = SEASHELL1
colors['seashell2'] = SEASHELL2
colors['seashell3'] = SEASHELL3
colors['seashell4'] = SEASHELL4
colors['sepia'] = SEPIA
colors['sgibeet'] = SGIBEET
colors['sgibrightgray'] = SGIBRIGHTGRAY
colors['sgichartreuse'] = SGICHARTREUSE
colors['sgidarkgray'] = SGIDARKGRAY
colors['sgigray12'] = SGIGRAY12
colors['sgigray16'] = SGIGRAY16
colors['sgigray32'] = SGIGRAY32
colors['sgigray36'] = SGIGRAY36
colors['sgigray52'] = SGIGRAY52
colors['sgigray56'] = SGIGRAY56
colors['sgigray72'] = SGIGRAY72
colors['sgigray76'] = SGIGRAY76
colors['sgigray92'] = SGIGRAY92
colors['sgigray96'] = SGIGRAY96
colors['sgilightblue'] = SGILIGHTBLUE
colors['sgilightgray'] = SGILIGHTGRAY
colors['sgiolivedrab'] = SGIOLIVEDRAB
colors['sgisalmon'] = SGISALMON
colors['sgislateblue'] = SGISLATEBLUE
colors['sgiteal'] = SGITEAL
colors['sienna'] = SIENNA
colors['sienna1'] = SIENNA1
colors['sienna2'] = SIENNA2
colors['sienna3'] = SIENNA3
colors['sienna4'] = SIENNA4
colors['silver'] = SILVER
colors['skyblue'] = SKYBLUE
colors['skyblue1'] = SKYBLUE1
colors['skyblue2'] = SKYBLUE2
colors['skyblue3'] = SKYBLUE3
colors['skyblue4'] = SKYBLUE4
colors['slateblue'] = SLATEBLUE
colors['slateblue1'] = SLATEBLUE1
colors['slateblue2'] = SLATEBLUE2
colors['slateblue3'] = SLATEBLUE3
colors['slateblue4'] = SLATEBLUE4
colors['slategray'] = SLATEGRAY
colors['slategray1'] = SLATEGRAY1
colors['slategray2'] = SLATEGRAY2
colors['slategray3'] = SLATEGRAY3
colors['slategray4'] = SLATEGRAY4
colors['snow1'] = SNOW1
colors['snow2'] = SNOW2
colors['snow3'] = SNOW3
colors['snow4'] = SNOW4
colors['springgreen'] = SPRINGGREEN
colors['springgreen1'] = SPRINGGREEN1
colors['springgreen2'] = SPRINGGREEN2
colors['springgreen3'] = SPRINGGREEN3
colors['steelblue'] = STEELBLUE
colors['steelblue1'] = STEELBLUE1
colors['steelblue2'] = STEELBLUE2
colors['steelblue3'] = STEELBLUE3
colors['steelblue4'] = STEELBLUE4
colors['tan'] = TAN
colors['tan1'] = TAN1
colors['tan2'] = TAN2
colors['tan3'] = TAN3
colors['tan4'] = TAN4
colors['teal'] = TEAL
colors['thistle'] = THISTLE
colors['thistle1'] = THISTLE1
colors['thistle2'] = THISTLE2
colors['thistle3'] = THISTLE3
colors['thistle4'] = THISTLE4
colors['tomato1'] = TOMATO1
colors['tomato2'] = TOMATO2
colors['tomato3'] = TOMATO3
colors['tomato4'] = TOMATO4
colors['turquoise'] = TURQUOISE
colors['turquoise1'] = TURQUOISE1
colors['turquoise2'] = TURQUOISE2
colors['turquoise3'] = TURQUOISE3
colors['turquoise4'] = TURQUOISE4
colors['turquoiseblue'] = TURQUOISEBLUE
colors['violet'] = VIOLET
colors['violetred'] = VIOLETRED
colors['violetred1'] = VIOLETRED1
colors['violetred2'] = VIOLETRED2
colors['violetred3'] = VIOLETRED3
colors['violetred4'] = VIOLETRED4
colors['warmgrey'] = WARMGREY
colors['wheat'] = WHEAT
colors['wheat1'] = WHEAT1
colors['wheat2'] = WHEAT2
colors['wheat3'] = WHEAT3
colors['wheat4'] = WHEAT4
colors['white'] = WHITE
colors['whitesmoke'] = WHITESMOKE
colors['whitesmoke'] = WHITESMOKE
colors['yellow1'] = YELLOW1
colors['yellow2'] = YELLOW2
colors['yellow3'] = YELLOW3
colors['yellow4'] = YELLOW4
colors = OrderedDict(sorted(colors.items(), key=lambda t: t[0]))



colors

from turtle import *

tur.clear()

from turtle import *
import turtle as tur
def petal1(t1, r, ang):
    for i in range(2):
        t1.circle(r, ang)
        t1.left(180 - ang)
def flower1(t1, n, r, ang):
    for i in range(n):
        petal1(t1, r, ang)
        t1.left(360.0 / n)
def move(t1, len):
        win = tur.Screen()
        win.bgcolor("cyan")
        t1.pu()
        t1.fd(len)
        t1.pd()
ws = tur.Turtle()
ws.speed(100)
ws.color("pink")
ws.shape("turtle")
move(ws, -150)
ws.begin_fill()
flower1(ws, 7, 50.0, 50.0)
ws.end_fill()
ws.color("blue")
move(ws, 150)
ws.begin_fill()
flower1(ws, 9, 20.0, 60.0)
ws.end_fill()
ws.color("green")
move(ws, 150)
ws.begin_fill()
flower1(ws, 13, 60.0, 40.0)
ws.end_fill()
tur.mainloop()
tur.Screen().exitonclick()
tur.done()

https://onecompiler.com/python/3xngvdswe

from turtle import*

bgcolor('darkblue')
speed(0)
hideturtle()
for i in range(420):
  color('red')
  circle(i)
  color('orange')
  circle(i*0.8)
  right(3)
  forward(3)
done()

"""
H-Tree Fractal using recursion and Turtle Graphics.
Robin Andrews - https://compucademy.net/
"""

import turtle

SPEED = 5
BG_COLOR = "blue"
PEN_COLOR = "lightgreen"
SCREEN_WIDTH = 800
SCREEN_HEIGHT = 800
DRAWING_WIDTH = 700
DRAWING_HEIGHT = 700
PEN_WIDTH = 5
TITLE = "H-Tree Fractal with Python Turtle Graphics"
FRACTAL_DEPTH = 3


def draw_line(tur, pos1, pos2):
    # print("Drawing from", pos1, "to", pos2)  # Uncomment for tracing the algorithm.
    tur.penup()
    tur.goto(pos1[0], pos1[1])
    tur.pendown()
    tur.goto(pos2[0], pos2[1])


def recursive_draw(tur, x, y, width, height, count):
    draw_line(
        tur,
        [x + width * 0.25, height // 2 + y],
        [x + width * 0.75, height // 2 + y],
    )
    draw_line(
        tur,
        [x + width * 0.25, (height * 0.5) // 2 + y],
        [x + width * 0.25, (height * 1.5) // 2 + y],
    )
    draw_line(
        tur,
        [x + width * 0.75, (height * 0.5) // 2 + y],
        [x + width * 0.75, (height * 1.5) // 2 + y],
    )

    if count <= 0:  # The base case
        return
    else:  # The recursive step
        count -= 1
        # Top left
        recursive_draw(tur, x, y, width // 2, height // 2, count)
        # Top right
        recursive_draw(tur, x + width // 2, y, width // 2, height // 2, count)
        # Bottom left
        recursive_draw(tur, x, y + width // 2, width // 2, height // 2, count)
        # Bottom right
        recursive_draw(tur, x + width // 2, y + width // 2, width // 2, height // 2, count)


if __name__ == "__main__":
    # Screen setup
    screen = turtle.Screen()
    screen.setup(SCREEN_WIDTH, SCREEN_HEIGHT)
    screen.title(TITLE)
    screen.bgcolor(BG_COLOR)

    # Turtle artist (pen) setup
    artist = turtle.Turtle()
    artist.hideturtle()
    artist.pensize(PEN_WIDTH)
    artist.color(PEN_COLOR)
    artist.speed(SPEED)

    # Initial call to recursive draw function
    recursive_draw(artist, - DRAWING_WIDTH / 2, - DRAWING_HEIGHT / 2, DRAWING_WIDTH, DRAWING_HEIGHT, FRACTAL_DEPTH)

    # Every Python Turtle program needs this (or an equivalent) to work correctly.
    turtle.done()

import os
import importlib
import turtle
importlib.reload(turtle)
sc = Screen()
sc.setup(600,600)
image = os.path.expanduser("alien.gif")
sc.register_shape(image)

t = turtle.Turtle()
t.shape(image)
sc.exitonclick()

import time
from turtle import *
import turtle
def recurse(n):
    inc = 0
    if n>0:
        inc = inc+1
        left(10)
        
        if n<100 :
            forward(5)
        if n<200:
            forward(5)    
                
        else:
            forward(5+inc)
        recurse(n-1)

recurse(300)
turtle.exitonclick()

import sys
from turtle import Turtle, Screen

def hilbert_curve(n, turtle, angle=90):
    if n <= 0:
        return

    turtle.left(angle)
    hilbert_curve(n - 1, turtle, -angle)
    turtle.forward(1)
    turtle.right(angle)
    hilbert_curve(n - 1, turtle, angle)
    turtle.forward(1)
    hilbert_curve(n - 1, turtle, angle)
    turtle.right(angle)
    turtle.forward(1)
    hilbert_curve(n - 1, turtle, -angle)
    turtle.left(angle)

depth = int(6)
size = 2 ** depth

screen = Screen()
screen.setworldcoordinates(0, 0, size, size)

yertle = Turtle('turtle')
yertle.speed('fastest')
yertle.penup()
yertle.goto(0.5, 0.5)
yertle.pendown()

hilbert_curve(depth, yertle)

yertle.hideturtle()

screen.exitonclick()

def iterative():
    # np being the previous value
    np = 14

    # starts from 2 since the first case is already assumed to known (14)
    # go up to the 100th number
    for n in range(2, 101):
        result = np - 4
        print("%5d" % n, end="")
        print("%12d" % result)
        np = result
    return 0

iterative()

import turtle
tur.bye()
#tur.done()
#tur.clear()
tur.reset()
turtle.done()
tur.clearscreen()
tur.exitonclick()
turtle.exitonclick()
turtle.done()
turtle.reset()

#https://cs111.wellesley.edu/~cs111/archive/cs111_spring15/public_html/notes/lectures/10_turtle_recursion_4up.pdf
    
import turtle

def drawTriangle(points,color,myTurtle):
    myTurtle.fillcolor(color)
    myTurtle.up()
    myTurtle.goto(points[0][0],points[0][1])
    myTurtle.down()
    myTurtle.begin_fill()
    myTurtle.goto(points[1][0],points[1][1])
    myTurtle.goto(points[2][0],points[2][1])
    myTurtle.goto(points[0][0],points[0][1])
    myTurtle.end_fill()

def getMid(p1,p2):
    return ( (p1[0]+p2[0]) / 2, (p1[1] + p2[1]) / 2)

def sierpinski(points,degree,myTurtle):
    colormap = ['blue','red','green','white','yellow','violet','orange']
    drawTriangle(points,colormap[degree],myTurtle)
    if degree > 0:
        sierpinski([points[0],
                        getMid(points[0], points[1]),
                        getMid(points[0], points[2])],
                   degree-1, myTurtle)
        sierpinski([points[1],
                        getMid(points[0], points[1]),
                        getMid(points[1], points[2])],
                   degree-1, myTurtle)
        sierpinski([points[2],
                        getMid(points[2], points[1]),
                        getMid(points[0], points[2])],
                   degree-1, myTurtle)

def main():
   myTurtle = turtle.Turtle()
   myWin = turtle.Screen()
   myPoints = [[-100,-50],[0,100],[100,-50]]
   sierpinski(myPoints,3,myTurtle)
   myWin.exitonclick()

main()


#https://runestone.academy/ns/books/published/pythonds/Recursion/pythondsintro-VisualizingRecursion.html
import turtle

def tree(branchLen,t):
    if branchLen > 5:
        t.forward(branchLen)
        t.right(20)
        tree(branchLen-15,t)
        t.left(40)
        tree(branchLen-15,t)
        t.right(20)
        t.backward(branchLen)

def main():
    t = turtle.Turtle()
    myWin = turtle.Screen()
    t.left(90)
    t.up()
    t.backward(100)
    t.down()
    t.color("green")
    tree(75,t)
    myWin.exitonclick()

main()


from turtle import *
color('red', 'yellow')
begin_fill()
while True:
    forward(200)
    left(170)
    if abs(pos()) < 1:
        break
end_fill()
done()

from svg_turtle import SvgTurtle


def draw_spiral(t):
    t.fillcolor('blue')
    t.begin_fill()
    for i in range(20):
        d = 50 + i*i*1.5
        t.pencolor(0, 0.05*i, 0)
        t.width(i)
        t.forward(d)
        t.right(144)
    t.end_fill()


def write_file(draw_func, filename, width, height):
    t = SvgTurtle(width, height)
    draw_func(t)
    t.save_as(filename)


def main():
    write_file(draw_spiral, 'example.svg', 500, 500)
    print('Done.')


if __name__ == '__main__':
    main()


!ls *.gif

import turtle

screen = turtle.Screen()
screen.title("U.S. States Game")
image = "10.gif"
screen.addshape(image)
turtle.shape(image)

screen.exitonclick()
#turtle.done()
turtle.reset()

turtle.done()
turtle.reset()

https://casual-effects.com/codeheart/turtle/
https://docs.python.org/3/library/turtle.html

%%javascript
// You can find the Turtle API reference here: https://turtletoy.net/syntax
var Canvas = document.getElementById('myChart')

Canvas.setpenopacity(1);

// Global code will be evaluated once.
const turtle = new Turtle();
turtle.penup();
turtle.goto(-50,-20);
turtle.pendown();

// The walk function will be called until it returns false.
function au(x){
x=x+10
return x
}
function walk(i) {
    turtle.forward(106+au(2));
    turtle.right(144+au(2));
    return i < 4;
}

<div id="myChart"></div>

# Python program to draw square
# using Turtle Programming
import turtle
skk = turtle.Turtle()
 
for i in range(4):
    skk.forward(50)
    skk.right(90)
     
turtle.done()

If you are using PHP, try adding the following code at the beginning of the php file:

If you are using localhost, try this:

header("Access-Control-Allow-Origin: *");
If you are using external domains such as server, try this:

header("Access-Control-Allow-Origin: http://www.website.com");

turtle.mainloop() aka turtle.Screen().mainloop() Turns control over to tkinter's event loop. Usually, a lack of turtle.Screen().mainloop() (or turtle.Screen().exitonclick(), etc.) will cause the window to close just because the program will end, closing everything. This, or one of its variants, should be the last statement in a turtle graphics program unless the script is run from within Python IDLE -n.

turtle.done() (Does not close window nor reset anything.) A synonym for turtle.mainloop()

turtle.clear() Deletes everything this turtle has drawn (not just the last thing). Otherwise doesn't affect the state of the turtle.

turtle.reset() Does a turtle.clear() and then resets this turtle's state (i.e. direction, position, etc.)

turtle.clearscreen() aka turtle.Screen().clear() Deletes all drawing and all turtles, reseting the window to it's original state.

turtle.resetscreen() aka turtle.Screen().reset() Resets all turtles on the screen to their initial state.

turtle.bye() aka turtle.Screen().bye() Closes the turtle graphics window. I don't see a way to use any turtle graphics commands after this is invoked.

turtle.exitonclick() aka turtle.Screen().exitonclick() After binding the screen click event to do a turtle.Screen().bye() invokes turtle.Screen().mainloop()

It's not clear that you can close and reopen the graphics window from within turtle without dropping down to the tkinter level that underpins turtle (and Zelle's graphics.py)

For purposes of starting a new hand in your blackjack game, I'd guess turtle.reset() or turtle.resetscreen() are your best bet.

https://trinket.io/python/88dd6c94d1

from PIL import Image
im = Image.open("/home/jack/Desktop/R-Studio/qt-colors.png") 
im

# http://interactivepython.org/runestone/static/thinkcspy/index.html

import turtle
from myturtle import MyTurtle, create_turtles, move_turtles, writer

number_of_turtles = 10

screen = turtle.Screen()

def draw_shape(x, y, n = 20, clear = True):
  if clear:
    writer.clear()
  screen.tracer(0)
  for turtle in screen.turtles():
    turtle.penup()
    turtle.goto(x,y)
    turtle.pendown()
  screen.tracer(1)
  for i in range(n):
    screen.tracer(0)
    move_turtles(screen)
    screen.tracer(1)

create_turtles(screen, number_of_turtles)

draw_shape(0,-150, clear = False)

screen.onclick(draw_shape)

screen.listen()
turtle.done()

turtle.done()
turtle.reset()
turtle.clear()

turtle.clear()

!python myturtle.py

# Inspired by Brad Miller's illustration for How to Think like a
# Computer Scientist:
# http://interactivepython.org/runestone/static/thinkcspy/index.html

import turtle
from myturtle import MyTurtle, create_turtles, move_turtles, writer

number_of_turtles = 10

screen = turtle.Screen()

def draw_shape(x, y, n = 20, clear = True):
    if clear:
        writer.clear()
        screen.tracer(0)
    for turtle in screen.turtles():
        turtle.penup()
        turtle.goto(x,y)
        turtle.pendown()
        screen.tracer(1)
    for i in range(n):
        screen.tracer(0)
        move_turtles(screen)
        screen.tracer(1)

create_turtles(screen, number_of_turtles)
draw_shape(0,-150, clear = False)
screen.onclick(draw_shape)
screen.listen()
turtle.done()

import turtle
import time


def draw_rect(t):
    for i in range(0, 4):
        t.forward(100)
        t.right(90)

    time.sleep(3)
    turtle.clearscreen()
    t.screen.exitonclick()
    turtle.TurtleScreen._RUNNING = True


def draw_circle(t):
    for i in range(0, 200):
        t.circle(50+i)  
        t.left(120+i)
        t.forward(100+i)    

    time.sleep(3)
    turtle.clearscreen()
    t.screen.exitonclick()
    turtle.TurtleScreen._RUNNING = True


def draw_triangle(t):
    t.forward(100)

    for i in range(0, 200):
        t.left(120+i)
        t.forward(100+i)

    time.sleep(3)
    turtle.clearscreen()
    t.screen.exitonclick()
    turtle.TurtleScreen._RUNNING = True


draw_triangle(turtle)

draw_circle(turtle)

%%writefile myturtle.py
import turtle
from time import sleep
class MyTurtle(turtle.Turtle):
    def __init__(self, screen = turtle.Screen()):
        turtle.Turtle.__init__(self, screen)
        self.hideturtle()
    
    def create_turtles(screen, n = 10):
        for i in range(n):
            MyTurtle(screen)
    
    def move_turtles(screen, dist=10, angle = 4):
        for i, turtle in enumerate(screen.turtles()):
            turtle.left(angle*(1+i))
            turtle.forward(dist)
            x, y = turtle.pos()
            try:
                turtle.color(abs(x), abs(y), abs(x+y))
            except:
                pass
    
writer = MyTurtle()
writer.penup()
writer.goto(0,100)
writer.write("Click Me!", font=("Arial",30), align = "center")
sleep(10)

width = 1000
height = 1000

from drawperlin import *
#import Painter
#import utils
width = 1000
height = 1000
p = Painter.Painter(width, height)
# Allow smooth drawing
p.setRenderHint(p.Antialiasing)
# Draw the background color
p.fillRect(0, 0, width, height, QColor(255,211,155))
# Set the pen color
p.setPen(QPen(QColor(0, 120, 180, 10), 3))
x_i = 500
y_i = 500
length=100
angle=0
x_f = x_i + length*math.cos(math.radians(angle))
y_f = y_i - length*math.sin(math.radians(angle))
x_f = int(x_f)
y_f = int(y_f)
for inc in range(0,500):
    incs = 0
    if inc %5 == 0:incs=incs-3
    p.drawLine(x_i+inc, y_i+inc, x_f-incs, y_f-incs)

v_path = "ZZZ"
 # Save the vector image
save(p, fname=v_path, folder='XXXX/New/')
#p.setPen(QColor(120,120,0))

import turtle
import turtle as t
import time


def draw_rect(t):
    for i in range(0, 4):
        t.forward(100)
        t.right(90)

    time.sleep(3)
    turtle.clearscreen()
    t.screen.exitonclick()
    turtle.TurtleScreen._RUNNING = True


def draw_circle(t):
    t.circle(50)

    time.sleep(3)
    turtle.clearscreen()
    t.screen.exitonclick()
    turtle.TurtleScreen._RUNNING = True


def draw_triangle(t):
    t.forward(100)

    for i in range(0, 2):
        t.left(120)
        t.forward(100)

    time.sleep(3)
    turtle.clearscreen()
    t.screen.exitonclick()
    turtle.TurtleScreen._RUNNING = True


t=3
draw_triangle(t)

from turtle import *
import turtle as tur
def petal1(t1, r, ang):
    for i in range(2):
        t1.circle(r, ang)
        t1.left(180 - ang)
def flower1(t1, n, r, ang):
    for i in range(n):
        petal1(t1, r, ang)
        t1.left(360.0 / n)
def move(t1, len):
        win = tur.Screen()
        win.bgcolor("cyan")
        t1.pu()
        t1.fd(len)
        t1.pd()
ws = tur.Turtle()
ws.speed(100)
ws.color("pink")
ws.shape("turtle")
move(ws, -150)
ws.begin_fill()
flower1(ws, 7, 50.0, 50.0)
ws.end_fill()
ws.color("blue")
move(ws, 150)
ws.begin_fill()
flower1(ws, 9, 20.0, 60.0)
ws.end_fill()
ws.color("green")
move(ws, 150)
ws.begin_fill()
flower1(ws, 13, 60.0, 40.0)
ws.end_fill()
tur.mainloop()
tur.Screen().exitonclick()
tur.done()

https://onecompiler.com/python/3xngvdswe

from turtle import*

bgcolor('darkblue')
speed(0)
hideturtle()
for i in range(420):
  color('red')
  circle(i)
  color('orange')
  circle(i*0.8)
  right(3)
  forward(3)
done()

"""
H-Tree Fractal using recursion and Turtle Graphics.
Robin Andrews - https://compucademy.net/
"""

import turtle

SPEED = 5
BG_COLOR = "blue"
PEN_COLOR = "lightgreen"
SCREEN_WIDTH = 800
SCREEN_HEIGHT = 800
DRAWING_WIDTH = 700
DRAWING_HEIGHT = 700
PEN_WIDTH = 5
TITLE = "H-Tree Fractal with Python Turtle Graphics"
FRACTAL_DEPTH = 3


def draw_line(tur, pos1, pos2):
    # print("Drawing from", pos1, "to", pos2)  # Uncomment for tracing the algorithm.
    tur.penup()
    tur.goto(pos1[0], pos1[1])
    tur.pendown()
    tur.goto(pos2[0], pos2[1])


def recursive_draw(tur, x, y, width, height, count):
    draw_line(
        tur,
        [x + width * 0.25, height // 2 + y],
        [x + width * 0.75, height // 2 + y],
    )
    draw_line(
        tur,
        [x + width * 0.25, (height * 0.5) // 2 + y],
        [x + width * 0.25, (height * 1.5) // 2 + y],
    )
    draw_line(
        tur,
        [x + width * 0.75, (height * 0.5) // 2 + y],
        [x + width * 0.75, (height * 1.5) // 2 + y],
    )

    if count <= 0:  # The base case
        return
    else:  # The recursive step
        count -= 1
        # Top left
        recursive_draw(tur, x, y, width // 2, height // 2, count)
        # Top right
        recursive_draw(tur, x + width // 2, y, width // 2, height // 2, count)
        # Bottom left
        recursive_draw(tur, x, y + width // 2, width // 2, height // 2, count)
        # Bottom right
        recursive_draw(tur, x + width // 2, y + width // 2, width // 2, height // 2, count)


if __name__ == "__main__":
    # Screen setup
    screen = turtle.Screen()
    screen.setup(SCREEN_WIDTH, SCREEN_HEIGHT)
    screen.title(TITLE)
    screen.bgcolor(BG_COLOR)

    # Turtle artist (pen) setup
    artist = turtle.Turtle()
    artist.hideturtle()
    artist.pensize(PEN_WIDTH)
    artist.color(PEN_COLOR)
    artist.speed(SPEED)

    # Initial call to recursive draw function
    recursive_draw(artist, - DRAWING_WIDTH / 2, - DRAWING_HEIGHT / 2, DRAWING_WIDTH, DRAWING_HEIGHT, FRACTAL_DEPTH)

    # Every Python Turtle program needs this (or an equivalent) to work correctly.
    turtle.done()

import os
import importlib
import turtle
importlib.reload(turtle)
sc = Screen()
sc.setup(600,600)
image = os.path.expanduser("alien.gif")
sc.register_shape(image)

t = turtle.Turtle()
t.shape(image)
sc.exitonclick()

import time
from turtle import *
import turtle
def recurse(n):
    inc = 0
    if n>0:
        inc = inc+1
        left(10)
        
        if n<100 :
            forward(5)
        if n<200:
            forward(5)    
                
        else:
            forward(5+inc)
        recurse(n-1)

recurse(300)
turtle.exitonclick()

import sys
from turtle import Turtle, Screen

def hilbert_curve(n, turtle, angle=90):
    if n <= 0:
        return

    turtle.left(angle)
    hilbert_curve(n - 1, turtle, -angle)
    turtle.forward(1)
    turtle.right(angle)
    hilbert_curve(n - 1, turtle, angle)
    turtle.forward(1)
    hilbert_curve(n - 1, turtle, angle)
    turtle.right(angle)
    turtle.forward(1)
    hilbert_curve(n - 1, turtle, -angle)
    turtle.left(angle)

depth = int(6)
size = 2 ** depth

screen = Screen()
screen.setworldcoordinates(0, 0, size, size)

yertle = Turtle('turtle')
yertle.speed('fastest')
yertle.penup()
yertle.goto(0.5, 0.5)
yertle.pendown()

hilbert_curve(depth, yertle)

yertle.hideturtle()

screen.exitonclick()

def iterative():
    # np being the previous value
    np = 14

    # starts from 2 since the first case is already assumed to known (14)
    # go up to the 100th number
    for n in range(2, 101):
        result = np - 4
        print("%5d" % n, end="")
        print("%12d" % result)
        np = result
    return 0

iterative()

import turtle
tur.bye()
#tur.done()
#tur.clear()
tur.reset()
turtle.done()
tur.clearscreen()
tur.exitonclick()
turtle.exitonclick()
turtle.done()
turtle.reset()

#https://cs111.wellesley.edu/~cs111/archive/cs111_spring15/public_html/notes/lectures/10_turtle_recursion_4up.pdf
    
import turtle

def drawTriangle(points,color,myTurtle):
    myTurtle.fillcolor(color)
    myTurtle.up()
    myTurtle.goto(points[0][0],points[0][1])
    myTurtle.down()
    myTurtle.begin_fill()
    myTurtle.goto(points[1][0],points[1][1])
    myTurtle.goto(points[2][0],points[2][1])
    myTurtle.goto(points[0][0],points[0][1])
    myTurtle.end_fill()

def getMid(p1,p2):
    return ( (p1[0]+p2[0]) / 2, (p1[1] + p2[1]) / 2)

def sierpinski(points,degree,myTurtle):
    colormap = ['blue','red','green','white','yellow','violet','orange']
    drawTriangle(points,colormap[degree],myTurtle)
    if degree > 0:
        sierpinski([points[0],
                        getMid(points[0], points[1]),
                        getMid(points[0], points[2])],
                   degree-1, myTurtle)
        sierpinski([points[1],
                        getMid(points[0], points[1]),
                        getMid(points[1], points[2])],
                   degree-1, myTurtle)
        sierpinski([points[2],
                        getMid(points[2], points[1]),
                        getMid(points[0], points[2])],
                   degree-1, myTurtle)

def main():
   myTurtle = turtle.Turtle()
   myWin = turtle.Screen()
   myPoints = [[-100,-50],[0,100],[100,-50]]
   sierpinski(myPoints,3,myTurtle)
   myWin.exitonclick()

main()


#https://runestone.academy/ns/books/published/pythonds/Recursion/pythondsintro-VisualizingRecursion.html
import turtle

def tree(branchLen,t):
    if branchLen > 5:
        t.forward(branchLen)
        t.right(20)
        tree(branchLen-15,t)
        t.left(40)
        tree(branchLen-15,t)
        t.right(20)
        t.backward(branchLen)

def main():
    t = turtle.Turtle()
    myWin = turtle.Screen()
    t.left(90)
    t.up()
    t.backward(100)
    t.down()
    t.color("green")
    tree(75,t)
    myWin.exitonclick()

main()


from turtle import *
color('red', 'yellow')
begin_fill()
while True:
    forward(200)
    left(170)
    if abs(pos()) < 1:
        break
end_fill()
done()

from svg_turtle import SvgTurtle


def draw_spiral(t):
    t.fillcolor('blue')
    t.begin_fill()
    for i in range(20):
        d = 50 + i*i*1.5
        t.pencolor(0, 0.05*i, 0)
        t.width(i)
        t.forward(d)
        t.right(144)
    t.end_fill()


def write_file(draw_func, filename, width, height):
    t = SvgTurtle(width, height)
    draw_func(t)
    t.save_as(filename)


def main():
    write_file(draw_spiral, 'example.svg', 500, 500)
    print('Done.')


if __name__ == '__main__':
    main()


!ls *.gif

import turtle

screen = turtle.Screen()
screen.title("U.S. States Game")
image = "10.gif"
screen.addshape(image)
turtle.shape(image)

screen.exitonclick()
#turtle.done()
turtle.reset()

turtle.done()
turtle.reset()

https://casual-effects.com/codeheart/turtle/
https://docs.python.org/3/library/turtle.html

%%javascript
// You can find the Turtle API reference here: https://turtletoy.net/syntax
var Canvas = document.getElementById('myChart')

Canvas.setpenopacity(1);

// Global code will be evaluated once.
const turtle = new Turtle();
turtle.penup();
turtle.goto(-50,-20);
turtle.pendown();

// The walk function will be called until it returns false.
function au(x){
x=x+10
return x
}
function walk(i) {
    turtle.forward(106+au(2));
    turtle.right(144+au(2));
    return i < 4;
}

<div id="myChart"></div>

# Python program to draw square
# using Turtle Programming
import turtle
skk = turtle.Turtle()
 
for i in range(4):
    skk.forward(50)
    skk.right(90)
     
turtle.done()

If you are using PHP, try adding the following code at the beginning of the php file:

If you are using localhost, try this:

header("Access-Control-Allow-Origin: *");
If you are using external domains such as server, try this:

header("Access-Control-Allow-Origin: http://www.website.com");

turtle.mainloop() aka turtle.Screen().mainloop() Turns control over to tkinter's event loop. Usually, a lack of turtle.Screen().mainloop() (or turtle.Screen().exitonclick(), etc.) will cause the window to close just because the program will end, closing everything. This, or one of its variants, should be the last statement in a turtle graphics program unless the script is run from within Python IDLE -n.

turtle.done() (Does not close window nor reset anything.) A synonym for turtle.mainloop()

turtle.clear() Deletes everything this turtle has drawn (not just the last thing). Otherwise doesn't affect the state of the turtle.

turtle.reset() Does a turtle.clear() and then resets this turtle's state (i.e. direction, position, etc.)

turtle.clearscreen() aka turtle.Screen().clear() Deletes all drawing and all turtles, reseting the window to it's original state.

turtle.resetscreen() aka turtle.Screen().reset() Resets all turtles on the screen to their initial state.

turtle.bye() aka turtle.Screen().bye() Closes the turtle graphics window. I don't see a way to use any turtle graphics commands after this is invoked.

turtle.exitonclick() aka turtle.Screen().exitonclick() After binding the screen click event to do a turtle.Screen().bye() invokes turtle.Screen().mainloop()

It's not clear that you can close and reopen the graphics window from within turtle without dropping down to the tkinter level that underpins turtle (and Zelle's graphics.py)

For purposes of starting a new hand in your blackjack game, I'd guess turtle.reset() or turtle.resetscreen() are your best bet.

import pygame, math

pygame.init()
window = pygame.display.set_mode((600, 600))
pygame.display.set_caption("Fractal Tree")
screen = pygame.display.get_surface()

def drawTree(x1, y1, angle, depth):
    fork_angle = 20
    base_len = 10.0
    if depth > 0:
        x2 = x1 + int(math.cos(math.radians(angle)) * depth * base_len)
        y2 = y1 + int(math.sin(math.radians(angle)) * depth * base_len)
        pygame.draw.line(screen, (255,255,255), (x1, y1), (x2, y2), 2)
        drawTree(x2, y2, angle - fork_angle, depth - 1)
        drawTree(x2, y2, angle + fork_angle, depth - 1)

def input(event):
    if event.type == pygame.QUIT:
        exit(0)

drawTree(300, 550, -90, 9)
pygame.display.flip()
while True:
    input(pygame.event.wait())
    

reset



import os 
os.chdir("/home/jack/Desktop/dockercommands")

from random import randint
Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
       "#styletransfer #PythonGraphics #PIL\n",
       "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
       "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
       "#CreativeCoding #AI #genart","#p5js #Generative\n",
       "#codefor30days #Python #100DaysOfCode\n",
       "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
hashs

from randtext import randTXT
STR = randTXT()
print (STR[:240])

from PIL import Image
import random
from random import randint
import time
import cv2
from PIL import ImageChops
dim = (720, 480)
def binarize(filein, fileout):
    # read the image file
    img = cv2.imread(filein, 2)
    ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    # converting to its binary form
    bw = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    cv2.imwrite(fileout, bw_img)
    output = fileout
    return output

path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")
path1 = r"/home/jack/Documents/jpg/720/"
base_image1 = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image1)

background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")
path2 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
  
file3=(path2+base_image3)

path3 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path3)
   if os.path.isfile(os.path.join(path3, x))
   ])
file4=(path3+base_image3)
   
filein=file1
filein=file2
filein=file3
#filein=file4
print("file1",file1)
print("file2",file2)
print("file3",file3)
print("file4",file4)
fileout = "junk/mask.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
binarize(filein, fileout)
im = Image.open("junk/mask.png")
im.save("/home/jack/Desktop/TENSORFLOW/images/BINARY_"+timestr+"_.png")

#binarize(filein, fileout)
img = Image.open("junk/mask.png").convert('L')
img = ImageChops.invert(img)
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/BINARY2"+timestr+"_.png"
outlineP(filename1,outfile_png)


from PIL import ImageDraw
frames =["frames/01-print.png","frames/abstract-print.png","frames/perferations.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/usal-perferations.png",\
"frames/04-print.png","frames/frame-lite.png","frames/usa-perferations.png",\
"frames/05-print.png","frames/frames.png","frames/usar-perferations.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/frames/leopard-print.png","frames/wood-blur-frame.png",\
"frames/08-print.png","lined-frame.png","frames/09-print.png","frames/perferations+.png"]
Num = randint( 0, len(frames)-1)
BOARDER = frames[Num]
imagebackground = Image.open(outfile_png)
border=Image.open(BOARDER).convert('RGBA') 
imagebackground.paste(border, (0,0), mask=mask)
imagebackground

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]

hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
x = 720//2
y = 480//2
text = hashs
x = imagebackground.width//2
y = imagebackground.height//2
blurred = Image.new('RGBA', imagebackground.size)
draw = ImageDraw.Draw(blurred)
CH = randint(0,1)
if CH == 0:COLor = ["white","black"]
elif CH == 1:COLor = ["black","white"]  
draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(2))
# Paste soft text onto background
bg.paste(blurred,blurred)
# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')





def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", img1)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    #STR = randTXT()
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/01-print.png","frames/abstract-print.png","frames/perferations.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/usal-perferations.png",\
"frames/04-print.png","frames/frame-lite.png","frames/usa-perferations.png",\
"frames/05-print.png","frames/frames.png","frames/usar-perferations.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/frames/leopard-print.png","frames/wood-blur-frame.png",\
"frames/08-print.png","lined-frame.png","frames/09-print.png","frames/perferations+.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]

    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):
count = 1
creatmased(count)

from PIL import Image
import random
from random import randint
import time
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
print("file1",file1)
#file2 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file1 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")
path1 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image)
print (file2)
background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")
path2 = r"//home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
  
file3=(path2+base_image)
print(file2) 

path2 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
   
file3=(path2+base_image)
print("file3",file3) 
img = Image.open(file3).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/XX"+timestr+"_.png"
outlineP(filename1,outfile_png)


from PIL import Image
import random
from random import randint
import time
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
print("file1",file1)
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")


path1 = r"/home/jack/Documents/jpg/720/"
base_image1 = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image1)
print ("file2: ",file2)
background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")

"""
path2 = r"/home/jack/Documents/jpg/720/"
base_image2 = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
file3=(path2+base_image2)
file3 = "/home/jack/Desktop/TENSORFLOW/images/warrior-20.png"
print("file3: ",file3) 
print("------------------------")
path3 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path3)
   if os.path.isfile(os.path.join(path3, x))
   ])
"""   
file4=(path3+base_image2)
file4=(path2+base_image1)
file4=(path1+base_image)
#file4=(path+base_image)
print("file4: ",file2) 
img = Image.open(file2).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)
timestr = time.strftime("%Y%m%d-%H%M%S")
png = "/home/jack/Desktop/TENSORFLOW/images/MASK"+timestr+"_.png"
mask.save(png)
print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/XX"+timestr+"_.png"
outlineP(filename1,outfile_png)


from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png" 
outlineP(filename1,outfile_png)





from PIL import Image
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
file2 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file1 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print(original.size)
path1 = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image)
print (file2)





background = Image.open(file2).convert('RGB')

background  = background.resize(dim, Image.NEAREST)

print(background.size)
path2 = r"/home/jack/Desktop/Imagedata/2-binary_images/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
   
file3=(path2+base_image)
#file3="/home/jack/Desktop/Imagedata/2-binary_images/binar_44_20220925161510.jpg"
print(file3) 



img = Image.open(file3).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print(mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
result

#file1 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file2 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
room = cv2.imread(file2)
logo = cv2.imread(file1)
path2 = r"/home/jack/Desktop/Imagedata/2-binary_images/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
    
file3=(path2+base_image)
#--- Resizing the logo to the shape of room image ---
logo = cv2.imread(file3)
logo = cv2.resize(logo, (room.shape[1], room.shape[0]))

#--- Apply Otsu threshold to blue channel of the logo image ---
ret, logo_mask = cv2.threshold(logo[:,:,0], 0, 255, cv2.THRESH_BINARY|cv2.THRESH_OTSU)
#cv2.imshow('logo_mask', logo_mask)

room2 = room.copy() 

#--- Copy pixel values of logo image to room image wherever the mask is white ---
room2[np.where(logo_mask == 255)] = logo[np.where(logo_mask == 255)]
timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "/home/jack/Desktop/TENSORFLOW/junk/XX"+timestr+"_"+str(count)+".png"
cv2.imwrite(filename, room2)



import cv2
  
def binarize(filein, fileout):
    # read the image file
    img = cv2.imread(filein, 2)
    ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    # converting to its binary form
    bw = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    cv2.imwrite(fileout, bw_img)
    output = fileout
    return output
    
filein = '/home/jack/Desktop/TENSORFLOW/images/MASK20221109-224545_.png'
fileout = "/home/jack/Desktop/TENSORFLOW/images/binary2.png"
binarize(filein, fileout)

original = Image.open(fileout).convert('RGB')
original

original = Image.open(filename).convert('RGB')
original

import mathsvg
image = mathsvg.SvgImage(pixel_density = 10, view_window = (( -1, -1 ), ( 1, 1 )))
image.draw_circle([0, 0], 1.1)
image.save("simple-example.svg")



#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
print (STR)
import cv2
from PIL import Image
randomframes = []
images=[]
count = 0
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    print(filename)
    return nim
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/test001.mp4"
filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", img1)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    #STR = randTXT()
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/01-print.png","frames/abstract-print.png","frames/perferations.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/usal-perferations.png",\
"frames/04-print.png","frames/frame-lite.png","frames/usa-perferations.png",\
"frames/05-print.png","frames/frames.png","frames/usar-perferations.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/frames/leopard-print.png","frames/wood-blur-frame.png",\
"frames/08-print.png","lined-frame.png","frames/09-print.png","frames/perferations+.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]

    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
im = Image.open(PATH)
im

In this 5th part of the image processing series, we discuss more on the Arithmetic and bitwise operations, and masking of images in Python.

It is recommended that the previous articles be run through, before starting off on your masked learning adventure here.
Setting up the environment

The following lines of code are used in all of the applications given below. We’ll include those here instead so you don’t have to read through a huge block of code.

Helps reduce clutter :)

# importing numpy to work with pixels
import numpy as np

# importing argument parsers
import argparse

# importing the OpenCV module
import cv2


# initializing an argument parser object
ap = argparse.ArgumentParser()

# adding the argument, providing the user an option
# to input the path of the image
ap.add_argument("-i", "--image", required=True, help="Path to the image")

# parsing the argument
args = vars(ap.parse_args())

# reading the image location through args
# and reading the image using cv2.imread
image = cv2.imread(args["image"])

Arithmetic Operations on Images using Python

Arithmetic Operations allow us to enhance a lot of aspects of an image.

We can work with lighting, shadows, the red, blue, and green color enhancement.

A lot of image filters on applications use the same method to alter and beautify photographs as well.

So, let’s get started with all of the code!

First, in order to understand whether the limit can go over 255 or 0, we can conduct a simple test, which provides us with 255 and 0.

# printing out details of image min, max and the wrap around
print("max of 255 :", str(cv2.add(np.uint8([200]), np.uint8([100]))))
print("min of 0 :", str(cv2.subtract(np.uint8([50]), np.uint8([100]))))

print("wrap around :", str(np.uint8([200]) + np.uint8([100])))
print("wrap around :", str(np.uint8([50]) - np.uint8([100])))

In this example, we are increasing the intensity of all the pixels in the image by 100.

# adding pixels of value 255 (white) to the image
M = np.ones(image.shape, dtype="uint8") * 100
added = cv2.add(image, M)
cv2.imshow("Added", added)
cv2.waitKey(0)

This is done by constructing a matrix with the same size as our images using the NumPy module, and adding it with our image.

In case we wish to darken an image, we subtract from the pixel values of the image, as shown below,

# adding pixels of value 0 (black) to the image
M = np.ones(image.shape, dtype="uint8") * 50
subtracted = cv2.subtract(image, M)
cv2.imshow("Subtracted", subtracted)
cv2.waitKey(0)

This should provide you with two different variations of the original image, one lighter, and the other darker.
Bitwise Operations

We use Bitwise operations a lot of the times while attempting to mask images.

This feature of OpenCV allows us to filter out the part of the image that is relevant to us.
Setting up

To work on Bitwise operations, we’ll first need two variables or images that we can conduct the operations on.

So, let’s create a bitwise square and a bitwise circle through which we can use the bitwise operations.

Note that bitwise operations require the images to be black and white.

# creating a square of zeros using a variable
rectangle = np.zeros((300, 300), dtype="uint8")
cv2.rectangle(rectangle, (25, 25), (275, 275), 255, -1)
cv2.imshow("Rectangle : ", rectangle)

# creating a circle of zeros using a variable
circle = np.zeros((300, 300), dtype="uint8")
cv2.circle(circle, (150, 150), 150, 255, -1)
cv2.imshow("Circle : ", circle)

The output images that you receive should look like this,
Image 4
Bit Square
Combine with the AND operation

Bitwise addition refers to the addition of two different images, and decide which is to be displayed using an AND operation on each pixel of the images.

# the bitwise_and function executes the AND operation
# on both the images
bitwiseAnd = cv2.bitwise_and(rectangle, circle)
cv2.imshow("AND", bitwiseAnd)
cv2.waitKey(0)

Bitwise addition of both the circle and the square gives us an output which should look like this,
Image 6
AND Bit Square
Given a choice with the OR operation

Bitwise OR provides us with a product of the two images with an OR operation performed on each pixel of the images.

# the bitwise_or function executes the OR operation
# on both the images
bitwiseOr = cv2.bitwise_or(rectangle, circle)
cv2.imshow("OR", bitwiseOr)
cv2.waitKey(0)

Upon performing the operation Bitwise OR, you should receive something like this,
Image 7
OR Bit Square
Exclusivity with the XOR operation

Another operation that is provided by the cv2 module is the XOR operation, which we can use through the bitwise_xor function.

# the bitwise_xor function executes the XOR operation
# on both the images
bitwiseXor = cv2.bitwise_xor(rectangle, circle)
cv2.imshow("XOR", bitwiseXor)
cv2.waitKey(0)

Image 8
XOR Bit Square
Negation using the NOT operation

Lastly, we have the negation operation, which is performed using the bitwise_not function.

The NOT operation only requires a single image as we’re not adding or subtracting anything here.

We still use it on both here however, that’s also an option.

# the bitwise_not function executes the NOT operation
# on both the images
bitwiseNot = cv2.bitwise_not(rectangle, circle)
cv2.imshow("NOT", bitwiseNot)
cv2.waitKey(0)

The circle is inside the square in this case, and as such is not visible,
Image 9
Not Bit Square
Masking of images using Python OpenCV

Masking is used in Image Processing to output the Region of Interest, or simply the part of the image that we are interested in.

We tend to use bitwise operations for masking as it allows us to discard the parts of the image that we do not need.

So, let’s get started with masking!

The process of masking images

We have three steps in masking.

    Creating a black canvas with the same dimensions as the image, and naming it as mask.
    Changing the values of the mask by drawing any figure in the image and providing it with a white color.
    Performing the bitwise ADD operation on the image with the mask.

Following the same process, let’s create a few masks and use them on our image.

First, let’s work with a rectangle mask.

# creating a mask of that has the same dimensions of the image
# where each pixel is valued at 0
mask = np.zeros(image.shape[:2], dtype="uint8")

# creating a rectangle on the mask
# where the pixels are valued at 255
cv2.rectangle(mask, (0, 90), (290, 450), 255, -1)
cv2.imshow("Mask", mask)

# performing a bitwise_and with the image and the mask
masked = cv2.bitwise_and(image, image, mask=mask)
cv2.imshow("Mask applied to Image", masked)
cv2.waitKey(0)

Now, let’s try it out with a circle mask.

# creating a mask of that has the same dimensions of the image
# where each pixel is valued at 0
mask = np.zeros(image.shape[:2], dtype="uint8")

# creating a rectangle on the mask
# where the pixels are valued at 255
cv2.circle(mask, (145, 200), 100, 255, -1)
cv2.imshow("Mask", mask)

# performing a bitwise_and with the image and the mask
masked = cv2.bitwise_and(image, image, mask=mask)
cv2.imshow("Mask applied to Image", masked)
cv2.waitKey(0)

If everything works out just fine, we should receive outputs which look something like this,
Image 10
Rectangular Mask
Conclusion

We’re finally getting started with the core of Image Processing, and understanding bitwise operations and masking in it is important.

It helps us to block out parts or only take in parts of the image that we are interested in, so, quite a useful concept.

import os 
os.chdir("/home/jack/Desktop/dockercommands")

from PIL import Image
import random
from random import randint
import time
import cv2
from PIL import ImageChops
dim = (720, 480)
def binarize(filein, fileout):
    # read the image file
    img = cv2.imread(filein, 2)
    ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    # converting to its binary form
    bw = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    cv2.imwrite(fileout, bw_img)
    output = fileout
    return output

path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")
path1 = r"/home/jack/Documents/jpg/720/"
base_image1 = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image1)

background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")
path2 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
  
file3=(path2+base_image3)

path3 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path3)
   if os.path.isfile(os.path.join(path3, x))
   ])
file4=(path3+base_image3)
   
filein=file1
filein=file2
filein=file3
#filein=file4
print("file1",file1)
print("file2",file2)
print("file3",file3)
print("file4",file4)
fileout = "junk/mask.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
binarize(filein, fileout)
im = Image.open("junk/mask.png")
im.save("/home/jack/Desktop/TENSORFLOW/images/BINARY_"+timestr+"_.png")

#binarize(filein, fileout)
img = Image.open("junk/mask.png").convert('L')
img = ImageChops.invert(img)
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
result.save('resultunlined.png')
timestr = time.strftime("%Y%m%d-%H%M%S")
filen = "onevid/"+timestr+"unlined.png"
result.save(filen)
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
#outfile_png = "/home/jack/Desktop/TENSORFLOW/images/BINARY2"+timestr+"_.png"
outfile_png = "onevid/resultmasked-outlined.png"
outlineP(filename1,outfile_png)
result.save('onevid/resultunmasked.png')
result

outfile_png = "onevid/resultmasked-outlined.png"
im =Image.open(outfile_png)
im

from PIL import Image
import random
from random import randint
import time
import cv2
from PIL import ImageChops
dim = (720, 480)
def binarize(filein, fileout):
    # read the image file
    img = cv2.imread(filein, 2)
    ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    # converting to its binary form
    bw = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    cv2.imwrite(fileout, bw_img)
    output = fileout
    return output

path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")
path1 = r"/home/jack/Documents/jpg/720/"
base_image1 = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image1)

background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")
path2 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
  
file3=(path2+base_image3)

path3 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path3)
   if os.path.isfile(os.path.join(path3, x))
   ])
file4=(path3+base_image3)
   
filein=file1
filein=file2
filein=file3
#filein=file4
print("file1",file1)
print("file2",file2)
print("file3",file3)
print("file4",file4)
fileout = "junk/mask.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
binarize(filein, fileout)
im = Image.open("junk/mask.png")
im.save("/home/jack/Desktop/TENSORFLOW/images/BINARY_"+timestr+"_.png")

#binarize(filein, fileout)
img = Image.open("junk/mask.png").convert('L')
img = ImageChops.invert(img)
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
#outfile_png = "/home/jack/Desktop/TENSORFLOW/images/BINARY2"+timestr+"_.png"
outfile_png = "onevid/resultmasked-outlined.png"
outlineP(filename1,outfile_png)


def creatmasked():   # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("/home/jack/Desktop/dockercommands/resultmasked.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)

    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')
    bg.save("MAIN.png")
    
    filename1 = "onevid/temp.png" 

    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/01-print.png","frames/abstract-print.png","frames/09-print.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/lined-frame.png",\
"frames/04-print.png","frames/frame-lite.png","frames/08-print.png",\
"frames/05-print.png","frames/frames.png","frames/wood-blur-frame.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/leopard-print.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]


    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):

creatmasked()


from OutlineImage import outlineP
#filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
#outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
im = Image.open(PATH)
im

def creatmasked():   # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("/home/jack/Desktop/dockercommands/resultmasked.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)

    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')
    bg.save("MAIN.png")
    
    filename1 = "onevid/temp.png" 

    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/01-print.png","frames/abstract-print.png","frames/09-print.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/lined-frame.png",\
"frames/04-print.png","frames/frame-lite.png","frames/08-print.png",\
"frames/05-print.png","frames/frames.png","frames/wood-blur-frame.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/leopard-print.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]


    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):

creatmasked()


from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
im = Image.open(PATH)
im

def creatmasked():   # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("/home/jack/Desktop/dockercommands/resultmasked.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)

    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')
    bg.save("MAIN.png")
    
 
    from OutlineImage import outlineP
    filename1 = "MAIN.png" 
    outfile_png = "onevid/temp2.png"
    outlineP(filename1,outfile_png)
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/01-print.png","frames/abstract-print.png","frames/09-print.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/lined-frame.png",\
"frames/04-print.png","frames/frame-lite.png","frames/08-print.png",\
"frames/05-print.png","frames/frames.png","frames/wood-blur-frame.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/leopard-print.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]


    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    imgsize=Image.open("/home/jack/Desktop/dockercommands/onevid/temp2.png")
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):

creatmasked()


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
im = Image.open(PATH)
im

response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

from random import randint
Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
       "#styletransfer #PythonGraphics #PIL\n",
       "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
       "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
       "#CreativeCoding #AI #genart","#p5js #Generative\n",
       "#codefor30days #Python #100DaysOfCode\n",
       "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
hashs

from randtext import randTXT
STR = randTXT()
print (STR[:240])

import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io





from PIL import ImageDraw
frames =["frames/01-print.png","frames/abstract-print.png","frames/perferations.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/usal-perferations.png",\
"frames/04-print.png","frames/frame-lite.png","frames/usa-perferations.png",\
"frames/05-print.png","frames/frames.png","frames/usar-perferations.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/frames/leopard-print.png","frames/wood-blur-frame.png",\
"frames/08-print.png","lined-frame.png","frames/09-print.png","frames/perferations+.png"]
Num = randint( 0, len(frames)-1)
BOARDER = frames[Num]
imagebackground = Image.open(outfile_png)
border=Image.open(BOARDER)#.convert('p') 
imagebackground.paste(border, (0,0), mask=mask)
imagebackground

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
truetype/dejavu/DejaVuSans-Bold.ttf', 16)
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
x = 720//2
y = 480//2
text = hashs
x = imagebackground.width//2
y = imagebackground.height//2
blurred = Image.new('RGBA', imagebackground.size)
draw = ImageDraw.Draw(blurred)
CH = randint(0,1)
if CH == 0:COLor = ["white","black"]
elif CH == 1:COLor = ["black","white"]  
draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(2))
# Paste soft text onto background
imagebackground.paste(blurred,blurred)
# Draw on sharp text
draw = ImageDraw.Draw(imagebackground)
draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')
imagebackground.save("onevid/temp.png")
imagebackground

from randtext import randTXT

"""
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)
"""

CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
im = Image.open(PATH)
im

!pwd

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
PATH = "/home/jack/Desktop/dockercommands/onevid/resultmasked-outlined0.png"
photo = open(PATH,'rb')
im = Image.open(PATH)
im

response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])



import os 
os.chdir("/home/jack/Desktop/dockercommands")

from random import randint
Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
       "#styletransfer #PythonGraphics #PIL\n",
       "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
       "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
       "#CreativeCoding #AI #genart","#p5js #Generative\n",
       "#codefor30days #Python #100DaysOfCode\n",
       "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
hashs

from randtext import randTXT
STR = randTXT()
print (STR[:240])

from PIL import Image
import random
from random import randint
import time
import cv2
from PIL import ImageChops
dim = (720, 480)
def binarize(filein, fileout):
    # read the image file
    img = cv2.imread(filein, 2)
    ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    # converting to its binary form
    bw = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    cv2.imwrite(fileout, bw_img)
    output = fileout
    return output

path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")
path1 = r"/home/jack/Documents/jpg/720/"
base_image1 = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image1)

background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")
path2 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
  
file3=(path2+base_image3)

path3 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path3)
   if os.path.isfile(os.path.join(path3, x))
   ])
file4=(path3+base_image3)
   
filein=file1
filein=file2
filein=file3
#filein=file4
print("file1",file1)
print("file2",file2)
print("file3",file3)
print("file4",file4)
fileout = "junk/mask.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
binarize(filein, fileout)
im = Image.open("junk/mask.png")
im.save("/home/jack/Desktop/TENSORFLOW/images/BINARY_"+timestr+"_.png")

#binarize(filein, fileout)
img = Image.open("junk/mask.png").convert('L')
img = ImageChops.invert(img)
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/BINARY2"+timestr+"_.png"
outlineP(filename1,outfile_png)


from PIL import Image
import random
from random import randint
import time
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
print("file1",file1)
#file2 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file1 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")
path1 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image)
print (file2)
background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")
path2 = r"//home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
  
file3=(path2+base_image)
print(file2) 

path2 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
   
file3=(path2+base_image)
print("file3",file3) 
img = Image.open(file3).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/XX"+timestr+"_.png"
outlineP(filename1,outfile_png)


from PIL import Image
import random
from random import randint
import time
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
print("file1",file1)
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")


path1 = r"/home/jack/Documents/jpg/720/"
base_image1 = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image1)
print ("file2: ",file2)
background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")

"""
path2 = r"/home/jack/Documents/jpg/720/"
base_image2 = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
file3=(path2+base_image2)
file3 = "/home/jack/Desktop/TENSORFLOW/images/warrior-20.png"
print("file3: ",file3) 
print("------------------------")
path3 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path3)
   if os.path.isfile(os.path.join(path3, x))
   ])
"""   
file4=(path3+base_image2)
file4=(path2+base_image1)
file4=(path1+base_image)
#file4=(path+base_image)
print("file4: ",file2) 
img = Image.open(file2).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)
timestr = time.strftime("%Y%m%d-%H%M%S")
png = "/home/jack/Desktop/TENSORFLOW/images/MASK"+timestr+"_.png"
mask.save(png)
print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/XX"+timestr+"_.png"
outlineP(filename1,outfile_png)


from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png" 
outlineP(filename1,outfile_png)





from PIL import Image
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
file2 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file1 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print(original.size)
path1 = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image)
print (file2)





background = Image.open(file2).convert('RGB')

background  = background.resize(dim, Image.NEAREST)

print(background.size)
path2 = r"/home/jack/Desktop/Imagedata/2-binary_images/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
   
file3=(path2+base_image)
#file3="/home/jack/Desktop/Imagedata/2-binary_images/binar_44_20220925161510.jpg"
print(file3) 



img = Image.open(file3).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print(mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
result

#file1 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file2 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
room = cv2.imread(file2)
logo = cv2.imread(file1)
path2 = r"/home/jack/Desktop/Imagedata/2-binary_images/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
    
file3=(path2+base_image)
#--- Resizing the logo to the shape of room image ---
logo = cv2.imread(file3)
logo = cv2.resize(logo, (room.shape[1], room.shape[0]))

#--- Apply Otsu threshold to blue channel of the logo image ---
ret, logo_mask = cv2.threshold(logo[:,:,0], 0, 255, cv2.THRESH_BINARY|cv2.THRESH_OTSU)
#cv2.imshow('logo_mask', logo_mask)

room2 = room.copy() 

#--- Copy pixel values of logo image to room image wherever the mask is white ---
room2[np.where(logo_mask == 255)] = logo[np.where(logo_mask == 255)]
timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "/home/jack/Desktop/TENSORFLOW/junk/XX"+timestr+"_"+str(count)+".png"
cv2.imwrite(filename, room2)



import cv2
  
def binarize(filein, fileout):
    # read the image file
    img = cv2.imread(filein, 2)
    ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    # converting to its binary form
    bw = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    cv2.imwrite(fileout, bw_img)
    output = fileout
    return output
    
filein = '/home/jack/Desktop/TENSORFLOW/images/MASK20221109-224545_.png'
fileout = "/home/jack/Desktop/TENSORFLOW/images/binary2.png"
binarize(filein, fileout)

original = Image.open(fileout).convert('RGB')
original

original = Image.open(filename).convert('RGB')
original

import mathsvg
image = mathsvg.SvgImage(pixel_density = 10, view_window = (( -1, -1 ), ( 1, 1 )))
image.draw_circle([0, 0], 1.1)
image.save("simple-example.svg")


In this 5th part of the image processing series, we discuss more on the Arithmetic and bitwise operations, and masking of images in Python.

It is recommended that the previous articles be run through, before starting off on your masked learning adventure here.
Setting up the environment

The following lines of code are used in all of the applications given below. We’ll include those here instead so you don’t have to read through a huge block of code.

Helps reduce clutter :)

# importing numpy to work with pixels
import numpy as np

# importing argument parsers
import argparse

# importing the OpenCV module
import cv2


# initializing an argument parser object
ap = argparse.ArgumentParser()

# adding the argument, providing the user an option
# to input the path of the image
ap.add_argument("-i", "--image", required=True, help="Path to the image")

# parsing the argument
args = vars(ap.parse_args())

# reading the image location through args
# and reading the image using cv2.imread
image = cv2.imread(args["image"])

Arithmetic Operations on Images using Python

Arithmetic Operations allow us to enhance a lot of aspects of an image.

We can work with lighting, shadows, the red, blue, and green color enhancement.

A lot of image filters on applications use the same method to alter and beautify photographs as well.

So, let’s get started with all of the code!

First, in order to understand whether the limit can go over 255 or 0, we can conduct a simple test, which provides us with 255 and 0.

# printing out details of image min, max and the wrap around
print("max of 255 :", str(cv2.add(np.uint8([200]), np.uint8([100]))))
print("min of 0 :", str(cv2.subtract(np.uint8([50]), np.uint8([100]))))

print("wrap around :", str(np.uint8([200]) + np.uint8([100])))
print("wrap around :", str(np.uint8([50]) - np.uint8([100])))

In this example, we are increasing the intensity of all the pixels in the image by 100.

# adding pixels of value 255 (white) to the image
M = np.ones(image.shape, dtype="uint8") * 100
added = cv2.add(image, M)
cv2.imshow("Added", added)
cv2.waitKey(0)

This is done by constructing a matrix with the same size as our images using the NumPy module, and adding it with our image.

In case we wish to darken an image, we subtract from the pixel values of the image, as shown below,

# adding pixels of value 0 (black) to the image
M = np.ones(image.shape, dtype="uint8") * 50
subtracted = cv2.subtract(image, M)
cv2.imshow("Subtracted", subtracted)
cv2.waitKey(0)

This should provide you with two different variations of the original image, one lighter, and the other darker.
Bitwise Operations

We use Bitwise operations a lot of the times while attempting to mask images.

This feature of OpenCV allows us to filter out the part of the image that is relevant to us.
Setting up

To work on Bitwise operations, we’ll first need two variables or images that we can conduct the operations on.

So, let’s create a bitwise square and a bitwise circle through which we can use the bitwise operations.

Note that bitwise operations require the images to be black and white.

# creating a square of zeros using a variable
rectangle = np.zeros((300, 300), dtype="uint8")
cv2.rectangle(rectangle, (25, 25), (275, 275), 255, -1)
cv2.imshow("Rectangle : ", rectangle)

# creating a circle of zeros using a variable
circle = np.zeros((300, 300), dtype="uint8")
cv2.circle(circle, (150, 150), 150, 255, -1)
cv2.imshow("Circle : ", circle)

The output images that you receive should look like this,
Image 4
Bit Square
Combine with the AND operation

Bitwise addition refers to the addition of two different images, and decide which is to be displayed using an AND operation on each pixel of the images.

# the bitwise_and function executes the AND operation
# on both the images
bitwiseAnd = cv2.bitwise_and(rectangle, circle)
cv2.imshow("AND", bitwiseAnd)
cv2.waitKey(0)

Bitwise addition of both the circle and the square gives us an output which should look like this,
Image 6
AND Bit Square
Given a choice with the OR operation

Bitwise OR provides us with a product of the two images with an OR operation performed on each pixel of the images.

# the bitwise_or function executes the OR operation
# on both the images
bitwiseOr = cv2.bitwise_or(rectangle, circle)
cv2.imshow("OR", bitwiseOr)
cv2.waitKey(0)

Upon performing the operation Bitwise OR, you should receive something like this,
Image 7
OR Bit Square
Exclusivity with the XOR operation

Another operation that is provided by the cv2 module is the XOR operation, which we can use through the bitwise_xor function.

# the bitwise_xor function executes the XOR operation
# on both the images
bitwiseXor = cv2.bitwise_xor(rectangle, circle)
cv2.imshow("XOR", bitwiseXor)
cv2.waitKey(0)

Image 8
XOR Bit Square
Negation using the NOT operation

Lastly, we have the negation operation, which is performed using the bitwise_not function.

The NOT operation only requires a single image as we’re not adding or subtracting anything here.

We still use it on both here however, that’s also an option.

# the bitwise_not function executes the NOT operation
# on both the images
bitwiseNot = cv2.bitwise_not(rectangle, circle)
cv2.imshow("NOT", bitwiseNot)
cv2.waitKey(0)

The circle is inside the square in this case, and as such is not visible,
Image 9
Not Bit Square
Masking of images using Python OpenCV

Masking is used in Image Processing to output the Region of Interest, or simply the part of the image that we are interested in.

We tend to use bitwise operations for masking as it allows us to discard the parts of the image that we do not need.

So, let’s get started with masking!

The process of masking images

We have three steps in masking.

    Creating a black canvas with the same dimensions as the image, and naming it as mask.
    Changing the values of the mask by drawing any figure in the image and providing it with a white color.
    Performing the bitwise ADD operation on the image with the mask.

Following the same process, let’s create a few masks and use them on our image.

First, let’s work with a rectangle mask.

# creating a mask of that has the same dimensions of the image
# where each pixel is valued at 0
mask = np.zeros(image.shape[:2], dtype="uint8")

# creating a rectangle on the mask
# where the pixels are valued at 255
cv2.rectangle(mask, (0, 90), (290, 450), 255, -1)
cv2.imshow("Mask", mask)

# performing a bitwise_and with the image and the mask
masked = cv2.bitwise_and(image, image, mask=mask)
cv2.imshow("Mask applied to Image", masked)
cv2.waitKey(0)

Now, let’s try it out with a circle mask.

# creating a mask of that has the same dimensions of the image
# where each pixel is valued at 0
mask = np.zeros(image.shape[:2], dtype="uint8")

# creating a rectangle on the mask
# where the pixels are valued at 255
cv2.circle(mask, (145, 200), 100, 255, -1)
cv2.imshow("Mask", mask)

# performing a bitwise_and with the image and the mask
masked = cv2.bitwise_and(image, image, mask=mask)
cv2.imshow("Mask applied to Image", masked)
cv2.waitKey(0)

If everything works out just fine, we should receive outputs which look something like this,
Image 10
Rectangular Mask
Conclusion

We’re finally getting started with the core of Image Processing, and understanding bitwise operations and masking in it is important.

It helps us to block out parts or only take in parts of the image that we are interested in, so, quite a useful concept.

import os 
os.chdir("/home/jack/Desktop/dockercommands")

from random import randint
Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
       "#styletransfer #PythonGraphics #PIL\n",
       "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
       "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
       "#CreativeCoding #AI #genart","#p5js #Generative\n",
       "#codefor30days #Python #100DaysOfCode\n",
       "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
hashs

from randtext import randTXT
STR = randTXT()
print (STR[:240])

import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
print (STR)
from random import randint
import time
dim = (720, 480)
def binarize(filein, fileout):
    # read the image file
    img = cv2.imread(filein, 2)
    ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    # converting to its binary form
    bw = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    cv2.imwrite(fileout, bw_img)
    output = fileout
    return output

path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")
path1 = r"/home/jack/Documents/jpg/720/"
base_image1 = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image1)

background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")
path2 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
  
file3=(path2+base_image3)

path3 = r"/home/jack/Documents/jpg/720/"
base_image3 = random.choice([
   x for x in os.listdir(path3)
   if os.path.isfile(os.path.join(path3, x))
   ])
file4=(path3+base_image3)
   
filein=file1
filein=file2
filein=file3
#filein=file4
print("file1",file1)
print("file2",file2)
print("file3",file3)
print("file4",file4)
fileout = "junk/mask.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
binarize(filein, fileout)
im = Image.open("junk/mask.png")
im.save("/home/jack/Desktop/TENSORFLOW/images/BINARY_"+timestr+"_.png")

#binarize(filein, fileout)
img = Image.open("junk/mask.png").convert('L')
img = ImageChops.invert(img)
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/BINARY2"+timestr+"_.png"
outlineP(filename1,outfile_png)


from PIL import Image
import random
from random import randint
import time
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
print("file1",file1)
#file2 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file1 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")
path1 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image)
print (file2)
background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")
path2 = r"//home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
  
file3=(path2+base_image)
print(file2) 

path2 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
   
file3=(path2+base_image)
print("file3",file3) 
img = Image.open(file3).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/XX"+timestr+"_.png"
outlineP(filename1,outfile_png)


from PIL import Image
import random
from random import randint
import time
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
print("file1",file1)
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print("original.size",original.size)
print("------------------------")


path1 = r"/home/jack/Documents/jpg/720/"
base_image1 = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image1)
print ("file2: ",file2)
background = Image.open(file2).convert('RGB')
background  = background.resize(dim, Image.NEAREST)
print("background.size",background.size)
print("------------------------")

"""
path2 = r"/home/jack/Documents/jpg/720/"
base_image2 = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
file3=(path2+base_image2)
file3 = "/home/jack/Desktop/TENSORFLOW/images/warrior-20.png"
print("file3: ",file3) 
print("------------------------")
path3 = r"/home/jack/Documents/jpg/720/"
base_image = random.choice([
   x for x in os.listdir(path3)
   if os.path.isfile(os.path.join(path3, x))
   ])
"""   
file4=(path3+base_image2)
file4=(path2+base_image1)
file4=(path1+base_image)
#file4=(path+base_image)
print("file4: ",file2) 
img = Image.open(file2).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)
timestr = time.strftime("%Y%m%d-%H%M%S")
png = "/home/jack/Desktop/TENSORFLOW/images/MASK"+timestr+"_.png"
mask.save(png)
print("mask.size",mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/XX"+timestr+"_.png"
outlineP(filename1,outfile_png)


from OutlineImage import outlineP
filename1 = "resultmasked.png" 
outfile_png = "resultmasked-outlined.png" 
outlineP(filename1,outfile_png)





from PIL import Image
dim = (720, 480)
path = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
file1=(path+base_image)
file2 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file1 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
original = Image.open(file1).convert('RGB')
original = original.resize(dim, Image.NEAREST)
print(original.size)
path1 = r"/home/jack/Desktop/TENSORFLOW/images/"
base_image = random.choice([
    x for x in os.listdir(path1)
    if os.path.isfile(os.path.join(path1, x))
    ])
file2=(path1+base_image)
print (file2)





background = Image.open(file2).convert('RGB')

background  = background.resize(dim, Image.NEAREST)

print(background.size)
path2 = r"/home/jack/Desktop/Imagedata/2-binary_images/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
   
file3=(path2+base_image)
#file3="/home/jack/Desktop/Imagedata/2-binary_images/binar_44_20220925161510.jpg"
print(file3) 



img = Image.open(file3).convert('L')
dim = (720, 480)
mask =img.resize(dim, Image.NEAREST)


print(mask.size)
result = Image.composite(original, background, mask)
result.save('resultmasked.png')
result

#file1 = "/home/jack/Desktop/TENSORFLOW/otoro-neural-network/00003.png"
#file2 = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png"
room = cv2.imread(file2)
logo = cv2.imread(file1)
path2 = r"/home/jack/Desktop/Imagedata/2-binary_images/"
base_image = random.choice([
   x for x in os.listdir(path2)
   if os.path.isfile(os.path.join(path2, x))
   ])
    
file3=(path2+base_image)
#--- Resizing the logo to the shape of room image ---
logo = cv2.imread(file3)
logo = cv2.resize(logo, (room.shape[1], room.shape[0]))

#--- Apply Otsu threshold to blue channel of the logo image ---
ret, logo_mask = cv2.threshold(logo[:,:,0], 0, 255, cv2.THRESH_BINARY|cv2.THRESH_OTSU)
#cv2.imshow('logo_mask', logo_mask)

room2 = room.copy() 

#--- Copy pixel values of logo image to room image wherever the mask is white ---
room2[np.where(logo_mask == 255)] = logo[np.where(logo_mask == 255)]
timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "/home/jack/Desktop/TENSORFLOW/junk/XX"+timestr+"_"+str(count)+".png"
cv2.imwrite(filename, room2)



import cv2
  
def binarize(filein, fileout):
    # read the image file
    img = cv2.imread(filein, 2)
    ret, bw_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    # converting to its binary form
    bw = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    cv2.imwrite(fileout, bw_img)
    output = fileout
    return output
    
filein = '/home/jack/Desktop/TENSORFLOW/images/MASK20221109-224545_.png'
fileout = "/home/jack/Desktop/TENSORFLOW/images/binary2.png"
binarize(filein, fileout)

original = Image.open(fileout).convert('RGB')
original

original = Image.open(filename).convert('RGB')
original

import mathsvg
image = mathsvg.SvgImage(pixel_density = 10, view_window = (( -1, -1 ), ( 1, 1 )))
image.draw_circle([0, 0], 1.1)
image.save("simple-example.svg")


In this 5th part of the image processing series, we discuss more on the Arithmetic and bitwise operations, and masking of images in Python.

It is recommended that the previous articles be run through, before starting off on your masked learning adventure here.
Setting up the environment

The following lines of code are used in all of the applications given below. We’ll include those here instead so you don’t have to read through a huge block of code.

Helps reduce clutter :)

# importing numpy to work with pixels
import numpy as np

# importing argument parsers
import argparse

# importing the OpenCV module
import cv2


# initializing an argument parser object
ap = argparse.ArgumentParser()

# adding the argument, providing the user an option
# to input the path of the image
ap.add_argument("-i", "--image", required=True, help="Path to the image")

# parsing the argument
args = vars(ap.parse_args())

# reading the image location through args
# and reading the image using cv2.imread
image = cv2.imread(args["image"])

Arithmetic Operations on Images using Python

Arithmetic Operations allow us to enhance a lot of aspects of an image.

We can work with lighting, shadows, the red, blue, and green color enhancement.

A lot of image filters on applications use the same method to alter and beautify photographs as well.

So, let’s get started with all of the code!

First, in order to understand whether the limit can go over 255 or 0, we can conduct a simple test, which provides us with 255 and 0.

# printing out details of image min, max and the wrap around
print("max of 255 :", str(cv2.add(np.uint8([200]), np.uint8([100]))))
print("min of 0 :", str(cv2.subtract(np.uint8([50]), np.uint8([100]))))

print("wrap around :", str(np.uint8([200]) + np.uint8([100])))
print("wrap around :", str(np.uint8([50]) - np.uint8([100])))

In this example, we are increasing the intensity of all the pixels in the image by 100.

# adding pixels of value 255 (white) to the image
M = np.ones(image.shape, dtype="uint8") * 100
added = cv2.add(image, M)
cv2.imshow("Added", added)
cv2.waitKey(0)

This is done by constructing a matrix with the same size as our images using the NumPy module, and adding it with our image.

In case we wish to darken an image, we subtract from the pixel values of the image, as shown below,

# adding pixels of value 0 (black) to the image
M = np.ones(image.shape, dtype="uint8") * 50
subtracted = cv2.subtract(image, M)
cv2.imshow("Subtracted", subtracted)
cv2.waitKey(0)

This should provide you with two different variations of the original image, one lighter, and the other darker.
Bitwise Operations

We use Bitwise operations a lot of the times while attempting to mask images.

This feature of OpenCV allows us to filter out the part of the image that is relevant to us.
Setting up

To work on Bitwise operations, we’ll first need two variables or images that we can conduct the operations on.

So, let’s create a bitwise square and a bitwise circle through which we can use the bitwise operations.

Note that bitwise operations require the images to be black and white.

# creating a square of zeros using a variable
rectangle = np.zeros((300, 300), dtype="uint8")
cv2.rectangle(rectangle, (25, 25), (275, 275), 255, -1)
cv2.imshow("Rectangle : ", rectangle)

# creating a circle of zeros using a variable
circle = np.zeros((300, 300), dtype="uint8")
cv2.circle(circle, (150, 150), 150, 255, -1)
cv2.imshow("Circle : ", circle)

The output images that you receive should look like this,
Image 4
Bit Square
Combine with the AND operation

Bitwise addition refers to the addition of two different images, and decide which is to be displayed using an AND operation on each pixel of the images.

# the bitwise_and function executes the AND operation
# on both the images
bitwiseAnd = cv2.bitwise_and(rectangle, circle)
cv2.imshow("AND", bitwiseAnd)
cv2.waitKey(0)

Bitwise addition of both the circle and the square gives us an output which should look like this,
Image 6
AND Bit Square
Given a choice with the OR operation

Bitwise OR provides us with a product of the two images with an OR operation performed on each pixel of the images.

# the bitwise_or function executes the OR operation
# on both the images
bitwiseOr = cv2.bitwise_or(rectangle, circle)
cv2.imshow("OR", bitwiseOr)
cv2.waitKey(0)

Upon performing the operation Bitwise OR, you should receive something like this,
Image 7
OR Bit Square
Exclusivity with the XOR operation

Another operation that is provided by the cv2 module is the XOR operation, which we can use through the bitwise_xor function.

# the bitwise_xor function executes the XOR operation
# on both the images
bitwiseXor = cv2.bitwise_xor(rectangle, circle)
cv2.imshow("XOR", bitwiseXor)
cv2.waitKey(0)

Image 8
XOR Bit Square
Negation using the NOT operation

Lastly, we have the negation operation, which is performed using the bitwise_not function.

The NOT operation only requires a single image as we’re not adding or subtracting anything here.

We still use it on both here however, that’s also an option.

# the bitwise_not function executes the NOT operation
# on both the images
bitwiseNot = cv2.bitwise_not(rectangle, circle)
cv2.imshow("NOT", bitwiseNot)
cv2.waitKey(0)

The circle is inside the square in this case, and as such is not visible,
Image 9
Not Bit Square
Masking of images using Python OpenCV

Masking is used in Image Processing to output the Region of Interest, or simply the part of the image that we are interested in.

We tend to use bitwise operations for masking as it allows us to discard the parts of the image that we do not need.

So, let’s get started with masking!

The process of masking images

We have three steps in masking.

    Creating a black canvas with the same dimensions as the image, and naming it as mask.
    Changing the values of the mask by drawing any figure in the image and providing it with a white color.
    Performing the bitwise ADD operation on the image with the mask.

Following the same process, let’s create a few masks and use them on our image.

First, let’s work with a rectangle mask.

# creating a mask of that has the same dimensions of the image
# where each pixel is valued at 0
mask = np.zeros(image.shape[:2], dtype="uint8")

# creating a rectangle on the mask
# where the pixels are valued at 255
cv2.rectangle(mask, (0, 90), (290, 450), 255, -1)
cv2.imshow("Mask", mask)

# performing a bitwise_and with the image and the mask
masked = cv2.bitwise_and(image, image, mask=mask)
cv2.imshow("Mask applied to Image", masked)
cv2.waitKey(0)

Now, let’s try it out with a circle mask.

# creating a mask of that has the same dimensions of the image
# where each pixel is valued at 0
mask = np.zeros(image.shape[:2], dtype="uint8")

# creating a rectangle on the mask
# where the pixels are valued at 255
cv2.circle(mask, (145, 200), 100, 255, -1)
cv2.imshow("Mask", mask)

# performing a bitwise_and with the image and the mask
masked = cv2.bitwise_and(image, image, mask=mask)
cv2.imshow("Mask applied to Image", masked)
cv2.waitKey(0)

If everything works out just fine, we should receive outputs which look something like this,
Image 10
Rectangular Mask
Conclusion

We’re finally getting started with the core of Image Processing, and understanding bitwise operations and masking in it is important.

It helps us to block out parts or only take in parts of the image that we are interested in, so, quite a useful concept.

!pwd

from TwitterAPI import TwitterAPI

data_in = open("Tweetsfull.txt","w")
data_in.close()

data_in = open("Tweetsfull.txt","a")
from APIkey import APIkey
consumer_key=APIkey()[0]
consumer_secret=APIkey()[1]
access_token_key=APIkey()[2]
access_token_secret=APIkey()[3]
from TwitterAPI import TwitterAPI
api = TwitterAPI(consumer_key, consumer_secret, access_token_key, access_token_secret)
r = api.request('search/tweets', {'q':'Trump',
        'query':'Ukrain',
        'tweet.fields':'author_id',
        'expansions':'author_id'})
for item in r:
        print(item)
        data_in.write("______________________________")
        data_in.write(str(item))
        data_in.write("______________________________")

data_in = open("Tweets.txt","a")
from APIkey import APIkey
consumer_key=APIkey()[0]
consumer_secret=APIkey()[1]
access_token_key=APIkey()[2]
access_token_secret=APIkey()[3]
from TwitterAPI import TwitterAPI
api = TwitterAPI(consumer_key, consumer_secret, access_token_key, access_token_secret)
r = api.request('search/tweets', {'q':'freedom'})
for item in r:
        text = item['text']
        #name = item['screen_name']
        #data_in.write(name)
        data_in.write(text)
        print(text,"\n")


from APIkey import APIkey
APIkey()[0]
APIkey()[1]
APIkey()[2]
APIkey()[3]

text = my_dict['text']

import os
from twython import Twython
from APIkey import APIkey
# Election Democrates Republicans 
SEARCH = 'Republicans'
filename = SEARCH+'_TwitterTwython.txt'
if os.path.exists(filename):
    append_write = 'a' # append if already exists
else:
    append_write = 'w' # make a new file if not
dData = open(filename,append_write)
APP_KEY = APIkey()[0]
APP_SECRET = APIkey()[1]
twitter= Twython(app_key=APP_KEY,app_secret=APP_SECRET)
for status in twitter.search(q= SEARCH ,count =100)["statuses"]:
    user =status["user"]["screen_name"].encode('utf-8')
    text =status["text"]
    data = "{0} {1} {2}".format(user ,text,'\n')
    print(data)
    dData.write(data)
dData.close()    



import tweepy

import tweepy
 
# API keyws that yous saved earlier
api_key = "EteOELZAulNVMe2Vn1fjRNiLF"
api_secrets = "zqhlZOsnZsoQaeFMhaVCWbK7vSwukLr1MUHmbDnXappWesRDNc"
access_token = "296906916-xiWeizroDnU5p60XG9YzzRy0fe4v9XqxTSFshWa6"
access_secret = "sJi0jI3YdkJTSDZ2tYFosHVqFhM2ksXnaAZMML0V1eBB9"
 
# Authenticate to Twitter
auth = tweepy.OAuthHandler(api_key,api_secrets)
auth.set_access_token(access_token,access_secret)
 
api = tweepy.API(auth)
 
try:
    api.verify_credentials()
    print('Successful Authentication')
except:
    print('Failed authentication')

API Key  ZXbM2vxRLwu3ZrenMBl1ZILe1
API Key Secret  nbfyN3SfMrrerqTFFCfOuFjECy2lRqfmxwJrivqx2CxnpbAuH7
Access Token 296906916-xiWeizroDnU5p60XG9YzzRy0fe4v9XqxTSFshWa6
Access Token Secret sJi0jI3YdkJTSDZ2tYFosHVqFhM2ksXnaAZMML0V1eBB9



Developer Agreement

Effective: March 10, 2020

This Twitter Developer Agreement (“Agreement”) is made between you (either an individual or an entity, referred to herein as “you”) and Twitter (as defined below) and governs your access to and use of the Licensed Material (as defined below).Your use of Twitter’s websites, SMS, APIs, email notifications, applications, buttons, embeds, ads, and our other covered services is governed by our general Terms of Service and Privacy Policy.

PLEASE READ THE TERMS AND CONDITIONS OF THIS AGREEMENT CAREFULLY, INCLUDING ANY LINKED TERMS REFERENCED BELOW, WHICH ARE PART OF THIS LICENSE AGREEMENT. BY USING THE LICENSED MATERIAL, YOU ARE AGREEING THAT YOU HAVE READ, AND THAT YOU AGREE TO COMPLY WITH AND TO BE BOUND BY THE TERMS AND CONDITIONS OF THIS AGREEMENT AND ALL APPLICABLE LAWS AND REGULATIONS IN THEIR ENTIRETY WITHOUT LIMITATION OR QUALIFICATION. IF YOU DO NOT AGREE TO BE BOUND BY THIS AGREEMENT, THEN YOU MAY NOT ACCESS OR OTHERWISE USE THE LICENSED MATERIAL. THIS AGREEMENT IS EFFECTIVE AS OF THE FIRST DATE THAT YOU USE THE LICENSED MATERIAL (“EFFECTIVE DATE”).

IF YOU ARE AN INDIVIDUAL REPRESENTING AN ENTITY, YOU ACKNOWLEDGE THAT YOU HAVE THE APPROPRIATE AUTHORITY TO ACCEPT THIS AGREEMENT ON BEHALF OF SUCH ENTITY. YOU MAY NOT USE THE LICENSED MATERIAL AND MAY NOT ACCEPT THIS AGREEMENT IF YOU ARE NOT OF LEGAL AGE TO FORM A BINDING CONTRACT WITH TWITTER, OR YOU ARE BARRED FROM USING OR RECEIVING THE LICENSED MATERIAL UNDER APPLICABLE LAW.

I. Twitter API and Twitter Content

A. Definitions

    Broadcast ID - A unique identification number generated for each Periscope Broadcast.

    Developer Site ‒ Twitter’s developer site located at https://developer.twitter.com

    End Users ‒ Users of your Services.

    Licensed Material ‒ A collective term for the Twitter API, Twitter Content and Twitter Marks.

    Periscope Broadcast - A live or on-demand video stream that is publicly displayed on Twitter Applications and is generated by a user via Twitter’s Periscope Producer feature (as set forth at https://help.periscope.tv/customer/en/portal/articles/2600293).

    Services ‒ Your services, websites, applications and other offerings (including research) that display Twitter Content or otherwise use the Licensed Material.

    Tweet ID ‒ A unique identification number generated for each Tweet.

    Tweet ‒ a posting made on Twitter Applications.

    Twitter Content ‒ Tweets, Tweet IDs, Twitter end user profile information, Periscope Broadcasts, Broadcast IDs and any other data and information made available to you through the Twitter API or by any other means authorized by Twitter, and any copies and derivative works thereof.

    “Twitter” means Twitter, Inc., with an office located at 1355 Market Street, Suite 900, San Francisco, CA, 94103, USA.  If you enter into this Agreement or an Order outside of the United States, Canada or Latin America, Twitter International Company with its registered offices at One Cumberland Place, Fenian Street, Dublin 2, D02 AX07, Ireland (“TIC”) is the contracting entity. 

    Direct Message - A message that is privately sent on Twitter Applications by one end user to one or more specific end user(s) using Twitter’s Direct Message function.

    Twitter API ‒ The Twitter Application Programming Interface (“API”), Software Development Kit (“SDK”) and/or the related documentation, data, code, and other materials provided by Twitter with the API, as updated from time to time, including without limitation through the Developer Site.

    Twitter Marks ‒ The Twitter name, trademarks, and logos that Twitter makes available to you, including via the Developer Site.

    Twitter Applications ‒ Twitter’s consumer facing products, services, applications, websites, web pages, platforms, and other offerings, including without limitation, those offered via https://twitter.com and Twitter’s mobile applications.

B. License from Twitter. Subject to the terms and conditions in this Agreement and the Developer Policy (as a condition to the grant below), Twitter hereby grants you and you accept a non-exclusive, royalty free, non-transferable, non-sublicensable, revocable license solely to:

    Use the Twitter API to integrate Twitter Content into your Services or conduct analysis of such Twitter Content, as explicitly approved by Twitter;

    Copy a reasonable amount of and display the Twitter Content on and through your Services to End Users, as permitted by this Agreement;

    Modify Twitter Content only to format it for display on your Services; and

    Use and display Twitter Marks, solely to attribute Twitter’s offerings as the source of the Twitter Content, as set forth herein.

C. License to Twitter You hereby grant Twitter and Twitter accepts a non-exclusive, royalty free, non-transferable, non-sublicensable revocable license to access, index, and cache by any means, including web spiders and/or crawlers, any webpage or applications on which you display Twitter Content using embedded Tweets or embedded timelines.

D. Incorporated Terms. Your use of the Licensed Material is further subject to and governed by the following terms and conditions:

    the Twitter Developer Policy;

    the API Restricted Use Rules;

    the Twitter Rules;

    as it relates to your display of any of the Twitter Content, the Display Requirements;

    as it relates to your use and display of the Twitter Marks, the Twitter Brand Resources; 

    as it relates to taking automated actions on your account, the Automation Rules;

    as it relates to your use of Periscope, the Periscope Community Guidelines, and the Periscope Trademark Guidelines.

The Developer Policy, API Restricted Use Rules, Twitter Rules, Display Requirements, Brand Resources, Automation Rules, Periscope Community Guidelines, and Periscope Trademark Guidelines are collectively referred to herein as the “Incorporated Developer Terms”. You agree to the Incorporated Developer Terms, which are hereby incorporated by reference and are available in hardcopy upon request to Twitter. In the event of a conflict between the Incorporated Developer Terms and this Agreement, this Agreement shall control. None of the Incorporated Developer Terms expand or extend the license to the Twitter API, Twitter Content or Twitter Marks granted in this Agreement.
 

II. Restrictions on Use of Licensed Materials

A. Reverse Engineering and other Restrictions. You will not or attempt to (and will not allow others to) a) reverse engineer, decompile, disassemble or translate the Twitter API, or otherwise attempt to derive source code, trade secrets or know-how in or underlying any Twitter API or any portion thereof; b) interfere with, modify, disrupt or disable features or functionality of the Twitter API, including without limitation any such mechanism used to restrict or control the functionality, or defeat, avoid, bypass, remove, deactivate or otherwise circumvent any software protection or monitoring mechanisms of the Twitter API; b) sell, rent, lease, sublicense, distribute, redistribute, syndicate, create derivative works of, assign or otherwise transfer or provide access to, in whole or in part, the Licensed Material to any third party except as expressly permitted herein; d) provide use of the Twitter API on a service bureau, rental or managed services basis or permit other individuals or entities to create links to the Twitter API or "frame" or "mirror" the Twitter API on any other server, or wireless or Internet-based device, or otherwise make available to a third party, any token, key, password or other login credentials to the Twitter API; or e) use the Licensed Material for any illegal, unauthorized or other improper purposes, including without limitation to store or transmit infringing, libelous, or otherwise unlawful or tortious material, to store or transmit malicious code, or to store or transmit material in violation of third-party privacy rights;  (f) utilize the Licensed Material to derive or obtain non-public information of individual Twitter users, including without limitation a user’s location; (g) interfere with or disrupt the integrity or performance of the Twitter Applications, Twitter API or Twitter Content contained therein, including by disrupting the ability of any other person to use or enjoy the Twitter Applications, Twitter API or Twitter Content, or attempt to gain unauthorized access to the Twitter Applications, Twitter API, Twitter Content or related systems or networks; (h) remove or alter any proprietary notices or marks on the Twitter Content; or (i) use Twitter Content, by itself or bundled with third party data, or derivative analysis therefrom, to target users with advertising outside of the Twitter Applications, including without limitation on other advertising networks, via data brokers, or through any other advertising or monetization services.

B. Commercial Use Restrictions. If your Project (located in your Twitter developer portal) is designated as ‘non-commercial’ you shall not make Commercial Use of the Licensed Materials. Commercial Use restrictions may not apply to those officially registered: (1) non-profits or (2) NGOs.  “Commercial Use” means any use of the Licensed Materials (which includes access to the Twitter API): (i) by (or on behalf of) a business (i.e. entity whose primary purpose is to earn revenue through a product or service) or (ii) as part of a product or service that is monetized (including but not limited to website advertising, licensing fees, in-app promotions, and sponsorships).  

C. No Monitoring or Measuring. Despite any other provision herein, you may only use the following information for non-commercial, internal purposes (e.g., to improve the functionality of the Services): (a) aggregate Twitter Applications user metrics, such as number of active users or accounts on Twitter Applications; (b) the responsiveness of Twitter Applications; and (c) results, usage statistics, data or other information (in the aggregate or otherwise) derived from analyzing, using, or regarding the performance of the Twitter API. All such information is Twitter’s Confidential Information.

D. Rate Limits. You will not attempt to exceed or circumvent limitations on access, calls and use of the Twitter API ("Rate Limits"), or otherwise use the Twitter API in a manner that exceeds reasonable request volume, constitutes excessive or abusive usage, or otherwise fails to comply or is inconsistent with any part of this Agreement. If you exceed or Twitter reasonably believes that you have attempted to circumvent Rate Limits, controls to limit use of the Twitter APIs or the terms and conditions of this Agreement, then your ability to use the Licensed Materials may be temporarily suspended or permanently blocked. Twitter may monitor your use of the Twitter API to improve the Licensed Materials and Twitter Applications  and to ensure your compliance with this Agreement and Incorporated Developer Terms.

E. Location Data. You will not  (and you will not allow others to) aggregate, cache, or store location data and other geographic information contained in the Twitter Content, except in conjunction with the Twitter Content to which it is attached. You may only use such location data and geographic information to identify the location tagged by the Twitter Content. You may not use location data or geographic information on a standalone basis.

F. Use of Twitter Marks. The Twitter Marks may not be included in or as part of your registered corporate name, any of your logos, or any of your service or product names. Moreover, you may not create any derivative works of the Twitter Marks or use the Twitter Marks in a manner that creates or reasonably implies an inaccurate sense of endorsement, sponsorship, or association with Twitter. You will not otherwise use business names and/or logos in a manner that can mislead, confuse, or deceive users of your Services. All use of the Twitter Marks and all goodwill arising out of such use, will inure to Twitter's benefit. You shall not use the Twitter Marks except as expressly authorized herein without Twitter's prior consent. You will not remove or alter any proprietary notices or Twitter Marks on the Licensed Material.

G. Security. You will maintain the security of the Twitter API and will not make available to a third party, any token, key, password or other login credentials to the Twitter API. You will use industry standard security measures to prevent unauthorized access or use of any of the features and functionality of the Twitter API, including access by viruses, worms, or any other harmful code or material. Additionally, you will keep Twitter Content (including, where applicable, personal data) confidential and secure from unauthorized access by using industry-standard organizational and technical safeguards for such data, and with no less care than it uses in connection with securing similar data you store. You will immediately notify Twitter consult and cooperate with investigations, assist with any required notices, and provide any information reasonably requested by Twitter if you know of or suspects any breach of security or potential vulnerability related to the Licensed Material and will promptly remedy such breach or potential vulnerability resulting from Your access to the Licensed Material.
 

III. Updates and Removals

A. Updates. You acknowledge that Twitter may update or modify the Licensed Materials from time to time, and at its sole discretion (in each instance, an “Update”). You are required to implement and use the most current version of the Licensed Materials and to make any changes to your Services that are required as a result of such Update, at your sole cost and expense. Updates may adversely affect the manner in which your Services access or communicate with the Twitter API or display Twitter Content. Your continued access or use of the Licensed Materials following an update or modification will constitute binding acceptance of the Update.

B. Removals. If Twitter Content is deleted, gains protected status, or is otherwise suspended, withheld, modified, or removed from the Twitter Applications (including removal of location information), you will make all reasonable efforts to delete or modify such Twitter Content (as applicable) as soon as possible, and in any case within 24 hours after a written request to do so by Twitter or by a Twitter user with regard to their Twitter Content, unless prohibited by applicable law or regulation and with the express written permission of Twitter.

IV. Ownership and Feedback

A. Ownership. The Licensed Material is licensed, not sold, and Twitter retains and reserves all rights not expressly granted in this Agreement. You expressly acknowledge that Twitter, its licensors and its end users retain all worldwide right, title and interest in and to the Licensed Material, including all rights in patents, trademarks, trade names, copyrights, trade secrets, know-how, data (including all applications therefor), and all proprietary rights under the laws of the United States, any other jurisdiction or any treaty ("IP Rights"). You agree not to do anything inconsistent with such ownership, including without limitation, challenging Twitter’s ownership of the Twitter Marks, challenging the validity of the licenses granted herein, or otherwise copying or exploiting the Twitter Marks during or after the termination of this Agreement, except as specifically authorized herein. If you acquire any rights in the Twitter Marks or any confusingly similar marks, by operation of law or otherwise, you will, at no expense to Twitter, immediately assign such rights to Twitter.

B. Feedback. You may provide Twitter with comments concerning the Licensed Material, Twitter Applications or your evaluation and use thereof (collectively, "Feedback"). You hereby grant Twitter all rights, title and ownership of such Feedback (including all intellectual property rights therein), and Twitter may use the Feedback for any and all commercial and non-commercial purposes with no obligation of any kind to you.

V. Termination. Twitter may immediately terminate or suspend this Agreement, any rights granted herein, and/or your license to the Licensed Materials, at its sole discretion at any time, for any reason by providing notice to you. You may terminate this Agreement at any time by ceasing your access to the Twitter API and use of all Twitter Content. Upon termination of this Agreement, (a) all licenses granted herein immediately expire and you must cease use of all Licensed Materials; and (b) you shall permanently delete all Licensed Material and Twitter Marks in all forms and types of media, and copies thereof, in your possession. The parties to this Agreement will not be liable to each other for any damages resulting solely from termination of this Agreement as permitted under this Agreement. Sections II (Restrictions on Use of Licensed Materials), IV (Ownership and Feedback), V (Termination), VI (Confidentiality), VII (Compliance Audit), VIII (Warranty Disclaimer), IX (Indemnification), X (Limitation of Liability) and XII (Miscellaneous) of this Agreement will survive the termination of this Agreement.

VI. Confidentiality. You may be given access to certain non-public information, software, and specifications relating to the Licensed Material (“Confidential Information”), which is confidential and proprietary to Twitter. You may use this Confidential Information only as necessary in exercising your rights granted in this Agreement. You may not disclose any of this Confidential Information to any third party without Twitter’s prior written consent. You agree that you will protect this Confidential Information from unauthorized use, access, or disclosure in the same manner that you would use to protect your own confidential and proprietary information of a similar nature and in no event with less than a reasonable degree of care.

VII. Compliance Audit. Twitter, or a third party agent subject to obligations of confidentiality, shall be entitled to inspect and audit any records or activity related to your access to the Licensed Material for the purpose of verifying compliance with this Agreement. Twitter may exercise its audit right at any time upon notice. You will provide your full cooperation and assistance with such audit and provide access to all Licensed Material in your possession or control, applicable agreements and records. Without limiting the generality of the foregoing, as part of the audit, Twitter may request, and you agree to provide, a written report, signed by an authorized representative, listing your then-current deployment of the Licensed Material and Twitter Content. The rights and requirements of this section will survive for one (1) year following the termination of this Agreement.

VIII. Warranty Disclaimer. THE LICENSED MATERIAL IS PROVIDED TO YOU “AS IS”, “WHERE IS”, WITH ALL FAULTS AND EACH PARTY DISCLAIMS ALL WARRANTIES, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, INCLUDING WITHOUT LIMITATION WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, AND ANY WARRANTIES OR CONDITIONS ARISING OUT OF THIS AGREEMENT, COURSE OF DEALING OR USAGE OF TRADE. TWITTER DOES NOT WARRANT THAT THE LICENSED MATERIAL OR ANY OTHER TWITTER PRODUCT OR SERVICE PROVIDED HEREUNDER WILL MEET ANY OF YOUR REQUIREMENTS OR THAT USE OF SUCH LICENSED MATERIAL OR OTHER PRODUCTS OR SERVICES WILL BE ERROR-FREE, UNINTERRUPTED, VIRUS-FREE OR SECURE. THIS DISCLAIMER OF WARRANTY MAY NOT BE VALID IN SOME JURISDICTIONS AND YOU MAY HAVE WARRANTY RIGHTS UNDER LAW WHICH MAY NOT BE WAIVED OR DISCLAIMED. ANY SUCH WARRANTY EXTENDS ONLY FOR THIRTY (30) DAYS FROM THE EFFECTIVE DATE OF THIS AGREEMENT (UNLESS SUCH LAW PROVIDES OTHERWISE).

IX. Indemnification. You shall defend Twitter against any and all actions, demands, claims and suits (including without limitation product liability claims), and indemnify and hold Twitter harmless from any and all liabilities, damages and costs (including without limitation reasonable attorneys' fees) to the extent arising out of: (i) your use of the Licensed Material in any manner that is inconsistent with this Agreement; or (ii) the performance, promotion, sale or distribution of your Services. In the event Twitter seeks indemnification or defense from you under this provision, Twitter will promptly notify you in writing of the claim(s) brought against Twitter for which it seeks indemnification or defense. Twitter reserves the right, at its option and sole discretion, to assume full control of the defense of claims with legal counsel of its choice. You may not enter into any third party agreement, which would, in any manner whatsoever, affect the rights of Twitter, constitute an admission of fault by Twitter or bind Twitter in any manner, without the prior written consent of Twitter. In the event Twitter assumes control of the defense of such claim, Twitter shall not settle any such claim requiring payment from you without your prior written approval.

X. Limitation of Liability. IN NO EVENT WILL TWITTER BE LIABLE TO YOU OR ANY END USERS FOR ANY INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, PUNITIVE OR CONSEQUENTIAL DAMAGES OR ANY LOSS OF OR DAMAGE TO USE, DATA, BUSINESS, GOODWILL OR PROFITS ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT. IN ANY CASE, TWITTER'S AGGREGATE LIABILITY FOR ANY AND ALL CLAIMS UNDER THIS AGREEMENT WILL NOT EXCEED $50.00 USD. THE FOREGOING LIMITATIONS, EXCLUSIONS AND DISCLAIMERS SHALL APPLY REGARDLESS OF WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, AND WHETHER OR NOT TWITTER HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. INSOFAR AS APPLICABLE LAW PROHIBITS ANY LIMITATION ON LIABILITY HEREIN, THE PARTIES AGREE THAT SUCH LIMITATION WILL BE AUTOMATICALLY MODIFIED, BUT ONLY TO THE EXTENT SO AS TO MAKE THE LIMITATION COMPLIANT WITH APPLICABLE LAW. THE PARTIES AGREE THAT THE LIMITATIONS ON LIABILITIES SET FORTH HEREIN ARE AGREED ALLOCATIONS OF RISK AND SUCH LIMITATIONS WILL APPLY NOTWITHSTANDING THE FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY.

XI. Agreement Updates. Twitter may update or modify this Agreement or any of the Incorporated Developer Terms from time to time at its sole discretion by posting the changes on this site or by otherwise notifying you (such notice may be via email). You acknowledge that these updates and modifications may adversely affect how your Service accesses or communicates with the Twitter API. If any change is unacceptable to you, your only recourse is to cease all use of the Licensed Material. Your continued access or use of the Licensed Material will constitute binding acceptance of such updates and modifications.

XII. Miscellaneous.

A. Assignment. You may not assign any of the rights or obligations granted hereunder, in whole or in part, whether voluntarily or by operation of law, contract, merger (whether you are the surviving or disappearing entity), stock or asset sale, consolidation, dissolution, through government action or otherwise, except with the prior written consent of Twitter, Inc. Twitter, Inc. is authorized to sign modifications and consents on behalf of Twitter International Company, an Irish company responsible for the information of Twitter users who live outside the United States. Any attempted assignment in violation of this paragraph is null and void, and Twitter may terminate this Agreement.

B. User Protection. Unless explicitly approved otherwise by Twitter in writing, you may not use, or knowingly display, distribute, or otherwise make Twitter Content, or information derived from Twitter Content, available to any entity for the purpose of: (a) conducting or providing surveillance or gathering intelligence, including but not limited to investigating or tracking Twitter users or Twitter Content; (b) conducting or providing analysis or research for any unlawful or discriminatory purpose, or in a manner that would be inconsistent with Twitter users' reasonable expectations of privacy; (c) monitoring sensitive events (including but not limited to protests, rallies, or community organizing meetings); or (d) targeting, segmenting, or profiling individuals based on sensitive personal information, including their health (e.g., pregnancy), negative financial status or condition, political affiliation or beliefs, racial or ethnic origin, religious or philosophical affiliation or beliefs, sex life or sexual orientation, trade union membership, Twitter Content relating to any alleged or actual commission of a crime, or any other sensitive categories of personal information prohibited by law.

C. Government Use. If you will display, distribute, or otherwise make available any Twitter Content to End Users that are, or that act on behalf of, any government-related entity (each a “Government End User”), You will identify all such Government End Users when submitting that use case for review to Twitter and will thereafter notify Twitter in writing of any new Government End Users, and any new use cases with existing Government End Users, prior to the Services displaying, distributing, or otherwise making available any Twitter Content to a Government End User or for any new use case, and Twitter will have the right at anytime to prohibit you from making Twitter Content available to any  new Government End User. In no event shall your use, or knowingly display, distribute, or otherwise make Twitter Content, or information derived from Twitter Content, available to any Government End User whose primary function or mission includes conducting surveillance or gathering intelligence. If law enforcement personnel request information about Twitter or its users for the purposes of an ongoing investigation, You may refer them to Twitter’s Guidelines for Law Enforcement located at https://t.co/le. The Twitter API and Twitter Content are "commercial items" as that term is defined at 48 C.F.R. 2.101, consisting of "commercial computer software" and "commercial computer software documentation" as such terms are used in 48 C.F.R. 12.212.  Any use, modification, derivative, reproduction, release, performance, display, disclosure or distribution of the Twitter API or Twitter Content by any government entity is prohibited, except as expressly permitted by the terms of this Agreement. Additionally, any use by U.S. government entities must be in accordance with 48 C.F.R. 12.212 and 48 C.F.R. 227.7202-1 through 227.7202-4. If you use the Twitter API or Twitter Content in your official capacity as an employee or representative of a U.S., state or local government entity and you are legally unable to accept the indemnity, jurisdiction, venue or other clauses herein, then those clauses do not apply to such entity, but only to the extent as required by applicable law. Contractor/manufacturer is Twitter, Inc. 1355 Market Street, Suite 900, San Francisco, California 94103.

D. Compliance with Laws; Export and Import. Each party will comply with all applicable foreign, federal, state, and local laws, rules and regulations, including without limitation, all applicable laws relating to bribery and/or corruption. The Licensed Material is subject to U.S. export laws and may be subject to import and use laws of the country where it is delivered or used. You agree to abide by these laws. Under these laws, the Licensed Material may not be sold, leased, downloaded, moved, exported, re-exported, or transferred across borders without a license, or approval from the relevant government authority, to any country or to any foreign national restricted by these laws, including countries embargoed by the U.S. Government (currently Cuba, Iran, North Korea, Northern Sudan and Syria); or to any restricted or denied end-user including, but not limited to, any person or entity prohibited by the U.S. Office of Foreign Assets Control; or for any restricted end-use. You will maintain all rights and licenses that are required with respect to your Services.

E. Data Protection Addendum. Twitter International Company (“TIC”), an Irish registered company, controls some of the Twitter Content, as set forth in the Twitter Privacy Policy, and has authorized Twitter to license such Twitter Content under this Agreement (such Twitter Content is “Twitter European Data”). To the extent that you receive Twitter European Data, you agree that in addition to this Agreement, the Twitter Controller-to-Controller Data Protection Addendum located at https://gdpr.twitter.com/en/controller-to-controller-transfers.html shall apply to Twitter European Data and is hereby incorporated by reference.

F. Governing Law; Dispute Resolution. This Agreement will be governed by and construed in accordance with the laws of the State of California, without regard to or application of conflicts of law rules or principles. Any dispute, claim or controversy arising out of or relating to this Agreement or the breach, termination, enforcement, interpretation or validity thereof, including the determination of the scope or applicability of this Agreement to arbitrate, shall be determined by arbitration in San Francisco, CA before a single arbitrator. The arbitration shall be administered by JAMS pursuant to its Comprehensive Arbitration Rules and Procedures. Judgment on the Award may be entered in any court having jurisdiction. You and Twitter hereby expressly waive trial by jury. As an alternative, you may bring your claim in your local "small claims" court, if permitted by that small claims court's rules. You may bring claims only on your own behalf, and unless Twitter agrees, the arbitrator may not consolidate more than one person's claims. Despite the foregoing, you agree that money damages would be an inadequate remedy for Twitter in the event of a breach or threatened breach of a provision of this Agreement protecting Twitter's intellectual property or Confidential Information, and that in the event of such a breach or threat, Twitter, in addition to any other remedies to which it is entitled, is entitled to such preliminary or injunctive relief (including an order prohibiting Company from taking actions in breach of such provisions), without the need for posting bond, and specific performance as may be appropriate. The parties agree that neither the United Nations Convention on Contracts for the International Sale of Goods, nor the Uniform Computer Information Transaction Act (UCITA) shall apply to this Agreement, regardless of the states in which the parties do business or are incorporated. No waiver by Twitter of any covenant or right under this Agreement will be effective unless memorialized in a writing duly authorized by Twitter.

G. Severability. If any part of this Agreement is determined to be invalid or unenforceable by a court of competent jurisdiction, that provision will be enforced to the maximum extent permissible and the remaining provisions of this Agreement will remain in full force and effect.

H. Entire Agreement. This Agreement constitutes the entire agreement among the parties with respect to the subject matter and supersedes and merges all prior proposals, understandings and contemporaneous communications. Any modification to this Agreement must be in a writing signed by both you and Twitter, Inc. This Agreement does not create or imply any partnership, agency or joint venture.

 
Developer Policy
 
Twitter + Developers 

Twitter loves developers. We’re delighted and amazed by the tools and services this community creates by harnessing the power of Twitter data. As part of our commitment to this community, we aim to provide data access that is open and fair for developers, safe for people on Twitter, and beneficial for the Twitter platform as a whole. To further these goals we’ve crafted the Developer Policy as a guide to help people understand our rules and expectations about appropriate API and Twitter Content usage.

This Developer Policy (“Policy”) provides rules and guidelines for developers who interact with Twitter’s ecosystem of applications, services, website, web pages and content. It is part of your contract with Twitter governing access to and use of the Twitter API and Twitter Content (either as part of the Developer Agreement or other written agreement with Twitter). Policy violations are considered violations of your agreement. This Policy may be changed from time to time without notice. Capitalized terms used in this Policy, which are not defined in this Policy, will have the respective meanings ascribed to them in the Developer Agreement or the Master License Agreement.
 
Using this policy

We’ve structured this policy to make it as easy to follow as possible. Please keep information from the following policy sections top of mind as you use the Twitter API and Twitter Content:

    Set Yourself Up for Success - You are responsible for complying with all Twitter policies. It’s important that you review and understand this Policy, as well as the policies we link to in this document, before you access the Twitter API and Twitter Content. The time spent reviewing our policies may save you hours of rework down the road.
    Privacy and Control are Essential - Protecting and defending the privacy of people on Twitter is built into the core DNA of our company. As such, we prohibit the use of Twitter data in any way that would be inconsistent with people’s reasonable expectations of privacy. By building on the Twitter API or accessing Twitter Content, you have a special role to play in safeguarding this commitment, most importantly by respecting people’s privacy and providing them with transparency and control over how their data is used.
    Follow the Platform Usage Guidelines - Getting approved to access the Twitter API and Twitter Content is just the first step. Our Platform Usage Guidelines should be your first stop anytime you have questions about how to ensure policy compliance for your planned use of the Twitter platform.

We’ve provided a lot more detail on what each of these three key sections mean below. Please review them carefully to ensure that your usage of the Twitter API and Twitter Content is consistent with our policies. 

If we believe you are in violation of this Policy (or any other Twitter policy), we may suspend or permanently revoke your access to the Twitter API and Twitter Content. If this happens to you, do not apply for or register additional API keys. Instead, contact us via the API Policy Support form.  

Finally, please note that Twitter may monitor your use of the Twitter API to improve the Twitter Applications, to examine any commercial use, and to ensure your compliance with your approved use case and this Policy.

Thanks for reading, and thank you for building with us! We look forward to seeing what you create!
 
Set yourself up for success

You can avoid many potential pitfalls while using the Twitter API by ensuring that your service has been built the right way from day 1. This section of the Developer Policy contains rules that all developers must follow before using the Twitter API or Twitter Content.

We review all proposed uses of the Twitter developer platform to verify policy compliance — so you’re required to disclose (and update, as applicable) your planned use of the Twitter API and Twitter Content in order to be granted and to maintain access. All new developers must apply for a developer account to access the Twitter API. Current developers without an approved developer account must apply for one as directed to do so by Twitter. As part of this process, you’ll need to provide us with a written description of your intended uses of the Twitter API and Twitter Content.

Your use case description is binding on you, and any substantive deviation from it may constitute a violation of our rules and result in enforcement action. You must notify us of any substantive modification to your use case and receive approval before you may begin using Twitter Content for that new purpose. Failure to do so may result in suspension and termination of your API and data access. You can update your use case by visiting our API Policy Support form, selecting I need to update my developer use case, or as otherwise agreed by Twitter.

By building on the Twitter API or accessing Twitter Content, you must comply with ALL Twitter policies. These include this Developer Policy, the Automation Rules, the Display Requirements, the API Restricted Uses Rules, the Twitter Rules, the Twitter Brand Resources, the Periscope Community Guidelines, and the Periscope Trademark Guidelines, as well as any other agreements you enter into with Twitter relating to your use of the Twitter API or Twitter Content, including but not limited to the Developer Agreement or a Master Licensing Agreement or Order (as applicable). You must also comply with any modifications to these policies and any new policies launched by Twitter. It is your responsibility to monitor the use of your service and to design your service to prevent violations of Twitter policy by people who use it. Failure to do so may result in suspension or termination of your API and Twitter Content access.

You may not register multiple applications for a single use case or substantially similar or overlapping use cases. In this context, a “use case” is a consistent set of analyses, displays, or actions performed via an application. Please note that providing the same service or application to different people (including “white label” versions of a tool or service) counts as a single use case.

As a single exception to these rules, you may create and use a maximum of 3 applications for development, staging, and production instances of the same service. These apps must be registered to a single account, and should be clearly identified (in the name and description) as dev, staging, and prod instances of a single service. You may not use development or staging applications for production purposes.

You must keep all API keys or other access credentials private. You may not use, and may not encourage or facilitate others to use, API keys or other access credentials owned by others.

Your license agreement with Twitter limits your use of the Twitter API and Twitter Content. Among other things, the Twitter API has rate limits which help to ensure fair data usage and to combat spam on the platform. You may not exceed or circumvent rate limits, or any other limitations or restrictions described in this Policy or your agreement with Twitter, listed on the Developer Site, or communicated to you by Twitter.

You may not remove or alter any proprietary notices or marks on Twitter Content received via the Twitter API. This helps to make sure that people know where Twitter Content is coming from, and who it belongs to.

For data integrity and platform health reasons, you may not interfere with, intercept, disrupt, or disable any features of the Twitter API or the Twitter service. In other words, use the APIs as intended and documented on developer.twitter.com. Refer to our HackerOne guidelines for more details about acceptable use.
 
Privacy and control are essential

Twitter takes privacy seriously, and we expect everyone using Twitter Content and the Twitter API to do the same. Any use of the Twitter developer platform, Twitter API, or Twitter Content in a manner that is inconsistent with peoples’ reasonable expectations of privacy may be subject to enforcement action, which can include suspension and termination of API and Twitter Content access.

Your commitment to privacy and control must extend to all uses of Twitter Content and all aspects of the service that you build using our API. To that end, the people using your service must understand and consent to how you use their data, and how you access Twitter on their behalf. This can be accomplished through providing people with a clear, comprehensive, and transparent privacy policy, as well as ensuring that you get express and informed consent from each person using your service before taking any action on their behalf. Please note that a person authenticating into your service does not by itself constitute consent.
 
Consent & permissions

In particular, you must get express and informed consent from people before doing any of the following:

    Taking any actions on their behalf. This includes (but is not limited to): 

        Posting content to Twitter

        Following/unfollowing accounts

        Modifying profile or account information

        Starting a Periscope Broadcast

        Adding hashtags or any other content to Tweets
         

    Republishing content accessed by means other than via the Twitter API or other Twitter tools

    Using someone’s Twitter Content to promote a product or service

    Storing non-public content such as Direct Messages (DMs), or any other private or confidential information

    Sharing or publishing protected content, or any other private or confidential information

If your service allows people to post content to Twitter you must do the following before publishing:

    Show exactly what will be published

    Make it clear to people using your service what geo information (if any) will be added to the content

If your service allows people to post content to both your service and Twitter, you must do the following before publishing:

    Obtain permission to post the content

    Explain where you will post the content

You must respect the protected and blocked status of all Twitter Content. You may not serve content obtained using one person’s authentication token to a different person who is not authorized to view that content.

    Protected accounts: A protected account’s content is only available to people who have been approved by the owner to follow that account. So, if you run a service that accesses protected accounts, you may only do so to serve such content to the specific people with permission to view that content.

    Blocked accounts: People on Twitter are able to block access to their accounts for any reason they choose. Commingling information obtained from tokens (or any other API-based action) to bypass this choice is not permitted.

As Direct Messages (DMs) are non-public in nature, services that provide DM features must take extra steps to safeguard personal privacy. You may not serve DM content to people who are not authorized to view that content. If your service provides DM functionality you must also:

    Notify people if you send read receipt events for DMs. You can do this by providing a notice directly in your service, or by displaying read receipts from other participants in a conversation.

    Get consent before configuring media to be sent in a DM as "shared" (i.e. reusable across multiple DMs). If you do allow media in a DM to be “shared,” you must provide a clear notice that this content will be accessible to anyone with the media’s URL.

 
Content compliance

If you store Twitter Content offline, you must keep it up to date with the current state of that content on Twitter. Specifically, you must delete or modify any content you have if it is deleted or modified on Twitter. This must be done as soon as reasonably possible, or within 24 hours after receiving a request to do so by Twitter or the applicable Twitter account owner, or as otherwise required by your agreement with Twitter or applicable law. This must be done unless otherwise prohibited by law, and only then with the express written permission of Twitter.

Modified content can take various forms. This includes (but is not limited to): 

    Content that has been made private or gained protected status

    Content that has been suspended from the platform

    Content that has had geotags removed from it

    Content that has been withheld or removed from Twitter

 
Off-Twitter matching

We limit the circumstances under which you may match a person on Twitter to information obtained or stored off-Twitter. Off-Twitter matching involves associating Twitter Content, including a Twitter @handle or user ID, with a person, household, device, browser, or other off-Twitter identifier. You may only do this if you have express opt-in consent from the person before making the association, or as described below.

In situations in which you don’t have a person’s express, opt-in consent to link their Twitter identity to an off-Twitter identifier, we require that any connection you draw be based only on information that someone would reasonably expect to be used for that purpose. In addition, absent a person’s express opt-in consent you may only attempt to match your records about someone to a Twitter identity based on:

    Information provided directly to you by the person. Note that records about individuals with whom you have no prior relationship, including data about individuals obtained from third parties, do not meet this standard; and/or

    Public data. “Public data” in this context refers to:

        Information about a person that you obtained from a public, generally-available resource (such as a directory of members of a professional association)

        Information on Twitter about a person that is publicly available, including:

            Tweets

            Profile information, including an account bio and publicly-stated location

            Display name and @handle

 
Your privacy policy

You must display your service’s privacy policy to people before they are permitted to download, install, or sign up to your service. It must disclose at least the following information:

    The information that you collect from people who use your service

    How you use and share that information (including with Twitter)

    How people can contact you with inquiries and requests regarding their information

Your privacy policy must be consistent with all applicable laws, and be no less protective of people than Twitter’s Privacy Policy and the privacy policy of our other services and corporate affiliates. You must cease your access to the Twitter API and the use of all Twitter Content if you are unable to comply with your and/or Twitter’s Privacy Policy.
 
Using geo-data

Use of geo data comes with additional restrictions due to the sensitive nature of this information. If your service adds location information to Tweets or Periscope Broadcasts, you must disclose to people:

    When you add location information

    Whether you add location information as a geotag or annotations data

    Whether your location information is listed as a place, or as geographic coordinates

If your application allows people to Tweet with their location you must comply with Twitter’s geo guidelines in full. 

Any use of location data or geographic information on a standalone basis is prohibited. You may not (and may not permit others to) store, aggregate, or cache location data and other geographic information contained in Twitter Content, except as part of a Tweet or Periscope Broadcast. For example, you may not separate location data or geographic information out from Tweets to show where individuals have been over time. Heat maps and related tools that show aggregated geo activity (e.g.: the number of people in a city using a hashtag) are permitted.
 
Twitter passwords

You may not store Twitter passwords, or request that people provide their Twitter password, account credentials, or developer application information (including consumer key) to you directly. We suggest the use of Sign-in with Twitter as the authentication tool to link your service and people on Twitter.
 
Platform usage guidelines

Have you taken care to review Twitter’s policies and set up your API access the right way? Does your service follow Twitter’s privacy and control guidelines? If you can answer yes to these two questions, then you are ready to start using the Twitter API and Twitter Content. Twitter’s Platform Usage Guidelines provide the assistance needed to ensure that your use of Twitter Content is compliant from day 1 throughout the lifecycle of your service. We suggest reviewing these rules on a regular basis to make sure that your integration is operating in a way that is safe and beneficial to people on Twitter and the Twitter platform as a whole.
 
Spam, bots, and automation

The use of the Twitter API and developer products to create spam, or engage in any form of platform manipulation, is prohibited. You should review the Twitter Rules on platform manipulation and spam, and ensure that your service does not, and does not enable people to, violate our policies.

Services that perform write actions, including posting Tweets, following accounts, or sending Direct Messages, must follow the Automation Rules. In particular, you should: 

    Always get explicit consent before sending people automated replies or Direct Messages

    Immediately respect requests to opt-out of being contacted by you

    Never perform bulk, aggressive, or spammy actions, including bulk following

    Never post identical or substantially similar content across multiple accounts

If you’re operating an API-based bot account you must clearly indicate what the account is and who is responsible for it. You should never mislead or confuse people about whether your account is or is not a bot. A good way to do this is by including a statement that the account is a bot in the profile bio.
 
Twitter performance benchmarking

You may not use the Twitter API to measure the availability, performance, functionality, or usage of Twitter for benchmarking, competitive, or commercial purposes. For example, you should never use the Twitter API to:

    Calculate aggregate Twitter metrics, such as the total number of Monthly Actives (MAs) or Daily Actives (DAs)

    Calculate aggregate Periscope metrics, such as total number of broadcast views

    Calculate aggregate Twitter Tweet metrics, such as the total number of Tweets posted per day, or the number of account engagements

    Measure or analyze the responsiveness of Twitter

    Measure or analyze spam or security on Twitter, except as permitted below

We support research that helps improve conversational health on Twitter. You may use the Twitter API and Twitter Content to measure and analyze topics like spam, abuse, or other platform health-related topics for non-commercial research purposes. You may not develop, create, or offer commercial services using the Twitter API or Twitter Content that measure, analyze, or attempt to identify behaviors or content which violate Twitter policies without express written permission from Twitter.

If you have questions about whether your use case qualifies as non-commercial research for this purpose please submit a request via the API Policy Support form.
 
Public display of Tweets

You must maintain the integrity of all Twitter Content that you display publicly or to people who use your service. If you don’t use Twitter for Websites to display content, then you must use the Twitter API to retrieve the most current version available for display. If displayed content ceases to be available through the Twitter API, then you must remove it from your service as soon as reasonably possible, or within 24 hours after the receipt of a removal request from Twitter, or the applicable Twitter account owner, or as otherwise required by applicable law.

There are specific rules you must follow if you display Twitter Content offline.  Follow the guidelines for using Tweets in broadcast if you display Tweets offline. Follow the guidelines for using Periscope Broadcasts if you display Periscope Broadcasts offline.

If you embed or display Tweets, you must contact us about your Twitter API access if your site exceeds 10 million daily impressions. Twitter reserves the right to require additional terms as a condition to your use of the Twitter API.  Additional restrictions on Twitter for Websites developer use include:

    Embedded Tweets and/or embedded timelines

        You must provide people with legally sufficient notice that fully discloses Twitter’s collection and use of data about browsing activities on your website, including for interest-based advertising and personalization. You must also obtain legally sufficient consent from people for such collection and use
        You must provide legally sufficient instructions on how people can opt out of Twitter’s interest-based advertising and personalization as described here
         
    Twitter for Websites widgets

        You must ensure that people are provided with clear and comprehensive information about, and consent to, the storing and accessing of cookies or other information on their devices as described in Twitter’s cookie use, where providing such information and obtaining such consent is required by law
         
    Services targeted to children under 13

        Services targeted to children under 13 must opt out of tailoring Twitter in any embedded Tweet and/or embedded timelines by setting the opt-out parameter to be ‘true’ as described here

 
Content redistribution

The best place to get Twitter Content is directly from Twitter. Consequently, we restrict the redistribution of Twitter Content to third parties.  If you provide Twitter Content to third parties, including downloadable datasets or via an API, you may only distribute Tweet IDs, Direct Message IDs, and/or User IDs (except as described below). We also grant special permissions to academic researchers sharing Tweet IDs and User IDs for non-commercial research purposes.

In total, you may not distribute more than 1,500,000 Tweet IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from Twitter. In addition, all developers may provide up to 50,000 public Tweets Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs).

Academic researchers are permitted to distribute an unlimited number of Tweet IDs and/or User IDs if they are doing so on behalf of an academic institution and for the sole purpose of non-commercial research. For example, you are permitted to share an unlimited number of Tweet IDs for the purpose of enabling peer review or validation of your research. If you have questions about whether your use case qualifies under this category please submit a request via the API Policy Support form.

Any Twitter Content provided to third parties remains subject to this Policy, and those third parties must agree to the Twitter Terms of Service, Privacy Policy, Developer Agreement, and Developer Policy before receiving such downloads. You may not enable any entity to circumvent any other limitations or restrictions on the distribution of Twitter Content as contained in this Policy, the Developer Agreement, or any other agreement with Twitter.
 
Pay to Engage

Your service shouldn’t compensate people to take actions on Twitter, as that results in inauthentic engagement that degrades the health of the platform. As you use the Twitter API you may not sell or receive monetary or virtual compensation for any Twitter or Periscope actions. This includes, but is not limited to, Tweets, follows, unfollows, retweets, likes, comments, and replies.
 
Service authenticity

You must clearly identify your service so that people can understand its source and purpose. Don’t use names, logos, or URLs that mask your service’s identity and features, or that falsely imply an affiliation with Twitter or third parties. Note that creating applications for the purpose of selling names, or to prevent others from using names, is prohibited.

You may not use any URL (including shortened URLs) for your service that directs people to:

    A site that is unrelated to your service

    A spam or malware site

    A site that encourages people to violate Twitter policy

 
Twitter name, logo, and likeness

You may only use and display the Twitter name and logo to identify Twitter as the source of Twitter Content. You should never use the Twitter name and logo, the Twitter Official Partner Program badge, or any other similar marks or names in a manner that creates a false sense of endorsement, sponsorship, or association with Twitter. The Twitter Brand Resources contain detailed information to help you use the Twitter brand in the right way.

You may only use the Twitter Verified Account badge and any other enhanced account categorization as it is reported to you by Twitter through the API. This helps people know that the content your service displays is equivalent to that shown on Twitter.
 
Advertising on Twitter

There are restrictions regarding how and where you are allowed to advertise around Twitter Content. To start, your advertisements can’t resemble or reasonably be confused by people as a Tweet or Periscope Broadcast. Other rules on advertising include:

    There must be a clear separation between Twitter Content and your advertisements. You may not place any advertisements within the Twitter timeline or on or within Periscope Broadcasts on your service other than Twitter Ads or advertisements made available through the official Twitter Kit integration with MoPub.

    Twitter reserves the right to serve advertising via the Twitter API. If you decide to serve Twitter Ads once we start delivering them via the API, we will share a portion of advertising revenue with you in accordance with the relevant terms and conditions.

    You may not use Twitter Content, or information obtained from the Twitter API to target people with advertising outside of the Twitter platform.

The following additional rules apply for any use of the Twitter services or features listed below:

Twitter Login

You must present people with easy to find options to log into and out of Twitter, for example via the OAuth protocol. The Sign in with Twitter option must be displayed at least as prominently as any other sign-up or sign-in feature on your service. You must also provide people without a Twitter account the opportunity to create one via Twitter.

Once someone on your service authenticates via Sign in with Twitter you must clearly display their Twitter identity. Twitter identity includes the person’s current Twitter @handle, avatar, and Twitter logo. Any display of someone’s Twitter followers on your service must clearly show that the relationship is associated with Twitter.

Twitter Cards

To ensure a quality experience you must develop your Card to render across all platforms where Cards are displayed. Additional rules that you must follow when using Cards include:

    You must mark your Tweet as ‘true’ for sensitive media if you plan to display such media within a Card

    You must use HTTPS for hosting all assets within your Card. Your Card should never generate active mixed content browser warnings

    Audio and video content should include stop or pause controls, and default to ‘sound off’ for videos that automatically play content

You may not exceed or circumvent Twitter’s limitations placed on any Cards, including the Card’s intended use. Additional restrictions on Cards use include:

    You may not place third-party sponsored content within Cards without Twitter’s approval

    You may not attach monetary incentives (including virtual currency) within your Card or on Twitter from your Card

    You may not include content or actions within your Card that are misleading or not contextually relevant, such as URLs and media.

    You may only attach an App Card to a Tweet when someone is explicitly promoting or referring to the app in the Tweet

Periscope Producer

You must contact us about your Twitter API access if you expect your service to exceed 10 million daily broadcasts. You may be subject to additional terms if you exceed this threshold. Additional restrictions on Periscope developer use include:

    You must provide a reasonable user-agent, as described in the Periscope Producer technical documentation, for your service when accessing the Periscope API

    You must honor requests from people to log out of their Periscope account on your service
    You may not provide tools in your service to allow people to circumvent technological protection measures



from twython import Twython
CONSUMER_KEY='EteOELZAulNVMe2Vn1fjRNiLF'
CONSUMER_SECRET='zqhlZOsnZsoQaeFMhaVCWbK7vSwukLr1MUHmbDnXappWesRDNc'
BEARER_TOKEN='AAAAAAAAAAAAAAAAAAAAAJ3negEAAAAA7cYYpESPIy%2FtJ0%2FhK%2FZllipGIOg%3Dld2zu8PPOoclOwShOMha6BgLW4smQ3Vf9I40DxluSUcelknoEA'
ACCESS_TOKEN='296906916-Ex4plVIJ13vxsDd90YGxJ2zxcN9yJwboTNU1ZRvt'
twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN,OAUTH_TOKEN_SECRET)
twitter.verify_credentials()

App details
Name: python-jln
App id: 24831901
    access_token 296906916-xiWeizroDnU5p60XG9YzzRy0fe4v9XqxTSFshWa6
    access_token_secret sJi0jI3YdkJTSDZ2tYFosHVqFhM2ksXnaAZMML0V1eBB9



To get Consumer Key & Consumer Secret, you have to create an app in Twitter via

https://developer.twitter.com/en/apps

Then you'll be taken to a page containing Consumer Key & Consumer Secret.

Hopefully this information will clarify OAuth essentials for Twitter:

    Create a Twitter account if you don't already have one
    Visit 'https://apps.twitter.com' and follow the required prompts to create a developer project (Twitter requires you to answer some questions before they will approve your account. Approval was nearly instant in my case.)
    Requesting the API key and secret via the Developer Portal causes Twitter to produce the following three things:

    API key (this is your 'consumer key')
    API secret key (this is your 'consumer secret')
    Bearer token

    Next, visit the 'Authentication Tokens' area of the Developer Portal and generate an 'Access token & secret'. This will provide you with the following two items:

    Access token (this is your 'token key')
    Access token secret (this is your 'token secret')

    The consumer key, consumer secret, token key, and token secret should be sufficient to do Twitter API calls (they were for me). Good luck!



%%writefile /home/jack/hidden/xxAPIkey.py
def APIkey():
    #removed keys for privacy reasons
    CONSUMER_KEY = '123456'
    CONSUMER_SECRET = 'abcdefg'
    ACCESS_KEY = 'A large monkey in a cherry tree'
    ACCESS_SECRET = 'How to hide secrets'
    keys = (CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
    return keys

from xxAPIkey import APIkey
CONSUMER_KEY = APIkey()[0]
CONSUMER_SECRET = APIkey()[1]
ACCESS_KEY = APIkey()[2]
ACCESS_SECRET = APIkey()[3]

print(CONSUMER_KEY)
print(CONSUMER_SECRET)
print(ACCESS_KEY)
print(ACCESS_SECRET)

import twython
from twython import Twython
import time
import os
import sys
import shutil
from randtext import randTXT
#removed keys for privacy reasons
from APIkey import APIkey
CONSUMER_KEY = APIkey()[0]
CONSUMER_SECRET = APIkey()[1]
ACCESS_KEY = APIkey()[2]
ACCESS_SECRET = APIkey()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)

# Path to Image to be posted
PATH = "images/useresult.png"
timestr = time.strftime("%Y%m%d-%H%M%S")
#python program to check if a directory exists
savepath = "posted"
# Check whether the specified path exists or not
isExist = os.path.exists(savepath)
if not isExist:
    # Create a new directory because it does not exist
    os.makedirs(savepath)
    print("The new directory is created!")
# Every image posted will be copied into savepath with datetime string    
shutil.copy(PATH, "posted/"+timestr+".png")
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
#photo = open("images/waves1.gif","rb")
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])
im = Image.open(PATH)
STR0 = randTXT()
STRu= STR0[:180]
print(STRu)
im

message ="Working with Python Mahotas. Mahotas is a computer vision and image processing library for Python. Mahotas currently has over 100 functions for image processing and computer vision and it keeps growing."

print (len(message))

from APIkey import APIkey
twitter = APIkey()[:3]
print(twitter)

import twython
from twython import Twython
from APIkey import APIkey
CONSUMER_KEY = APIkey()[0]
CONSUMER_SECRET = APIkey()[1]
ACCESS_KEY = APIkey()[2]
ACCESS_SECRET = APIkey()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)

def post(message):
    #twitter.update_status(status='See how easy using Twython is!')
    twitter.update_status(status=message)#, in_reply_to_status_id=twitter_id)


message ="""What a retired life I live. Playing Game of Thrones winter is coming. 
#GameOfThrones on the computer to my right. The computer in front I program process
images withPython and Tweet from my #Jupyternotebook"""
print(len(message))


post(message)

from twython
import Twython
from APIkey import APIkey
CONSUMER_KEY = APIkey()[0]
CONSUMER_SECRET = APIkey()[1]
ACCESS_KEY = APIkey()[2]
ACCESS_SECRET = APIkey()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)

photo = open('/path/to/file/image.jpg', 'rb')
response = twitter.upload_media(media = photo)
twitter.update_status(status = 'Checkout this cool image!', media_ids = [response['media_id']])

video = open('/path/to/file/video.mp4', 'rb')
response = twitter.upload_video(media = video, media_type = 'video/mp4')
twitter.update_status(status = 'Checkout this cool video!', media_ids = [response['media_id']])

response = twitter.upload_media(media=photo)
twitter.update_status(status=STRu, media_ids=[response['media_id']])

from twython import Twython, TwythonError
from APIkey import APIkey
CONSUMER_KEY = APIkey()[0]
CONSUMER_SECRET = APIkey()[1]
ACCESS_KEY = APIkey()[2]
ACCESS_SECRET = APIkey()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)

search_results = twitter.search(count=1, q='aiartcommunity')

try:
    for tweet in search_results['statuses']:
        print ('Tweet ID: ', tweet['id'])
except TwythonError as e:
    print(e)

status = twitter.show_status(id="1579675608816447488")
print(status)

def tweetstuff(STRINGS):
    tweetstuff = open("tweetstuff.txt", "a")
    tweetstuff.write(STRINGS)
works = status
works = str(works)
work = works.split("{")
cnt = 0
for lines in work:
    if len(lines)>3:
        cnt=cnt+1
        STRINGS = str(cnt)+": "+lines+"\n"
        tweetstuff(STRINGS)
        print (str(cnt),lines,"\n")


works = twitter.get_home_timeline()
works = status
works = str(works)
work = works.split("{")
cnt = 0
for lines in work:
    if len(lines)>3:
        cnt=cnt+1
        STRINGS = str(cnt)+": "+lines+"\n"
        tweetstuff(STRINGS)
        print (str(cnt),lines,"\n")

from twython import Twython
#twitter = Twython()
import datetime
from twython import Twython, TwythonError
from APIkey import APIkey
CONSUMER_KEY = APIkey()[0]
CONSUMER_SECRET = APIkey()[1]
ACCESS_KEY = APIkey()[2]
ACCESS_SECRET = APIkey()[3]

# https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets
results = twitter.cursor(twitter.search, q="#aiartcommunity", result_type='recent', count=25, tweet_mode='extended')

max_str_id = None
for _result in results:
    str_id = _result['id_str']
    #if str_id > max_str_id:
    #   max_str_id = str_id

    # if tweet_mode='extended', use _result['full_text']
    text = _result['text'] if 'text' in _result else _result['full_text']

    # check if is retweet
    is_retweet = True if 'retweeted_status' in _result or 'quoted_status' in _result else False

    # generate tweet url
    user_id = _result['user']['id_str']
    username = _result['user']['screen_name']
    post_id = _result['id_str']
    url = "https://twitter.com/{}/status/{}".format(username, post_id)

    # Mon Sep 24 03:35:21 +0000 2012
    created = datetime.datetime.strptime(_result['created_at'], '%a %b %d %H:%M:%S +0000 %Y')    

    # hashtags
    hashtags = [_hashtag['text'].lower() for _hashtag in _result['entities']['hashtags']]
    print(hashtags)
# you might want to save max_str_id if you plan to use since_id in next query.

def tweetstuff(STRINGS):
    tweetstuff = open("tweetstuff.txt", "a")
    tweetstuff.write(STRINGS)

print(len(results))



import ipywidgets as widgets
widgets.IntSlider(
    value=7,
    min=0,
    max=10,
    step=1,
    description='Test:',
    disabled=False,
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='d'
)

import ipywidgets as widgets
widgets.IntSlider(
    value=7,
    min=0,
    max=10,
    step=1,
    description='Test:',
    disabled=False,
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='d'
)

import os
import os.path
title = "database.list"
f= open(title,"a");f.close()
count=0
for dirpath, dirnames, filenames in os.walk("/home"):
    filenames = [f for f in filenames if not f[0] == '.']
    dirnames[:] = [d for d in dirnames if not d[0] == '.']
    for filename in [f for f in filenames if f.endswith(".db")]:
        count=count+1
        Path = os.path.join(dirpath, filename)
        with open(title, 'a') as outfile:
            path = Path+"\n"
            outfile.write(path)

f = open("database.list").readlines()
for database in f:
    database = database.replace("\n", "")
    print (database)



import sqlite3
# This reads an unknown Sqlite3 Database, thens creates or reuses a database
# to store the information retrieved
def Sdbinfo(unknown, storage):
    con = sqlite3.connect(storage)
    co = con.cursor()
    co.execute("""
    CREATE VIRTUAL TABLE IF NOT EXISTS data 
    USING FTS4(unknown, row, columns);
    """)
    con.commit()
    
    conn = sqlite3.connect(unknown)
    conn.text_factory = str
    c = conn.cursor()
    res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
    row = c.fetchone()
    row = str(row)
    row = row.replace("(","");row = row.replace(",)","")
    row = row.replace("'","");row = row.replace(",","")
    cur = c.execute("select * from \"%s\"" %  row)
    columns = [description[0] for description in cur.description]
    col = str(columns)
    col = col.replace("[","");col = col.replace("]","")
    coll = col.replace("'","")
    print (row, coll)
    co.execute("INSERT into data (unknown, row, columns) values (?,?,?)",(unknown, row, coll))
    con.commit()
    conn.close()
    conn.close()
    return columns

# picked out a single file to test
unknown = "/home/jake/pas.bak/GOT.db"
storage = "Store.db"
Sdbinfo(unknown, storage)

import sqlite3
# This reads an unknown Sqlite3 Database, thens creates or reuses a database
# to store the information retrieved
def Sdbinfo(unknown, storage):
    con = sqlite3.connect(storage)
    co = con.cursor()
    co.execute("""
    CREATE VIRTUAL TABLE IF NOT EXISTS data 
    USING FTS4(unknown, row, columns);
    """)
    con.commit()
    
    conn = sqlite3.connect(unknown)
    conn.text_factory = str
    c = conn.cursor()
    res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
    row = c.fetchone()
    row = str(row)
    row = row.replace("(","");row = row.replace(",)","")
    row = row.replace("'","");row = row.replace(",","")
    cur = c.execute("select * from \"%s\"" %  row)
    columns = [description[0] for description in cur.description]
    col = str(columns)
    col = col.replace("[","");col = col.replace("]","")
    coll = col.replace("'","")
    print (row, coll)
    co.execute("INSERT into data (unknown, row, columns) values (?,?,?)",(unknown, row, coll))
    con.commit()
    conn.close()
    conn.close()
    return columns

# picked out a single file to test
unknown = "/home/jack/Desktop/DataScience/work/COVID-19-Jupyter-Notebooks/DATA/DATAcsv.db"
storage = "Store.db"
Sdbinfo(unknown, storage)

import sqlite3
conn = sqlite3.connect("/home/jack/Desktop/DataScience/work/COVID-19-Jupyter-Notebooks/DATA/DATAcsv.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM CORONA"):
    print ("Row id:              ", row[0])
    print ("TEXT:  ", row[1])
 

# Print a specific column of the cvs entered in the database

import sqlite3
conn = sqlite3.connect("/home/jack/Desktop/DataScience/work/COVID-19-Jupyter-Notebooks/DATA/DATAcsv.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM CORONA"):
    #print ("Row id:              ", row[0])
    #print ("TEXT:  ", row[1])
    columns = row[1].split(",")
    print(columns[4],columns[5])

import sqlite3
conn = sqlite3.connect("/home/jake/pas.bak/GOT.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM PROJECT"):
    print ("Row id:              ", row[0])
    print ("File path and Name:  ", row[1])
 

import sqlite3
conn = sqlite3.connect("Store.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM data"):
    print ("Row id:              ", row[0])
    print ("File path and Name:  ", row[1])
    print ("Print Table:         ", row[2])
    print ("Columns:             ", row[3])

import sqlite3
conn = sqlite3.connect("/home/jack/nltk_data/corpora/city_database/city.db")
c = conn.cursor()
count = 0
for row in c.execute("SELECT ROWID, * FROM SequelizeMeta"):
    print "Row id:        ", row[0]
    print "City:          ", row[1]
    print "County:        ", row[2]
    print "Population:    ", row[3]
    print "---------------------"
    count = count +1
    if count >4:
        break

import sqlite3
conn = sqlite3.connect("/home/jack/nltk_data/corpora/city_database/city.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM city_table"):
    print "Row id:        ", row[0]
    print "City:          ", row[1]
    print "County:        ", row[2]
    print "Population:    ", row[3]

%%writefile SdbINFO.py
import sqlite3
def Sdbinfo(unknown, storage):
    con = sqlite3.connect(storage)
    co = con.cursor()
    co.execute("""
    CREATE VIRTUAL TABLE IF NOT EXISTS data 
    USING FTS4(unknown, row, columns);
    """)
    con.commit()
    
    conn = sqlite3.connect(unknown)
    conn.text_factory = str
    c = conn.cursor()
    res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
    row = c.fetchone()
    row = str(row)
    row = row.replace("(","");row = row.replace(",)","")
    row = row.replace("'","");row = row.replace(",","")
    cur = c.execute("select * from \"%s\"" %  row)
    columns = [description[0] for description in cur.description]
    col = str(columns)
    col = col.replace("[","");col = col.replace("]","")
    coll = col.replace("'","")
    print (row, coll)
    co.execute("INSERT into data (unknown, row, columns) values (?,?,?)",(unknown, row, coll))
    con.commit()
    conn.close()
    conn.close()
    return columns

!pwd

import SdbINFO
fname = "database.list"
def file_len(fname):
    with open(fname, encoding = 'unicode_escape') as f:
        for i, l in enumerate(f):
            pass
    total = i + 1
    return total

def readAll(fname, encoding = 'unicode_escape'):
    f = open(fname , 'r' )
    lines = f.readlines()
    f.close()
    return lines

def file_head(filein, encoding = 'unicode_escape'):
    count = 0
    fn = 0
    with open(filein, encoding = 'unicode_escape') as fin:
        for fline in fin:
            fline = str(fline)
            # the first line of an SQLITE data file reads " SQLite format 3@ "
            # this splits the line at the @ 
            fin = fline.split("@")
            for fi in fin:
                # This line seeks the string SQLite, That will identify the file as an SQLite database
                if count < 2 and "SQLite" in fi:
                    if len(fi) < 25:
                        return filein 
        

tl = file_len(fname)
#limit_memory(1000)
print ("Total Lines : ",tl)

for line in readAll(fname, encoding = 'unicode_escape'):
    line = line.replace("\n", "")
    sqlite = file_head(line)
    try:
        if len(sqlite) >6:
            
            storage = "NEW.db"
            database = sqlite.replace("\n", "")
            print (database)
            SdbINFO.Sdbinfo(database, storage)
    except:
        pass



storage = "NEW.db"
import sqlite3
con = sqlite3.connect(storage)
co = con.cursor()
for row in co.execute("SELECT ROWID, * from data"):
    print ("ROWID : ",row[0])
    print ("DATABASE : ",row[1])
    print ("TABLE : ",row[2])
    print ("COLUMNS : ",row[3])
    print ("-----------------")

con.close()

# /home/jack/rollerball/Library/AssetVersioning.db
# assetversion changeset, guid, name, parent, assettype, digest, oldversion, parentfolderid
# this is a databse created by reading RSS feeds
#database = "/home/jack/hubiC/Databases/bigfeedfts.db"


database ="/home/jack/miniconda3/pkgs/obspy-1.2.2-py37ha757849_2/lib/python3.7/site-packages/obspy/clients/filesystem/tests/data/tsindex_data/timeseries.sqlite"
# TABLE tsindex 
#network, station, location, channel, quality, version, starttime, endtime, samplerate,
#filename, byteoffset, bytes, hash, timeindex, timespans, timerates, format, filemodtime, updated, scanned
import sqlite3
con = sqlite3.connect(database)
co = con.cursor()
count = 0
for row in co.execute("SELECT ROWID, * from  tsindex"):
    print ("ROWID :       ",row[0])
    print ("network :     ",row[1])
    print ("station :     ",row[2])
    print ("location :    ",row[3])
    print ("channel :     ",row[4])
    print ("quality :     ",row[5])
    print ("version :     ",row[6])
    print ("starttime :   ",row[7])
    print ("endtime :     ",row[8])    
    print ("samplerate :  ",row[9])
    print ("filename :    ",row[10])
    print ("byteoffset :  ",row[11])
    print ("bytes :       ",row[12])
    print ("hash :        ",row[13])
    print ("timeindex :   ",row[14])
    print ("timespans :   ",row[15])
    print ("timerates :   ",row[16])
    print ("format :      ",row[17])
    print ("filemodtime : ",row[18])
    print ("updated :     ",row[19])
    print ("scanned :     ",row[20])    
    
    
    
    print ("-----------------")
    count = count + 1
    if count >10:
        break

con.close()

#import resource

def limit_memory(maxsize):
    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))
fname = "database.list"
def file_len(fname):
    with open(fname, encoding = 'unicode_escape') as f:
        for i, l in enumerate(f):
            pass
    total = i + 1
    return total

def readAll(fname):
    f = open(fname, 'r', encoding = 'unicode_escape'  )
    lines = f.readlines()
    f.close()
    return lines

def file_head(filein):
    count = 0
    fn = 0
    with open(filein, encoding = 'unicode_escape' ) as fin:
        for fline in fin:
            fline = str(fline)
            # the first line of an SQLITE data file reads " SQLite format 3@ "
            # this splits the line at the @ 
            fin = fline.split("@")
            for fi in fin:
                # This line seeks the string SQLite, That will identify the file as an SQLite database
                if count < 2 and "SQLite" in fi:
                    if len(fi) < 25:
                        return filein 
        

tl = file_len(fname)
#limit_memory(1000)
print ("Total Lines : ",tl)

for line in readAll(fname):
    line = line.replace("\n", "")
    sqlite = file_head(line)
    try:
        if len(sqlite) >6:
            print (sqlite)
    except:
        pass



import SdbINFO
database = "/home/jack/Downloads/basicCRUDops_NodeJs_sqlite-main/database/employee.db"
storage = "DBinfo.db"
SdbINFO.Sdbinfo(database, storage)

def file_head(filein):
    count = 0
    with open(filein, encoding = 'unicode_escape' ) as fin:
        for fline in fin:
            while count < 1:
                print (count,fline)
                count = count+1
            return    
filein = "/home/jack/unique-End2.db" 
file_head(filein)

def file_head(filein):
    count = 0
    with open(filein) as fin:
        for fline in fin:
            while count < 1:
                print (count,fline)
                count = count+1
            return    
filein = "/home/jack/data/db.sqlite3" 
file_head(filein)

def file_head(filein):
    count = 0
    with open(filein, encoding= 'unicode_escape') as fin:
        for fline in fin:
            while count < 8:
                print (count,fline)
                count = count+1
            return    
filein = "/home/jack/data/db.sqlite3" 
file_head(filein)

#import resource

def limit_memory(maxsize):
    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))
fname = "database.list"
def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    total = i + 1
    return total

def readAll(fname):
    f = open(fname , 'r' )
    lines = f.readlines()
    f.close()
    return lines

def file_head(filein):
    count = 0
    fn = 0
    with open(filein) as fin:
        for fline in fin:
            fline = str(fline)
            # the first line of an SQLITE data file reads " SQLite format 3@ "
            # this splits the line at the @ 
            fin = fline.split("@")
            for fi in fin:
                # This line seeks the string SQLite, That will identify the file as an SQLite database
                if count < 2 and "SQLite" in fi:
                    if len(fi) < 25:
                        print filein
                        print fi
                        print "----------------------------"  
                        count = count+1
                        return filein 
        

tl = file_len(fname)
#limit_memory(1000)
print "Total Lines : ",tl

for line in readAll(fname):
    line = line.replace("\n", "")
    file_head(line)

from time import sleep
def file_head(filein):
    count = 0
    with open(filein) as fin:
        for fline in fin:
            fline = str(fline)
            fin = fline.split(" ")
            print "\n",filein,"\n________________\n"
            for fi in fin:
                if count < 4:
                    print count,fi             
                    count = count+1
            return  
            
filein = "/home/jack/unique-End2.db"            
file_head(filein)            


fname = "database.list"
def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    total = i + 1
    return total

def readAll(fname):
    f = open(fname , 'r' )
    lines = f.readlines()
    f.close()
    return lines

def file_head(filein):
    count = 0
    with open(filein) as fin:
        for fline in fin:
            fline = str(fline)
            fin = fline.split(" ")
            for fi in fin:
                while count < 10:
                    print "\n",filein,"\n",count,fi,"\n________________\n"
                    count = count+1
                return    
        

tl = file_len(fname)
print "Total Lines : ",tl
for line in readAll(fname):
    line = line.replace("\n", "")
    file_head(line)

f = open( 'database.list' , 'r' )
line = f.readline()
while line :
    print line
    line = f.readline()
f.close()

import SdbINFO13
from time import sleep
import sys
def main():
    title = "database.list"
    filein = open(title,"r").readlines()
    fn = 0
    while databases in filein:
        count = 0
        fn = fn+1
        database = str(databases)
        database = database.replace("\n","")
        unknown = database
        print "1 : ",unknown
        while count < 6:
            count =0
            with open((unknown).decode('utf8')) as infile:
                for lines in infile:
                    line = lines.split(" ")
                    for li in line:

                        sleep(.5)
                        print fn,"/",count," : ",li
                        count = count +1
                        
                        
main()                        
                        

import SdbINFO13
from time import sleep
import sys
count = 0
storage = "Exp.db"
title = "database.list"
filein = open(title,"r").readlines()
for databases in filein:
    database = str(databases)
    database = database.replace("\n","")
    unknown = database
    print unknown
    SdbINFO13.Sdbinfo(unknown, storage)
    sleep(.5)
    count = count +1
    if count > 10:
        break

import SdbINFO
from time import sleep
title = "database.list"
for database in open(title,"r").readlines():
    try:
        storage = "DBinfo.db"
    except OperationalError:
        print title," is not a database."
        pass

    
    database = database.replace("\n","")
    #database = '/home/jack/Desktop/databases/PDE2db.db'
    #databasx = "/home/jack/Desktop/databases/PDE2db.db"
    #cur = c.execute("select * from \"%s\"" %  row)
    #SdbINFO.Sdbinfo("?", storage),(database)
    unknown = database
    print unknown

    SdbINFO.Sdbinfo(unknown, storage)
    sleep(.5)


!ls /home/jack/unique-End2.db

test = "bunch o junk"
total = "this is \'%s\'" % test
print total

test = "bunch o junk"
total = "this is '?'",(test) 
print total[0],total[1]

import SdbINFO
database = "/home/jack/unique-End2.db"
storage = "DBinfo.db"
SdbINFO.Sdbinfo(database, storage)

import os
count = 0
fn = 0
title = "database.list"
for lines in open(title,"r").readlines():
    fn = fn + 1
    filename = os.path.basename(lines)
    lines = lines.replace("\n","")
    b = os.path.getsize(lines)
    a = float(b/1000)
    filename = filename.replace("\n","")
    print fn," :",a,"k  -",lines
    count = count +1

from time import sleep
import sys
count = 0
title = "/home/jack/nltk_data/corpora/city_database/city.db"
# ��j!!�tablecity_tablecity_tableCREATE TABLE city_table

title = "/home/jack/Desktop/databases/PDE2db.db"
#SQLite format 3@  f�
#tablepde_docsizepde_docsizeCREATE TABLE 'pde_docsize'(docid INTEGER P

title = "/home/jack/rollerball/Library/AssetVersioning.db"
SQLite format 3@  -����)��d!!�tablerow_countsrow_countCREATE TABLE row_counts 

#title = "/home/jack/nltk_data/corpora/city_database/city.db"
#title = "/home/jack/nltk_data/corpora/city_database/city.db"
for lines in open(title,"r").readlines():
    sleep(.5)
    print lines
    count = count +1
    if count > 6:sys.exit()

from time import sleep
import sys
count = 0
title = "database.list"
for lines in open(title,"r").readlines():
    sleep(.5)
    print lines
    count = count +1
    if count > 6:
        break

from time import sleep
import sys
count = 0
title = "database.list"
for lines in open(title,"r").readlines():
    sleep(.5)
    print lines
    lines = lines.replace("\n","")
    count = count +1
    if count > 6:sys.exit()

f = open("DATAbase.list").readlines()
count =0
for database in f:
    count=count+1
    database = database.replace("\n", "")
    print (database)
print (count)    

import sqlite3

unknown = "/home/jack/.local/share/zeitgeist/activity.sqlite"

conn = sqlite3.connect(unknown)
conn.text_factory = str
c = conn.cursor()
res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
rows = c.fetchone()
row = str(rows)
row = row.replace("(","");row = row.replace(",)","")
row = row.replace("'","");row = row.replace(",","")
cur = c.execute("select * from \"%s\"" %  row)
columns = [description[0] for description in cur.description]
col = str(columns)
col = col.replace("[","");col = col.replace("]","")
coll = col.replace("'","")
print(rows)
print ("Table|ID|Field1 :  ",row, coll)
print(columns)
print(coll)

rm

import sqlite3
conn = sqlite3.connect("/home/jack/.local/share/zeitgeist/activity.sqlite")
c = conn.cursor()
count = 0
for row in c.execute("SELECT ROWID, * FROM uri"):
    #print ("Row id:", row[0])
    count=count+1
    if count<40:
        print ("ID: ", row[1],"Uri: ",row[2])
print(count)        

import sqlite3

unknown = "/home/jack/.cache/tracker/meta.db"

conn = sqlite3.connect(unknown)
conn.text_factory = str
c = conn.cursor()
res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
rows = c.fetchone()
row = str(rows)
row = row.replace("(","");row = row.replace(",)","")
row = row.replace("'","");row = row.replace(",","")
cur = c.execute("select * from \"%s\"" %  row)
columns = [description[0] for description in cur.description]
col = str(columns)
col = col.replace("[","");col = col.replace("]","")
coll = col.replace("'","")
print(rows)
print ("Table|ID|Field1|Field2 :  ",row, coll)
print(columns)
print(coll)

import sqlite3
conn = sqlite3.connect("/home/jack/.cache/tracker/meta.db")
c = conn.cursor()
count = 0
for row in c.execute("SELECT ROWID, * FROM Resource"):
    #print ("Row id:", row[0])
    count=count+1
    if count<30:
        print ("ID: ", row[1],"Uri: ",row[2],"Refcount: ",row[3])
print(count)        

import sqlite3
# This reads an unknown Sqlite3 Database, thens creates or reuses a database
# to store the information retrieved
def Sdbinfo(unknown, storage):
    con = sqlite3.connect(storage)
    co = con.cursor()
    co.execute("""
    CREATE VIRTUAL TABLE IF NOT EXISTS data 
    USING FTS4(unknown, row, columns);
    """)
    con.commit()
    
    conn = sqlite3.connect(unknown)
    conn.text_factory = str
    c = conn.cursor()
    res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
    row = c.fetchone()
    row = str(row)
    row = row.replace("(","");row = row.replace(",)","")
    row = row.replace("'","");row = row.replace(",","")
    cur = c.execute("select * from \"%s\"" %  row)
    columns = [description[0] for description in cur.description]
    col = str(columns)
    col = col.replace("[","");col = col.replace("]","")
    coll = col.replace("'","")
    print (row, coll)
    co.execute("INSERT into data (unknown, row, columns) values (?,?,?)",(unknown, row, coll))
    con.commit()
    conn.close()
    conn.close()
    return columns

# picked out a single file to test
unknown = "/home/jack/Desktop/ChatterBot-Stuff/chatbot.sqlite"
storage = "Store.db"
Sdbinfo(unknown, storage)

import sqlite3
# This reads an unknown Sqlite3 Database, thens creates or reuses a database
# to store the information retrieved
def Sdbinfo(unknown, storage):
    con = sqlite3.connect(storage)
    co = con.cursor()
    co.execute("""
    CREATE VIRTUAL TABLE IF NOT EXISTS data 
    USING FTS4(unknown, row, columns);
    """)
    con.commit()
    
    conn = sqlite3.connect(unknown)
    conn.text_factory = str
    c = conn.cursor()
    res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
    row = c.fetchone()
    row = str(row)
    row = row.replace("(","");row = row.replace(",)","")
    row = row.replace("'","");row = row.replace(",","")
    cur = c.execute("select * from \"%s\"" %  row)
    columns = [description[0] for description in cur.description]
    col = str(columns)
    col = col.replace("[","");col = col.replace("]","")
    coll = col.replace("'","")
    print (row, coll)
    co.execute("INSERT into data (unknown, row, columns) values (?,?,?)",(unknown, row, coll))
    con.commit()
    conn.close()
    conn.close()
    return columns

# picked out a single file to test
unknown = "/home/jack/Desktop/DataScience/work/COVID-19-Jupyter-Notebooks/DATA/DATAcsv.db"
storage = "Store.db"
Sdbinfo(unknown, storage)

import sqlite3
conn = sqlite3.connect("/home/jack/Desktop/ChatterBot-Stuff/chatbot.sqlite")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM words"):
    #print ("Row id:", row[0])
    print ("TEXT:  ", row[1])
    

import sqlite3
conn = sqlite3.connect("/home/jack/Desktop/DataScience/work/COVID-19-Jupyter-Notebooks/DATA/DATAcsv.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM CORONA"):
    print ("Row id:              ", row[0])
    print ("TEXT:  ", row[1])
 

# Print a specific column of the cvs entered in the database

import sqlite3
conn = sqlite3.connect("/home/jack/Desktop/DataScience/work/COVID-19-Jupyter-Notebooks/DATA/DATAcsv.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM CORONA"):
    #print ("Row id:              ", row[0])
    #print ("TEXT:  ", row[1])
    columns = row[1].split(",")
    print(columns[4],columns[5])

import sqlite3
conn = sqlite3.connect("/home/jake/pas.bak/GOT.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM PROJECT"):
    print ("Row id:              ", row[0])
    print ("File path and Name:  ", row[1])
 

import sqlite3
conn = sqlite3.connect("Store.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM data"):
    print ("Row id:              ", row[0])
    print ("File path and Name:  ", row[1])
    print ("Print Table:         ", row[2])
    print ("Columns:             ", row[3])

import sqlite3
conn = sqlite3.connect("/home/jack/nltk_data/corpora/city_database/city.db")
c = conn.cursor()
count = 0
for row in c.execute("SELECT ROWID, * FROM SequelizeMeta"):
    print "Row id:        ", row[0]
    print "City:          ", row[1]
    print "County:        ", row[2]
    print "Population:    ", row[3]
    print "---------------------"
    count = count +1
    if count >4:
        break

import sqlite3
conn = sqlite3.connect("/home/jack/nltk_data/corpora/city_database/city.db")
c = conn.cursor()
for row in c.execute("SELECT ROWID, * FROM city_table"):
    print "Row id:        ", row[0]
    print "City:          ", row[1]
    print "County:        ", row[2]
    print "Population:    ", row[3]

%%writefile SdbINFO.py
import sqlite3
def Sdbinfo(unknown, storage):
    con = sqlite3.connect(storage)
    co = con.cursor()
    co.execute("""
    CREATE VIRTUAL TABLE IF NOT EXISTS data 
    USING FTS4(unknown, row, columns);
    """)
    con.commit()
    
    conn = sqlite3.connect(unknown)
    conn.text_factory = str
    c = conn.cursor()
    res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
    row = c.fetchone()
    row = str(row)
    row = row.replace("(","");row = row.replace(",)","")
    row = row.replace("'","");row = row.replace(",","")
    cur = c.execute("select * from \"%s\"" %  row)
    columns = [description[0] for description in cur.description]
    col = str(columns)
    col = col.replace("[","");col = col.replace("]","")
    coll = col.replace("'","")
    print (row, coll)
    co.execute("INSERT into data (unknown, row, columns) values (?,?,?)",(unknown, row, coll))
    con.commit()
    conn.close()
    conn.close()
    return columns

import os
import os.path
title = "DATAbase.list"
f= open(title,"a");f.close()
count=0
for dirpath, dirnames, filenames in os.walk("/home"):
    filenames = [f for f in filenames if not f[0] == '.']
    dirnames[:] = [d for d in dirnames if not d[0] == '.']
    for filename in [f for f in filenames if f.endswith(".db")]:
        count=count+1
        Path = os.path.join(dirpath, filename)
        with open(title, 'a') as outfile:
            path = Path+"\n"
            outfile.write(path)

!pwd

import SdbINFO
fname = "DATAbase.list"
def file_len(fname):
    with open(fname, encoding = 'unicode_escape') as f:
        for i, l in enumerate(f):
            pass
    total = i + 1
    return total

def readAll(fname, encoding = 'unicode_escape'):
    f = open(fname , 'r' )
    lines = f.readlines()
    f.close()
    return lines

def file_head(filein, encoding = 'unicode_escape'):
    count = 0
    fn = 0
    with open(filein, encoding = 'unicode_escape') as fin:
        for fline in fin:
            fline = str(fline)
            # the first line of an SQLITE data file reads " SQLite format 3@ "
            # this splits the line at the @ 
            fin = fline.split("@")
            for fi in fin:
                # This line seeks the string SQLite, That will identify the file as an SQLite database
                if count < 2 and "SQLite" in fi:
                    if len(fi) < 25:
                        return filein 
        

tl = file_len(fname)
#limit_memory(1000)
print ("Total Lines : ",tl)
try:
    for line in readAll(fname, encoding = 'unicode_escape'):
        line = line.replace("\n", "")
        sqlite = file_head(line)
 
        if len(sqlite) >6:
            
            storage = "NEW.db"
            database = sqlite.replace("\n", "")
            print (database)
            SdbINFO.Sdbinfo(database, storage)
except:
    pass



storage = "NEW.db"
import sqlite3
con = sqlite3.connect(storage)
co = con.cursor()
for row in co.execute("SELECT ROWID, * from data"):
    print ("ROWID : ",row[0])
    print ("DATABASE : ",row[1])
    print ("TABLE : ",row[2])
    print ("COLUMNS : ",row[3])
    print ("-----------------")

con.close()

# /home/jack/rollerball/Library/AssetVersioning.db
# assetversion changeset, guid, name, parent, assettype, digest, oldversion, parentfolderid
# this is a databse created by reading RSS feeds
#database = "/home/jack/hubiC/Databases/bigfeedfts.db"


database ="/home/jack/miniconda3/pkgs/obspy-1.2.2-py37ha757849_2/lib/python3.7/site-packages/obspy/clients/filesystem/tests/data/tsindex_data/timeseries.sqlite"
# TABLE tsindex 
#network, station, location, channel, quality, version, starttime, endtime, samplerate,
#filename, byteoffset, bytes, hash, timeindex, timespans, timerates, format, filemodtime, updated, scanned
import sqlite3
con = sqlite3.connect(database)
co = con.cursor()
count = 0
for row in co.execute("SELECT ROWID, * from  tsindex"):
    print ("ROWID :       ",row[0])
    print ("network :     ",row[1])
    print ("station :     ",row[2])
    print ("location :    ",row[3])
    print ("channel :     ",row[4])
    print ("quality :     ",row[5])
    print ("version :     ",row[6])
    print ("starttime :   ",row[7])
    print ("endtime :     ",row[8])    
    print ("samplerate :  ",row[9])
    print ("filename :    ",row[10])
    print ("byteoffset :  ",row[11])
    print ("bytes :       ",row[12])
    print ("hash :        ",row[13])
    print ("timeindex :   ",row[14])
    print ("timespans :   ",row[15])
    print ("timerates :   ",row[16])
    print ("format :      ",row[17])
    print ("filemodtime : ",row[18])
    print ("updated :     ",row[19])
    print ("scanned :     ",row[20])    
    
    
    
    print ("-----------------")
    count = count + 1
    if count >10:
        break

con.close()

#import resource

def limit_memory(maxsize):
    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))
fname = "database.list"
def file_len(fname):
    with open(fname, encoding = 'unicode_escape') as f:
        for i, l in enumerate(f):
            pass
    total = i + 1
    return total

def readAll(fname):
    f = open(fname, 'r', encoding = 'unicode_escape'  )
    lines = f.readlines()
    f.close()
    return lines

def file_head(filein):
    count = 0
    fn = 0
    with open(filein, encoding = 'unicode_escape' ) as fin:
        for fline in fin:
            fline = str(fline)
            # the first line of an SQLITE data file reads " SQLite format 3@ "
            # this splits the line at the @ 
            fin = fline.split("@")
            for fi in fin:
                # This line seeks the string SQLite, That will identify the file as an SQLite database
                if count < 2 and "SQLite" in fi:
                    if len(fi) < 25:
                        return filein 
        

tl = file_len(fname)
#limit_memory(1000)
print ("Total Lines : ",tl)

for line in readAll(fname):
    line = line.replace("\n", "")
    sqlite = file_head(line)
    try:
        if len(sqlite) >6:
            print (sqlite)
    except:
        pass



import SdbINFO
database = "/home/jack/Downloads/basicCRUDops_NodeJs_sqlite-main/database/employee.db"
storage = "DBinfo.db"
SdbINFO.Sdbinfo(database, storage)

def file_head(filein):
    count = 0
    with open(filein, encoding = 'unicode_escape' ) as fin:
        for fline in fin:
            while count < 1:
                print (count,fline)
                count = count+1
            return    
filein = "/home/jack/unique-End2.db" 
file_head(filein)

def file_head(filein):
    count = 0
    with open(filein) as fin:
        for fline in fin:
            while count < 1:
                print (count,fline)
                count = count+1
            return    
filein = "/home/jack/data/db.sqlite3" 
file_head(filein)

def file_head(filein):
    count = 0
    with open(filein, encoding= 'unicode_escape') as fin:
        for fline in fin:
            while count < 8:
                print (count,fline)
                count = count+1
            return    
filein = "/home/jack/data/db.sqlite3" 
file_head(filein)

#import resource

def limit_memory(maxsize):
    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))
fname = "database.list"
def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    total = i + 1
    return total

def readAll(fname):
    f = open(fname , 'r' )
    lines = f.readlines()
    f.close()
    return lines

def file_head(filein):
    count = 0
    fn = 0
    with open(filein) as fin:
        for fline in fin:
            fline = str(fline)
            # the first line of an SQLITE data file reads " SQLite format 3@ "
            # this splits the line at the @ 
            fin = fline.split("@")
            for fi in fin:
                # This line seeks the string SQLite, That will identify the file as an SQLite database
                if count < 2 and "SQLite" in fi:
                    if len(fi) < 25:
                        print filein
                        print fi
                        print "----------------------------"  
                        count = count+1
                        return filein 
        

tl = file_len(fname)
#limit_memory(1000)
print "Total Lines : ",tl

for line in readAll(fname):
    line = line.replace("\n", "")
    file_head(line)

from time import sleep
def file_head(filein):
    count = 0
    with open(filein) as fin:
        for fline in fin:
            fline = str(fline)
            fin = fline.split(" ")
            print "\n",filein,"\n________________\n"
            for fi in fin:
                if count < 4:
                    print count,fi             
                    count = count+1
            return  
            
filein = "/home/jack/unique-End2.db"            
file_head(filein)            


fname = "database.list"
def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    total = i + 1
    return total

def readAll(fname):
    f = open(fname , 'r' )
    lines = f.readlines()
    f.close()
    return lines

def file_head(filein):
    count = 0
    with open(filein) as fin:
        for fline in fin:
            fline = str(fline)
            fin = fline.split(" ")
            for fi in fin:
                while count < 10:
                    print "\n",filein,"\n",count,fi,"\n________________\n"
                    count = count+1
                return    
        

tl = file_len(fname)
print "Total Lines : ",tl
for line in readAll(fname):
    line = line.replace("\n", "")
    file_head(line)

f = open( 'database.list' , 'r' )
line = f.readline()
while line :
    print line
    line = f.readline()
f.close()

import SdbINFO13
from time import sleep
import sys
def main():
    title = "database.list"
    filein = open(title,"r").readlines()
    fn = 0
    while databases in filein:
        count = 0
        fn = fn+1
        database = str(databases)
        database = database.replace("\n","")
        unknown = database
        print "1 : ",unknown
        while count < 6:
            count =0
            with open((unknown).decode('utf8')) as infile:
                for lines in infile:
                    line = lines.split(" ")
                    for li in line:

                        sleep(.5)
                        print fn,"/",count," : ",li
                        count = count +1
                        
                        
main()                        
                        

import SdbINFO13
from time import sleep
import sys
count = 0
storage = "Exp.db"
title = "database.list"
filein = open(title,"r").readlines()
for databases in filein:
    database = str(databases)
    database = database.replace("\n","")
    unknown = database
    print unknown
    SdbINFO13.Sdbinfo(unknown, storage)
    sleep(.5)
    count = count +1
    if count > 10:
        break

import SdbINFO
from time import sleep
title = "database.list"
for database in open(title,"r").readlines():
    try:
        storage = "DBinfo.db"
    except OperationalError:
        print title," is not a database."
        pass

    
    database = database.replace("\n","")
    #database = '/home/jack/Desktop/databases/PDE2db.db'
    #databasx = "/home/jack/Desktop/databases/PDE2db.db"
    #cur = c.execute("select * from \"%s\"" %  row)
    #SdbINFO.Sdbinfo("?", storage),(database)
    unknown = database
    print unknown

    SdbINFO.Sdbinfo(unknown, storage)
    sleep(.5)


!ls /home/jack/unique-End2.db

test = "bunch o junk"
total = "this is \'%s\'" % test
print total

test = "bunch o junk"
total = "this is '?'",(test) 
print total[0],total[1]

import SdbINFO
database = "/home/jack/unique-End2.db"
storage = "DBinfo.db"
SdbINFO.Sdbinfo(database, storage)

import os
count = 0
fn = 0
title = "database.list"
for lines in open(title,"r").readlines():
    fn = fn + 1
    filename = os.path.basename(lines)
    lines = lines.replace("\n","")
    b = os.path.getsize(lines)
    a = float(b/1000)
    filename = filename.replace("\n","")
    print fn," :",a,"k  -",lines
    count = count +1

from time import sleep
import sys
count = 0
title = "/home/jack/nltk_data/corpora/city_database/city.db"
# ��j!!�tablecity_tablecity_tableCREATE TABLE city_table

title = "/home/jack/Desktop/databases/PDE2db.db"
#SQLite format 3@  f�
#tablepde_docsizepde_docsizeCREATE TABLE 'pde_docsize'(docid INTEGER P

title = "/home/jack/rollerball/Library/AssetVersioning.db"
SQLite format 3@  -����)��d!!�tablerow_countsrow_countCREATE TABLE row_counts 

#title = "/home/jack/nltk_data/corpora/city_database/city.db"
#title = "/home/jack/nltk_data/corpora/city_database/city.db"
for lines in open(title,"r").readlines():
    sleep(.5)
    print lines
    count = count +1
    if count > 6:sys.exit()

from time import sleep
import sys
count = 0
title = "database.list"
for lines in open(title,"r").readlines():
    sleep(.5)
    print lines
    count = count +1
    if count > 6:
        break

from time import sleep
import sys
count = 0
title = "database.list"
for lines in open(title,"r").readlines():
    sleep(.5)
    print lines
    lines = lines.replace("\n","")
    count = count +1
    if count > 6:sys.exit()

import sys

sys.path.remove('/home/jack/hidden')

# %load main_dpir_deblur.py
import os.path
import cv2
import logging

import numpy as np
from datetime import datetime
from collections import OrderedDict
import hdf5storage
from scipy import ndimage

import torch

from utils import utils_deblur
from utils import utils_logger
from utils import utils_model
from utils import utils_pnp as pnp
from utils import utils_sisr as sr
from utils import utils_image as util


"""
Spyder (Python 3.7)
PyTorch 1.6.0
Windows 10 or Linux
Kai Zhang (cskaizhang@gmail.com)
github: https://github.com/cszn/DPIR
        https://github.com/cszn/IRCNN
        https://github.com/cszn/KAIR
@article{zhang2020plug,
  title={Plug-and-Play Image Restoration with Deep Denoiser Prior},
  author={Zhang, Kai and Li, Yawei and Zuo, Wangmeng and Zhang, Lei and Van Gool, Luc and Timofte, Radu},
  journal={arXiv preprint},
  year={2020}
}
% If you have any question, please feel free to contact with me.
% Kai Zhang (e-mail: cskaizhang@gmail.com; homepage: https://cszn.github.io/)
by Kai Zhang (01/August/2020)

# --------------------------------------------
|--model_zoo               # model_zoo
   |--drunet_gray          # model_name, for color images
   |--drunet_color
|--testset                 # testsets
|--results                 # results
# --------------------------------------------
"""

def main():

    # ----------------------------------------
    # Preparation
    # ----------------------------------------

    noise_level_img = 7.65/255.0         # default: 0, noise level for LR image
    noise_level_model = noise_level_img  # noise level of model, default 0
    model_name = 'drunet_gray'           # 'drunet_gray' | 'drunet_color' | 'ircnn_gray' | 'ircnn_color'
    testset_name = 'Set3C'               # test set,  'set5' | 'srbsd68'
    x8 = True                            # default: False, x8 to boost performance
    iter_num = 8                         # number of iterations
    modelSigma1 = 49
    modelSigma2 = noise_level_model*255.

    show_img = False                     # default: False
    save_L = True                        # save LR image
    save_E = True                        # save estimated image
    save_LEH = False                     # save zoomed LR, E and H images
    border = 0

    # --------------------------------
    # load kernel
    # --------------------------------

    kernels = hdf5storage.loadmat(os.path.join('kernels', 'Levin09.mat'))['kernels']

    sf = 1
    task_current = 'deblur'              # 'deblur' for deblurring
    n_channels = 3 if 'color' in  model_name else 1  # fixed
    model_zoo = 'model_zoo'              # fixed
    testsets = 'testsets'                # fixed
    results = 'results'                  # fixed
    result_name = testset_name + '_' + task_current + '_' + model_name
    model_path = os.path.join(model_zoo, model_name+'.pth')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    torch.cuda.empty_cache()

    # ----------------------------------------
    # L_path, E_path, H_path
    # ----------------------------------------

    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images
    E_path = os.path.join(results, result_name)   # E_path, for Estimated images
    util.mkdir(E_path)

    logger_name = result_name
    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+'.log'))
    log_path=os.path.join(E_path, logger_name+'.log')
    print(log_path)                      
    logger = logging.getLogger(logger_name)

    # ----------------------------------------
    # load model
    # ----------------------------------------

    if 'drunet' in model_name:
        from models.network_unet import UNetRes as net
        model = net(in_nc=n_channels+1, out_nc=n_channels, nc=[64, 128, 256, 512], nb=4, act_mode='R', downsample_mode="strideconv", upsample_mode="convtranspose")
        model.load_state_dict(torch.load(model_path), strict=True)
        model.eval()
        for _, v in model.named_parameters():
            v.requires_grad = False
        model = model.to(device)
    elif 'ircnn' in model_name:
        from models.network_dncnn import IRCNN as net
        model = net(in_nc=n_channels, out_nc=n_channels, nc=64)
        model25 = torch.load(model_path)
        former_idx = 0

    logger.info('model_name:{}, image sigma:{:.3f}, model sigma:{:.3f}'.format(model_name, noise_level_img, noise_level_model))
    logger.info('Model path: {:s}'.format(model_path))
    logger.info(L_path)
    L_paths = util.get_image_paths(L_path)

    test_results_ave = OrderedDict()
    test_results_ave['psnr'] = []  # record average PSNR for each kernel

    for k_index in range(kernels.shape[1]):

        logger.info('-------k:{:>2d} ---------'.format(k_index))
        test_results = OrderedDict()
        test_results['psnr'] = []
        k = kernels[0, k_index].astype(np.float64)
        util.imshow(k) if show_img else None

        for idx, img in enumerate(L_paths):

            # --------------------------------
            # (1) get img_L
            # --------------------------------

            img_name, ext = os.path.splitext(os.path.basename(img))
            img_H = util.imread_uint(img, n_channels=n_channels)
            img_H = util.modcrop(img_H, 8)  # modcrop

            img_L = ndimage.filters.convolve(img_H, np.expand_dims(k, axis=2), mode='wrap')
            util.imshow(img_L) if show_img else None
            img_L = util.uint2single(img_L)

            np.random.seed(seed=0)  # for reproducibility
            img_L += np.random.normal(0, noise_level_img, img_L.shape) # add AWGN

            # --------------------------------
            # (2) get rhos and sigmas
            # --------------------------------

            rhos, sigmas = pnp.get_rho_sigma(sigma=max(0.255/255., noise_level_model), iter_num=iter_num, modelSigma1=modelSigma1, modelSigma2=modelSigma2, w=1.0)
            rhos, sigmas = torch.tensor(rhos).to(device), torch.tensor(sigmas).to(device)

            # --------------------------------
            # (3) initialize x, and pre-calculation
            # --------------------------------

            x = util.single2tensor4(img_L).to(device)

            img_L_tensor, k_tensor = util.single2tensor4(img_L), util.single2tensor4(np.expand_dims(k, 2))
            [k_tensor, img_L_tensor] = util.todevice([k_tensor, img_L_tensor], device)
            FB, FBC, F2B, FBFy = sr.pre_calculate(img_L_tensor, k_tensor, sf)

            # --------------------------------
            # (4) main iterations
            # --------------------------------

            for i in range(iter_num):

                # --------------------------------
                # step 1, FFT
                # --------------------------------

                tau = rhos[i].float().repeat(1, 1, 1, 1)
                x = sr.data_solution(x, FB, FBC, F2B, FBFy, tau, sf)

                if 'ircnn' in model_name:
                    current_idx = np.int(np.ceil(sigmas[i].cpu().numpy()*255./2.)-1)
        
                    if current_idx != former_idx:
                        model.load_state_dict(model25[str(current_idx)], strict=True)
                        model.eval()
                        for _, v in model.named_parameters():
                            v.requires_grad = False
                        model = model.to(device)
                    former_idx = current_idx

                # --------------------------------
                # step 2, denoiser
                # --------------------------------

                if x8:
                    x = util.augment_img_tensor4(x, i % 8)

                if 'drunet' in model_name:
                    x = torch.cat((x, sigmas[i].float().repeat(1, 1, x.shape[2], x.shape[3])), dim=1)
                    x = utils_model.test_mode(model, x, mode=2, refield=32, min_size=256, modulo=16)
                elif 'ircnn' in model_name:
                    x = model(x)

                if x8:
                    if i % 8 == 3 or i % 8 == 5:
                        x = util.augment_img_tensor4(x, 8 - i % 8)
                    else:
                        x = util.augment_img_tensor4(x, i % 8)

            # --------------------------------
            # (3) img_E
            # --------------------------------

            img_E = util.tensor2uint(x)
            if n_channels == 1:
                img_H = img_H.squeeze()

            if save_E:
                util.imsave(img_E, os.path.join(E_path, img_name+'_k'+str(k_index)+'_'+model_name+'.png'))

            # --------------------------------
            # (4) img_LEH
            # --------------------------------

            if save_LEH:
                img_L = util.single2uint(img_L)
                k_v = k/np.max(k)*1.0
                k_v = util.single2uint(np.tile(k_v[..., np.newaxis], [1, 1, 3]))
                k_v = cv2.resize(k_v, (3*k_v.shape[1], 3*k_v.shape[0]), interpolation=cv2.INTER_NEAREST)
                img_I = cv2.resize(img_L, (sf*img_L.shape[1], sf*img_L.shape[0]), interpolation=cv2.INTER_NEAREST)
                img_I[:k_v.shape[0], -k_v.shape[1]:, :] = k_v
                img_I[:img_L.shape[0], :img_L.shape[1], :] = img_L
                util.imshow(np.concatenate([img_I, img_E, img_H], axis=1), title='LR / Recovered / Ground-truth') if show_img else None
                util.imsave(np.concatenate([img_I, img_E, img_H], axis=1), os.path.join(E_path, img_name+'_k'+str(k_index)+'_LEH.png'))

            if save_L:
                util.imsave(util.single2uint(img_L), os.path.join(E_path, img_name+'_k'+str(k_index)+'_LR.png'))

            psnr = util.calculate_psnr(img_E, img_H, border=border)  # change with your own border
            test_results['psnr'].append(psnr)
            logger.info('{:->4d}--> {:>10s} --k:{:>2d} PSNR: {:.2f}dB'.format(idx+1, img_name+ext, k_index, psnr))


        # --------------------------------
        # Average PSNR
        # --------------------------------

        ave_psnr = sum(test_results['psnr']) / len(test_results['psnr'])
        logger.info('------> Average PSNR of ({}), kernel: ({}) sigma: ({:.2f}): {:.2f} dB'.format(testset_name, k_index, noise_level_model, ave_psnr))
        test_results_ave['psnr'].append(ave_psnr)

if __name__ == '__main__':

    main()


log_path=os.path.join(E_path, logger_name+'.log')
print(log_path)           

# %load main_dpir_denoising.py
import os.path
import logging

import numpy as np
from collections import OrderedDict

import torch

from utils import utils_logger
from utils import utils_model
from utils import utils_image as util


"""
Spyder (Python 3.7)
PyTorch 1.6.0
Windows 10 or Linux
Kai Zhang (cskaizhang@gmail.com)
github: https://github.com/cszn/DPIR
        https://github.com/cszn/IRCNN
        https://github.com/cszn/KAIR
@article{zhang2020plug,
  title={Plug-and-Play Image Restoration with Deep Denoiser Prior},
  author={Zhang, Kai and Li, Yawei and Zuo, Wangmeng and Zhang, Lei and Van Gool, Luc and Timofte, Radu},
  journal={arXiv preprint},
  year={2020}
}
% If you have any question, please feel free to contact with me.
% Kai Zhang (e-mail: cskaizhang@gmail.com; homepage: https://cszn.github.io/)
by Kai Zhang (01/August/2020)

# --------------------------------------------
|--model_zoo               # model_zoo
   |--drunet_gray          # model_name, for color images
   |--drunet_color
|--testset                 # testsets
   |--set12                # testset_name
   |--bsd68
   |--cbsd68
|--results                 # results
   |--set12_dn_drunet_gray # result_name = testset_name + '_' + 'dn' + model_name
   |--set12_dn_drunet_color
# --------------------------------------------
"""


def main():

    # ----------------------------------------
    # Preparation
    # ----------------------------------------

    noise_level_img = 15                 # set AWGN noise level for noisy image
    noise_level_model = noise_level_img  # set noise level for model
    model_name = 'drunet_gray'           # set denoiser model, 'drunet_gray' | 'drunet_color'
    testset_name = 'bsd68'               # set test set,  'bsd68' | 'cbsd68' | 'set12'
    x8 = False                           # default: False, x8 to boost performance
    show_img = False                     # default: False
    border = 0                           # shave boader to calculate PSNR and SSIM

    if 'color' in model_name:
        n_channels = 3                   # 3 for color image
    else:
        n_channels = 1                   # 1 for grayscale image

    model_pool = 'model_zoo'             # fixed
    testsets = 'testsets'                # fixed
    results = 'results'                  # fixed
    task_current = 'dn'                  # 'dn' for denoising
    result_name = testset_name + '_' + task_current + '_' + model_name

    model_path = os.path.join(model_pool, model_name+'.pth')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    torch.cuda.empty_cache()

    # ----------------------------------------
    # L_path, E_path, H_path
    # ----------------------------------------

    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images
    E_path = os.path.join(results, result_name)   # E_path, for Estimated images
    util.mkdir(E_path)

    logger_name = result_name
    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+'.log'))
    logger = logging.getLogger(logger_name)

    # ----------------------------------------
    # load model
    # ----------------------------------------

    from models.network_unet import UNetRes as net
    model = net(in_nc=n_channels+1, out_nc=n_channels, nc=[64, 128, 256, 512], nb=4, act_mode='R', downsample_mode="strideconv", upsample_mode="convtranspose")
    model.load_state_dict(torch.load(model_path), strict=True)
    model.eval()
    for k, v in model.named_parameters():
        v.requires_grad = False
    model = model.to(device)
    logger.info('Model path: {:s}'.format(model_path))
    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))
    logger.info('Params number: {}'.format(number_parameters))

    test_results = OrderedDict()
    test_results['psnr'] = []
    test_results['ssim'] = []

    logger.info('model_name:{}, model sigma:{}, image sigma:{}'.format(model_name, noise_level_img, noise_level_model))
    logger.info(L_path)
    L_paths = util.get_image_paths(L_path)

    for idx, img in enumerate(L_paths):

        # ------------------------------------
        # (1) img_L
        # ------------------------------------

        img_name, ext = os.path.splitext(os.path.basename(img))
        # logger.info('{:->4d}--> {:>10s}'.format(idx+1, img_name+ext))
        img_H = util.imread_uint(img, n_channels=n_channels)
        img_L = util.uint2single(img_H)

        # Add noise without clipping
        np.random.seed(seed=0)  # for reproducibility
        img_L += np.random.normal(0, noise_level_img/255., img_L.shape)

        util.imshow(util.single2uint(img_L), title='Noisy image with noise level {}'.format(noise_level_img)) if show_img else None

        img_L = util.single2tensor4(img_L)
        img_L = torch.cat((img_L, torch.FloatTensor([noise_level_model/255.]).repeat(1, 1, img_L.shape[2], img_L.shape[3])), dim=1)
        img_L = img_L.to(device)

        # ------------------------------------
        # (2) img_E
        # ------------------------------------

        if not x8 and img_L.size(2)//8==0 and img_L.size(3)//8==0:
            img_E = model(img_L)
        elif not x8 and (img_L.size(2)//8!=0 or img_L.size(3)//8!=0):
            img_E = utils_model.test_mode(model, img_L, refield=64, mode=5)
        elif x8:
            img_E = utils_model.test_mode(model, img_L, mode=3)

        img_E = util.tensor2uint(img_E)

        # --------------------------------
        # PSNR and SSIM
        # --------------------------------

        if n_channels == 1:
            img_H = img_H.squeeze() 
        psnr = util.calculate_psnr(img_E, img_H, border=border)
        ssim = util.calculate_ssim(img_E, img_H, border=border)
        test_results['psnr'].append(psnr)
        test_results['ssim'].append(ssim)
        logger.info('{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.'.format(img_name+ext, psnr, ssim))

        # ------------------------------------
        # save results
        # ------------------------------------

        util.imsave(img_E, os.path.join(E_path, img_name+ext))

    ave_psnr = sum(test_results['psnr']) / len(test_results['psnr'])
    ave_ssim = sum(test_results['ssim']) / len(test_results['ssim'])
    logger.info('Average PSNR/SSIM(RGB) - {} - PSNR: {:.2f} dB; SSIM: {:.4f}'.format(result_name, ave_psnr, ave_ssim))


if __name__ == '__main__':

    main()