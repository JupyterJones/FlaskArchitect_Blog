Python AI with sqlite3
user
isn't it possible to build a python coding AI using sqlite3
ChatGPT
Yes, it is possible to build a Python coding AI that utilizes `sqlite3` for database operations. `sqlite3` can be used to store various pieces of information, such as code snippets, programming concepts, user inputs, and AI responses. Here's a high-level outline of how you might build such a system:

1. **Database Design**:
    - Create tables to store code snippets, user queries, AI responses, and any other relevant information.
    
2. **AI Model**:
    - Use an existing AI model for code generation, such as OpenAI's Codex or any other model suitable for the task.
    
3. **Integration**:
    - Integrate the AI model with your application to process user queries and generate code.
    
4. **Query Handling**:
    - Handle user queries by retrieving relevant information from the database and using the AI model to generate responses.
    
5. **Logging and Updating Database**:
    - Log user interactions and AI responses in the database for future reference and improvement.

Here’s a basic example to get you started:

### 1. Setting Up the Database

First, let's create an SQLite database to store user queries and AI responses.

```python
import sqlite3

# Connect to SQLite database (or create it if it doesn't exist)
conn = sqlite3.connect('coding_ai.db')
cursor = conn.cursor()

# Create tables
cursor.execute('''
CREATE TABLE IF NOT EXISTS user_queries (
    id INTEGER PRIMARY KEY,
    query TEXT NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
)
''')

cursor.execute('''
CREATE TABLE IF NOT EXISTS ai_responses (
    id INTEGER PRIMARY KEY,
    query_id INTEGER,
    response TEXT NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (query_id) REFERENCES user_queries(id)
)
''')

conn.commit()
conn.close()
```

### 2. Handling User Queries

Next, let's create a function to handle user queries and generate responses using a placeholder AI model.

```python
import datetime
import logging

logging.basicConfig(level=logging.INFO)

def handle_query(query):
    # Connect to the database
    conn = sqlite3.connect('coding_ai.db')
    cursor = conn.cursor()

    # Insert user query into the database
    cursor.execute('INSERT INTO user_queries (query) VALUES (?)', (query,))
    query_id = cursor.lastrowid

    # Placeholder AI response generation (replace this with actual AI model integration)
    ai_response = generate_ai_response(query)

    # Insert AI response into the database
    cursor.execute('INSERT INTO ai_responses (query_id, response) VALUES (?, ?)', (query_id, ai_response))

    # Commit the transaction and close the connection
    conn.commit()
    conn.close()

    logging.info(f"Query: {query}")
    logging.info(f"AI Response: {ai_response}")

    return ai_response

def generate_ai_response(query):
    # Placeholder function for AI response generation
    # Replace with actual AI model logic
    return f"Generated code for: {query}"

# Example usage
query = "How to sort a list in Python?"
response = handle_query(query)
print(response)
```

### 3. Enhancing AI Response Generation

To enhance AI response generation, you would integrate with a pre-trained model such as OpenAI Codex, GPT-4, or any other relevant model. Here’s a simple example using OpenAI’s API:

```python
import openai

# Make sure to replace 'your-api-key' with your actual OpenAI API key
openai.api_key = 'your-api-key'

def generate_ai_response(query):
    response = openai.Completion.create(
        engine="davinci-codex",
        prompt=query,
        max_tokens=150
    )
    return response.choices[0].text.strip()
```

### 4. Running the Application

Now you can run your application to handle user queries and generate code snippets.

```python
if __name__ == "__main__":
    while True:
        user_query = input("Enter your coding question: ")
        if user_query.lower() == 'exit':
            break
        response = handle_query(user_query)
        print(f"AI Response: {response}")
```

This basic setup stores user queries and AI responses in an SQLite database and uses a placeholder function to generate AI responses. You can enhance this by integrating a more sophisticated AI model and adding more features as needed.
user
can a free ai model be used ?
ChatGPT
Yes, a free AI model can be used to build a Python coding AI. There are several free and open-source AI models that you can use for code generation and assistance. One popular choice is the GPT-2 model by OpenAI, which has been released for free and can be fine-tuned on specific tasks, including code generation. Another option is Hugging Face's Transformers library, which provides access to a variety of pre-trained models.

Here's an example of how to use GPT-2 with the Hugging Face Transformers library for generating code snippets based on user queries:

### 1. Setting Up the Environment

First, you'll need to install the necessary libraries:

```bash
pip install transformers torch sqlite3
```

### 2. Database Setup

Use the same database setup as in the previous example to store user queries and AI responses.

### 3. Loading the GPT-2 Model

Load the GPT-2 model using the Hugging Face Transformers library:

```python
import sqlite3
import logging
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Configure logging
logging.basicConfig(level=logging.INFO)

# Load the GPT-2 model and tokenizer
model_name = 'gpt2-medium'  # You can also use 'gpt2' or 'gpt2-large'
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

def handle_query(query):
    # Connect to the database
    conn = sqlite3.connect('coding_ai.db')
    cursor = conn.cursor()

    # Insert user query into the database
    cursor.execute('INSERT INTO user_queries (query) VALUES (?)', (query,))
    query_id = cursor.lastrowid

    # Generate AI response
    ai_response = generate_ai_response(query)

    # Insert AI response into the database
    cursor.execute('INSERT INTO ai_responses (query_id, response) VALUES (?, ?)', (query_id, ai_response))

    # Commit the transaction and close the connection
    conn.commit()
    conn.close()

    logging.info(f"Query: {query}")
    logging.info(f"AI Response: {ai_response}")

    return ai_response

def generate_ai_response(query):
    inputs = tokenizer.encode(query, return_tensors='pt')
    outputs = model.generate(inputs, max_length=150, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Example usage
if __name__ == "__main__":
    while True:
        user_query = input("Enter your coding question: ")
        if user_query.lower() == 'exit':
            break
        response = handle_query(user_query)
        print(f"AI Response: {response}")
```

### Explanation

1. **Install Dependencies**: Ensure you have `transformers`, `torch`, and `sqlite3` installed.
2. **Load GPT-2 Model**: Use the Hugging Face Transformers library to load the GPT-2 model and tokenizer.
3. **Generate Response**: Encode the user query, generate a response using the GPT-2 model, and decode the response.
4. **Database Operations**: Store the user query and AI response in the SQLite database.
5. **Run the Application**: Continuously accept user queries and generate AI responses.

This setup provides a basic framework for a Python coding AI using a free GPT-2 model. You can further enhance it by fine-tuning the model on specific coding datasets, adding more features, and improving the user interface.
user
how would format your data for training?
ChatGPT
Training an AI model for coding assistance involves formatting your data appropriately. The data typically consists of code snippets, problem statements, and corresponding solutions. Here's a structured approach to preparing and formatting your data for training:

### 1. Data Collection

Collect data from various sources such as:

- **Programming websites** (e.g., Stack Overflow, GitHub)
- **Educational resources** (e.g., coding tutorials, problem-solving websites)
- **Personal projects** (your own code and problem-solving methods)

### 2. Data Structuring

Structure your data in a way that pairs problem statements or queries with their corresponding solutions. A common format is JSON, where each entry contains a query and a corresponding response.

#### Example JSON Format

```json
[
    {
        "query": "How to sort a list in Python?",
        "response": "You can sort a list in Python using the `sort()` method or the `sorted()` function. Example:\n\nmy_list = [3, 1, 4, 1, 5, 9]\nmy_list.sort()\nprint(my_list)  # Output: [1, 1, 3, 4, 5, 9]"
    },
    {
        "query": "How to reverse a string in Python?",
        "response": "You can reverse a string in Python using slicing. Example:\n\nmy_string = 'hello'\nreversed_string = my_string[::-1]\nprint(reversed_string)  # Output: 'olleh'"
    }
]
```

### 3. Preprocessing

Preprocess the data to ensure consistency and quality. This includes:

- **Cleaning**: Remove any unnecessary whitespace, special characters, or incorrect data.
- **Normalization**: Standardize the format of the code snippets.
- **Tokenization**: Tokenize the data if required by the model. For GPT-2, tokenization is handled internally by the tokenizer.

### 4. Fine-Tuning GPT-2 with Hugging Face

You can fine-tune the GPT-2 model using the Hugging Face Transformers library. Below is an example script for fine-tuning:

#### Install Necessary Libraries

```bash
pip install transformers datasets
```

#### Prepare the Dataset

```python
import json
from datasets import load_dataset, Dataset

# Load your JSON data
with open('coding_data.json', 'r') as f:
    data = json.load(f)

# Convert to Hugging Face dataset
dataset = Dataset.from_dict({"text": [f"Query: {entry['query']} Response: {entry['response']}" for entry in data]})
dataset = dataset.train_test_split(test_size=0.1)  # Split into training and testing sets

# Save dataset
dataset.save_to_disk('coding_dataset')
```

#### Fine-Tune the Model

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import load_from_disk

# Load dataset
dataset = load_from_disk('coding_dataset')

# Load model and tokenizer
model_name = 'gpt2-medium'
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Set training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
)

# Fine-tune the model
trainer.train()

# Save the model
model.save_pretrained('./fine-tuned-coding-gpt2')
tokenizer.save_pretrained('./fine-tuned-coding-gpt2')
```

### Explanation

1. **Data Collection**: Gather diverse examples of coding problems and solutions.
2. **Data Structuring**: Organize the data in a JSON format with clear pairs of queries and responses.
3. **Preprocessing**: Clean and tokenize the data to ensure it’s in the correct format for training.
4. **Fine-Tuning**: Use Hugging Face Transformers to fine-tune the GPT-2 model on your custom dataset.

By following these steps, you can create a custom-trained AI model tailored to generate coding solutions and assist with programming queries.
user
Is there anyway you use you with Python and selenium or chrome divers??
ChatGPT
Yes, you can use Selenium with Python and ChromeDriver to automate web browser tasks. Selenium is a powerful tool for web scraping, web testing, and automation.

### Setting Up Selenium and ChromeDriver

1. **Install Selenium**:
   ```bash
   pip install selenium
   ```

2. **Download ChromeDriver**:
   - Download ChromeDriver from [here](https://sites.google.com/a/chromium.org/chromedriver/downloads) and ensure it matches your installed Chrome version.
   - Extract the `chromedriver` executable and place it in a directory included in your system's PATH, or specify its path directly in your script.

### Basic Example with Selenium and ChromeDriver

Here’s an example of using Selenium to open a webpage, search for a term on Google, and print the titles of the search results:

#### 1. Setting Up the Script

```python
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)

# Path to the ChromeDriver executable
chrome_driver_path = "/path/to/chromedriver"  # Update this with the actual path

# Initialize the ChromeDriver
driver = webdriver.Chrome(executable_path=chrome_driver_path)

try:
    # Open Google
    driver.get("https://www.google.com")
    logging.info("Opened Google")

    # Find the search box
    search_box = driver.find_element_by_name("q")
    logging.info("Found the search box")

    # Enter a search term
    search_term = "Python programming"
    search_box.send_keys(search_term)
    logging.info(f"Entered search term: {search_term}")

    # Press Enter
    search_box.send_keys(Keys.RETURN)
    logging.info("Pressed Enter")

    # Wait for the results to load
    time.sleep(3)

    # Retrieve search results
    results = driver.find_elements_by_css_selector("h3")
    logging.info(f"Found {len(results)} results")

    # Print the titles of the search results
    for result in results:
        logging.info(result.text)

finally:
    # Close the browser
    driver.quit()
    logging.info("Closed the browser")
```

### Explanation

1. **Install Selenium**: Install the Selenium package using `pip`.
2. **Download ChromeDriver**: Download the ChromeDriver binary that matches your Chrome browser version.
3. **Initialize ChromeDriver**: Set the path to the `chromedriver` executable and initialize the `webdriver.Chrome` instance.
4. **Open a Webpage**: Use `driver.get()` to navigate to the desired webpage.
5. **Find Elements**: Locate web elements using methods like `find_element_by_name`, `find_element_by_css_selector`, etc.
6. **Interact with Elements**: Perform actions such as sending keystrokes with `send_keys`.
7. **Retrieve Results**: Extract information from the webpage, such as the titles of search results.
8. **Logging**: Use Python's `logging` module to log the actions and results.
9. **Close the Browser**: Ensure the browser is closed using `driver.quit()`.

### Using a Headless Browser

If you want to run the browser in headless mode (without a graphical interface), you can set the Chrome options accordingly:

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)

# Path to the ChromeDriver executable
chrome_driver_path = "/path/to/chromedriver"  # Update this with the actual path

# Set Chrome options to run headless
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")

# Initialize the ChromeDriver with options
driver = webdriver.Chrome(executable_path=chrome_driver_path, options=chrome_options)

try:
    # Open Google
    driver.get("https://www.google.com")
    logging.info("Opened Google")

    # Find the search box
    search_box = driver.find_element_by_name("q")
    logging.info("Found the search box")

    # Enter a search term
    search_term = "Python programming"
    search_box.send_keys(search_term)
    logging.info(f"Entered search term: {search_term}")

    # Press Enter
    search_box.send_keys(Keys.RETURN)
    logging.info("Pressed Enter")

    # Wait for the results to load
    time.sleep(3)

    # Retrieve search results
    results = driver.find_elements_by_css_selector("h3")
    logging.info(f"Found {len(results)} results")

    # Print the titles of the search results
    for result in results:
        logging.info(result.text)

finally:
    # Close the browser
    driver.quit()
    logging.info("Closed the browser")
```

By running the browser in headless mode, you can perform web scraping and automation tasks on a server without a graphical user interface, which is particularly useful for continuous integration and deployment environments.
user
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import json
import os
from sys import argv


DIR = os.path.dirname(os.path.abspath(__file__))
Filename = 'CHATGPT/conversations.json'
print(Filename)

with open(Filename, 'r', encoding='utf-8') as file:
    json_data = json.load(file)

# Initialize the result string
result_str = ""

# Define a function to get conversation messages similar to the JavaScript logic
def get_conversation_messages(conversation):
    messages = []
    current_node = conversation.get('current_node')
    while current_node:
        node = conversation['mapping'][current_node]
        message = node.get('message')
        if (message and message.get('content') and message['content'].get('content_type') == 'text' and
                len(message['content'].get('parts', [])) > 0 and len(message['content']['parts'][0]) > 0 and
                (message['author']['role'] != 'system' or message.get('metadata', {}).get('is_user_system_message'))):
            author = message['author']['role']
            if author == 'assistant':
                author = 'ChatGPT'
            elif author == 'system' and message['metadata'].get('is_user_system_message'):
                author = 'Custom user info'
            messages.append({'author': author, 'text': message['content']['parts'][0]})
        current_node = node.get('parent')
    return messages[::-1]  # Reverse the list to maintain chronological order

# Iterate over each conversation in the JSON data and process it
for conversation in json_data:
    # Get the conversation title and messages
    title = conversation.get('title', '')
    messages = get_conversation_messages(conversation)

    # Append the title and messages to the result string
    result_str += title + '\n'
    for message in messages:
        result_str += message['author'] + '\n' + message['text'] + '\n'
    result_str += '\n'  # Add a newline between conversations
conversation=json_data 
get_conversation_messages(conversation)
# Return the processed result string
TEXTfile =  Filename[:-5] + ".txt"  # Changed this line to properly handle the filename
print("TEXTfile: ", TEXTfile)
with open(TEXTfile, "w", encoding='utf-8') as Input:
    result_str = result_str.replace("
", "\n")
    for line in result_str.split(""):
        Input.write(line)
what am i doing wrong here?
ChatGPT
Your script has a few issues that need to be addressed. Here are the main problems and their solutions:

1. **File Path Issue**: The script assumes that the `Filename` is relative to the current directory, but it might not be the correct path.
2. **Loop Over Conversations**: The script tries to call `get_conversation_messages(conversation)` outside of the loop, which doesn't make sense since `json_data` is a list of conversations.
3. **File Write Logic**: The line replacement logic before writing to the file is incorrect.

Here's a corrected version of your script:

```python
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import json
import os

DIR = os.path.dirname(os.path.abspath(__file__))
Filename = os.path.join(DIR, 'CHATGPT/conversations.json')
print("JSON Filename:", Filename)

with open(Filename, 'r', encoding='utf-8') as file:
    json_data = json.load(file)

# Initialize the result string
result_str = ""

# Define a function to get conversation messages similar to the JavaScript logic
def get_conversation_messages(conversation):
    messages = []
    current_node = conversation.get('current_node')
    while current_node:
        node = conversation['mapping'][current_node]
        message = node.get('message')
        if (message and message.get('content') and message['content'].get('content_type') == 'text' and
                len(message['content'].get('parts', [])) > 0 and len(message['content']['parts'][0]) > 0 and
                (message['author']['role'] != 'system' or message.get('metadata', {}).get('is_user_system_message'))):
            author = message['author']['role']
            if author == 'assistant':
                author = 'ChatGPT'
            elif author == 'system' and message['metadata'].get('is_user_system_message'):
                author = 'Custom user info'
            messages.append({'author': author, 'text': message['content']['parts'][0]})
        current_node = node.get('parent')
    return messages[::-1]  # Reverse the list to maintain chronological order

# Iterate over each conversation in the JSON data and process it
for conversation in json_data:
    # Get the conversation title and messages
    title = conversation.get('title', '')
    messages = get_conversation_messages(conversation)

    # Append the title and messages to the result string
    result_str += title + '\n'
    for message in messages:
        result_str += message['author'] + '\n' + message['text'] + '\n'
    result_str += '\n'  # Add a newline between conversations

# Write the processed result string to a text file
TEXTfile = Filename[:-5] + ".txt"  # Changed this line to properly handle the filename
print("TEXTfile:", TEXTfile)
with open(TEXTfile, "w", encoding='utf-8') as output_file:
    result_str = result_str.replace("
", "\n")
    output_file.write(result_str)

print("Processing complete. Output written to:", TEXTfile)
```

### Explanation of Changes

1. **Correct File Path**: The `Filename` is constructed using `os.path.join(DIR, 'CHATGPT/conversations.json')` to ensure the path is correctly formed.
2. **Iterate Correctly**: The `get_conversation_messages(conversation)` is called within the loop that iterates over each conversation in the `json_data`.
3. **File Write Logic**: The `result_str.replace("
", "\n")` is corrected to ensure newlines are properly handled. The replacement of `
` with `\n` was removed as it didn't make sense in this context.

With these changes, your script should read the JSON data correctly, process the conversations, and write the results to a text file.
user
i want to copy conversations.txt ../
ChatGPT
To copy the generated `conversations.txt` file to the parent directory (`../`), you can use Python's `shutil` module to handle file operations. Here's the updated script:

```python
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import json
import os
import shutil

# Define the current directory and the filename for the JSON data
DIR = os.path.dirname(os.path.abspath(__file__))
json_filename = os.path.join(DIR, 'CHATGPT/conversations.json')
print("JSON Filename:", json_filename)

# Read the JSON data from the file
with open(json_filename, 'r', encoding='utf-8') as file:
    json_data = json.load(file)

# Initialize the result string
result_str = ""

# Define a function to get conversation messages similar to the JavaScript logic
def get_conversation_messages(conversation):
    messages = []
    current_node = conversation.get('current_node')
    while current_node:
        node = conversation['mapping'][current_node]
        message = node.get('message')
        if (message and message.get('content') and message['content'].get('content_type') == 'text' and
                len(message['content'].get('parts', [])) > 0 and len(message['content']['parts'][0]) > 0 and
                (message['author']['role'] != 'system' or message.get('metadata', {}).get('is_user_system_message'))):
            author = message['author']['role']
            if author == 'assistant':
                author = 'ChatGPT'
            elif author == 'system' and message['metadata'].get('is_user_system_message'):
                author = 'Custom user info'
            messages.append({'author': author, 'text': message['content']['parts'][0]})
        current_node = node.get('parent')
    return messages[::-1]  # Reverse the list to maintain chronological order

# Iterate over each conversation in the JSON data and process it
for conversation in json_data:
    # Get the conversation title and messages
    title = conversation.get('title', '')
    messages = get_conversation_messages(conversation)

    # Append the title and messages to the result string
    result_str += title + '\n'
    for message in messages:
        result_str += message['author'] + '\n' + message['text'] + '\n'
    result_str += '\n'  # Add a newline between conversations

# Write the processed result string to a text file
txt_filename = json_filename[:-5] + ".txt"  # Change this line to properly handle the filename
print("TXT Filename:", txt_filename)
with open(txt_filename, "w", encoding='utf-8') as output_file:
    result_str = result_str.replace("
", "\n")
    output_file.write(result_str)

# Copy the generated text file to the parent directory
destination_path = os.path.join(DIR, '..', os.path.basename(txt_filename))
shutil.copy(txt_filename, destination_path)
print("Copied TXT file to:", destination_path)

print("Processing complete. Output written to:", txt_filename)
```

### Explanation of Changes

1. **Copying the File**: After writing the processed result string to the text file, the script uses `shutil.copy()` to copy the file to the parent directory.
   - The `destination_path` is defined as the parent directory of the current directory (`DIR`), combined with the base name of the text file.
   - `shutil.copy(txt_filename, destination_path)` copies the file to the specified location.

This script will now generate the `conversations.txt` file and copy it to the parent directory (`../`).
user
This works great!
ChatGPT
Jack successfully implemented a script to process JSON conversation data and copy the resulting text file to the parent directory using Python and `shutil.copy`.
tool
Model set context updated.
ChatGPT
I'm glad to hear that! If you have any more questions or need further assistance with your projects, feel free to ask. Happy coding!
user
What is the best way to morph a directory of jpgs to create a video
ChatGPT
To create a video by morphing a directory of JPEG images, you can use several tools and libraries. One effective approach involves using `moviepy`, `opencv`, and `numpy`. This method will allow you to apply morphing effects between consecutive images and compile them into a video.

### Steps to Morph Images and Create a Video

1. **Install Necessary Libraries**:
   ```bash
   pip install moviepy opencv-python-headless numpy
   ```

2. **Morphing Function**:
   Create a function that interpolates between two images to create a morphing effect.

3. **Generate Interpolated Frames**:
   Use the morphing function to generate intermediate frames between each pair of consecutive images.

4. **Compile Frames into a Video**:
   Use `moviepy` to compile the frames into a video.

Here's a complete example:

### Script to Morph Images and Create a Video

```python
import os
import cv2
import numpy as np
from moviepy.editor import ImageSequenceClip

def load_images_from_directory(directory):
    image_files = [f for f in os.listdir(directory) if f.endswith('.jpg')]
    image_files.sort()  # Ensure the images are in the correct order
    images = [cv2.imread(os.path.join(directory, f)) for f in image_files]
    return images

def morph_images(img1, img2, alpha):
    return cv2.addWeighted(img1, 1 - alpha, img2, alpha, 0)

def generate_interpolated_frames(images, num_frames_between=10):
    interpolated_frames = []
    for i in range(len(images) - 1):
        for j in range(num_frames_between):
            alpha = j / float(num_frames_between)
            morphed_image = morph_images(images[i], images[i+1], alpha)
            interpolated_frames.append(morphed_image)
    interpolated_frames.append(images[-1])  # Append the last image
    return interpolated_frames

def save_video(frames, output_path, fps=24):
    clip = ImageSequenceClip([cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in frames], fps=fps)
    clip.write_videofile(output_path, codec='libx264')

def main(input_directory, output_video_path):
    images = load_images_from_directory(input_directory)
    interpolated_frames = generate_interpolated_frames(images)
    save_video(interpolated_frames, output_video_path)

if __name__ == '__main__':
    input_directory = 'path/to/your/jpgs'  # Replace with the path to your directory of jpgs
    output_video_path = 'output_video.mp4'
    main(input_directory, output_video_path)
```

### Explanation

1. **Load Images**: The `load_images_from_directory` function reads all JPEG images from the specified directory and sorts them to ensure they are in the correct order.
2. **Morphing Function**: The `morph_images` function interpolates between two images based on the `alpha` value.
3. **Generate Interpolated Frames**: The `generate_interpolated_frames` function creates a specified number of intermediate frames between each pair of consecutive images using the morphing function.
4. **Save Video**: The `save_video` function uses `moviepy` to compile the frames into a video file.
5. **Main Function**: The `main` function ties everything together, processing the images and saving the resulting video.

### Running the Script

Replace `'path/to/your/jpgs'` with the path to your directory containing the JPEG images and run the script. The resulting video will be saved as `output_video.mp4` in the current directory.

This script will create a smooth morphing effect between the images and compile them into a video, providing a visually appealing transition effect.

