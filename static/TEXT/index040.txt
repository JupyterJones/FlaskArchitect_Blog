-

til = Image.new("RGB",(640,640))
im =  Image.new("RGB",(620,620),(150,150,150))
til.paste(im,(10,10))
til2 = Image.new("RGB",(600,160),(250,250,250))
til.paste(til2,(20,450))
w,h = im.size
text0 = "LUA Text Generator Using maching learning / AI"
def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]
infile = "MYTEXT.txt"    
Text = generate_the_word(infile)
#print text
draw = ImageDraw.Draw(til)
#font = ImageFont.truetype("/home/jack/.fonts/Daubmark.ttf", 30)
font = ImageFont.truetype("/home/jack/.fonts/dontmix.ttf", 20)
lines = textwrap.wrap(Text, width=75)
#print lines
y_text = h
#print "::",y_text
for line in lines:
    width, height = font.getsize(line)
    #print lines,w - width / 2, y_text
    W = (w - width) / 2
    H =  y_text-280
    print "++",W,H
    draw.text(((w - width) / 2, y_text-480), line, font=font, fill= "black")
    #draw.text((100, 100), line, font=font, fill= "black")
    
    y_text += height
    print y_text
    
    
font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 22)
draw.text((20,40), text0, font=font, fill= "red")    
    
tm = time.strftime("%Y-%m%d%H%M%S")

filename = tm+"testtiles.png"
print filename
til.save(filename)
til

# read a text file as a list of lines
# find the last line, change to a file you have
filename = "/home/jack/Desktop/char-rnn/uranbible_.txt"
fileHandle = open (filename,"r" )
lineList = fileHandle.readlines()
fileHandle.close()
line = lineList[-1]
print "This is the loss of the last model: ",line[-9:-3]
print "This is the Epoch: ",line[-14:-10]
print "Current learning_rate: ", line[-51:-38]
print "rnn-size: ", line[-28:-25]


til2 = Image.new("RGB",(600,160),(250,250,250))

# read a text file as a list of lines
# find the last line, change to a file you have
filename = "/home/jack/Desktop/char-rnn/uranbible_.txt"
fileHandle = open (filename,"r" )
lineList = fileHandle.readlines()
fileHandle.close()
line = lineList[-1]
text = "This is the loss of the last model: ",line[-9:-3]
text1 = "This is the Epoch: ",line[-14:-10]
text2 = "Current learning_rate: ", line[-51:-38]
text3 = "rnn-size: ", line[-28:-25]
font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 22)
draw.text((20,40), text0, font=font, fill= "red")
draw.text((20,40), text1, font=font, fill= "red")
draw.text((20,40), text2, font=font, fill= "red")
draw.text((20,40), text3, font=font, fill= "red")

f = open("/home/jack/Desktop/char-rnn/uranbible_.txt").read()
print f


f = open("MYTEXT.txt", "r").readlines()
count = 0
for line in f:
    line = line.replace("\n", "")
    count = count +1
    if count<45:
        print line

from PIL import Image
from PIL import ImageFont, ImageDraw
import textwrap
import time
til = Image.new("RGB",(640,640))
im = Image.new("RGB",(600,500),(210,210,230))
til.paste(im,(20,30))
til2 = Image.new("RGB",(600,160),(230,230,230))
til.paste(til2,(20,450))
w,h = im.size

text0 ="""
 The Most High was a world and an angel of God and the sins of the Father's life in the spiritual 
personality of the earth. The Paradise spirit of the Morning Spirit of God is a being of the spirit of 
God and in the divine different spirit of the Universal Father and the Deity Spirit. The Paradise Father 
is the superficial experience of the Father and the Son of Days and the spiritual father of God and 
provision universes to ask the Father's life in the well-half the ancient personality of the Deity 
Trinity of Deity and the Absolute is the first constellation of the experience of the Paradise Creator 
Son in the long experience of the Paradise Creator Son and the experience of God.
"""
draw = ImageDraw.Draw(til)
#font = ImageFont.truetype("/home/jack/.fonts/Daubmark.ttf", 30)
font = ImageFont.truetype("/home/jack/.fonts/dontmix.ttf", 24)
lines = textwrap.wrap(text0, width=60)
y_text = h
for line in lines:
    
    width, height = font.getsize(line)
    #print lines,w - width / 2, y_text
    draw.text(((w - width) / 2, y_text-450), line, font=font, fill= "black")
    y_text += height

text = """
wiki/GRU-0.002200000000-lm_lstm_epoch_0.39_1.3764.t7:   Iteration 11000 of 1420850
Loss is at 1.3764. _epoch_0.39 shows is is only 39% of a full Epoch.
"""
draw = ImageDraw.Draw(til)
#font = ImageFont.truetype("/home/jack/.fonts/Daubmark.ttf", 30)
font = ImageFont.truetype("/home/jack/.fonts/dontmix.ttf", 30)
lines = textwrap.wrap(text, width=40)
y_text = h
for line in lines:
    
    width, height = font.getsize(line)
    #print lines,w - width / 2, y_text
    draw.text(((w - width) / 2, y_text-30), line, font=font, fill= "black")
    y_text += height
    
tm = time.strftime("%Y-%m%d%H%M%S")

filename = tm+"test.png"
print filename
til.save(filename)
til

til2 = Image.new("RGB",(600,160),(250,250,250))

# read a text file as a list of lines
# find the last line, change to a file you have
filename = "/home/jack/Desktop/char-rnn/uranbible_.txt"
fileHandle = open (filename,"r" )
lineList = fileHandle.readlines()
fileHandle.close()
line = lineList[-1]
text = "This is the loss of the last model: ",line[-9:-3]
text1 = "This is the Epoch: ",line[-14:-10]
text2 = "Current learning_rate: ", line[-51:-38]
text3 = "rnn-size: ", line[-28:-25]
font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 22)
draw.text((20,40), text0, font=font, fill= "red")
draw.text((20,40), text1, font=font, fill= "red")
draw.text((20,40), text2, font=font, fill= "red")
draw.text((20,40), text3, font=font, fill= "red")

from PIL import Image
from PIL import ImageFont, ImageDraw
import textwrap
til = Image.new("RGB",(640,640))
#image = "img = 'wiki/Training-Loss-Graph-forwiki_2018-0609062255.png'"
im = Image.open("Last-GRU-network-loss-1.1179-plot-for-Directory-uranbible_2018-0617131414.png") #600x400

til.paste(im,(20,35))
til2 = Image.new("RGB",(600,160),(250,250,250))
til.paste(til2,(20,450))
w,h = im.size
#fname = "Here shows the effect of changing the LearningRate"
fname = "Filename parsed for data:"
# read a text file as a list of lines
# find the last line, change to a file you have
filename = "/home/jack/Desktop/char-rnn/uranbible_.txt"
fileHandle = open (filename,"r" )
lineList = fileHandle.readlines()
fileHandle.close()
line = lineList[-1]
text0 = "This is the loss of the last model: ",line[-9:-3]
text1 = "This is the Epoch: ",line[-14:-10]
text2 = "Current learning_rate: ", line[-51:-38]
text3 = "rnn-size: ", line[-28:-25]

text = str(line[-51:])
SPACE = "-----------------------------------------------"
text0 = str(text0)
text1 = str(text1)
text2 = str(text2)
text3 = str(text3)
text3 = str(text3)
draw = ImageDraw.Draw(til)
font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 18)
draw.text((70,450), fname, font=font, fill= "black")
font = ImageFont.truetype("/home/jack/.fonts/dontmix.ttf", 20)
#draw.text((320,450), fname0, font=font, fill= "black")
#font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 16)
#draw.text((70,474), text0, font=font, fill= "red")
font = ImageFont.truetype("/home/jack/.fonts/dontmix.ttf", 22)
draw.text((70,472), text, font=font, fill= "navy")
draw.text((70,490), SPACE, font=font, fill= "black")
draw.text((70,510), text0, font=font, fill= "navy")  
draw.text((70,530), text1, font=font, fill= "navy")    
draw.text((70,550), text2, font=font, fill= "navy")    
draw.text((70,570), text3, font=font, fill= "navy")    
#draw.text((70,552), text4, font=font, fill= "navy")    
#draw.text((70,572), text5, font=font, fill= "navy")




tm = time.strftime("%Y-%m%d%H%M%S")

filename = tm+"testtiles.png"
print filename
til.save(filename)

til



// This sample is copied from https://github.com/wcharczuk/go-chart/tree/master/_examples/stacked_bar

import (
    "bytes"
    "fmt"

    "github.com/wcharczuk/go-chart"
)

sbc := chart.StackedBarChart{
    Title:      "Test Stacked Bar Chart",
    TitleStyle: chart.StyleShow(),
    Background: chart.Style{
        Padding: chart.Box{
            Top: 40,
        },
    },
    Height: 512,
    XAxis: chart.Style{
        Show: true,
    },
    YAxis: chart.Style{
        Show: true,
    },
    Bars: []chart.StackedBar{
        {
            Name: "This is a very long string to test word break wrapping.",
            Values: []chart.Value{
                {Value: 5, Label: "Blue"},
                {Value: 5, Label: "Green"},
                {Value: 4, Label: "Gray"},
                {Value: 3, Label: "Orange"},
                {Value: 3, Label: "Test"},
                {Value: 2, Label: "??"},
                {Value: 1, Label: "!!"},
            },
        }, {
            Name: "Test",
            Values: []chart.Value{
                {Value: 10, Label: "Blue"},
                {Value: 5, Label: "Green"},
                {Value: 1, Label: "Gray"},
            },
        },
        {
            Name: "Test 2",
            Values: []chart.Value{
                {Value: 10, Label: "Blue"},
                {Value: 6, Label: "Green"},
                {Value: 4, Label: "Gray"},
            },
        },
    },
}
var buf bytes.Buffer
err := sbc.Render(chart.PNG, &buf)
if err != nil {
    fmt.Printf("Error rendering chart: %v\n", err)
    return
}
_ctx.Display.PNG(buf.Bytes(), nil)

// This sample is copied from https://github.com/gonum/plot/wiki/Example-plots#plotutil
import (
    "bytes"
    "math/rand"

    "gonum.org/v1/plot"
    "gonum.org/v1/plot/plotter"
    "gonum.org/v1/plot/plotutil"
    "gonum.org/v1/plot/vg"
)

func DisplayPlot(p *plot.Plot) {
    // Save the plot to a PNG file.
    var buf bytes.Buffer
    c, err := p.WriterTo(4*vg.Inch, 4*vg.Inch, "png")
    if err != nil {
        panic(err)
    }
    if _, err := c.WriteTo(&buf); err != nil {
        panic(err)
    }
    _ctx.Display.PNG(buf.Bytes(), nil)
}

// randomPoints returns some random x, y points.
func randomPoints(n int) plotter.XYs {
    pts := make(plotter.XYs, n)
    for i := range pts {
        if i == 0 {
            pts[i].X = rand.Float64()
        } else {
            pts[i].X = pts[i-1].X + rand.Float64()
        }
        pts[i].Y = pts[i].X + 10*rand.Float64()
    }
    return pts
}

{
    p, err := plot.New()
    if err != nil {
        panic(err)
    }
    
    p.Title.Text = "Plotutil example"
    p.X.Label.Text = "X"
    p.Y.Label.Text = "Y"

    err = plotutil.AddLinePoints(
        p,
        "First", randomPoints(15),
        "Second", randomPoints(15),
        "Third", randomPoints(15))
    if err != nil {
        panic(err)
    }

    DisplayPlot(p)
}



allcountries = []

import plotly.graph_objects as go
import time
from PIL import Image
from GlobalData import *


LASTFILE="COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"


STAT =LASTFILE[57:-4]
DataIn = open(LASTFILE).readlines()



X=GlobalData(SEARCH)
x=X.split(",")
population=x[4]
print(SEARCH,"Population:", population)
cnt = 0
CNTS=0
counts=[]
for line in DataIn:
    if cnt==0:print(line)
    cnt=cnt+1
    line=line.lstrip(",")
    #if SEARCH in line:print(line)
    if SEARCH in line:


import plotly.graph_objects as go
import time
from PIL import Image
from GlobalData import *
DATA=[]
counts=[]
cnt=0
LASTFILE="COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"
DataIn = open(LASTFILE).readlines()
for line in DataIn:
    cnt=cnt+1
    if cnt>1:
        line=line.lstrip(",")
        line=line.rstrip("\n")
        line="0,0"+line.split("0,0",1)[-1]
        entry=line.split(",")
        print(entry)


import plotly.graph_objects as go
import time
from PIL import Image
from GlobalData import *


LASTFILE="COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"


STAT =LASTFILE[57:-4]
DataIn = open(LASTFILE).readlines()

#SEARCH = input("SEARCH: ")
#SEARCH = "Brazil"
SEARCH = "Spain"
#SEARCH = "Philippine"
#SEARCH = "Ecuador"
#SEARCH = "Germany"
#SEARCH = "Japan"
#SEARCH = "US"
X=GlobalData(SEARCH)
x=X.split(",")
population=x[4]
print(SEARCH,"Population:", population)
cnt = 0
CNTS=0
counts=[]
for line in DataIn:
    if cnt==0:print(line)
    cnt=cnt+1
    line=line.lstrip(",")
    #if SEARCH in line:print(line)
    if SEARCH in line:

        line="0,0"+line.split("0,0",1)[-1]

        print
        entry=line.split(",")
        for num in entry:
            counts.append(int(num))
print(SEARCH," counts/population",int(counts[-1])/int(population)) 
allcountries.append([SEARCH," counts/population",int(counts[-1])/int(population)])

import plotly.graph_objects as go
import time
from PIL import Image
from GlobalData import *


LASTFILE="COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"


STAT =LASTFILE[57:-4]
DataIn = open(LASTFILE).readlines()

#SEARCH = input("SEARCH: ")
#SEARCH = "Brazil"
SEARCH = "Spain"
#SEARCH = "Philippine"
#SEARCH = "Ecuador"
#SEARCH = "Germany"
#SEARCH = "Japan"
#SEARCH = "US"
X=GlobalData(SEARCH)
x=X.split(",")
population=x[4]
print(SEARCH,"Population:", population)
cnt = 0
CNTS=0
counts=[]
for line in DataIn:
    if cnt==0:print(line)
    cnt=cnt+1
    line=line.lstrip(",")
    #if SEARCH in line:print(line)
    if SEARCH in line:

        line="0,0"+line.split("0,0",1)[-1]

        print
        entry=line.split(",")
        for num in entry:
            counts.append(int(num))
print(SEARCH," counts/population",int(counts[-1])/int(population)) 
allcountries.append([SEARCH," counts/population",int(counts[-1])/int(population)])

import plotly.graph_objects as go
import time
from PIL import Image
from GlobalData import *


LASTFILE="COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"


STAT =LASTFILE[57:-4]
DataIn = open(LASTFILE).readlines()

#SEARCH = input("SEARCH: ")
#SEARCH = "Brazil"
SEARCH = "Spain"
#SEARCH = "Philippine"
#SEARCH = "Ecuador"
#SEARCH = "Germany"
#SEARCH = "Japan"
#SEARCH = "US"
X=GlobalData(SEARCH)
x=X.split(",")
population=x[4]
print(SEARCH,"Population:", population)
cnt = 0
CNTS=0
counts=[]
for line in DataIn:
    if cnt==0:print(line)
    cnt=cnt+1
    line=line.lstrip(",")
    #if SEARCH in line:print(line)
    if SEARCH in line:

        line="0,0"+line.split("0,0",1)[-1]

        print
        entry=line.split(",")
        for num in entry:
            counts.append(int(num))
print(SEARCH," counts/population",int(counts[-1])/int(population)) 
allcountries.append([SEARCH," counts/population",int(counts[-1])/int(population)])
filename0 = time.strftime("images/"+SEARCH+"_"+STAT+"Deaths_%Y%m%d%H%M%S.png")            
fig = go.Figure()
fig.add_trace(go.Scatter(y=counts))
fig.add_trace(go.Bar(y=counts))
fig.update_layout(title = SEARCH+' CONDID-19 Deaths')
fig.show() 



IncreasePerDay=[]
All = (len(counts))
for x in range(0,All):
    try:
        Sum = (counts[x+1]-counts[x])
        print(Sum, end = " ")
        IncreasePerDay.append(Sum)
    except:
        pass
    
filename1 = time.strftime("images/"+SEARCH+"_"+STAT+"IncreasePerDay"+"_%Y%m%d%H%M%S.png")
fig = go.Figure()
fig.add_trace(go.Scatter(y=IncreasePerDay))
fig.add_trace(go.Bar(y=IncreasePerDay))
fig.update_layout(title = SEARCH+' Increase Each day CONDID-19 Cases')
fig.show() 

from GlobalData import *
data=str(DATA[4]).split(",")
print(data[3])



%%writefile  GlobalData.py
'''
Usage:
To get Zambia Population-2019 
from GlobalData import *
X=GlobalData("Zambia")
x=X.split(",")
print(x[4])

To get Zambia Population Change
from GlobalData import *
X=GlobalData("Zambia")
x=X.split(",")
print(x[5])

from GlobalData import *
print(DATA[4])
>>> ['American Samoa,Oceania,Polynesia,55465,55312,−0.28%']

from GlobalData import *
data=str(DATA[4]).split(",")
print(data[3])
>>> 55465
'''
DATA=[['Country,UN Continental,Unstatistical,Population-2018,Population-2019,Population Change'],
['Afghanistan,Asia,Southern Asia,37171921,38041754,+2.34%'],
['Albania,Europe,Southern Europe,2882740,2880917,−0.06%'],
['Algeria,Africa,Northern Africa,42228408,43053054,+1.95%'],
['American Samoa,Oceania,Polynesia,55465,55312,−0.28%'],
['Andorra,Europe,Southern Europe,77006,77142,+0.18%'],
['Angola,Africa,Middle Africa,30809787,31825295,+3.30%'],
['Anguilla,Americas,Caribbean,14731,14869,+0.94%'],
['Antigua and Barbuda,Americas,Caribbean,96286,97118,+0.86%'],
['Argentina,Americas,South America,44361150,44780677,+0.95%'],
['Armenia,Asia,Western Asia,2951745,2957731,+0.20%'],
['Aruba,Americas,Caribbean,105845,106314,+0.44%'],
['Australia,Oceania,Australia and New Zealand,24898152,25203198,+1.23%'],
['Austria,Europe,Western Europe,8891388,8955102,+0.72%'],
['Azerbaijan,Asia,Western Asia,9949537,10047718,+0.99%'],
['Bahamas,Americas,Caribbean,385637,389482,+1.00%'],
['Bahrain,Asia,Western Asia,1569446,1641172,+4.57%'],
['Bangladesh,Asia,Southern Asia,161376708,163046161,+1.03%'],
['Barbados,Americas,Caribbean,286641,287025,+0.13%'],
['Belarus,Europe,Eastern Europe,9452617,9452411,0.00%'],
['Belgium,Europe,Western Europe,11482178,11539328,+0.50%'],
['Belize,Americas,Central America,383071,390353,+1.90%'],
['Benin,Africa,Western Africa,11485044,11801151,+2.75%'],
['Bermuda,Americas,Northern America,62756,62506,−0.40%'],
['Bhutan,Asia,Southern Asia,754388,763092,+1.15%'],
['Bolivia,Americas,South America,11353142,11513100,+1.41%'],
['Bosnia and Herzegovina,Europe,Southern Europe,3323925,3301000,−0.69%'],
['Botswana,Africa,Southern Africa,2254068,2303697,+2.20%'],
['Brazil,Americas,South America,209469323,211049527,+0.75%'],
['British Virgin Islands,Americas,Caribbean,29802,30030,+0.77%'],
['Brunei,Asia,South-eastern Asia,428963,433285,+1.01%'],
['Bulgaria,Europe,Eastern Europe,7051608,7000119,−0.73%'],
['Burkina Faso,Africa,Western Africa,19751466,20321378,+2.89%'],
['Burundi,Africa,Eastern Africa,10524117,10864245,+3.23%'],
['Cambodia,Asia,South-eastern Asia,16249792,16486542,+1.46%'],
['Cameroon,Africa,Middle Africa,25216267,25876380,+2.62%'],
['Canada,Americas,Northern America,37074562,37411047,+0.91%'],
['Cape Verde,Africa,Western Africa,543767,549935,+1.13%'],
['Caribbean Netherlands[t],Americas,Caribbean,25711,25979,+1.04%'],
['Cayman Islands,Americas,Caribbean,64174,64948,+1.21%'],
['Central African Republic,Africa,Middle Africa,4666368,4745185,+1.69%'],
['Chad,Africa,Middle Africa,15477729,15946876,+3.03%'],
['Chile,Americas,South America,18729160,18952038,+1.19%'],
['China[a],Asia,Eastern Asia,1427647786,1433783686,+0.43%'],
['Colombia,Americas,South America,49661048,50339443,+1.37%'],
['Comoros,Africa,Eastern Africa,832322,850886,+2.23%'],
['Congo,Africa,Middle Africa,5244359,5380508,+2.60%'],
['Cook Islands,Oceania,Polynesia,17518,17548,+0.17%'],
['Costa Rica,Americas,Central America,4999441,5047561,+0.96%'],
['Croatia,Europe,Southern Europe,4156405,4130304,−0.63%'],
['Cuba,Americas,Caribbean,11338134,11333483,−0.04%'],
['Curaçao,Americas,Caribbean,162752,163424,+0.41%'],
['Cyprus,Asia,Western Asia,1170125,1179551,+0.81%'],
['Czech Republic,Europe,Eastern Europe,10665677,10689209,+0.22%'],
['Denmark,Europe,Northern Europe,5752126,5771876,+0.34%'],
['Djibouti,Africa,Eastern Africa,958923,973560,+1.53%'],
['Dominica,Americas,Caribbean,71625,71808,+0.26%'],
['Dominican Republic,Americas,Caribbean,10627141,10738958,+1.05%'],
['DR Congo,Africa,Middle Africa,84068091,86790567,+3.24%'],
['East Timor,Asia,South-eastern Asia,1267974,1293119,+1.98%'],
['Ecuador,Americas,South America,17084358,17373662,+1.69%'],
['Egypt,Africa,Northern Africa,98423598,100388073,+2.00%'],
['El Salvador,Americas,Central America,6420746,6453553,+0.51%'],
['Equatorial Guinea,Africa,Middle Africa,1308975,1355986,+3.59%'],
['Eritrea,Africa,Eastern Africa,3452786,3497117,+1.28%'],
['Estonia,Europe,Northern Europe,1322920,1325648,+0.21%'],
['Eswatini,Africa,Southern Africa,1136281,1148130,+1.04%'],
['Ethiopia,Africa,Eastern Africa,109224414,112078730,+2.61%'],
['F.S. Micronesia,Oceania,Micronesia,112640,113815,+1.04%'],
['Falkland Islands,Americas,South America,3234,3377,+4.42%'],
['Faroe Islands,Europe,Northern Europe,48497,48678,+0.37%'],
['Fiji,Oceania,Melanesia,883483,889953,+0.73%'],
['Finland,Europe,Northern Europe,5522576,5532156,+0.17%'],
['France,Europe,Western Europe,64990511,65129728,+0.21%'],
['French Guiana,Americas,South America,275713,282731,+2.55%'],
['French Polynesia,Oceania,Polynesia,277679,279287,+0.58%'],
['Gabon,Africa,Middle Africa,2119275,2172579,+2.52%'],
['Gambia,Africa,Western Africa,2280094,2347706,+2.97%'],
['Georgia,Asia,Western Asia,4002942,3996765,−0.15%'],
['Germany,Europe,Western Europe,83124418,83517045,+0.47%'],
['Ghana,Africa,Western Africa,28206728,28833629,+2.22%'],
['Gibraltar,Europe,Southern Europe,33718,33701,−0.05%'],
['United Kingdom,Europe,Northern Europe,67141684,67530172,+0.58%'],
['Greece,Europe,Southern Europe,10522246,10473455,−0.46%'],
['Greenland,Americas,Northern America,56564,56672,+0.19%'],
['Grenada,Americas,Caribbean,111454,112003,+0.49%'],
['Guadeloupe[s],Americas,Caribbean,446928,447905,+0.22%'],
['Guam,Oceania,Micronesia,165768,167294,+0.92%'],
['Guatemala,Americas,Central America,17247849,17581472,+1.93%'],
['Guernsey and  Jersey,Europe,Northern Europe,170499,172259,+1.03%'],
['Guinea,Africa,Western Africa,12414293,12771246,+2.88%'],
['Guinea-Bissau,Africa,Western Africa,1874303,1920922,+2.49%'],
['Guyana,Americas,South America,779006,782766,+0.48%'],
['Haiti,Americas,Caribbean,11123178,11263770,+1.26%'],
['Honduras,Americas,Central America,9587522,9746117,+1.65%'],
['Hong Kong,Asia,Eastern Asia,7371730,7436154,+0.87%'],
['Hungary,Europe,Eastern Europe,9707499,9684679,−0.24%'],
['Iceland,Europe,Northern Europe,336713,339031,+0.69%'],
['India,Asia,Southern Asia,1352642280,1366417754,+1.02%'],
['Indonesia,Asia,South-eastern Asia,267670543,270625568,+1.10%'],
['Iran,Asia,Southern Asia,81800188,82913906,+1.36%'],
['Iraq,Asia,Western Asia,38433600,39309783,+2.28%'],
['Ireland,Europe,Northern Europe,4818690,4882495,+1.32%'],
['Isle of Man,Europe,Northern Europe,84077,84584,+0.60%'],
['Israel,Asia,Western Asia,8381516,8519377,+1.64%'],
['Italy,Europe,Southern Europe,60627291,60550075,−0.13%'],
['Ivory Coast,Africa,Western Africa,25069230,25716544,+2.58%'],
['Jamaica,Americas,Caribbean,2934847,2948279,+0.46%'],
['Japan,Asia,Eastern Asia,127202192,126860301,−0.27%'],
['Jordan,Asia,Western Asia,9965318,10101694,+1.37%'],
['Kazakhstan,Asia,Central Asia,18319618,18551427,+1.27%'],
['Kenya,Africa,Eastern Africa,51392565,52573973,+2.30%'],
['Kiribati,Oceania,Micronesia,115847,117606,+1.52%'],
['Kuwait,Asia,Western Asia,4137312,4207083,+1.69%'],
['Kyrgyzstan,Asia,Central Asia,6304030,6415850,+1.77%'],
['Laos,Asia,South-eastern Asia,7061507,7169455,+1.53%'],
['Latvia,Europe,Northern Europe,1928459,1906743,−1.13%'],
['Lebanon,Asia,Western Asia,6859408,6855713,−0.05%'],
['Lesotho,Africa,Southern Africa,2108328,2125268,+0.80%'],
['Liberia,Africa,Western Africa,4818973,4937374,+2.46%'],
['Libya,Africa,Northern Africa,6678559,6777452,+1.48%'],
['Liechtenstein,Europe,Western Europe,37910,38019,+0.29%'],
['Lithuania,Europe,Northern Europe,2801264,2759627,−1.49%'],
['Luxembourg,Europe,Western Europe,604245,615729,+1.90%'],
['Macau,Asia,Eastern Asia,631636,640445,+1.39%'],
['Madagascar,Africa,Eastern Africa,26262313,26969307,+2.69%'],
['Malawi,Africa,Eastern Africa,18143217,18628747,+2.68%'],
['Malaysia,Asia,South-eastern Asia,31528033,31949777,+1.34%'],
['Maldives,Asia,Southern Asia,515696,530953,+2.96%'],
['Mali,Africa,Western Africa,19077749,19658031,+3.04%'],
['Malta,Europe,Southern Europe,439248,440372,+0.26%'],
['Marshall Islands,Oceania,Micronesia,58413,58791,+0.65%'],
['Martinique,Americas,Caribbean,375673,375554,−0.03%'],
['Mauritania,Africa,Western Africa,4403313,4525696,+2.78%'],
['Mauritius,Africa,Eastern Africa,1189265,1198575,+0.78%'],
['Mayotte,Africa,Eastern Africa,259531,266150,+2.55%'],
['Mexico,Americas,Central America,126190788,127575529,+1.10%'],
['Moldova,Europe,Eastern Europe,4051944,4043263,−0.21%'],
['Monaco,Europe,Western Europe,38682,38964,+0.73%'],
['Mongolia,Asia,Eastern Asia,3170216,3225167,+1.73%'],
['Montenegro,Europe,Southern Europe,627809,627987,+0.03%'],
['Montserrat,Americas,Caribbean,4993,4989,−0.08%'],
['Morocco,Africa,Northern Africa,36029093,36471769,+1.23%'],
['Mozambique,Africa,Eastern Africa,29496004,30366036,+2.95%'],
['Myanmar,Asia,South-eastern Asia,53708320,54045420,+0.63%'],
['Namibia,Africa,Southern Africa,2448301,2494530,+1.89%'],
['Nauru,Oceania,Micronesia,10670,10756,+0.81%'],
['Nepal,Asia,Southern Asia,28095714,28608710,+1.83%'],
['Netherlands,Europe,Western Europe,17059560,17097130,+0.22%'],
['New Caledonia,Oceania,Melanesia,279993,282750,+0.98%'],
['New Zealand,Oceania,Australia and New Zealand,4743131,4783063,+0.84%'],
['Nicaragua,Americas,Central America,6465501,6545502,+1.24%'],
['Niger,Africa,Western Africa,22442822,23310715,+3.87%'],
['Nigeria,Africa,Western Africa,195874683,200963599,+2.60%'],
['Niue,Oceania,Polynesia,1620,1615,−0.31%'],
['North Korea,Asia,Eastern Asia,25549604,25666161,+0.46%'],
['North Macedonia,Europe,Southern Europe,2082957,2083459,+0.02%'],
['Northern Mariana Islands,Oceania,Micronesia,56882,56188,−1.22%'],
['Norway,Europe,Northern Europe,5337962,5378857,+0.77%'],
['Oman,Asia,Western Asia,4829473,4974986,+3.01%'],
['Pakistan,Asia,Southern Asia,212228286,216565318,+2.04%'],
['Palau,Oceania,Micronesia,17907,18008,+0.56%'],
['Palestine[n],Asia,Western Asia,4862979,4981420,+2.44%'],
['Panama,Americas,Central America,4176869,4246439,+1.67%'],
['Papua New Guinea,Oceania,Melanesia,8606323,8776109,+1.97%'],
['Paraguay,Americas,South America,6956066,7044636,+1.27%'],
['Peru,Americas,South America,31989260,32510453,+1.63%'],
['Philippines,Asia,South-eastern Asia,106651394,108116615,+1.37%'],
['Poland,Europe,Eastern Europe,37921592,37887768,−0.09%'],
['Portugal,Europe,Southern Europe,10256193,10226187,−0.29%'],
['Puerto Rico,Americas,Caribbean,3039596,2933408,−3.49%'],
['Qatar,Asia,Western Asia,2781682,2832067,+1.81%'],
['Réunion,Africa,Eastern Africa,882526,888927,+0.73%'],
['Romania,Europe,Eastern Europe,19506114,19364557,−0.73%'],
['Russia,Europe,Eastern Europe,145734038,145872256,+0.09%'],
['Rwanda,Africa,Eastern Africa,12301970,12626950,+2.64%'],
['Saint Helena Ascension and Tristan da Cunha,Africa,Western Africa,6035,6059,+0.40%'],
['Saint Kitts and Nevis,Americas,Caribbean,52441,52823,+0.73%'],
['Saint Lucia,Americas,Caribbean,181889,182790,+0.50%'],
['Saint Pierre and Miquelon,Americas,Northern America,5849,5822,−0.46%'],
['Saint Vincent and the Grenadines,Americas,Caribbean,110211,110589,+0.34%'],
['Samoa,Oceania,Polynesia,196129,197097,+0.49%'],
['San Marino,Europe,Southern Europe,33785,33860,+0.22%'],
['São Tomé and Príncipe,Africa,Middle Africa,211028,215056,+1.91%'],
['Saudi Arabia,Asia,Western Asia,33702756,34268528,+1.68%'],
['Senegal,Africa,Western Africa,15854323,16296364,+2.79%'],
['Serbia[k],Europe,Southern Europe,8802754,8772235,−0.35%'],
['Seychelles,Africa,Eastern Africa,97096,97739,+0.66%'],
['Sierra Leone,Africa,Western Africa,7650150,7813215,+2.13%'],
['Singapore,Asia,South-eastern Asia,5757499,5804337,+0.81%'],
['Sint Maarten,Americas,Caribbean,41940,42388,+1.07%'],
['Slovakia,Europe,Eastern Europe,5453014,5457013,+0.07%'],
['Slovenia,Europe,Southern Europe,2077837,2078654,+0.04%'],
['Solomon Islands,Oceania,Melanesia,652857,669823,+2.60%'],
['Somalia,Africa,Eastern Africa,15008226,15442905,+2.90%'],
['South Africa,Africa,Southern Africa,57792518,58558270,+1.33%'],
['South Korea,Asia,Eastern Asia,51171706,51225308,+0.10%'],
['South Sudan,Africa,Eastern Africa,10975927,11062113,+0.79%'],
['Spain[d],Europe,Southern Europe,46692858,46736776,+0.09%'],
['Sri Lanka,Asia,Southern Asia,21228763,21323733,+0.45%'],
['Sudan,Africa,Northern Africa,41801533,42813238,+2.42%'],
['Suriname,Americas,South America,575990,581372,+0.93%'],
['Sweden,Europe,Northern Europe,9971638,10036379,+0.65%'],
['Switzerland,Europe,Western Europe,8525611,8591365,+0.77%'],
['Syria,Asia,Western Asia,16945057,17070135,+0.74%'],
['Taiwan,Asia,Eastern Asia,23726460,23773876,+0.20%'],
['Tajikistan,Asia,Central Asia,9100835,9321018,+2.42%'],
['Tanzania[c],Africa,Eastern Africa,56313438,58005463,+3.00%'],
['Thailand,Asia,South-eastern Asia,68863514,69037513,+0.25%'],
['Togo,Africa,Western Africa,7889093,8082366,+2.45%'],
['Tokelau,Oceania,Polynesia,1319,1340,+1.59%'],
['Tonga,Oceania,Polynesia,110589,110940,+0.32%'],
['Trinidad and Tobago,Americas,Caribbean,1389843,1394973,+0.37%'],
['Tunisia,Africa,Northern Africa,11565201,11694719,+1.12%'],
['Turkey,Asia,Western Asia,82340088,83429615,+1.32%'],
['Turkmenistan,Asia,Central Asia,5850901,5942089,+1.56%'],
['Turks and Caicos Islands,Americas,Caribbean,37665,38191,+1.40%'],
['Tuvalu,Oceania,Polynesia,11508,11646,+1.20%'],
['U.S. Virgin Islands,Americas,Caribbean,104680,104578,−0.10%'],
['Uganda,Africa,Eastern Africa,42729036,44269594,+3.61%'],
['Ukraine,Europe,Eastern Europe,44246156,43993638,−0.57%'],
['United Arab Emirates,Asia,Western Asia,9630959,9770529,+1.45%'],
['United States,Americas,Northern America,327096265,329064917,+0.60%'],
['US,Americas,Northern America,327096265,329064917,+0.60%'],      
['Uruguay,Americas,South America,3449285,3461734,+0.36%'],
['Uzbekistan,Asia,Central Asia,32476244,32981716,+1.56%'],
['Vanuatu,Oceania,Melanesia,292680,299882,+2.46%'],
['Vatican City,Europe,Southern Europe,801,799,−0.25%'],
['Venezuela,Americas,South America,28887118,28515829,−1.29%'],
['Vietnam,Asia,South-eastern Asia,95545962,96462106,+0.96%'],
['Wallis and Futuna,Oceania,Polynesia,11661,11432,−1.96%'],
['Western Sahara,Africa,Northern Africa,567402,582463,+2.65%'],
['Yemen,Asia,Western Asia,28498683,29161922,+2.33%'],
['Zambia,Africa,Eastern Africa,17351708,17861030,+2.94%'],
['Zimbabwe,Africa,Eastern Africa,14438802,14645468,+1.43%']]
HELP="""'help,Usage:\
To get Zambia Population-2019 \
from GlobalData import *\
X=GlobalData("Zambia")\
x=X.split(",")\
print(x[4])\
\n\
To get Zambia Population Change\
from GlobalData import *\
X=GlobalData("Zambia")\
x=X.split(",")\
print(x[5])\
\n\
from GlobalData import *\
print(DATA[4])\
>>> [\'American Samoa,Oceania,Polynesia,55465,55312,−0.28%\']\
\n\
from GlobalData import *\
data=str(DATA[4]).split(",")\
print(data[3])\
>>> 55465']]"""
    
def GlobalData(country):
    if country=="HELP":print(HELP)
    for i in range(0,len(DATA)):
        if country in DATA[i][0]:
            
            res=str(DATA[i])
            return res

from GlobalData import *
X=GlobalData("Zambia")
x=X.split(",")
print(x[4])

from GlobalData import *
GlobalData("Turkey")

from GlobalData import *
print(GlobalData(HELP))



#100 cases
#10 ill

10/100



%%writefile wallet
#!/bin/bash
# do not forget to make this file executable: chmod +x wallet
# if you cp or symlink wallet to /usr/bin it can be used in a cron job
#echo curly brackets and the date into the datafile > wallet.balance
echo "{ \"Date\": `date +%s`," >>/home/jack/wallet.balance
# sent the results of the command: lbrynet wallet balance into the file > wallet.balance
lbrynet wallet balance >>/home/jack/wallet.balance 
# echo and ending curly-brackets to  > wallet.balance
echo "}" >>/home/jack/wallet.balance
# Then look at > wallet.balance
cat /home/jack/wallet.balance

if not os.path.exists('images'):
    os.makedirs('images')
if not os.path.exists('post'):
    os.makedirs('post')

# The result of the `wallet` being executed 
# Each time it is run it will append the wallet.balance file
# This may be set up torun as a cron job
!wallet

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
f = open("/home/jack/wallet.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))
        
for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))
        
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)

plt.figure(figsize=(12, 5))
#fig, ax = plt.subplots()
plt.subplot(221)
plt.grid(True)
#ax.plot(t, s, color='green')
plt.plot(t, s, color='green')
plt.xlabel('Number TIP entries', fontsize=14, color='green')
plt.subplot(222)
plt.grid(True)
#ax.plot(t, S, color='red')
plt.plot(t, S, color='red')
plt.xlabel('Number SUPPORT entries', fontsize=14, color='red')
plt.subplot(223)
plt.grid(True)
plt.plot(t, S+s, color='blue')
plt.xlabel('Number TOTAL entries', fontsize=14, color='blue')
#ax.plot(t, S+s, color='blue')
plt.subplot(224)
plt.grid(True)
plt.plot(t, s, color='green')
plt.plot(t, S+s, color='blue')
plt.plot(t, S, color='red')
plt.suptitle('Green=Tips                      Red=Support                           Blue=Total')
plt.xlabel('All three compared', fontsize=14, color='blue')

plt.subplots_adjust(top=0.92, bottom=0.01, left=0.10, right=0.95, hspace=0.55,
                    wspace=0.35)

plt.show()


f = open("/home/jack/wallet.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))
        
for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))
        
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)
plt.figure(figsize=(16, 6))
#fig, ax = plt.subplots()
plt.subplot(131)
plt.grid(True)
#ax.plot(t, s, color='green')
plt.plot(t, s, color='green')
plt.xlabel('Number TIP entries', fontsize=14, color='green')
plt.subplot(132)
plt.grid(True)
#ax.plot(t, S, color='red')
plt.plot(t, S, color='red')
plt.xlabel('Number SUPPORT entries', fontsize=14, color='red')
plt.subplot(133)
plt.grid(True)
plt.plot(t, S+s, color='blue')
#ax.plot(t, S+s, color='blue')
plt.suptitle('Green=Tips                      Red=Support                           Blue=Total')
plt.xlabel('Number TOTAL of Support and Tips', fontsize=14, color='blue')
plt.ylabel='Values (LBC)'
#ax.set(xlabel='time (t)', ylabel='Values (LBC)',
#       title='Blue=Total / Green=Tips / Red=Support')
#plt.subplot(231)(t, S, color='red')
plt.show()


f = open("/home/jack/wallet.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))
        
for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))
        
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)
plt.figure(figsize=(12, 8))
fig, ax = plt.subplots()
plt.subplot(131)
plt.grid(True)
#ax.plot(t, s, color='green')
plt.plot(t, s, color='green')
plt.subplot(132)
plt.grid(True)
#ax.plot(t, S, color='red')
plt.plot(t, S, color='red')
plt.subplot(133)
plt.grid(True)
plt.plot(t, S+s, color='blue')
#ax.plot(t, S+s, color='blue')
plt.suptitle('Green=Tips                      Red=Support                           Blue=Total')
ax.set(xlabel='time (t)', ylabel='Values (LBC)',
       title='Blue=Total / Green=Tips / Red=Support')
plt.show()


# %load /home/jack/wallet2.balance
{ "Date": 1579654802,
{
  "available": "6.395654",
  "reserved": "14303.268",
  "reserved_subtotals": {
    "claims": "910.158",
    "supports": "6205.0",
    "tips": "7188.11"
  },
  "total": "14309.663654"
}
}
{ "Date": 1579658401,
{
  "available": "6.395654",
  "reserved": "14303.268",
  "reserved_subtotals": {
    "claims": "910.158",
    "supports": "6205.0",
    "tips": "7188.11"
  },
  "total": "14309.663654"
}
}
{ "Date": 1579662001,
{
  "available": "6.395654",
  "reserved": "14303.268",
  "reserved_subtotals": {
    "claims": "910.158",
    "supports": "6205.0",
    "tips": "7188.11"
  },
  "total": "14309.663654"
}
}


f = open("/home/jack/wallet2.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))
        
for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))
        
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)
plt.figure(figsize=(12, 5))

plt.subplot(131)
plt.grid(True)
plt.plot(t, s, color='green')

plt.subplot(132)
plt.grid(True)
plt.plot(t, S, color='red')

plt.subplot(133)
plt.grid(True)
plt.plot(t, S+s, color='blue')

plt.suptitle('Green=Tips                      Red=Support                           Blue=Total')
ax.set(xlabel='time (t)', ylabel='Values (LBC)',
       title='Blue=Total / Green=Tips / Red=Support')
plt.show()


import matplotlib
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
import time
DT = time.strftime("%Y%m%d%H%M")
f = open("/home/jack/wallet.balance").readlines()
x = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))

# Data for plotting
t=range(0,len(x))
s = np.array(x)

fig, ax = plt.subplots()
ax.plot(t, s)
ax.set(xlabel='time (s)', ylabel='voltage (mV)',
       title='Total Support in LBC')
ax.grid()
filename = "images/"+DT+"test.png"
print filename
fig.savefig(filename)

plt.show()

import time
DT = time.strftime("%Y-%m-%d-%H:%M")
print DT

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time
%matplotlib inline
import matplotlib.patches as mpatches
DT = time.strftime("%Y-%m-%d-%H:%M")
# use for filename
dt = time.strftime("%Y%m%d%H%M-")
f = open("/home/jack/wallet.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))

for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))        
        
# Data for plotting
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)

fig, ax = plt.subplots(dpi=150)
ax.grid(color = "salmon", alpha = .9, linewidth = 2)

red_patch = mpatches.Patch(color='green', alpha = .5, label='Supports')
blue_patch = mpatches.Patch(color='blue', alpha = .5, label='Tips')
plt.legend(handles=[red_patch, blue_patch])
ax.set(xlabel='DATE: '+DT)
'''
ax.set(xlabel='DATE: '+DT+'  Samples(Scale = '+NumOfSamples+' samples)   Dropout '+ Dropout, ylabel='Last Training Loss: '+Trainingloss,
       title='Plot Losses from Last '+NumOfSamples+' Samples.   Epoch: '+Epoch+' \n Last: EvaluationLoss: \
'+Evaluationloss+"    LearningRate: "+LearningRate )
'''
print "Green = Supports"
print "Blue = Tips"
fig.patch.set_facecolor('salmon')
#plt.plot(yinc, y, color= 'green', alpha = .5)
#plt.plot(yinc, y, color= 'red', alpha = .5)
plt.plot(t, s, color= 'blue')
plt.plot(t, S, color= 'green')
filename = "images/"+dt+"test1.png"
filename0 = "images/"+dt+"full.png"
print filename," ",filename0
fig.savefig(filename)
plt.savefig(filename0 , bbox_inches="tight", facecolor="salmon")

plt.show()

from PIL import Image
files = glob('images/*full.png') # * means all *full.png
filename = max(files, key=os.path.getctime)
IMG = Image.open(filename)
IMG

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time
%matplotlib inline
import matplotlib.patches as mpatches
DT = time.strftime("%Y-%m-%d:%H")
dt = time.strftime("%Y%m%d%H%M-")
f = open("/home/jack/wallet2.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))

for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))        
        
# Data for plotting
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)

fig, ax = plt.subplots(dpi=150)
ax.grid(color = "salmon", alpha = .9, linewidth = 2)

red_patch = mpatches.Patch(color='green', alpha = .5, label='Supports')
blue_patch = mpatches.Patch(color='blue', alpha = .5, label='Tips')
plt.legend(handles=[red_patch, blue_patch])
ax.set(xlabel='DATE: '+DT)
'''
ax.set(xlabel='DATE: '+DT+'  Samples(Scale = '+NumOfSamples+' samples)   Dropout '+ Dropout, ylabel='Last Training Loss: '+Trainingloss,
       title='Plot Losses from Last '+NumOfSamples+' Samples.   Epoch: '+Epoch+' \n Last: EvaluationLoss: \
'+Evaluationloss+"    LearningRate: "+LearningRate )
'''
print "Green = Supports"
print "Blue = Tips"
fig.patch.set_facecolor('salmon')
#plt.plot(yinc, y, color= 'green', alpha = .5)
#plt.plot(yinc, y, color= 'red', alpha = .5)
plt.plot(t, s, color= 'blue')
plt.plot(t, S, color= 'green')

#fig.savefig("test.png")
#plt.savefig(dt+'wallet_crone.png', bbox_inches="tight", facecolor="salmon")

plt.show()

!ls

# %load SignBaseImage
#!/usr/bin/python2
"""
Some imports are not needed, but are for future use. markovify will be used to generate status for 
the LBRY TwitterBot.
"""
import random
from random import randint
import subprocess
import time
#import markovify
import os
import sys
from glob import glob
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
#creates directories if they do not exist
if not os.path.exists('images'):
    os.makedirs('images')
if not os.path.exists('post'):
    os.makedirs('post')
#gets a list of all files in a directory    
all_files = glob('images/*full.png') # * means all *full.png
#find the file with the latest date
filename0 = max(all_files, key=os.path.getctime)
dt = time.strftime("%Y%m%d%H%M-")

bashCommand = "curl https://api.cryptonator.com/api/ticker/lbc-usd >crytonator/cryptonator"
output = subprocess.check_output(['bash','-c', bashCommand])
data = open("crytonator/cryptonator").readlines()
DATA = str(data).split(",")
     
img = Image.new('RGB', (220, 110), color = (255, 140, 105))
fnt = ImageFont.truetype('/home/jack/fonts/DejaVuSerif.ttf', 15)     
d = ImageDraw.Draw(img)
d.text((10,10), DATA[1], font=fnt, fill=(0,0,0))
d.text((10,30), DATA[2], font=fnt, fill=(0,0,0))
d.text((10,50), DATA[4][:-1], font=fnt, fill=(0,0,0))
d.text((10,70), DATA[5], font=fnt, fill=(0,0,0))
img.save("images/"+"pil_text.png")


def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_blurred_back(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', (1280,720), (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

if __name__ == '__main__':
    #inp = Image.open(filename0)
    inp = Image.new('RGBA', (1280,720), (21,175,75))
    font = ImageFont.truetype("/home/jack/fonts/Exo-Black.ttf", 30)
    text_title = (255, 255,230) # bright green
    blur_title = (0, 0, 0)   # black
    #textin = (generate_the_word("wordcloud.txt"))
    i2 = draw_blurred_back(inp, (2, 24), "@LBC-toolbox", font, text_title, blur_title)
    font0 = ImageFont.truetype("/home/jack/fonts/Exo-Black.ttf", 20)
    i2 = draw_blurred_back(i2, (2, 70), "LBRY LBC Support-Tips", font0, text_title, blur_title)    
    i2 = draw_blurred_back(i2, (2, 100), "LBC-TwitterPlotBot", font0, text_title, blur_title) 
    i2 = draw_blurred_back(i2, (2, 150), "Don't have the app -", font0, text_title, blur_title)
    i2 = draw_blurred_back(i2, (2, 180), "Visit me on lbry.tv", font0, text_title, blur_title) 

    # get a font
    fnt = ImageFont.truetype("/home/jack/fonts/Exo-Black.ttf", 30)
    # get a drawing context
    signature_ = "@MyLinuxToyBox     @TotallyInsaneArt      @PhilippineRetirement" 
    #get length in pixel of signature_
    sizeS,ln = fnt.getsize(signature_)
    #add 15 pixels to right border
    pt = sizeS+90
    width, height = inp.size
    #marginx starting point of signature_
    marginx = pt
    #bottom margin
    marginy = 80
    x = width - marginx
    y = height - marginy
    

    text_sig = (255, 255,230) # bright green
    blur_sig = (0, 0, 0)   # black
    txt=draw_blurred_back(i2,(x,y), signature_, fnt, text_sig, blur_sig)
    out = Image.alpha_composite(i2, txt)
    #out.save("images/TEMP_POST.png")
    background = Image.new('RGBA', (1280,720), (21,175,75))
    #txt=draw_blurred_back(background,(x,y), i2, fnt, text_sig, blur_sig)
    txt=draw_blurred_back(background,(x,y), signature_, fnt, text_sig, blur_sig)
    txt.paste(out, (50, 20))
    im2=Image.open(filename0)
    plot  = im2.resize((940,600) , Image.NEAREST)
    txt.paste(plot, (300,50))
    filename0 = "post/"+dt+"POST.png"
    txt.save(filename0, quality=95)
    
files = glob('post/*POST.png') # * means all *full.png
PATH = max(files, key=os.path.getctime)    
image = Image.open(PATH)
over = Image.open("images/pil_text.png")
over2 = Image.open("images/logo.png")
image.paste(over, (50,250))
image.paste(over2, (50,395))
filename = time.strftime("post/%Y%m%d%H%M-"+"final.png")
image.save(filename)
image

files = glob('images/*full.png') # * means all *full.png
filename0 = max(files, key=os.path.getctime)
print files



%%writefile wallet
#!/bin/bash
# do not forget to make this file executable: chmod +x wallet
# if you cp or symlink wallet to /usr/bin it can be used in a cron job
#echo curly brackets and the date into the datafile > wallet.balance
echo "{ \"Date\": `date +%s`," >>/home/jack/wallet.balance
# sent the results of the command: lbrynet wallet balance into the file > wallet.balance
lbrynet wallet balance >>/home/jack/wallet.balance 
# echo and ending curly-brackets to  > wallet.balance
echo "}" >>/home/jack/wallet.balance
# Then look at > wallet.balance
cat /home/jack/wallet.balance

if not os.path.exists('images'):
    os.makedirs('images')
if not os.path.exists('post'):
    os.makedirs('post')

# The result of the `wallet` being executed 
# Each time it is run it will append the wallet.balance file
# This may be set up torun as a cron job
!wallet

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
f = open("/home/jack/wallet.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))
        
for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))
        
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)

plt.figure(figsize=(12, 5))
#fig, ax = plt.subplots()
plt.subplot(221)
plt.grid(True)
#ax.plot(t, s, color='green')
plt.plot(t, s, color='green')
plt.xlabel('Number TIP entries', fontsize=14, color='green')
plt.subplot(222)
plt.grid(True)
#ax.plot(t, S, color='red')
plt.plot(t, S, color='red')
plt.xlabel('Number SUPPORT entries', fontsize=14, color='red')
plt.subplot(223)
plt.grid(True)
plt.plot(t, S+s, color='blue')
plt.xlabel('Number TOTAL entries', fontsize=14, color='blue')
#ax.plot(t, S+s, color='blue')
plt.subplot(224)
plt.grid(True)
plt.plot(t, s, color='green')
plt.plot(t, S+s, color='blue')
plt.plot(t, S, color='red')
plt.suptitle('Green=Tips                      Red=Support                           Blue=Total')
plt.xlabel('All three compared', fontsize=14, color='blue')

plt.subplots_adjust(top=0.92, bottom=0.01, left=0.10, right=0.95, hspace=0.55,
                    wspace=0.35)

plt.show()


f = open("/home/jack/wallet.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))
        
for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))
        
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)
plt.figure(figsize=(16, 6))
#fig, ax = plt.subplots()
plt.subplot(131)
plt.grid(True)
#ax.plot(t, s, color='green')
plt.plot(t, s, color='green')
plt.xlabel('Number TIP entries', fontsize=14, color='green')
plt.subplot(132)
plt.grid(True)
#ax.plot(t, S, color='red')
plt.plot(t, S, color='red')
plt.xlabel('Number SUPPORT entries', fontsize=14, color='red')
plt.subplot(133)
plt.grid(True)
plt.plot(t, S+s, color='blue')
#ax.plot(t, S+s, color='blue')
plt.suptitle('Green=Tips                      Red=Support                           Blue=Total')
plt.xlabel('Number TOTAL of Support and Tips', fontsize=14, color='blue')
plt.ylabel='Values (LBC)'
#ax.set(xlabel='time (t)', ylabel='Values (LBC)',
#       title='Blue=Total / Green=Tips / Red=Support')
#plt.subplot(231)(t, S, color='red')
plt.show()


f = open("/home/jack/wallet.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))
        
for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))
        
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)
plt.figure(figsize=(12, 8))
fig, ax = plt.subplots()
plt.subplot(131)
plt.grid(True)
#ax.plot(t, s, color='green')
plt.plot(t, s, color='green')
plt.subplot(132)
plt.grid(True)
#ax.plot(t, S, color='red')
plt.plot(t, S, color='red')
plt.subplot(133)
plt.grid(True)
plt.plot(t, S+s, color='blue')
#ax.plot(t, S+s, color='blue')
plt.suptitle('Green=Tips                      Red=Support                           Blue=Total')
ax.set(xlabel='time (t)', ylabel='Values (LBC)',
       title='Blue=Total / Green=Tips / Red=Support')
plt.show()


f = open("/home/jack/wallet2.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))
        
for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))
        
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)
plt.figure(figsize=(12, 5))

plt.subplot(131)
plt.grid(True)
plt.plot(t, s, color='green')

plt.subplot(132)
plt.grid(True)
plt.plot(t, S, color='red')

plt.subplot(133)
plt.grid(True)
plt.plot(t, S+s, color='blue')

plt.suptitle('Green=Tips                      Red=Support                           Blue=Total')
ax.set(xlabel='time (t)', ylabel='Values (LBC)',
       title='Blue=Total / Green=Tips / Red=Support')
plt.show()


import matplotlib
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
import time
DT = time.strftime("%Y%m%d%H%M")
f = open("/home/jack/wallet.balance").readlines()
x = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))

# Data for plotting
t=range(0,len(x))
s = np.array(x)

fig, ax = plt.subplots()
ax.plot(t, s)
ax.set(xlabel='time (s)', ylabel='voltage (mV)',
       title='Total Support in LBC')
ax.grid()
filename = "images/"+DT+"test.png"
print filename
fig.savefig(filename)

plt.show()

import time
DT = time.strftime("%Y-%m-%d-%H:%M")
print DT

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time
%matplotlib inline
import matplotlib.patches as mpatches
DT = time.strftime("%Y-%m-%d-%H:%M")
# use for filename
dt = time.strftime("%Y%m%d%H%M-")
f = open("/home/jack/wallet.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))

for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))        
        
# Data for plotting
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)

fig, ax = plt.subplots(dpi=150)
ax.grid(color = "salmon", alpha = .9, linewidth = 2)

red_patch = mpatches.Patch(color='green', alpha = .5, label='Supports')
blue_patch = mpatches.Patch(color='blue', alpha = .5, label='Tips')
plt.legend(handles=[red_patch, blue_patch])
ax.set(xlabel='DATE: '+DT)
'''
ax.set(xlabel='DATE: '+DT+'  Samples(Scale = '+NumOfSamples+' samples)   Dropout '+ Dropout, ylabel='Last Training Loss: '+Trainingloss,
       title='Plot Losses from Last '+NumOfSamples+' Samples.   Epoch: '+Epoch+' \n Last: EvaluationLoss: \
'+Evaluationloss+"    LearningRate: "+LearningRate )
'''
print "Green = Supports"
print "Blue = Tips"
fig.patch.set_facecolor('salmon')
#plt.plot(yinc, y, color= 'green', alpha = .5)
#plt.plot(yinc, y, color= 'red', alpha = .5)
plt.plot(t, s, color= 'blue')
plt.plot(t, S, color= 'green')
filename = "images/"+dt+"test1.png"
filename0 = "images/"+dt+"full.png"
print filename," ",filename0
fig.savefig(filename)
plt.savefig(filename0 , bbox_inches="tight", facecolor="salmon")

plt.show()

import os
from glob import glob
from PIL import Image
files = glob('images/*full.png') # * means all *full.png
filename = max(files, key=os.path.getctime)
IMG = Image.open(filename)
IMG

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time
%matplotlib inline
import matplotlib.patches as mpatches
DT = time.strftime("%Y-%m-%d:%H")
dt = time.strftime("%Y%m%d%H%M-")
f = open("/home/jack/wallet2.balance").readlines()
x = []
y = []
for line in f:
    if '"supports": "' in line:
        line =line.replace('"\n', '')
        line =line.replace('"supports": "', '')
        line =line.replace('",', '')
        num = float(line)
        x.append(int(num))

for lines in f:
    if '"tips": "' in lines:
        lines =lines.replace('"\n', '')
        lines =lines.replace('"tips": "', '')
        lines =lines.replace('",', '')
        nums = float(lines)
        y.append(int(nums))        
        
# Data for plotting
t=range(0,len(x))  
S = np.array(x)
s = np.array(y)

fig, ax = plt.subplots(dpi=150)
ax.grid(color = "salmon", alpha = .9, linewidth = 2)

red_patch = mpatches.Patch(color='green', alpha = .5, label='Supports')
blue_patch = mpatches.Patch(color='blue', alpha = .5, label='Tips')
plt.legend(handles=[red_patch, blue_patch])
ax.set(xlabel='DATE: '+DT)
'''
ax.set(xlabel='DATE: '+DT+'  Samples(Scale = '+NumOfSamples+' samples)   Dropout '+ Dropout, ylabel='Last Training Loss: '+Trainingloss,
       title='Plot Losses from Last '+NumOfSamples+' Samples.   Epoch: '+Epoch+' \n Last: EvaluationLoss: \
'+Evaluationloss+"    LearningRate: "+LearningRate )
'''
print "Green = Supports"
print "Blue = Tips"
fig.patch.set_facecolor('salmon')
#plt.plot(yinc, y, color= 'green', alpha = .5)
#plt.plot(yinc, y, color= 'red', alpha = .5)
plt.plot(t, s, color= 'blue')
plt.plot(t, S, color= 'green')

#fig.savefig("test.png")
#plt.savefig(dt+'wallet_crone.png', bbox_inches="tight", facecolor="salmon")

plt.show()

!ls

# %load SignBaseImage
#!/usr/bin/python2
"""
Some imports are not needed, but are for future use. markovify will be used to generate status for 
the LBRY TwitterBot.
"""
import random
from random import randint
import subprocess
import time
#import markovify
import os
import sys
from glob import glob
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
#creates directories if they do not exist
if not os.path.exists('images'):
    os.makedirs('images')
if not os.path.exists('post'):
    os.makedirs('post')
#gets a list of all files in a directory    
all_files = glob('images/*full.png') # * means all *full.png
#find the file with the latest date
filename0 = max(all_files, key=os.path.getctime)
dt = time.strftime("%Y%m%d%H%M-")

bashCommand = "curl https://api.cryptonator.com/api/ticker/lbc-usd >crytonator/cryptonator"
output = subprocess.check_output(['bash','-c', bashCommand])
data = open("crytonator/cryptonator").readlines()
DATA = str(data).split(",")
     
img = Image.new('RGB', (220, 110), color = (255, 140, 105))
fnt = ImageFont.truetype('/home/jack/fonts/DejaVuSerif.ttf', 15)     
d = ImageDraw.Draw(img)
d.text((10,10), DATA[1], font=fnt, fill=(0,0,0))
d.text((10,30), DATA[2], font=fnt, fill=(0,0,0))
d.text((10,50), DATA[4][:-1], font=fnt, fill=(0,0,0))
d.text((10,70), DATA[5], font=fnt, fill=(0,0,0))
img.save("images/"+"pil_text.png")


def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_blurred_back(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', (1280,720), (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

if __name__ == '__main__':
    #inp = Image.open(filename0)
    inp = Image.new('RGBA', (1280,720), (21,175,75))
    font = ImageFont.truetype("/home/jack/fonts/Exo-Black.ttf", 30)
    text_title = (255, 255,230) # bright green
    blur_title = (0, 0, 0)   # black
    #textin = (generate_the_word("wordcloud.txt"))
    i2 = draw_blurred_back(inp, (2, 24), "@LBC-toolbox", font, text_title, blur_title)
    font0 = ImageFont.truetype("/home/jack/fonts/Exo-Black.ttf", 20)
    i2 = draw_blurred_back(i2, (2, 70), "LBRY LBC Support-Tips", font0, text_title, blur_title)    
    i2 = draw_blurred_back(i2, (2, 100), "LBC-TwitterPlotBot", font0, text_title, blur_title) 
    i2 = draw_blurred_back(i2, (2, 150), "Don't have the app -", font0, text_title, blur_title)
    i2 = draw_blurred_back(i2, (2, 180), "Visit me on lbry.tv", font0, text_title, blur_title) 

    # get a font
    fnt = ImageFont.truetype("/home/jack/fonts/Exo-Black.ttf", 30)
    # get a drawing context
    signature_ = "@MyLinuxToyBox     @TotallyInsaneArt      @PhilippineRetirement" 
    #get length in pixel of signature_
    sizeS,ln = fnt.getsize(signature_)
    #add 15 pixels to right border
    pt = sizeS+90
    width, height = inp.size
    #marginx starting point of signature_
    marginx = pt
    #bottom margin
    marginy = 80
    x = width - marginx
    y = height - marginy
    

    text_sig = (255, 255,230) # bright green
    blur_sig = (0, 0, 0)   # black
    txt=draw_blurred_back(i2,(x,y), signature_, fnt, text_sig, blur_sig)
    out = Image.alpha_composite(i2, txt)
    #out.save("images/TEMP_POST.png")
    background = Image.new('RGBA', (1280,720), (21,175,75))
    #txt=draw_blurred_back(background,(x,y), i2, fnt, text_sig, blur_sig)
    txt=draw_blurred_back(background,(x,y), signature_, fnt, text_sig, blur_sig)
    txt.paste(out, (50, 20))
    im2=Image.open(filename0)
    plot  = im2.resize((940,600) , Image.NEAREST)
    txt.paste(plot, (300,50))
    filename0 = "post/"+dt+"POST.png"
    txt.save(filename0, quality=95)
    
files = glob('post/*POST.png') # * means all *full.png
PATH = max(files, key=os.path.getctime)    
image = Image.open(PATH)
over = Image.open("images/pil_text.png")
over2 = Image.open("images/logo.png")
image.paste(over, (50,250))
image.paste(over2, (50,395))
filename = time.strftime("post/%Y%m%d%H%M-"+"final.png")
image.save(filename)
image

files = glob('images/*full.png') # * means all *full.png
filename0 = max(files, key=os.path.getctime)
print files



%matplotlib inline
%config InlineBackend.figure_formats = set(['retina'])
import matplotlib.pyplot as plt
plt.plot([0,1],[1,0])

%matplotlib inline
%config InlineBackend.figure_formats = set(['retina'])
import matplotlib.pyplot as plt
plt.plot([0,1],[1,0])

%matplotlib inline
%config InlineBackend.figure_formats = set(['retina'])
import matplotlib.pyplot as plt
plt.plot([0,1],[1,0])

import numpy as np

import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("4mm")

phase = -np.pi / 2
for i in range(20):
    angles = np.linspace(0, 2 * np.pi, i + 4)
    vsk.polygon((i + 1) * np.cos(angles + phase), (i + 1) * np.sin(angles + phase))

vsk.display()#mode="matplotlib")
vsk.save("polygons.svg")

import numpy as np

import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("4mm")

phase = -np.pi / 2
for i in range(50):
    angles = np.linspace(0, 2 * np.pi, i + 3)
    vsk.polygon((i + 1) * np.cos(angles + phase), (i + 1) * np.sin(angles + phase))

vsk.display()#mode="matplotlib")
vsk.save("polygons.svg")

import numpy as np
from random import randint
import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("4mm")
def inc():
    var = randint(0,4)
    return var
phase = -np.pi / 2
for i in range(150):
    angles = np.linspace(0, 2 * np.pi, i + 3)
    vsk.polygon((i + 1) * np.cos(angles-inc() + phase+inc()), (i + 1) * np.sin(angles + phase))

vsk.display()#mode="matplotlib")
vsk.save("polygons.svg")

import numpy as np
from random import randint
import vsketch

vsk = vsketch.Vsketch()
vsk.size(1440,960, landscape=False)
vsk.scale("px")
def inc():
    var = randint(0,4)
    return var
phase = -np.pi / 2
for i in range(0,450,5):
    angles = np.linspace(0, 2 * np.pi, i + 3)
    vsk.polygon((i + 1) * np.cos(angles-inc() + phase+inc()), (i + 1) * np.sin(angles + phase))

vsk.display()#mode="matplotlib")
vsk.save("polygons-px.svg")

import numpy as np
from random import randint
import vsketch

vsk = vsketch.Vsketch()
vsk.size(1440,960, landscape=False)
vsk.scale("px")
def inc():
    var = randint(0,4)
    return var
num_lines = 850
x_coords = np.linspace(0., 250., 1000)
perlin = vsk.noise(x_coords * 0.1, np.linspace(0, 5., num_lines))
for j in range(0,num_lines,20):
    vsk.polygon(x_coords, j + perlin[:, j] * 45)

vsk.display()#mode="matplotlib")
vsk.save("polygons-px.svg")



import numpy as np

import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("4mm")

phase = -np.pi / 2
for i in range(20):
    angles = np.linspace(0, 2 * np.pi, i + 4)
    vsk.polygon((i + 1) * np.cos(angles + phase), (i + 1) * np.sin(angles + phase))

vsk.display(mode="matplotlib")
vsk.save("polygons.svg")

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

!pip install -q opencv-python

import csv
import cv2
import itertools
import numpy as np
import pandas as pd
import os
import sys
import tempfile
import tqdm

from matplotlib import pyplot as plt
from matplotlib.collections import LineCollection

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow import keras

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#@title Functions to run pose estimation with MoveNet

#@markdown You'll download the MoveNet Thunder model from [TensorFlow Hub](https://www.google.com/url?sa=D&q=https%3A%2F%2Ftfhub.dev%2Fs%3Fq%3Dmovenet), and reuse some inference and visualization logic from the [MoveNet Raspberry Pi (Python)](https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/raspberry_pi) sample app to detect landmarks (ear, nose, wrist etc.) from the input images.

#@markdown *Note: You should use the most accurate pose estimation model (i.e. MoveNet Thunder) to detect the keypoints and use them to train the pose classification model to achieve the best accuracy. When running inference, you can use a pose estimation model of your choice (e.g. either MoveNet Lightning or Thunder).*

# Download model from TF Hub and check out inference code from GitHub
!wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite
!git clone https://github.com/tensorflow/examples.git
pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')
sys.path.append(pose_sample_rpi_path)

# Load MoveNet Thunder model
import utils
from data import BodyPart
from ml import Movenet
movenet = Movenet('movenet_thunder')

# Define function to run pose estimation using MoveNet Thunder.
# You'll apply MoveNet's cropping algorithm and run inference multiple times on
# the input image to improve pose estimation accuracy.
def detect(input_tensor, inference_count=3):
  """Runs detection on an input image.
 
  Args:
    input_tensor: A [height, width, 3] Tensor of type tf.float32.
      Note that height and width can be anything since the image will be
      immediately resized according to the needs of the model within this
      function.
    inference_count: Number of times the model should run repeatly on the
      same input image to improve detection accuracy.
 
  Returns:
    A Person entity detected by the MoveNet.SinglePose.
  """
  image_height, image_width, channel = input_tensor.shape
 
  # Detect pose using the full input image
  movenet.detect(input_tensor.numpy(), reset_crop_region=True)
 
  # Repeatedly using previous detection result to identify the region of
  # interest and only croping that region to improve detection accuracy
  for _ in range(inference_count - 1):
    person = movenet.detect(input_tensor.numpy(), 
                            reset_crop_region=False)

  return person

#@title Functions to visualize the pose estimation results.

def draw_prediction_on_image(
    image, person, crop_region=None, close_figure=True,
    keep_input_size=False):
  """Draws the keypoint predictions on image.
 
  Args:
    image: An numpy array with shape [height, width, channel] representing the
      pixel values of the input image.
    person: A person entity returned from the MoveNet.SinglePose model.
    close_figure: Whether to close the plt figure after the function returns.
    keep_input_size: Whether to keep the size of the input image.
 
  Returns:
    An numpy array with shape [out_height, out_width, channel] representing the
    image overlaid with keypoint predictions.
  """
  # Draw the detection result on top of the image.
  image_np = utils.visualize(image, [person])
  
  # Plot the image with detection results.
  height, width, channel = image.shape
  aspect_ratio = float(width) / height
  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))
  im = ax.imshow(image_np)
 
  if close_figure:
    plt.close(fig)
 
  if not keep_input_size:
    image_np = utils.keep_aspect_ratio_resizer(image_np, (512, 512))

  return image_np

#@title Code to load the images, detect pose landmarks and save them into a CSV file

class MoveNetPreprocessor(object):
  """Helper class to preprocess pose sample images for classification."""
 
  def __init__(self,
               images_in_folder,
               images_out_folder,
               csvs_out_path):
    """Creates a preprocessor to detection pose from images and save as CSV.

    Args:
      images_in_folder: Path to the folder with the input images. It should
        follow this structure:
        yoga_poses
        |__ downdog
            |______ 00000128.jpg
            |______ 00000181.bmp
            |______ ...
        |__ goddess
            |______ 00000243.jpg
            |______ 00000306.jpg
            |______ ...
        ...
      images_out_folder: Path to write the images overlay with detected
        landmarks. These images are useful when you need to debug accuracy
        issues.
      csvs_out_path: Path to write the CSV containing the detected landmark
        coordinates and label of each image that can be used to train a pose
        classification model.
    """
    self._images_in_folder = images_in_folder
    self._images_out_folder = images_out_folder
    self._csvs_out_path = csvs_out_path
    self._messages = []

    # Create a temp dir to store the pose CSVs per class
    self._csvs_out_folder_per_class = tempfile.mkdtemp()
 
    # Get list of pose classes and print image statistics
    self._pose_class_names = sorted(
        [n for n in os.listdir(self._images_in_folder) if not n.startswith('.')]
        )
    
  def process(self, per_pose_class_limit=None, detection_threshold=0.1):
    """Preprocesses images in the given folder.
    Args:
      per_pose_class_limit: Number of images to load. As preprocessing usually
        takes time, this parameter can be specified to make the reduce of the
        dataset for testing.
      detection_threshold: Only keep images with all landmark confidence score
        above this threshold.
    """
    # Loop through the classes and preprocess its images
    for pose_class_name in self._pose_class_names:
      print('Preprocessing', pose_class_name, file=sys.stderr)

      # Paths for the pose class.
      images_in_folder = os.path.join(self._images_in_folder, pose_class_name)
      images_out_folder = os.path.join(self._images_out_folder, pose_class_name)
      csv_out_path = os.path.join(self._csvs_out_folder_per_class,
                                  pose_class_name + '.csv')
      if not os.path.exists(images_out_folder):
        os.makedirs(images_out_folder)
 
      # Detect landmarks in each image and write it to a CSV file
      with open(csv_out_path, 'w') as csv_out_file:
        csv_out_writer = csv.writer(csv_out_file, 
                                    delimiter=',', 
                                    quoting=csv.QUOTE_MINIMAL)
        # Get list of images
        image_names = sorted(
            [n for n in os.listdir(images_in_folder) if not n.startswith('.')])
        if per_pose_class_limit is not None:
          image_names = image_names[:per_pose_class_limit]

        valid_image_count = 0
 
        # Detect pose landmarks from each image
        for image_name in tqdm.tqdm(image_names):
          image_path = os.path.join(images_in_folder, image_name)

          try:
            image = tf.io.read_file(image_path)
            image = tf.io.decode_jpeg(image)
          except:
            self._messages.append('Skipped ' + image_path + '. Invalid image.')
            continue
          else:
            image = tf.io.read_file(image_path)
            image = tf.io.decode_jpeg(image)
            image_height, image_width, channel = image.shape
          
          # Skip images that isn't RGB because Movenet requires RGB images
          if channel != 3:
            self._messages.append('Skipped ' + image_path +
                                  '. Image isn\'t in RGB format.')
            continue
          person = detect(image)
          
          # Save landmarks if all landmarks were detected
          min_landmark_score = min(
              [keypoint.score for keypoint in person.keypoints])
          should_keep_image = min_landmark_score >= detection_threshold
          if not should_keep_image:
            self._messages.append('Skipped ' + image_path +
                                  '. No pose was confidentlly detected.')
            continue

          valid_image_count += 1

          # Draw the prediction result on top of the image for debugging later
          output_overlay = draw_prediction_on_image(
              image.numpy().astype(np.uint8), person, 
              close_figure=True, keep_input_size=True)
        
          # Write detection result into an image file
          output_frame = cv2.cvtColor(output_overlay, cv2.COLOR_RGB2BGR)
          cv2.imwrite(os.path.join(images_out_folder, image_name), output_frame)
        
          # Get landmarks and scale it to the same size as the input image
          pose_landmarks = np.array(
              [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score]
                for keypoint in person.keypoints],
              dtype=np.float32)

          # Write the landmark coordinates to its per-class CSV file
          coordinates = pose_landmarks.flatten().astype(np.str).tolist()
          csv_out_writer.writerow([image_name] + coordinates)

        if not valid_image_count:
          raise RuntimeError(
              'No valid images found for the "{}" class.'
              .format(pose_class_name))
      
    # Print the error message collected during preprocessing.
    print('\n'.join(self._messages))

    # Combine all per-class CSVs into a single output file
    all_landmarks_df = self._all_landmarks_as_dataframe()
    all_landmarks_df.to_csv(self._csvs_out_path, index=False)

  def class_names(self):
    """List of classes found in the training dataset."""
    return self._pose_class_names
  
  def _all_landmarks_as_dataframe(self):
    """Merge all per-class CSVs into a single dataframe."""
    total_df = None
    for class_index, class_name in enumerate(self._pose_class_names):
      csv_out_path = os.path.join(self._csvs_out_folder_per_class,
                                  class_name + '.csv')
      per_class_df = pd.read_csv(csv_out_path, header=None)
      
      # Add the labels
      per_class_df['class_no'] = [class_index]*len(per_class_df)
      per_class_df['class_name'] = [class_name]*len(per_class_df)

      # Append the folder name to the filename column (first column)
      per_class_df[per_class_df.columns[0]] = (os.path.join(class_name, '') 
        + per_class_df[per_class_df.columns[0]].astype(str))

      if total_df is None:
        # For the first class, assign its data to the total dataframe
        total_df = per_class_df
      else:
        # Concatenate each class's data into the total dataframe
        total_df = pd.concat([total_df, per_class_df], axis=0)
 
    list_name = [[bodypart.name + '_x', bodypart.name + '_y', 
                  bodypart.name + '_score'] for bodypart in BodyPart] 
    header_name = []
    for columns_name in list_name:
      header_name += columns_name
    header_name = ['file_name'] + header_name
    header_map = {total_df.columns[i]: header_name[i] 
                  for i in range(len(header_name))}
 
    total_df.rename(header_map, axis=1, inplace=True)

    return total_df

#@title (Optional) Code snippet to try out the Movenet pose estimation logic

#@markdown You can download an image from the internet, run the pose estimation logic on it and plot the detected landmarks on top of the input image. 

#@markdown *Note: This code snippet is also useful for debugging when you encounter an image with bad pose classification accuracy. You can run pose estimation on the image and see if the detected landmarks look correct or not before investigating the pose classification logic.*

test_image_url = "https://cdn.pixabay.com/photo/2017/03/03/17/30/yoga-2114512_960_720.jpg" #@param {type:"string"}
!wget -O /tmp/image.jpeg {test_image_url}

if len(test_image_url):
  image = tf.io.read_file('/tmp/image.jpeg')
  image = tf.io.decode_jpeg(image)
  person = detect(image)
  _ = draw_prediction_on_image(image.numpy(), person, crop_region=None, 
                               close_figure=False, keep_input_size=True)

is_skip_step_1 = False #@param ["False", "True"] {type:"raw"}

use_custom_dataset = False #@param ["False", "True"] {type:"raw"}

dataset_is_split = False #@param ["False", "True"] {type:"raw"}

#@markdown Be sure you run this cell. It's hiding the `split_into_train_test()` function that's called in the next code block.

import os
import random
import shutil

def split_into_train_test(images_origin, images_dest, test_split):
  """Splits a directory of sorted images into training and test sets.

  Args:
    images_origin: Path to the directory with your images. This directory
      must include subdirectories for each of your labeled classes. For example:
      yoga_poses/
      |__ downdog/
          |______ 00000128.jpg
          |______ 00000181.jpg
          |______ ...
      |__ goddess/
          |______ 00000243.jpg
          |______ 00000306.jpg
          |______ ...
      ...
    images_dest: Path to a directory where you want the split dataset to be
      saved. The results looks like this:
      split_yoga_poses/
      |__ train/
          |__ downdog/
              |______ 00000128.jpg
              |______ ...
      |__ test/
          |__ downdog/
              |______ 00000181.jpg
              |______ ...
    test_split: Fraction of data to reserve for test (float between 0 and 1).
  """
  _, dirs, _ = next(os.walk(images_origin))

  TRAIN_DIR = os.path.join(images_dest, 'train')
  TEST_DIR = os.path.join(images_dest, 'test')
  os.makedirs(TRAIN_DIR, exist_ok=True)
  os.makedirs(TEST_DIR, exist_ok=True)

  for dir in dirs:
    # Get all filenames for this dir, filtered by filetype
    filenames = os.listdir(os.path.join(images_origin, dir))
    filenames = [os.path.join(images_origin, dir, f) for f in filenames if (
        f.endswith('.png') or f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.bmp'))]
    # Shuffle the files, deterministically
    filenames.sort()
    random.seed(42)
    random.shuffle(filenames)
    # Divide them into train/test dirs
    os.makedirs(os.path.join(TEST_DIR, dir), exist_ok=True)
    os.makedirs(os.path.join(TRAIN_DIR, dir), exist_ok=True)
    test_count = int(len(filenames) * test_split)
    for i, file in enumerate(filenames):
      if i < test_count:
        destination = os.path.join(TEST_DIR, dir, os.path.split(file)[1])
      else:
        destination = os.path.join(TRAIN_DIR, dir, os.path.split(file)[1])
      shutil.copyfile(file, destination)
    print(f'Moved {test_count} of {len(filenames)} from class "{dir}" into test.')
  print(f'Your split dataset is in "{images_dest}"')

if use_custom_dataset:
  # ATTENTION:
  # You must edit these two lines to match your archive and images folder name:
  # !tar -xf YOUR_DATASET_ARCHIVE_NAME.tar
  !unzip -q YOUR_DATASET_ARCHIVE_NAME.zip
  dataset_in = 'YOUR_DATASET_DIR_NAME'

  # You can leave the rest alone:
  if not os.path.isdir(dataset_in):
    raise Exception("dataset_in is not a valid directory")
  if dataset_is_split:
    IMAGES_ROOT = dataset_in
  else:
    dataset_out = 'split_' + dataset_in
    split_into_train_test(dataset_in, dataset_out, test_split=0.2)
    IMAGES_ROOT = dataset_out

if not is_skip_step_1 and not use_custom_dataset:
  !wget -O yoga_poses.zip http://download.tensorflow.org/data/pose_classification/yoga_poses.zip
  !unzip -q yoga_poses.zip -d yoga_cg
  IMAGES_ROOT = "yoga_cg"

if not is_skip_step_1:
  images_in_train_folder = os.path.join(IMAGES_ROOT, 'train')
  images_out_train_folder = 'poses_images_out_train'
  csvs_out_train_path = 'train_data.csv'

  preprocessor = MoveNetPreprocessor(
      images_in_folder=images_in_train_folder,
      images_out_folder=images_out_train_folder,
      csvs_out_path=csvs_out_train_path,
  )

  preprocessor.process(per_pose_class_limit=None)

if not is_skip_step_1:
  images_in_test_folder = os.path.join(IMAGES_ROOT, 'test')
  images_out_test_folder = 'poses_images_out_test'
  csvs_out_test_path = 'test_data.csv'

  preprocessor = MoveNetPreprocessor(
      images_in_folder=images_in_test_folder,
      images_out_folder=images_out_test_folder,
      csvs_out_path=csvs_out_test_path,
  )

  preprocessor.process(per_pose_class_limit=None)

# Download the preprocessed CSV files which are the same as the output of step 1
if is_skip_step_1:
  !wget -O train_data.csv http://download.tensorflow.org/data/pose_classification/yoga_train_data.csv
  !wget -O test_data.csv http://download.tensorflow.org/data/pose_classification/yoga_test_data.csv

  csvs_out_train_path = 'train_data.csv'
  csvs_out_test_path = 'test_data.csv'
  is_skipped_step_1 = True

def load_pose_landmarks(csv_path):
  """Loads a CSV created by MoveNetPreprocessor.
  
  Returns:
    X: Detected landmark coordinates and scores of shape (N, 17 * 3)
    y: Ground truth labels of shape (N, label_count)
    classes: The list of all class names found in the dataset
    dataframe: The CSV loaded as a Pandas dataframe features (X) and ground
      truth labels (y) to use later to train a pose classification model.
  """

  # Load the CSV file
  dataframe = pd.read_csv(csv_path)
  df_to_process = dataframe.copy()

  # Drop the file_name columns as you don't need it during training.
  df_to_process.drop(columns=['file_name'], inplace=True)

  # Extract the list of class names
  classes = df_to_process.pop('class_name').unique()

  # Extract the labels
  y = df_to_process.pop('class_no')

  # Convert the input features and labels into the correct format for training.
  X = df_to_process.astype('float64')
  y = keras.utils.to_categorical(y)

  return X, y, classes, dataframe

# Load the train data
X, y, class_names, _ = load_pose_landmarks(csvs_out_train_path)

# Split training data (X, y) into (X_train, y_train) and (X_val, y_val)
X_train, X_val, y_train, y_val = train_test_split(X, y,
                                                  test_size=0.15)

# Load the test data
X_test, y_test, _, df_test = load_pose_landmarks(csvs_out_test_path)

def get_center_point(landmarks, left_bodypart, right_bodypart):
  """Calculates the center point of the two given landmarks."""

  left = tf.gather(landmarks, left_bodypart.value, axis=1)
  right = tf.gather(landmarks, right_bodypart.value, axis=1)
  center = left * 0.5 + right * 0.5
  return center


def get_pose_size(landmarks, torso_size_multiplier=2.5):
  """Calculates pose size.

  It is the maximum of two values:
    * Torso size multiplied by `torso_size_multiplier`
    * Maximum distance from pose center to any pose landmark
  """
  # Hips center
  hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, 
                                 BodyPart.RIGHT_HIP)

  # Shoulders center
  shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,
                                      BodyPart.RIGHT_SHOULDER)

  # Torso size as the minimum body size
  torso_size = tf.linalg.norm(shoulders_center - hips_center)

  # Pose center
  pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, 
                                     BodyPart.RIGHT_HIP)
  pose_center_new = tf.expand_dims(pose_center_new, axis=1)
  # Broadcast the pose center to the same size as the landmark vector to
  # perform substraction
  pose_center_new = tf.broadcast_to(pose_center_new,
                                    [tf.size(landmarks) // (17*2), 17, 2])

  # Dist to pose center
  d = tf.gather(landmarks - pose_center_new, 0, axis=0,
                name="dist_to_pose_center")
  # Max dist to pose center
  max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))

  # Normalize scale
  pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)

  return pose_size


def normalize_pose_landmarks(landmarks):
  """Normalizes the landmarks translation by moving the pose center to (0,0) and
  scaling it to a constant pose size.
  """
  # Move landmarks so that the pose center becomes (0,0)
  pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, 
                                 BodyPart.RIGHT_HIP)
  pose_center = tf.expand_dims(pose_center, axis=1)
  # Broadcast the pose center to the same size as the landmark vector to perform
  # substraction
  pose_center = tf.broadcast_to(pose_center, 
                                [tf.size(landmarks) // (17*2), 17, 2])
  landmarks = landmarks - pose_center

  # Scale the landmarks to a constant pose size
  pose_size = get_pose_size(landmarks)
  landmarks /= pose_size

  return landmarks


def landmarks_to_embedding(landmarks_and_scores):
  """Converts the input landmarks into a pose embedding."""
  # Reshape the flat input into a matrix with shape=(17, 3)
  reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)

  # Normalize landmarks 2D
  landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])

  # Flatten the normalized landmark coordinates into a vector
  embedding = keras.layers.Flatten()(landmarks)

  return embedding

# Define the model
inputs = tf.keras.Input(shape=(51))
embedding = landmarks_to_embedding(inputs)

layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)
layer = keras.layers.Dropout(0.5)(layer)
layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)
layer = keras.layers.Dropout(0.5)(layer)
outputs = keras.layers.Dense(len(class_names), activation="softmax")(layer)

model = keras.Model(inputs, outputs)
model.summary()

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Add a checkpoint callback to store the checkpoint that has the highest
# validation accuracy.
checkpoint_path = "weights.best.hdf5"
checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,
                             monitor='val_accuracy',
                             verbose=1,
                             save_best_only=True,
                             mode='max')
earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', 
                                              patience=20)

# Start training
history = model.fit(X_train, y_train,
                    epochs=200,
                    batch_size=16,
                    validation_data=(X_val, y_val),
                    callbacks=[checkpoint, earlystopping])

# Visualize the training history to see whether you're overfitting.
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['TRAIN', 'VAL'], loc='lower right')
plt.show()

# Evaluate the model using the TEST dataset
loss, accuracy = model.evaluate(X_test, y_test)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """Plots the confusion matrix."""
  if normalize:
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print("Normalized confusion matrix")
  else:
    print('Confusion matrix, without normalization')

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=55)
  plt.yticks(tick_marks, classes)
  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, format(cm[i, j], fmt),
              horizontalalignment="center",
              color="white" if cm[i, j] > thresh else "black")

  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.tight_layout()

# Classify pose in the TEST dataset using the trained model
y_pred = model.predict(X_test)

# Convert the prediction result to class name
y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]
y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]

# Plot the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))
plot_confusion_matrix(cm,
                      class_names,
                      title ='Confusion Matrix of Pose Classification Model')

# Print the classification report
print('\nClassification Report:\n', classification_report(y_true_label,
                                                          y_pred_label))

if is_skip_step_1:
  raise RuntimeError('You must have run step 1 to run this cell.')

# If step 1 was skipped, skip this step.
IMAGE_PER_ROW = 3
MAX_NO_OF_IMAGE_TO_PLOT = 30

# Extract the list of incorrectly predicted poses
false_predict = [id_in_df for id_in_df in range(len(y_test)) \
                if y_pred_label[id_in_df] != y_true_label[id_in_df]]
if len(false_predict) > MAX_NO_OF_IMAGE_TO_PLOT:
  false_predict = false_predict[:MAX_NO_OF_IMAGE_TO_PLOT]

# Plot the incorrectly predicted images
row_count = len(false_predict) // IMAGE_PER_ROW + 1
fig = plt.figure(figsize=(10 * IMAGE_PER_ROW, 10 * row_count))
for i, id_in_df in enumerate(false_predict):
  ax = fig.add_subplot(row_count, IMAGE_PER_ROW, i + 1)
  image_path = os.path.join(images_out_test_folder,
                            df_test.iloc[id_in_df]['file_name'])

  image = cv2.imread(image_path)
  plt.title("Predict: %s; Actual: %s"
            % (y_pred_label[id_in_df], y_true_label[id_in_df]))
  plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.show()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

print('Model size: %dKB' % (len(tflite_model) / 1024))

with open('pose_classifier.tflite', 'wb') as f:
  f.write(tflite_model)

with open('pose_labels.txt', 'w') as f:
  f.write('\n'.join(class_names))

def evaluate_model(interpreter, X, y_true):
  """Evaluates the given TFLite model and return its accuracy."""
  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]

  # Run predictions on all given poses.
  y_pred = []
  for i in range(len(y_true)):
    # Pre-processing: add batch dimension and convert to float32 to match with
    # the model's input data format.
    test_image = X[i: i + 1].astype('float32')
    interpreter.set_tensor(input_index, test_image)

    # Run inference.
    interpreter.invoke()

    # Post-processing: remove batch dimension and find the class with highest
    # probability.
    output = interpreter.tensor(output_index)
    predicted_label = np.argmax(output()[0])
    y_pred.append(predicted_label)

  # Compare prediction results with ground truth labels to calculate accuracy.
  y_pred = keras.utils.to_categorical(y_pred)
  return accuracy_score(y_true, y_pred)

# Evaluate the accuracy of the converted TFLite model
classifier_interpreter = tf.lite.Interpreter(model_content=tflite_model)
classifier_interpreter.allocate_tensors()
print('Accuracy of TFLite model: %s' %
      evaluate_model(classifier_interpreter, X_test, y_test))

!zip pose_classifier.zip pose_labels.txt pose_classifier.tflite

# Download the zip archive if running on Colab.
try:
  from google.colab import files
  files.download('pose_classifier.zip')
except:
  pass

import numpy
numpy.version.version
#'1.16.6' bakup-clonebase

#%%writefile tweetout
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR = randTXT()
Hash = ["#twitme #Python #100DaysOfCode\n","#Python #100DaysOfCode #PythonBots #codefor30days\n" ]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum]
# add the hash to STR generated with randTXT()
STR = hashs+STR
STR= STR[:225]
print(STR)
# Open background image and work out centre
x = 720//2
y = 480//2

# The text we want to add
#text = "NFT TwitterBot Project"
#text = hashs
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 12)

#nap=randint(10,400)
#time.sleep(nap)
path = r"/home/jack/Desktop/Imagedata/4-publish-images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename1=(path+base_image)

bg = Image.open(filename1).convert('RGB')
x = bg.width//2
y = bg.height//2
src_path = filename1
dst_path = "/home/jack/Desktop/Imagedata/Used_Images/"
shutil.move(src_path, dst_path)
# The text we want to add
#text = "NFT TwitterBot Project"
#text=STR[:249]
text = hashs
print("len(text): ",len(text))
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 12)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
CH = randint(0,1)
if CH == 0:COLor = ["white","black"]
elif CH == 1:COLor = ["black","white"]  
draw.text(xy=(x-20,y+170), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x-21,y+171), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x-19,y+169), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x-20,y+170), text=text, fill=COLor[0], font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(2))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x-20,y+170), text=text, fill=COLor[1], font=font, anchor='mm')
postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
frames =["lined-frame.png","black-blur-frame.png", "white-blur-frame.png"]
Num = randint( 0, len(frames)-1)
BOARDER = postage[Num]
mask=Image.open(BOARDER).convert('RGBA') 
bg.paste(mask, (0,0), mask=mask)
bg.save('result.png')
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "result.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])

from PIL import Image
im = Image.open(PATH)
im

from OutlineImage import outlineP
filename1 = "result.png" 
outfile_png = "resultXX.png" 
outlineP(filename1,outfile_png)

photo = open(outfile_png,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

!ls 

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
# Open background image and work out centre

path = r"2-resource_images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)


bg = Image.open(filename0).convert('RGB')
bg =bg.resize((720,480),Image.ANTIALIAS)
x = bg.width//2
y = bg.height//2

# The text we want to add
text = "NFT TwitterBot Project"

# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
#xx=0
#yy=0
#bg.paste("peferations.png", (xx,yy)) 
# paste an onerlay image
#perferations.png
#mask=Image.open("perferations.png").convert('RGBA') 
#bg.paste(mask, (x,y), mask=mask)
#bg.paste("peferations.png", box=(0, 0) + original.size) 
bg.save('result.png')
timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "records/"+timestr+"_.png"
bg.save(filename)
im =Image.open('result.png')
im



#nap=randint(10,400)
#time.sleep(nap)
path = r"3-resource_images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)

bg = Image.open(filename0).convert('RGB')
x = bg.width//2
y = bg.height//2

# The text we want to add
text = "NFT TwitterBot Project"


# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
mask=Image.open("perferations.png").convert('RGBA') 
bg.paste(mask, (0,0), mask=mask)
bg.save('result.png')
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
with open("sart.txt") as f:
    data = f.read()
data_model = markovify.Text(data)
STR = data_model.make_sentence()
#STR = ("#All_in_One - #WordCloud #Create - Added ability to randomly choose an image background  #Automated")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "/home/jack/Desktop/dockercommands/newtest/NEwtest2.png"
#PATH = "result.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


from PIL import Image
im = Image.open("result.png")
im

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
with open("sart.txt") as f:
    data = f.read()
data_model = markovify.Text(data)
STR = data_model.make_sentence()

PATH = "result7.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


Hash = ["#twitme #Python #100DaysOfCode\n","#Python #100DaysOfCode #PythonBots #codefor30days\n" ]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum]
hashs

tweet="UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUuu'place': None, 'contributors': None,\
'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorite_count': 0, \
'favorited': False0, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'en'}"
Text = (tweet[:240])

print("len(Text): ",len(Text),"\n",Text)

st

import numpy
numpy.version.version
#'1.16.6' bakup-clonebase

import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR = randTXT()
Hash = ["#programming #php #nodejs #javascript","#twitme #Python #100DaysOfCode\n","#Python #100DaysOfCode #PythonBots #codefor30days\n" ]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum]
# add the hash to STR generated with randTXT()
STR = hashs+STR
STR= STR[:225]
print(STR)
# Open background image and work out centre
x = 720//2
y = 480//2

# The text we want to add
#text = "NFT TwitterBot Project"
#text = hashs
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 12)

#nap=randint(10,400)
#time.sleep(nap)
path = r"/home/jack/Desktop/Imagedata/4-publish-images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename1=(path+base_image)
#filename1="/home/jack/Desktop/dockercommands/preped_images/Style_00332.jpg"


bg = Image.open(filename1).convert('RGB')
x = bg.width//2
y = bg.height//2
src_path = filename1
dst_path = "/home/jack/Desktop/Imagedata/Used_Images/"
shutil.move(src_path, dst_path)
# The text we want to add
#text = "NFT TwitterBot Project"
#text=STR[:249]
text = hashs
print("len(text): ",len(text))
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 14)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)

CH = randint(0,1)
if CH == 0:COLor = ["white","black"]
elif CH == 1:COLor = ["black","white"]  
draw.text(xy=(x+15,y+222), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+9,y+224), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+15,y+225), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+9,y+226), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x+15,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
#blurred = blurred.filter(ImageFilter.BoxBlur(2))
blurred = blurred.filter(ImageFilter.GaussianBlur(radius=2))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x+10,y+225), text=text, fill=COLor[1], font=font, anchor='mm')
#postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
frames =["wood-blur-frame.png","frames.png","lined-frame.png","black-blur-frame.png", "white-blur-frame.png","beige-blur-frame.png","frame-lite.png"]
Num = randint( 0, len(frames)-1)
BOARDER = frames[Num]
mask=Image.open(BOARDER).convert('RGBA') 
bg.paste(mask, (0,0), mask=mask)
bg.save('result.png')
#removerotisory d keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "result.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

#photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])

from OutlineImage import outlineP
filename1 = "result.png" 
outfile_png = "resultXX.png" 
outlineP(filename1,outfile_png)



from PIL import Image
im = Image.open(PATH)
im

from OutlineImage import outlineP
filename1 = "result.png" 
outfile_png = "resultXX.png" 
outlineP(filename1,outfile_png)

photo = open(outfile_png,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

!ls 

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
# Open background image and work out centre

path = r"2-resource_images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)


bg = Image.open(filename0).convert('RGB')
bg =bg.resize((720,480),Image.ANTIALIAS)
x = bg.width//2
y = bg.height//2

# The text we want to add
text = "NFT TwitterBot Project"

# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
#xx=0
#yy=0
#bg.paste("peferations.png", (xx,yy)) 
# paste an onerlay image
#perferations.png
#mask=Image.open("perferations.png").convert('RGBA') 
#bg.paste(mask, (x,y), mask=mask)
#bg.paste("peferations.png", box=(0, 0) + original.size) 
bg.save('result.png')
timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "records/"+timestr+"_.png"
bg.save(filename)
im =Image.open('result.png')
im



#nap=randint(10,400)
#time.sleep(nap)
path = r"3-resource_images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)

bg = Image.open(filename0).convert('RGB')
x = bg.width//2
y = bg.height//2

# The text we want to add
text = "NFT TwitterBot Project"


# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
mask=Image.open("perferations.png").convert('RGBA') 
bg.paste(mask, (0,0), mask=mask)
bg.save('result.png')
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
with open("sart.txt") as f:
    data = f.read()
data_model = markovify.Text(data)
STR = data_model.make_sentence()
#STR = ("#All_in_One - #WordCloud #Create - Added ability to randomly choose an image background  #Automated")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "/home/jack/Desktop/dockercommands/newtest/NEwtest2.png"
#PATH = "result.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


from PIL import Image
im = Image.open("result.png")
im

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
with open("sart.txt") as f:
    data = f.read()
data_model = markovify.Text(data)
STR = data_model.make_sentence()

PATH = "result7.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


Hash = ["#twitme #Python #100DaysOfCode\n","#Python #100DaysOfCode #PythonBots #codefor30days\n" ]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum]
hashs

tweet="UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUuu'place': None, 'contributors': None,\
'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorite_count': 0, \
'favorited': False0, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'en'}"
Text = (tweet[:240])

print("len(Text): ",len(Text),"\n",Text)

st

%%writefile tweetout
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR = randTXT()
Hash = ["#twitme #Python #100DaysOfCode\n","#Python #100DaysOfCode #PythonBots #codefor30days\n" ]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum]
# add the hash to STR generated with randTXT()
STR = hashs+STR
STR= STR[:225]
print(STR)
# Open background image and work out centre
x = 720//2
y = 480//2

# The text we want to add
#text = "NFT TwitterBot Project"
#text = hashs
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 12)

#nap=randint(10,400)
#time.sleep(nap)
path = r"/home/jack/Desktop/Imagedata/4-publish-images/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename1=(path+base_image)

bg = Image.open(filename1).convert('RGB')
x = bg.width//2
y = bg.height//2
src_path = filename1
dst_path = "/home/jack/Desktop/Imagedata/Used_Images/"
shutil.move(src_path, dst_path)
# The text we want to add
#text = "NFT TwitterBot Project"
#text=STR[:249]
text = hashs
print("len(text): ",len(text))
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 12)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
CH = randint(0,1)
if CH == 0:COLor = ["white","black"]
elif CH == 1:COLor = ["black","white"]  
draw.text(xy=(x-20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x-21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x-19,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
draw.text(xy=(x-20,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(2))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x-20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')
#postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
frames =["wood-blur-frame.png","frames.png","lined-frame.png","black-blur-frame.png", "white-blur-frame.png","beige-blur-frame.png","frame-lite.png"]
Num = randint( 0, len(frames)-1)
BOARDER = frames[Num]
mask=Image.open(BOARDER).convert('RGBA') 
bg.paste(mask, (0,0), mask=mask)
bg.save('result.png')
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "result.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

#photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])

from OutlineImage import outlineP
filename1 = "result.png" 
outfile_png = "resultXX.png" 
outlineP(filename1,outfile_png)

photo = open(outfile_png,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

!curl -F 'name=cloud-colored-image' -F 'file=@/home/jack/Desktop/LBRY-cli/videos-complete/junk1/cloud-colored-image.png' https://spee.ch/api/claim/publish

import urllib, json
import requests
requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": "cloud-colored-image","bid": "0.15", "file_path": "/home/jack/Desktop/LBRY-cli/VIDEOS/Clouds-Palette-Colors-Sound.mp4", "tags": "art","tags":"digital graphics", "tags":"gimp images", "languages": "en", "locations": "PH", "channel_name":"@MyLinuxToyBox","thumbnail_url":"https://spee.ch/2/cloud-colored-image"}}).json()


"claim_id": "f0e1ccae16cc1160cbfb028034d5176c98b3dd16",  MyLinuxToyBox

!curl -F 'name=Palette-Swapped-Image' -F 'file=@/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/Pallet-swapped/07a-Thum.jpg' https://spee.ch/api/claim/publish
----------------------------------------------------------------------------------------------------------------
{"success":true,"message":"publish completed successfully","data":{"name":"Palette-Swapped-Image","claimId":"e7c2e060400eebddbfca70c67e44ee776078c0ef","url":"https://spee.ch/e/Palette-Swapped-Image","showUrl":"https://spee.ch/e/Palette-Swapped-Image","serveUrl":"https://spee.ch/e/Palette-Swapped-Image.jpg","pushTo":"/e/Palette-Swapped-Image","claimData":{"name":"Palette-Swapped-Image","claimId":"e7c2e060400eebddbfca70c67e44ee776078c0ef","title":"Palette-Swapped-Image","description":"","address":"bFFR9aqXaamuihY8mowejqHWHgyjktx2nm","outpoint":"483ccccf2e8204e83fea4973ca137684050bbc721ce0669fa3bd2c39110b0af6:0","height":475584,"contentType":"image/jpeg","amount":"0.01","certificateId":null,"channelName":null}}}

import urllib, json
import requests
requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": "Palette-Swapped-Image","bid": "0.15", "file_path": "/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/Pallet-swapped/07a.jpg", "tags": "art_digital graphics_gimp images", "languages": "en", "locations": "PH", "channel_account_id": "@MyLinuxToyBox","thumbnail_url":"https://spee.ch/e/Palette-Swapped-Image.jpg"}}).json()

import urllib, json
import requests
IMAGE = "/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/ART-blended/blended20190407203210.jpg"
requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": "First Bot Post","bid": "0.15", "file_path": IMAGE, "tags": "art,digital graphics,images", "languages": "en", "locations": "PH", "channel_account_id": "@MyLinuxToyBox","thumbnail_url":"https://spee.ch/c/blended-thumb"}}).json()

import urllib, json
import requests
IMAGE = "/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/ART-blended/blended20190407203210.jpg"
requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": "FirstBotPost001","bid": ".5", "file_path": IMAGE, "tags": "art,digital graphics,images", "languages": "en", "locations": "PH", "channel_account_id": "@MyLinuxToyBox","funding_account_ids": ,"thumbnail_url":"https://spee.ch/c/blended-thumb"}}).json()

import urllib, json
import requests
requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": "Blended Images", "bid": "0.2", "file_path": "/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/ART-blended/blended20190407203210.jpg", "tags": "graphic art image", "languages": "en", "locations": "PH","title":"Blended Image" ,"channel_account_id": "@MyLinuxToyBox","thumbnail_url":"https://spee.ch/c/blended-thumb","preview": "false", "blocking": "false"}}).json()

import urllib, json
import requests
requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": "Blended Images", "bid": "0.2", "file_path": "/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/ART-blended/blended20190407203210.jpg", "tags": "art, test post, graphic image ", "languages": "en", "locations": "PH","title":"Blended Image" ,"channel_account_id": "@MyLinuxToyBox","thumbnail_url":"https://spee.ch/c/blended-thumb", "funding_account_ids": "PhilippineRetirement", "preview": "false", "blocking": "false"}}).json()

/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/ART-blended/blended20190407203210.gif
https://spee.ch/c/blended-thumb
<img alt="blended-thumb" src="https://spee.ch/c/blended-thumb.gif" />
blended-thumb#cb0fee9808bca38d405482c671330df754ea4822
https://open.lbry.com/blended-thumb#cb0fee9808bca38d405482c671330df754ea4822
    
https://open.lbry.com/@MyLinuxToyBox#f
MyLinuxToyBox
@MyLinuxToyBox

@elcer

!lbrynet publish --name=Posted CLI image --bid=0.2 --file_path=/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/ART-blended/blended20190407203210.jpg --description=Testing the description out --title=IMAGE-TEST-OO2 --thumbnail_url=https://spee.ch/c/blended-thumb --tags=art videos graphics --channel_account_id=@DigitalArt

from hiddenID import funding
ss=funding()
print ss

!lbrynet publish --name=Test-Image --bid=0.5 --file_path=/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/Images/ART-blended/blended20190407203210.jpg --title=IMAGE-TEST-two --channel_account_id=@DigitalArt

!lbrynet resolve Test-Image


    name
    str
    name of the content (can only consist of a-z A-Z 0-9 and -(dash))
    bid
    optionaldecimal
    amount to back the claim
    file_path
    optionalstr
    path to file to be associated with name.
    fee_currency
    optionalstring
    specify fee currency
    fee_amount
    optionaldecimal
    content download fee
    fee_address
    optionalstr
    address where to send fee payments, will use value from --claim_address if not provided
    title
    optionalstr
    title of the publication
    description
    optionalstr
    description of the publication
    author
    optionalstr
    author of the publication. The usage for this field is not the same as for channels. The author field is used to credit an author who is not the publisher and is not represented by the channel. For example, a pdf file of 'The Odyssey' has an author of 'Homer' but may by published to a channel such as '@classics', or to no channel at all
    tags
    optionallist
    add content tags
    languages
    optionallist
    languages used by the channel, using RFC 5646 format, eg: for English `--languages=en` for Spanish (Spain) `--languages=es-ES` for Spanish (Mexican) `--languages=es-MX` for Chinese (Simplified) `--languages=zh-Hans` for Chinese (Traditional) `--languages=zh-Hant`
    locations
    optionallist
    locations relevant to the stream, consisting of 2 letter `country` code and a `state`, `city` and a postal `code` along with a `latitude` and `longitude`. for JSON RPC: pass a dictionary with aforementioned attributes as keys, eg: ... "locations": [{'country': 'US', 'state': 'NH'}] ... for command line: pass a colon delimited list with values in the following order: "COUNTRY:STATE:CITY:CODE:LATITUDE:LONGITUDE" making sure to include colon for blank values, for example to provide only the city: ... --locations="::Manchester" with all values set: ... --locations="US:NH:Manchester:03101:42.990605:-71.460989" optionally, you can just pass the "LATITUDE:LONGITUDE": ... --locations="42.990605:-71.460989" finally, you can also pass JSON string of dictionary on the command line as you would via JSON RPC ... --locations="{'country': 'US', 'state': 'NH'}"
    license
    optionalstr
    publication license
    license_url
    optionalstr
    publication license url
    thumbnail_url
    optionalstr
    thumbnail url
    release_time
    optionalint
    original public release of content, seconds since UNIX epoch
    width
    optionalint
    image/video width, automatically calculated from media file
    height
    optionalint
    image/video height, automatically calculated from media file
    duration
    optionalint
    audio/video duration in seconds, automatically calculated
    channel_id
    optionalstr
    claim id of the publisher channel
    channel_name
    optionalstr
    name of publisher channel
    channel_account_id
    optionalstr
    one or more account ids for accounts to look in for channel certificates, defaults to all accounts.
    account_id
    optionalstr
    account to use for holding the transaction
    wallet_id
    optionalstr
    restrict operation to specific wallet
    funding_account_ids
    optionallist
    ids of accounts to fund this transaction
    claim_address
    optionalstr
    address where the claim is sent to, if not specified it will be determined automatically from the account
    preview
    optionalbool
    do not broadcast the transaction
    blocking
    optionalbool
    wait until transaction is in mempool


link lbry://blended-thumb#c

(base) jack@server1:~/lighthouse$ lbrynet resolve lbry://blended-thumb#c
{
  "lbry://blended-thumb#c": {
    "address": "bFFR9aqXaamuihY8mowejqHWHgyjktx2nm",
    "amount": "0.01",
    "canonical_url": "lbry://blended-thumb#c",
    "claim_id": "cb0fee9808bca38d405482c671330df754ea4822",
    "claim_op": "create",
    "confirmations": 55,
    "height": 696816,
    "meta": {
      "activation_height": 696816,
      "creation_height": 696816,
      "creation_timestamp": 1578620921,
      "effective_amount": "0.01",
      "expiration_height": 2799216,
      "is_controlling": true,
      "reposted": 0,
      "support_amount": "0.0",
      "take_over_height": 696816,
      "trending_global": 0.0,
      "trending_group": 0,
      "trending_local": 0.0,
      "trending_mixed": 0.0
    },
    "name": "blended-thumb",
    "normalized_name": "blended-thumb",
    "nout": 0,
    "permanent_url": "lbry://blended-thumb#cb0fee9808bca38d405482c671330df754ea4822",
    "short_url": "lbry://blended-thumb#c",
    "timestamp": 1578620921,
    "txid": "64f6d042b9d8f49f3e0640cde9f23a7ea3af860897eccf686639a0baddcb6a2a",
    "type": "claim",
    "value": {
      "author": "Spee.ch",
      "image": {
        "height": 720,
        "width": 1280
      },
      "languages": [
        "en"
      ],
      "source": {
        "hash": "ecf17ea796d7bc09dfc4ec348edb56e8fcbc5f236b049af294cda30bb6dec6d5774054d05232aa87b1ce64e48b586831",
        "media_type": "image/gif",
        "name": "hMP2zHe8APA1xXYus4B_nWR9.gif",
        "sd_hash": "74c46a3a85e91e05b95bd20c67e8d8e1fe77921b1aff95816c6eec2c1ef06a09ae5c8651fe9c20d0cf7e2f0615f614d1",
        "size": "1067432"
      },
      "stream_type": "image",
      "title": "blended-thumb"
    },
    "value_type": "stream"
  }
}


# This is a special path secifically for an env. It was entered in kernel.json
# kernel.json
#{
# "argv": [
#  "/home/jack/miniconda3/envs/deep/bin/python",
#  "-m",
#  "ipykernel_launcher",
#  "-f",
#  "{connection_file}"
# ],
# "display_name": "deep",
# "language": "python",
# "env": {
#          "PYTHONPATH": "/home/jack/hidden"
#        }
#}

import os
os.environ['PYTHONPATH'].split(os.pathsep)

import sys
for p in sys.path:
    print(p)

STR="""
Python and Twitter:
CONSUMER_KEY = APP_KEY 
CONSUMER_SECRET = APP_SECRET
ACCESS_KEY =  OAUTH_TOKEN
ACCESS_SECRET =  OAUTH_TOKEN_SECRET
It is all confusing enough without changing the names
"""

from twython import Twython
from APIkeys import KEYS
CONSUMER_KEY = KEYS()[0]
CONSUMER_SECRET =  KEYS()[1]
ACCESS_KEY =  KEYS()[2]
ACCESS_SECRET =  KEYS()[3]
BEARER_TOKEN  =  KEYS()[4]
APP_KEY = KEYS()[0]
APP_SECRET = KEYS()[1]
OAUTH_TOKEN = KEYS()[2]
OAUTH_TOKEN_SECRET = KEYS()[3]


twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN,OAUTH_TOKEN_SECRET)
twitter.verify_credentials()

!pwd

from APIkeys import KEYS
CONSUMER_KEY = KEYS()[0]
CONSUMER_SECRET =  KEYS()[1]
ACCESS_KEY =  KEYS()[2]
ACCESS_SECRET =  KEYS()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
#f = open("Mine.txt")
#text = f.read()
# Build the model.
#text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
#STR = (text_model.make_short_sentence(140))
#random.choice(open('Mine.txt').readlines())



##file_name = '/home/jack/Desktop/imagebot/Mine.txt'
#STR = get_random_line(file_name)
STR=("""
Python and Twitter:
CONSUMER_KEY = APP_KEY 
CONSUMER_SECRET = APP_SECRET
ACCESS_KEY =  OAUTH_TOKEN
ACCESS_SECRET =  OAUTH_TOKEN_SECRET
It is all confusing enough without changing the names
""")
#STR = ("Sometimes we have visitors at night. If lucky we don't wake up.")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
#PATH = "tmp/TM_POST.jpg"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

#photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])
#print STR
twitter.update_status(status=STR)

import requests
import os
import json
from APIkeys import KEYS
bearer_token= KEYS()[4]
# To set your environment variables in your terminal run the following line:
# export 'BEARER_TOKEN'='<your_bearer_token>'
#bearer_token = os.environ.get("BEARER_TOKEN")
#bearer_token="AAAAAAAAAAAAAAAAAAAAAJ3negEAAAAA7cYYpESPIy%2FtJ0%2FhK%2FZ\
#llipGIOg%3Dld2zu8PPOoclOwShOMha6BgLW4smQ3Vf9I40DxluSUcelknoEA"
search_url = "https://api.twitter.com/2/spaces/search"

search_term = 'New York' # Replace this value with your search term

# Optional params: host_ids,conversation_controls,created_at,creator_id,id,invited_user_ids,is_ticketed,lang,media_key,participants,scheduled_start,speaker_ids,started_at,state,title,updated_at
query_params = {'query': search_term, 'space.fields': 'title,created_at', 'expansions': 'creator_id'}


def create_headers(bearer_token):
    headers = {
        "Authorization": "Bearer {}".format(bearer_token),
        "User-Agent": "v2SpacesSearchPython"
    }
    return headers


def connect_to_endpoint(url, headers, params):
    response = requests.request("GET", search_url, headers=headers, params=params)
    print(response.status_code)
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()


def main():
    headers = create_headers(bearer_token)
    json_response = connect_to_endpoint(search_url, headers, query_params)
    print(json.dumps(json_response, indent=4, sort_keys=True))


if __name__ == "__main__":
    main()

      
!curl https://api.twitter.com/2/tweets/search/recent?query=cat%20has%3Amedia%20-grumpy&tweet.fields=created_at&max_results=100 --header "Authorization: Bearer AAAAAAAAAAAAAAAAAAAAAJ3negEAAAAA7cYYpESPIy%2FtJ0%2FhK%2FZllipGIOg%3Dld2zu8PPOoclOwShOMha6BgLW4smQ3Vf9I40DxluSUcelknoEA"

    

!curl --request GET 'https://api.twitter.com/2/tweets/search/recent?query=from:twitterdev' \
--header 'Authorization: Bearer AAAAAAAAAAAAAAAAAAAAAJ3negEAAAAA7cYYpESPIy%2FtJ0%2FhK%2FZllipGIOg%3Dld2zu8PPOoclOwShOMha6BgLW4smQ3Vf9I40DxluSUcelknoEA' >twitterdev


!GET https://api.twitter.com/1.1/AAAAAAAAAAAAAAAAAAAAAJ3negEAAAAA7cYYpESPIy%2FtJ0%2FhK%2FZllipGIOg%3Dld2zu8PPOoclOwShOMha6BgLW4smQ3Vf9I40DxluSUcelknoEA/settings.json


# %load twitterdev
{"data":[{"id":"1547249504633643008","text":"Working with OAuth 2.0 and the Twitter API? Check out @jessicagarson's tutorial for @thepracticaldev on working with refresh tokens. \uD83D\uDCBB\n\nRead it here ⬇️ \nhttps://t.co/wACONDbTuq"},{"id":"1547039584202067969","text":"Registration for Chirp is now open! \uD83C\uDF89 Join us on Nov. 16 in San Francisco and online to celebrate our developer community and connect with the Twitter Developer Platform Team. This is an event you don’t want to miss. \uD83D\uDC40 #Chirp\n\nRegister here ➡️ https://t.co/AMENQKRZcd https://t.co/ofGVvSU736"},{"id":"1547007901570662401","text":"The #Chirp Developer Conference returns on November 16. Sign up now or  ❤  this tweet to get reminded when registration is ending. https://t.co/DIbYQmXJ4b"},{"id":"1546987013475225600","text":"Register today for the Chirp Developer Conference! Come celebrate our developer community in San Francisco or online on Nov. 16, this is an event you don't want to miss. \uD83D\uDC40"},{"id":"1546987013160677382","text":"Registration is now open for the Chirp Developer Conference. Come connect with the Twitter Developer Platform Team on Nov. 16, in San Francisco or online. See you there!"},{"id":"1546987013085298688","text":"Registration is now open for the #Chirp Developer Conference. Come connect with the Twitter Developer Platform Team on Nov. 16, in San Francisco or online. See you there!"},{"id":"1546986887969026048","text":"Registration is now open for the #Chirp Developer Conference. Come connect with the Twitter Developer Platform Team on Nov. 16, in San Francisco or online. See you there!"},{"id":"1546986887318933504","text":"Register for the Chirp Developer Conference to:\n\n⚡️ Sync with devs AFK\n\uD83E\uDDED Git building opportunities\n\uD83C\uDF89 Root for all we’ve accomplished together"},{"id":"1546986761607266304","text":"Register for the Chirp Developer Conference to:\n\n⚡️ Sync with devs AFK\n\uD83E\uDDED Git building opportunities\n\uD83C\uDF89 Root for all we’ve accomplished together"},{"id":"1546986761594802178","text":"Registration is now open for the Chirp Developer Conference. Come connect with the Twitter Developer Platform Team on Nov. 16, in San Francisco or online. See you there!"}],"meta":{"newest_id":"1547249504633643008","oldest_id":"1546986761594802178","result_count":10,"next_token":"b26v89c19zqg8o3fpz2mvn9yqnfvvf64en88jnrrfuz5p"}}



# %load twitterdev
{"data":[{"id":"1546615344780914688","text":"If you’re curious about building with Tweepy v2, check out the second episode of Devs in the Details. \uD83C\uDFA5 \n\n@suhemparack shows how to migrate from Tweepy v1.1 to v2 and the different functionalities of the APIs. #WelcomeToOurTechTalk\n\nWatch it here ⬇️\nhttps://t.co/UYGLlcIAZY"},{"id":"1545437560301928453","text":"\uD83D\uDCAD Wondering how to Tweet with media using the Twitter API v2? We are still working on a v2 media endpoint but in the meantime, @jessicagarson walks you through how to accomplish this task in Python.\n\nLearn how here ⬇️\nhttps://t.co/xmSES9Ug3s"},{"id":"1545105640795738113","text":"\uD83C\uDF89 We are excited to launch our student ambassador program.\n\nIf you are passionate about building and empowering the student community to build with Twitter, apply to be a TwitterDev student ambassador at your university! \uD83D\uDCDA\uD83D\uDEE0\n\nLearn more ⤵️\nhttps://t.co/PI25NViPqT"}],"meta":{"newest_id":"1546615344780914688","oldest_id":"1545105640795738113","result_count":3}}

AAAAAAAAAAAAAAAAAAAAAJ3negEAAAAA7cYYpESPIy%2FtJ0%2FhK%2FZllipGIOg%3Dld2zu8PPOoclOwShOMha6BgLW4smQ3Vf9I40DxluSUcelknoEA

API Key  
EteOELZAulNVMe2Vn1fjRNiLF

API Key Secret
zqhlZOsnZsoQaeFMhaVCWbK7vSwukLr1MUHmbDnXappWesRDNc

Bearer Token
 AAAAAAAAAAAAAAAAAAAAAJ3negEAAAAA7cYYpESPIy%2FtJ0%2FhK%2FZllipGIOg%3Dld2zu8PPOoclOwShOMha6BgLW4smQ3Vf9I40DxluSUcelknoEA



from twython import Twython
APP_KEY="EteOELZAulNVMe2Vn1fjRNiLF"
APP_SECRET="zqhlZOsnZsoQaeFMhaVCWbK7vSwukLr1MUHmbDnXappWesRDNc"

OAUTH_TOKEN_SECRET="AAAAAAAAAAAAAAAAAAAAAJ3negEAAAAA7cYYpESPIy%2FtJ0%2FhK%2FZllipGIOg%3Dld2zu8PPOoclOwShOMha6BgLW4smQ3Vf9I40DxluSUcelknoEA"
twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN,OAUTH_TOKEN_SECRET)


from twython import Twython
APP_KEY="EteOELZAulNVMe2Vn1fjRNiLF"

ACCESS_TOKEN="zqhlZOsnZsoQaeFMhaVCWbK7vSwukLr1MUHmbDnXappWesRDNc"

twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)
#twitter.search(q='python')
twitter.verify_credentials()

get a twitter CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY

CONSUMER_KEY, = key.twiter()[0]
CONSUMER_SECRET = key.twiter()[1]
ACCESS_KEY = key

CONSUMER_KEY = key.twiter()[0]
CONSUMER_SECRET = key.twiter()[1]
ACCESS_KEY = key


https://www.1001fonts.com/exo-font.html
https://interversehq.com/qview/download/

!ls frames

!ls ~/Desktop/images/*.jpg

!cp /home/jack/Desktop/images/girl.jpg images

from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
im = Image.open('images/girl.jpg')
im

!ls fonts

!mkdir images

import random
from random import randint
import time
import markovify
import os
import sys
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter

filename0 = "images/girl.jpg"

def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_blurred_back(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

if __name__ == '__main__':
    inp = Image.open(filename0)
    inp = inp.resize((640,640), Image.ANTIALIAS)
    font = ImageFont.truetype("fonts/exo.extra-bold.otf", 30)
    text_title = (255, 255,230) # bright green
    blur_title = (0, 0, 0)   # black
    #textin = (generate_the_word("wordcloud.txt"))
    i2 = draw_blurred_back(inp, (15, 4), "A Couple DeepDreams", font, text_title, blur_title)
    font0 = ImageFont.truetype("fonts/exo.extra-bold.otf", 20)
    i2 = draw_blurred_back(i2, (15, 40), "8conv3fc_DSN-CaffeModel", font0, text_title, blur_title)    
    i2 = draw_blurred_back(i2, (15, 60), "pool4", font0, text_title, blur_title)    
    
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = ImageFont.truetype("fonts/exo.extra-bold.otf", 20)
    # get a drawing context
    signature_ = "@jacklnorthrup" 
    #get length in pixel of signature_
    sizeS,ln = fnt.getsize(signature_)
    #add 15 pixels to right border
    pt = sizeS+25
    width, height = inp.size
    #marginx starting point of signature_
    marginx = pt
    #bottom margin
    marginy = 30
    x = width - marginx
    y = height - marginy
    

    text_sig = (255, 255,230) # bright green
    blur_sig = (0, 0, 0)   # black
    txt=draw_blurred_back(i2,(x,y), signature_, fnt, text_sig, blur_sig)
    out = Image.alpha_composite(i2, txt)
    out.save("images/TEMP_POST.png")

PATH = "images/TEMP_POST.png"
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')
image = Image.open(PATH)
image

import os
import random
from PIL import Image
PATH = "images"
def get_picture_list(PATH):
    abs_path = os.path.join(os.getcwd(),PATH)
    print ('abs_path =', abs_path)
    dir_files = os.listdir(abs_path)
    FILE = random.choice(dir_files)
    files = "".join(PATH+"/"+FILE)
    return files

PATH = "images"
get_picture_list(PATH)

import os
import random
from PIL import Image
PATH = "images"
def get_picture_list(PATH):
    abs_path = os.path.join(os.getcwd(),PATH)
    print ('abs_path =', abs_path)
    dir_files = os.listdir(abs_path)
    for file in dir_files:
        #dir_file = file.endswith("*.png")
        #FILE = random.choice(dir_file)
        #files = "".join(PATH+"/"+FILE)
        return file

PATH = "images"
get_picture_list(PATH)

INFO Found Here:
31 
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def get_picture_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    #print dir_files
    return dir_files

picture_list = get_picture_list('snippets')
print picture_list
------------
get_picture_list, get list of files, get a file list in memory
files in memeory , file list memory 


INFO Found Here:
38 
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def get_picture_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    #print dir_files
    return dir_files

picture_list = get_picture_list('snippets')
print picture_list
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def create_or_open_db(db_file):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists PICTURES(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        PICTURE BLOB,
        TYPE TEXT,
        FILE_NAME TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
    return conn

def insert_picture(picture_file):
    with open(picture_file, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(picture_file)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO PICTURES
        (PICTURE, TYPE, FILE_NAME)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimages(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM PICTURES")
    for fn in picture_list:
        picture_file = path+"/"+fn
        insert_picture(picture_file)

    for r in c.execute("SELECT rowid, FILE_NAME FROM PICTURES"):
        print r[0],r[1]
   
    conn.commit()


def get_image(picture_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT PICTURE, TYPE, FILE_NAME FROM PICTURES WHERE id = ?;",(picture_id,))
    #sql = "SELECT PICTURE, TYPE, FILE_NAME FROM PICTURES WHERE id = 19"
    param = {'id': picture_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename


dbname = "ImageC.db"
db_file = create_or_open_db(dbname)
path = "snippets/"
loadimages(dbname, path)
filename = get_image(16)
print filename
Image(filename=filename)

-----------------
store, retrieve images,SQLite Databasestore,
retrieve images, from SQLite , Database


INFO Found Here:
40 
%%writefile Image2SQLite.py
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def getImage_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    return dir_files

def create_or_open_db(dbname):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists images(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        image BLOB,
        TYPE TEXT,
        imagE TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
        conn.commit()
        conn.close()
    return conn

def insertImage(dbname, imageFile):
    with open(imageFile, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(imageFile)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO images
        (image, TYPE, imagE)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimagE(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM images")
    for fn in image_list:
        imageFile = path+"/"+fn
        insertImage(imageFile)

    for r in c.execute("SELECT rowid, imagE FROM images"):
        print r[0],r[1]
   
    conn.commit()
    conn.close()

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, imagE FROM images")
    for row in rows:
        print row[0],row[2]+row[1]    
    return
    
def get_image(dbname,image_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT image, TYPE, imagE FROM images WHERE id = ?;",(image_id,))
    #sql = "SELECT image, TYPE, imagE FROM images WHERE id = 19"
    param = {'id': image_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename
---------------
USAGE:
import Image2Data
picture_list = Image2Data.get_picture_list('snippets')
print picture_list

import Image2Data
dbname = "ImageE.db"
Image2Data.create_or_open_db(dbname)

#insert one image
import Image2Data
dbname = "ImageD.db"
picture_file = "01.jpg"
Image2Data.insert_picture(dbname, picture_file)

import Image2Data
dbname = "ImageD.db"
path = "snippets"
loadimages(dbname, path)

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, FILE_NAME FROM PICTURES")
    for row in rows:
        print row[0],row[2]+row[1]
    
#list images by id
dbname = "ImageD.db"
image_id(dbname)

#retrieve image by id
filename = get_image(dbname,1)
print filename
Image(filename=filename)




INFO Found Here:
41 
%%writefile Image2SQLite.py
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def getImage_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    return dir_files

def create_or_open_db(dbname):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists images(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        image BLOB,
        TYPE TEXT,
        imagE TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
        conn.commit()
        conn.close()
    return conn

def insertImage(dbname, imageFile):
    with open(imageFile, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(imageFile)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO images
        (image, TYPE, imagE)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimagE(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM images")
    for fn in image_list:
        imageFile = path+"/"+fn
        insertImage(imageFile)

    for r in c.execute("SELECT rowid, imagE FROM images"):
        print r[0],r[1]
   
    conn.commit()
    conn.close()

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, imagE FROM images")
    for row in rows:
        print row[0],row[2]+row[1]    
    return
    
def get_image(dbname,image_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT image, TYPE, imagE FROM images WHERE id = ?;",(image_id,))
    #sql = "SELECT image, TYPE, imagE FROM images WHERE id = 19"
    param = {'id': image_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename
---------------
USAGE:
import Image2Data
picture_list = Image2Data.get_picture_list('snippets')
print picture_list

import Image2Data
dbname = "ImageE.db"
Image2Data.create_or_open_db(dbname)

#insert one image
import Image2Data
dbname = "ImageD.db"
picture_file = "01.jpg"
Image2Data.insert_picture(dbname, picture_file)

import Image2Data
dbname = "ImageD.db"
path = "snippets"
loadimages(dbname, path)

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, FILE_NAME FROM PICTURES")
    for row in rows:
        print row[0],row[2]+row[1]
    
#list images by id
dbname = "ImageD.db"
image_id(dbname)

#retrieve image by id
filename = get_image(dbname,1)
print filename
Image(filename=filename)
------------
images to database , image2data , store images, store images as data, SQLite images



INFO Found Here:
45 
import os
import timeit
def txsearch():
    # Ask to enter string to search
    Sstring = raw_input("Search Phrase")
    for fname in os.listdir('./'):
       # Apply file type filter   
       if fname.endswith(".txt"):
            # Open file for reading
            fo = open(fname)
            # Read the first line from the file
            line = fo.readline()
            # Initialize counter for line number
            line_no = 1
            # Loop until EOF
            while line != '' :
                    index = line.find(Sstring)
                    if ( index != -1) :
                        # Set some parameters no lines longer than 240 characters 
                        # or less than search phrase +30 characters 
                        if len(line)< 240 and len(line)> len(Sstring)+20 :
                            #print(fname, "[", line_no, ",", index, "] ", line)
                            #print fname,line[1:-8],"  "
                            print fname,line_no,line
                    # Read next line
                    line = fo.readline()  
                    # Increment line counter
                    line_no += 1
            # Close the files
            fo.close()
            

------
search text file search file module import searchfile




INFO Found Here:
77 
import sqlite3
#database = "FTS4_IPYNB.db"
database = "FTS4_IPYNB_indexed.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute(""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS4(file, content, description);
"")
conn.commit()
conn.close()
conn = sqlite3.connect(database)
c = conn.cursor()
count=0
while count<19:
    count=count+1
    if count==1:PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/"
    if count==2:PATH = "/home/jack/Desktop/text_stuff/"
    if count==3:PATH = "/home/jack/Desktop/imagebot/"
    if count==4:PATH = "/home/jack/Desktop/Snippet_Warehouse/"
    if count==5:PATH = "/home/jack/Desktop/gitjupyter/"
    if count==6:PATH = "/home/jack/Desktop/jack_watch/"
    if count==7:PATH = "/home/jack/Desktop/jack_watch/nltk/"
    if count==8:PATH = "/home/jack/Desktop/jack_watch/Python-Lectures/"
    if count==9:PATH = "/home/jack/Desktop/jack_watch/jupyter_examples-master/"
    if count==10:PATH = "/home/jack/Desktop/Books/numerical-python-book-code/"
    if count==11:PATH = "/home/jack/Desktop/Books/pydata-book/"
    if count==12:PATH = "/home/jack/Desktop/Ruby/"
    if count==13:PATH = "/home/jack/Desktop/alice/ChatterBot/"
    if count==14:PATH = "/home/jack/Desktop/deep-dream-generator/LOCAL-notebooks/"
    if count==15:PATH = "/home/jack/Desktop/numpy-array-filters/"
    if count==16:PATH = "/home/jack/Desktop/pycode/"
    if count==17:PATH = "/home/jack/Desktop/pycode/vpython2/TrigonometryBot/"
    if count==18:PATH = "/home/jack/Desktop/temp/args_csv_Twython_ImageBot/"
    if count==19:PATH = "/home/jack/python3-starter/notebooks/"
    for file in os.listdir(PATH):
        if file.endswith(".ipynb"):
            filename = PATH+file
            filein = PATH
            filein = filein.replace("/home/jack/", "")
            filein = filein.replace("/", "_")
            filein = filein+file
            description = filein
            description = description.replace("_", " ")
            description = description.replace("-", " ")
            description = description.replace("/", " ")
            description = description+"
"+PATH+file+"
"+file
            with open(filename, "rb") as input_file:
                    ablob = input_file.read()
                    content  = sqlite3.Binary(ablob)
                    c.execute("INSERT INTO ipynb (file, content, description) VALUES(?, ?, ?)", 
                              (filein, content, description))
                    print os.path.join(PATH, file, filein)
                    conn.commit()
            line = file
            #line ="Good-mouse-sizing-and-cropping.ipynb"
            title = "index"
            c.execute("INSERT INTO ipynb VALUES (?,?,?)", (title, file, description)) 
            conn.commit()

c.close()
conn.close() 
        
        


INFO Found Here:
87 
import os.path
from os import listdir, getcwd
def mp3list(PATH):
    abs_path = os.path.join(os.getcwd(),PATH)
    dir_files = os.listdir(abs_path)
    return dir_files

def mkFile(mfile, PATH):
    mFiles = open(mfile, "w");mFiles.close()
    lst = mp3list(PATH)
    lst = str(lst)
    lst = lst.replace(",","
");lst = lst.replace("'","")
    lst = lst.replace("[","");lst = lst.replace("]","")
    lst = lst.replace(" ","")
    print lst
    mFiles = open(mfile, "a")
    mFiles.write(lst)
    mFiles.close()
    
PATH = "MP3/"
mfile = "GISTstore/muz.list"
mkFile(mfile, PATH)
---
list directory, list to file directory, list to file


INFO Found Here:
107 
import os.path
from os import listdir, getcwd
def mp3list(PATH):
    abs_path = os.path.join(os.getcwd(),PATH)
    dir_files = os.listdir(abs_path)
    return dir_files

def mkFile(mfile, PATH):
    mFiles = open(mfile, "w");mFiles.close()
    lst = mp3list(PATH)
    lst = str(lst)
    lst = lst.replace(",","
");lst = lst.replace("'","")
    lst = lst.replace("[","");lst = lst.replace("]","")
    lst = lst.replace(" ","")
    print lst
    mFiles = open(mfile, "a")
    mFiles.write(lst)
    mFiles.close()
    
PATH = "MP3/"
mfile = "GISTstore/muz.list"
mkFile(mfile, PATH)
---
list directory, list to file directory, list to file


INFO Found Here:
117 
# Create Segmentation Art art Generator generate art computer generated art
from skimage import graph, data, io, segmentation, color
from skimage import future 
import skimage,os
from PIL import Image
import cv2
from matplotlib import pyplot as plt
%matplotlib inline 
path = r"/home/jack/Desktop/imagebot/greedy/"
#path = r"build/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
filename=(path+base_image)
img = io.imread(filename)
labels1 = segmentation.slic(img, compactness=10, n_segments=400)
out1 = color.label2rgb(labels1, img, kind='avg')
g = future.graph.rag_mean_color(img, labels1, mode='similarity')
labels2 = future.graph.cut_normalized(labels1, g)
out2 = color.label2rgb(labels2, img, kind='avg')
imfile = "output/out2b01"
cv2.imwrite(imfile+".png", out2)
im = Image.open(imfile+".png")
im0 = im.resize((640,640), Image.NEAREST)
im0.save(imfile+".jpg")
im0

----------------
Create Segmentation Art art Generator generate art computer generated art
Create SegmentationArt artGenerator generateart computergeneratedart


INFO Found Here:
118 
import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage.future import graph
from matplotlib import pyplot as plt
from skimage import io
path = r"/home/jack/Desktop/imagebot/greedy/"
#path = r"build/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
filename=(path+base_image)
img = io.imread(filename)
labels = segmentation.slic(img, compactness=20, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.5, 'count': 10}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    ""Call back called before merging 2 nodes.
    In this case we don't need to do any computation here."" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)
plt.figure(figsize=(20,10))
out = color.label2rgb(labels2, img, kind='avg')
plt.imshow(out)
plt.title('        Creating Segmentation Art')

plt.savefig('tmp/seg001.png', bbox_inches='tight')
im = Image.open('tmp/seg001.png');im1 = im.resize((640,640), Image.NEAREST)
filename = time.strftime("Images/segmented%Y%m%d%H%M%S.png")
im1.save(filename)
print filename
plt.close()
im1
  


INFO Found Here:
164 
# create a list in memory of a directory
from time import sleep
import os
LST = []
PATH = "640x640/"
for files in sorted(os.listdir(PATH)):
    LST.append(PATH+files)
    
    
Then it may be used with:
    
    
for line in LST:
    print line    ig file into memory.


INFO Found Here:
165 
shell commands shutil
os.remove() will remove a file.
os.rmdir() will remove an empty directory.
shutil.rmtree() will delete a directory and all its contents
import shutil
import os
source = os.listdir("/tmp/")
destination = "/tmp/newfolder/"
for files in source:
    if files.endswith(".txt"):
        shutil.copy(files,destination)
---
import shutil  
shutil.copyfile('/path/to/file', '/path/to/other/phile')

mv move 
import shutil
import os
source = os.listdir("/tmp/")
destination = "/tmp/newfolder/"
for files in source:
    if files.endswith(".txt"):
        shutil.move(files,destination)
---------
import shutil
import os
SOURCE = "samples"
BACKUP = "samples-bak"
# create a backup directory
shutil.copytree(SOURCE, BACKUP)
print os.listdir(BACKUP)

shutil.rmtree('one/two/three')



INFO Found Here:
166 
import os
PATH = "cd/"
count=0
for File in sorted(os.listdir(PATH)):
    count=count+1
    filein = PATH+File
    base = ntpath.basename(filein)
    im = Image.open(filein)
    width, height = im.size   # Get dimensions
    if width>height:new_width = height;new_height=new_width
    if width<height:new_width = height;new_height=new_width    
    left = (width - new_width)/2
    top = (height - new_height)/2
    right = (width + new_width)/2
    bottom = (height + new_height)/2
    im0 = im.crop((left, top, right, bottom))
    img = im0.resize((640, 640), Image.NEAREST)
    filename = "cd640/"+base
    img.save(filename)
    os.remove(filein)

_________________

os.listdir Command Completed
In [71]:

1
f = open("codetalk.txt")
2
text = f.read()
3
text_model = markovify.Text(text)

!NED -S os.listdir

INFO Found Here:
31 
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def get_picture_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    #print dir_files
    return dir_files

picture_list = get_picture_list('snippets')
print picture_list
------------
get_picture_list, get list of files, get a file list in memory
files in memeory , file list memory 


INFO Found Here:
38 
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def get_picture_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    #print dir_files
    return dir_files

picture_list = get_picture_list('snippets')
print picture_list
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def create_or_open_db(db_file):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists PICTURES(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        PICTURE BLOB,
        TYPE TEXT,
        FILE_NAME TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
    return conn

def insert_picture(picture_file):
    with open(picture_file, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(picture_file)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO PICTURES
        (PICTURE, TYPE, FILE_NAME)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimages(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM PICTURES")
    for fn in picture_list:
        picture_file = path+"/"+fn
        insert_picture(picture_file)

    for r in c.execute("SELECT rowid, FILE_NAME FROM PICTURES"):
        print r[0],r[1]
   
    conn.commit()


def get_image(picture_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT PICTURE, TYPE, FILE_NAME FROM PICTURES WHERE id = ?;",(picture_id,))
    #sql = "SELECT PICTURE, TYPE, FILE_NAME FROM PICTURES WHERE id = 19"
    param = {'id': picture_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename


dbname = "ImageC.db"
db_file = create_or_open_db(dbname)
path = "snippets/"
loadimages(dbname, path)
filename = get_image(16)
print filename
Image(filename=filename)

-----------------
store, retrieve images,SQLite Databasestore,
retrieve images, from SQLite , Database


INFO Found Here:
40 
%%writefile Image2SQLite.py
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def getImage_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    return dir_files

def create_or_open_db(dbname):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists images(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        image BLOB,
        TYPE TEXT,
        imagE TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
        conn.commit()
        conn.close()
    return conn

def insertImage(dbname, imageFile):
    with open(imageFile, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(imageFile)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO images
        (image, TYPE, imagE)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimagE(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM images")
    for fn in image_list:
        imageFile = path+"/"+fn
        insertImage(imageFile)

    for r in c.execute("SELECT rowid, imagE FROM images"):
        print r[0],r[1]
   
    conn.commit()
    conn.close()

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, imagE FROM images")
    for row in rows:
        print row[0],row[2]+row[1]    
    return
    
def get_image(dbname,image_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT image, TYPE, imagE FROM images WHERE id = ?;",(image_id,))
    #sql = "SELECT image, TYPE, imagE FROM images WHERE id = 19"
    param = {'id': image_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename
---------------
USAGE:
import Image2Data
picture_list = Image2Data.get_picture_list('snippets')
print picture_list

import Image2Data
dbname = "ImageE.db"
Image2Data.create_or_open_db(dbname)

#insert one image
import Image2Data
dbname = "ImageD.db"
picture_file = "01.jpg"
Image2Data.insert_picture(dbname, picture_file)

import Image2Data
dbname = "ImageD.db"
path = "snippets"
loadimages(dbname, path)

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, FILE_NAME FROM PICTURES")
    for row in rows:
        print row[0],row[2]+row[1]
    
#list images by id
dbname = "ImageD.db"
image_id(dbname)

#retrieve image by id
filename = get_image(dbname,1)
print filename
Image(filename=filename)




INFO Found Here:
41 
%%writefile Image2SQLite.py
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def getImage_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    return dir_files

def create_or_open_db(dbname):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists images(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        image BLOB,
        TYPE TEXT,
        imagE TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
        conn.commit()
        conn.close()
    return conn

def insertImage(dbname, imageFile):
    with open(imageFile, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(imageFile)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO images
        (image, TYPE, imagE)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimagE(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM images")
    for fn in image_list:
        imageFile = path+"/"+fn
        insertImage(imageFile)

    for r in c.execute("SELECT rowid, imagE FROM images"):
        print r[0],r[1]
   
    conn.commit()
    conn.close()

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, imagE FROM images")
    for row in rows:
        print row[0],row[2]+row[1]    
    return
    
def get_image(dbname,image_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT image, TYPE, imagE FROM images WHERE id = ?;",(image_id,))
    #sql = "SELECT image, TYPE, imagE FROM images WHERE id = 19"
    param = {'id': image_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename
---------------
USAGE:
import Image2Data
picture_list = Image2Data.get_picture_list('snippets')
print picture_list

import Image2Data
dbname = "ImageE.db"
Image2Data.create_or_open_db(dbname)

#insert one image
import Image2Data
dbname = "ImageD.db"
picture_file = "01.jpg"
Image2Data.insert_picture(dbname, picture_file)

import Image2Data
dbname = "ImageD.db"
path = "snippets"
loadimages(dbname, path)

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, FILE_NAME FROM PICTURES")
    for row in rows:
        print row[0],row[2]+row[1]
    
#list images by id
dbname = "ImageD.db"
image_id(dbname)

#retrieve image by id
filename = get_image(dbname,1)
print filename
Image(filename=filename)
------------
images to database , image2data , store images, store images as data, SQLite images



INFO Found Here:
45 
import os
import timeit
def txsearch():
    # Ask to enter string to search
    Sstring = raw_input("Search Phrase")
    for fname in os.listdir('./'):
       # Apply file type filter   
       if fname.endswith(".txt"):
            # Open file for reading
            fo = open(fname)
            # Read the first line from the file
            line = fo.readline()
            # Initialize counter for line number
            line_no = 1
            # Loop until EOF
            while line != '' :
                    index = line.find(Sstring)
                    if ( index != -1) :
                        # Set some parameters no lines longer than 240 characters 
                        # or less than search phrase +30 characters 
                        if len(line)< 240 and len(line)> len(Sstring)+20 :
                            #print(fname, "[", line_no, ",", index, "] ", line)
                            #print fname,line[1:-8],"  "
                            print fname,line_no,line
                    # Read next line
                    line = fo.readline()  
                    # Increment line counter
                    line_no += 1
            # Close the files
            fo.close()
            

------
search text file search file module import searchfile




INFO Found Here:
77 
import sqlite3
#database = "FTS4_IPYNB.db"
database = "FTS4_IPYNB_indexed.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute(""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS4(file, content, description);
"")
conn.commit()
conn.close()
conn = sqlite3.connect(database)
c = conn.cursor()
count=0
while count<19:
    count=count+1
    if count==1:PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/"
    if count==2:PATH = "/home/jack/Desktop/text_stuff/"
    if count==3:PATH = "/home/jack/Desktop/imagebot/"
    if count==4:PATH = "/home/jack/Desktop/Snippet_Warehouse/"
    if count==5:PATH = "/home/jack/Desktop/gitjupyter/"
    if count==6:PATH = "/home/jack/Desktop/jack_watch/"
    if count==7:PATH = "/home/jack/Desktop/jack_watch/nltk/"
    if count==8:PATH = "/home/jack/Desktop/jack_watch/Python-Lectures/"
    if count==9:PATH = "/home/jack/Desktop/jack_watch/jupyter_examples-master/"
    if count==10:PATH = "/home/jack/Desktop/Books/numerical-python-book-code/"
    if count==11:PATH = "/home/jack/Desktop/Books/pydata-book/"
    if count==12:PATH = "/home/jack/Desktop/Ruby/"
    if count==13:PATH = "/home/jack/Desktop/alice/ChatterBot/"
    if count==14:PATH = "/home/jack/Desktop/deep-dream-generator/LOCAL-notebooks/"
    if count==15:PATH = "/home/jack/Desktop/numpy-array-filters/"
    if count==16:PATH = "/home/jack/Desktop/pycode/"
    if count==17:PATH = "/home/jack/Desktop/pycode/vpython2/TrigonometryBot/"
    if count==18:PATH = "/home/jack/Desktop/temp/args_csv_Twython_ImageBot/"
    if count==19:PATH = "/home/jack/python3-starter/notebooks/"
    for file in os.listdir(PATH):
        if file.endswith(".ipynb"):
            filename = PATH+file
            filein = PATH
            filein = filein.replace("/home/jack/", "")
            filein = filein.replace("/", "_")
            filein = filein+file
            description = filein
            description = description.replace("_", " ")
            description = description.replace("-", " ")
            description = description.replace("/", " ")
            description = description+"
"+PATH+file+"
"+file
            with open(filename, "rb") as input_file:
                    ablob = input_file.read()
                    content  = sqlite3.Binary(ablob)
                    c.execute("INSERT INTO ipynb (file, content, description) VALUES(?, ?, ?)", 
                              (filein, content, description))
                    print os.path.join(PATH, file, filein)
                    conn.commit()
            line = file
            #line ="Good-mouse-sizing-and-cropping.ipynb"
            title = "index"
            c.execute("INSERT INTO ipynb VALUES (?,?,?)", (title, file, description)) 
            conn.commit()

c.close()
conn.close() 
        
        


INFO Found Here:
87 
import os.path
from os import listdir, getcwd
def mp3list(PATH):
    abs_path = os.path.join(os.getcwd(),PATH)
    dir_files = os.listdir(abs_path)
    return dir_files

def mkFile(mfile, PATH):
    mFiles = open(mfile, "w");mFiles.close()
    lst = mp3list(PATH)
    lst = str(lst)
    lst = lst.replace(",","
");lst = lst.replace("'","")
    lst = lst.replace("[","");lst = lst.replace("]","")
    lst = lst.replace(" ","")
    print lst
    mFiles = open(mfile, "a")
    mFiles.write(lst)
    mFiles.close()
    
PATH = "MP3/"
mfile = "GISTstore/muz.list"
mkFile(mfile, PATH)
---
list directory, list to file directory, list to file


INFO Found Here:
107 
import os.path
from os import listdir, getcwd
def mp3list(PATH):
    abs_path = os.path.join(os.getcwd(),PATH)
    dir_files = os.listdir(abs_path)
    return dir_files

def mkFile(mfile, PATH):
    mFiles = open(mfile, "w");mFiles.close()
    lst = mp3list(PATH)
    lst = str(lst)
    lst = lst.replace(",","
");lst = lst.replace("'","")
    lst = lst.replace("[","");lst = lst.replace("]","")
    lst = lst.replace(" ","")
    print lst
    mFiles = open(mfile, "a")
    mFiles.write(lst)
    mFiles.close()
    
PATH = "MP3/"
mfile = "GISTstore/muz.list"
mkFile(mfile, PATH)
---
list directory, list to file directory, list to file


INFO Found Here:
117 
# Create Segmentation Art art Generator generate art computer generated art
from skimage import graph, data, io, segmentation, color
from skimage import future 
import skimage,os
from PIL import Image
import cv2
from matplotlib import pyplot as plt
%matplotlib inline 
path = r"/home/jack/Desktop/imagebot/greedy/"
#path = r"build/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
filename=(path+base_image)
img = io.imread(filename)
labels1 = segmentation.slic(img, compactness=10, n_segments=400)
out1 = color.label2rgb(labels1, img, kind='avg')
g = future.graph.rag_mean_color(img, labels1, mode='similarity')
labels2 = future.graph.cut_normalized(labels1, g)
out2 = color.label2rgb(labels2, img, kind='avg')
imfile = "output/out2b01"
cv2.imwrite(imfile+".png", out2)
im = Image.open(imfile+".png")
im0 = im.resize((640,640), Image.NEAREST)
im0.save(imfile+".jpg")
im0

----------------
Create Segmentation Art art Generator generate art computer generated art
Create SegmentationArt artGenerator generateart computergeneratedart


INFO Found Here:
118 
import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage.future import graph
from matplotlib import pyplot as plt
from skimage import io
path = r"/home/jack/Desktop/imagebot/greedy/"
#path = r"build/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
filename=(path+base_image)
img = io.imread(filename)
labels = segmentation.slic(img, compactness=20, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.5, 'count': 10}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    ""Call back called before merging 2 nodes.
    In this case we don't need to do any computation here."" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)
plt.figure(figsize=(20,10))
out = color.label2rgb(labels2, img, kind='avg')
plt.imshow(out)
plt.title('        Creating Segmentation Art')

plt.savefig('tmp/seg001.png', bbox_inches='tight')
im = Image.open('tmp/seg001.png');im1 = im.resize((640,640), Image.NEAREST)
filename = time.strftime("Images/segmented%Y%m%d%H%M%S.png")
im1.save(filename)
print filename
plt.close()
im1
  


INFO Found Here:
164 
# create a list in memory of a directory
from time import sleep
import os
LST = []
PATH = "640x640/"
for files in sorted(os.listdir(PATH)):
    LST.append(PATH+files)
    
    
Then it may be used with:
    
    
for line in LST:
    print line    ig file into memory.


INFO Found Here:
165 
shell commands shutil
os.remove() will remove a file.
os.rmdir() will remove an empty directory.
shutil.rmtree() will delete a directory and all its contents
import shutil
import os
source = os.listdir("/tmp/")
destination = "/tmp/newfolder/"
for files in source:
    if files.endswith(".txt"):
        shutil.copy(files,destination)
---
import shutil  
shutil.copyfile('/path/to/file', '/path/to/other/phile')

mv move 
import shutil
import os
source = os.listdir("/tmp/")
destination = "/tmp/newfolder/"
for files in source:
    if files.endswith(".txt"):
        shutil.move(files,destination)
---------
import shutil
import os
SOURCE = "samples"
BACKUP = "samples-bak"
# create a backup directory
shutil.copytree(SOURCE, BACKUP)
print os.listdir(BACKUP)

shutil.rmtree('one/two/three')



INFO Found Here:
166 
import os
PATH = "cd/"
count=0
for File in sorted(os.listdir(PATH)):
    count=count+1
    filein = PATH+File
    base = ntpath.basename(filein)
    im = Image.open(filein)
    width, height = im.size   # Get dimensions
    if width>height:new_width = height;new_height=new_width
    if width<height:new_width = height;new_height=new_width    
    left = (width - new_width)/2
    top = (height - new_height)/2
    right = (width + new_width)/2
    bottom = (height + new_height)/2
    im0 = im.crop((left, top, right, bottom))
    img = im0.resize((640, 640), Image.NEAREST)
    filename = "cd640/"+base
    img.save(filename)
    os.remove(filein)

_________________

os.listdir Command Completed

f = open("codetalk.txt")
text = f.read()
text_model = markovify.Text(text)
STR = (text_model.make_short_sentence(140))
print STR

!showme copies/TM_POST1.jpg

!mkdir copies

def pil_image():
    ''' A View that Returns a PNG Image generated using PIL'''

    from PIL import Image, ImageDraw 

    size = (100,50)             # size of the image to create
    im = Image.new('RGB', size) # create the image
    draw = ImageDraw.Draw(im)   # create a drawing object that is
                                # used to draw on the new image
    red = (255,0,0)    # color of our text
    text_pos = (10,10) # top-left position of our text
    text = "Hello World!" # text to draw
    # Now, we'll do the drawing: 
    draw.text(text_pos, text, fill=red)
    
    del draw # I'm done drawing so I don't need this anymore
    
    # We need an HttpResponse object with the correct mimetype
    response = HttpResponse(mimetype="image/png")
    # now, we tell the image to save as a PNG to the 
    # provided file-like object
    im.save("test.png", 'PNG')

    return im # and we're done!

pil_image()

def pil_image():
    ''' A View that Returns a PNG Image generated using PIL'''

    from PIL import Image, ImageDraw 

    size = (100,50)             # size of the image to create
    im = Image.new('RGB', size) # create the image
    draw = ImageDraw.Draw(im)   # create a drawing object that is
                                # used to draw on the new image
    red = (255,0,0)    # color of our text
    text_pos = (10,10) # top-left position of our text
    text = "Hello World!" # text to draw
    # Now, we'll do the drawing: 
    draw.text(text_pos, text, fill=red)
    
    del draw # I'm done drawing so I don't need this anymore
    
    # We need an HttpResponse object with the correct mimetype
    #response = HttpResponse(mimetype="image/png")
    # now, we tell the image to save as a PNG to the 
    # provided file-like object
    im.save("test.png", 'PNG')

    return im # and we're done!

pil_image()

%%writefile saltpost
#!/bin/bash

while true; do
  python saltpost.py
  echo "posted :"
  date
  sleep 800s
done

%%writefile saltpost
#!/bin/bash

while true; do
  python saltpost.py
  echo "posted :"
  date
  sleep 800s
done

import random
from random import randint
import time
import markovify
import os
import sys
#sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
import sys
sys.path.insert(0,"/home/jack/hidden")
import key

#%%writefile saltpost.py
#ONE TIME MANUAL POSTS
#!/home/jack/anaconda2/python
import random
from random import randint
import time
import markovify
import os
import sys
#sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
import sys
sys.path.insert(0,"/home/jack/hidden")
import key
def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_text_with_halo(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

def rndcolor():
    r = randint(50,255)
    g = randint(50,255)
    b = randint(50,255)
    rndcolor = (r,g,b) 
    return rndcolor
def get_random_line(file_name):
    total_bytes = os.stat(file_name).st_size 
    random_point = random.randint(0, total_bytes)
    file = open(file_name)
    file.seek(random_point)
    file.readline() # skip this line to clear the partial line
    return file.readline()



if __name__ == '__main__':
    #nap = randint(500,1200)
    #time.sleep(nap)
    #isize = (640,640)     
    #inp = Image.new('RGB', isize)
    
    path2 = r"/home/jack/Desktop/pycode/vpython2/TrigonometryBot/images/"
    #path2 = r"bugs/advertisements1800/"
    random_filename2 = random.choice([
        y for y in os.listdir(path2)
        if os.path.isfile(os.path.join(path2, y))
    ])

    img1a = path2+"/"+random_filename2
    inp=Image.open(img1a)    
    inp = inp.resize((640,640), Image.NEAREST)
    
    
    font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 40)
    text_col = (255, 255,230) # bright green
    halo_col = (0, 0, 0)   # black
    textin = (generate_the_word("/home/jack/Desktop/imagebot/wordcloud.txt"))
    i2 = draw_text_with_halo(inp, (15, 8), "SaltMan", font, text_col, halo_col)
    
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
    # get a drawing context
    width, height = inp.size
    marginx = 225
    marginy = 35
    x = width - marginx
    y = height - marginy
    signature_ = "The TwitterBot Project" 
    #text_col2 = (150, 255, 150) # bright green
    #halo_col2 = (0, 0, 0)   # black
    text_col2 = (255, 255,230) # bright green
    halo_col2 = (0, 0, 0)   # black
    #text_col2 = (0, 0, 0)  # bright green
    #halo_col2 = (255, 255,230)  # black    
    txt1=draw_text_with_halo(i2,(x,y), signature_, fnt, text_col2, halo_col2)
    
    
    
    # get a font
    fs=randint(15,24)
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", fs)
    # get a drawing context
    width, height = inp.size
    marginx = 225
    marginy = 35
    x = width - marginx
    y = height - marginy
    signature_ = "The TwitterBot Project" 
    #text_col2 = (150, 255, 150) # bright green
    #halo_col2 = (0, 0, 0)   # black
    #text_col2 = (255, 255,230) # bright green
    text_col2 = rndcolor()
    halo_col2 = (0, 0, 0)   # black
    #text_col2 = (0, 0, 0)  # bright green
    #halo_col2 = (255, 255,230)  # black 
    yy=randint(70,290)
    xx=randint(5,60)
    #iword = (text_model.make_short_sentence(50))
    file_name = '/home/jack/Desktop/imagebot/saltman.txt'
    iword = get_random_line(file_name)
    
    txt3=draw_text_with_halo(txt1,(xx,yy), iword, fnt, text_col2, halo_col2)
   
    vv=randint(320,530)
    vvv=randint(5,10)
    #iword = (text_model.make_short_sentence(50))
    file_name = '/home/jack/Desktop/imagebot/saltman.txt'
    lword = get_random_line(file_name)        
    text_col3 = rndcolor()
    fs2=randint(15,24)
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", fs2)
    txt=draw_text_with_halo(txt3,(vvv,vv), lword, fnt, text_col3, halo_col2)
     
    out = Image.alpha_composite(i2, txt)
    out.save("tmp/TM_POST.jpg")

#removed keys for privacy reasons
CONSUMER_KEY = key.twiter()[0]
CONSUMER_SECRET = key.twiter()[1]
ACCESS_KEY = key.twiter()[2]
ACCESS_SECRET = key.twiter()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
#f = open("Mine.txt")
#text = f.read()
# Build the model.
#text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
#STR = (text_model.make_short_sentence(140))
#random.choice(open('Mine.txt').readlines())



file_name = '/home/jack/Desktop/imagebot/Mine.txt'
STR = get_random_line(file_name)

#STR = ("Sometimes we have visitors at night. If lucky we don't wake up.")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "tmp/TM_POST.jpg"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])
print STR


!showme /home/jack/Desktop/imagebot/images/Sranger-Tri-001-crop2b.jpg

import random
from random import randint
import time
import markovify
import os
import sys
sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
import Key
CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
file_name = '/home/jack/Desktop/imagebot/Mine.txt'
STR = get_random_line(file_name)
#STR = ("Sometimes we have visitors at night. If lucky we don't wake up.")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "tmp/TM_POST1.jpg"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])
print STR


!showme tmp/TM_POST1.jpg

import genim
path="junk/"
genim.RanFile(path)

!showme tmp/TM_POST.jpg

#ONE TIME MANUAL POSTS
#!/home/jack/anaconda2/python
import random
from random import randint
import time
import markovify
import os
import Key
import sys
sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
nap = randint(10,35)
time.sleep(nap)
path = r"publish/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)
def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_text_with_halo(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

if __name__ == '__main__':
    inp = Image.open(filename0)
    font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 40)
    text_col = (255, 255,230) # bright green
    halo_col = (0, 0, 0)   # black
    textin = (generate_the_word("wordcloud.txt"))
    i2 = draw_text_with_halo(inp, (15, 8), "I_FollowBack", font, text_col, halo_col)
    
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
    # get a drawing context
    width, height = inp.size
    marginx = 225
    marginy = 35
    x = width - marginx
    y = height - marginy
    signature_ = "The TwitterBot Project" 
    #text_col2 = (150, 255, 150) # bright green
    #halo_col2 = (0, 0, 0)   # black
    text_col2 = (255, 255,230) # bright green
    halo_col2 = (0, 0, 0)   # black
    txt=draw_text_with_halo(i2,(x,y), signature_, fnt, text_col2, halo_col2)
    out = Image.alpha_composite(i2, txt)
    out.save("tmp/TM_POST.jpg")

#removed keys for privacy reasons
#removed keys for privacy reasons
CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
f = open("art.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
STR = (text_model.make_short_sentence(140))
#STR = ("#All_in_One - #WordCloud #Create - Added ability to randomly choose an image background  #Automated")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "tmp/TM_POST.jpg"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

#photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])


Easy to customize with various title sizes, colors. and locations. 

%%writefile wordcloud.txt
Python
Programming
ImageBot
Enjoy
f4f always
Just for You
Good Stuff
Computer Graphics
Python Fun
Python Graphics
Generator
Followback
Word Cloud
Graphics
Fun w/Python
Python Stuff
PYTHON !!!
Follow4Follow
Love`en Python
Creative Python
Graphic Fun
ImageBot
Programming

%%writefile titlenpost
#!/bin/bash

while true; do
  python titlenpost.py
  echo "posted :"
  date
  sleep 1800s
done

%%writefile titlenpost
#!/bin/bash

while true; do
  python titlenpost.py
  echo "posted :"
  date
  sleep 1500s
done

!./titlenpost

#Great signature
import sys
import os
import time
import random
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
path = r"publish/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)
def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_text_with_halo(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

if __name__ == '__main__':
    inp = Image.open(filename0)
    font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 40)
    text_col = (255, 255,230) # bright green
    halo_col = (0, 0, 0)   # black
    textin = (generate_the_word("wordcloud.txt"))
    i2 = draw_text_with_halo(inp, (15, 8), textin, font, text_col, halo_col)
    
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
    
    width, height = inp.size
    marginx = 225
    marginy = 35
    x = width - marginx
    y = height - marginy
    signature_ = "The TwitterBot Project" 
    #text_col2 = (150, 255, 150) # bright green
    #halo_col2 = (0, 0, 0)   # black
    text_col2 = (255, 255,230) # bright green
    halo_col2 = (0, 0, 0)   # black
    txt=draw_text_with_halo(i2,(x,y), signature_, fnt, text_col2, halo_col2)
    # get a drawing context
    d = ImageDraw.Draw(txt)
    out = Image.alpha_composite(i2, txt)
    filename = time.strftime("tmp/%Y%m%d%H%M%S.jpg")


out

!ls key.py

!cp key.py Key.py


!showme tmp/TM_POST1.jpg





#%%writefile titlenpost.py
#!/home/jack/anaconda2/python
import random
from random import randint
import time
import markovify
import os
import sys
import Key
sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
#nap = randint(100,635)
#time.sleep(nap)
path = r"output/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)
def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_text_with_halo(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

if __name__ == '__main__':
    inp = Image.open(filename0)
    font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 40)
    text_col = (255, 255,230) # bright green
    halo_col = (0, 0, 0)   # black
    textin = (generate_the_word("/home/jack/Desktop/imagebot/wordcloud.txt"))
    i2 = draw_text_with_halo(inp, (15, 8), textin, font, text_col, halo_col)
    
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
    # get a drawing context
    width, height = inp.size
    marginx = 225
    marginy = 35
    x = width - marginx
    y = height - marginy
    signature_ = "" 
    #text_col2 = (150, 255, 150) # bright green
    #halo_col2 = (0, 0, 0)   # black
    text_col2 = (255, 255,230) # bright green
    halo_col2 = (0, 0, 0)   # black
    txt=draw_text_with_halo(i2,(x,y), signature_, fnt, text_col2, halo_col2)
    out = Image.alpha_composite(i2, txt)
    out.save("tmp/TM_POST1.jpg")


#removed keys for privacy reasons
CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
f = open("/home/jack/Desktop/imagebot/codetalk.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
STR = (text_model.make_short_sentence(140))
#STR = ("#All_in_One - #WordCloud #Create - Added ability to randomly choose an image background  #Automated")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "tmp/TM_POST1.jpg"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

!showme tmp/TM_POST1.jpg

!locate codetalk

from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
#images/perlin001-art.png
custom = "images/paper002.jpg"
filename0=(custom)
def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_blurred_back(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

if __name__ == '__main__':
    inp = Image.open(filename0)
    font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 40)
    text_title = (255, 255,230) # bright green
    blur_title = (0, 0, 0)   # black
    #textin = (generate_the_word("wordcloud.txt"))
    i2 = draw_blurred_back(inp, (15, 4), "Processing JS", font, text_title, blur_title)
    
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
    # get a drawing context
    signature_ = "@Jacknorthrup Instagram and @jacklnorthrup TwitterBot Project" 
    #get length in pixel of signature_
    sizeS,ln = fnt.getsize(signature_)
    #add 15 pixels to right border
    pt = sizeS+15
    width, height = inp.size
    #marginx starting point of signature_
    marginx = pt
    #bottom margin
    marginy = 35
    x = width - marginx
    y = height - marginy
    

    text_sig = (255, 255,230) # bright green
    blur_sig = (0, 0, 0)   # black
    txt=draw_blurred_back(i2,(x,y), signature_, fnt, text_sig, blur_sig)
    out = Image.alpha_composite(i2, txt)
    out.save("copies/test_image.jpg")
    im=Image.open("copies/test_image.jpg")

import sqlite3
from time import sleep 
import sys
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
filein = open("allsnippets.txt","w");filein.close()
filein = open("allsnippets.txt","a")
for row in c.execute('SELECT rowid, * FROM snippet'):    

    info= (row)[2]
    info = str(info)
    info = info+"\n\n"
    filein.write(info)
    
    
filein.close()    

!ls *.db

import sqlite3
from time import sleep 
import sys
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
filein = open("allsnippets.txt","w");filein.close()
filein = open("allsnippets.txt","a")
for row in c.execute('SELECT rowid, * FROM snippet'):    
    count=count+1
    sleep(.25)
    print "ID : ",(row)[0],(row)[2]," -- KEYWORDS",(row)[3],"\n"
    if count > req:
        conn.close()
        sys.exit()

import sqlite3
import sys
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
filein = open("allsnippets.txt","w");filein.close()
filein = open("allsnippets.txt","a")
rows = c.execute('SELECT snippet FROM snippet') 
for row in rows:
     print row

import sqlite3
import sys
from time import sleep
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
filein = open("allsnippets.txt","w");filein.close()
filein = open("allsnippets.txt","a")
rows = c.execute('SELECT rowid, * FROM snippet') 
for row in rows:
    #sleep(1)
    #print row[2]
    filein.write(row[2])

import sqlite3
import sys
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, * FROM snippet WHERE keywords MATCH ?', (search,)):    
    count=count+1
    print "ID : ",(row)[0],(row)[2]," -- KEYWORDS",(row)[3],"\n"
    if count > req:
        conn.close()
        sys.exit()

import sqlite3
import sys
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, * FROM snippet WHERE snippet MATCH ?', (search,)):    
    count=count+1
    print "ID : ",(row)[0],(row)[2]," -- KEYWORDS",(row)[3],"\n"
    if count > req:
        conn.close()
        sys.exit()
        
------------------        

import sqlite3
import base64
conn = sqlite3.connect('/home/jack/snippet.db') 
c = conn.cursor()
conn.text_factory = str
file = """
import json
import sys
from time import sleep
import sqlite3
import csv
conn = sqlite3.connect('notebooks.db')
conn.text_factory=str 
c = conn.cursor()
#c.execute("DELETE title, line FROM ipynb where rowid MATCH '362113' "):
#c.execute("delete from ipynb where rowid=362113;"):
c.execute("DELETE FROM ipynb WHERE rowid = ?", (362113,))
conn.commit()
conn.close()
        
"""
keywords = "delete sqlite by rowid delete id rowid ROWID"
encodedlistvalue=base64.b64encode(file)
c.execute("INSERT INTO snippet VALUES (?,?,?)", (encodedlistvalue, file, keywords))
conn.commit()
conn.close()

%%writefile UnicodeToAscii.py
def unicodetoascii(text):

    uni2ascii = {
            ord('\xe2\x80\x99'.decode('utf-8')): ord("'"),
            ord('\xe2\x80\x9c'.decode('utf-8')): ord('"'),
            ord('\xe2\x80\x9d'.decode('utf-8')): ord('"'),
            ord('\xe2\x80\x9e'.decode('utf-8')): ord('"'),
            ord('\xe2\x80\x9f'.decode('utf-8')): ord('"'),
            ord('\xc3\xa9'.decode('utf-8')): ord('e'),
            ord('\xe2\x80\x9c'.decode('utf-8')): ord('"'),
            ord('\xe2\x80\x93'.decode('utf-8')): ord('-'),
            ord('\xe2\x80\x92'.decode('utf-8')): ord('-'),
            ord('\xe2\x80\x94'.decode('utf-8')): ord('-'),
            ord('\xe2\x80\x94'.decode('utf-8')): ord('-'),
            ord('\xe2\x80\x98'.decode('utf-8')): ord("'"),
            ord('\xe2\x80\x9b'.decode('utf-8')): ord("'"),

            ord('\xe2\x80\x90'.decode('utf-8')): ord('-'),
            ord('\xe2\x80\x91'.decode('utf-8')): ord('-'),

            ord('\xe2\x80\xb2'.decode('utf-8')): ord("'"),
            ord('\xe2\x80\xb3'.decode('utf-8')): ord("'"),
            ord('\xe2\x80\xb4'.decode('utf-8')): ord("'"),
            ord('\xe2\x80\xb5'.decode('utf-8')): ord("'"),
            ord('\xe2\x80\xb6'.decode('utf-8')): ord("'"),
            ord('\xe2\x80\xb7'.decode('utf-8')): ord("'"),

            ord('\xe2\x81\xba'.decode('utf-8')): ord("+"),
            ord('\xe2\x81\xbb'.decode('utf-8')): ord("-"),
            ord('\xe2\x81\xbc'.decode('utf-8')): ord("="),
            ord('\xe2\x81\xbd'.decode('utf-8')): ord("("),
            ord('\xe2\x81\xbe'.decode('utf-8')): ord(")"),

                            }
    return text.decode('utf-8').translate(uni2ascii).encode('ascii')

#print unicodetoascii("weren\xe2\x80\x99t")  

import UnicodeToAscii
UnicodeToAscii.unicodetoascii()

import UnicodeToAscii
text = """
'\xe2\x81\xbafd fdfdf df \xe2\x81\xbb decode \xe2\x81\xbc code 'utf-8\xe2\x81\xbd' 
code'\xe2\x81\xbe'.decode('utf-8'))"""
UnicodeToAscii.unicodetoascii(text)

import sqlite3
import sys
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, * FROM snippet WHERE snippet MATCH ?', (search,)):    
    count=count+1
    print "ID : ",(row)[0],(row)[2]," -- KEYWORDS",(row)[3],"\n"
    if count > req:
        conn.close()
        sys.exit()

import sqlite3
import base64
conn = sqlite3.connect('/home/jack/snippet.db') 
c = conn.cursor()
conn.text_factory = str
file = """

"""
keywords = """

"""
encodedlistvalue=base64.b64encode(file)
c.execute("INSERT INTO snippet VALUES (?,?,?)", (encodedlistvalue, file, keywords))
conn.commit()
conn.close()

import os
import sys
from PIL import Image
import shutil
import time
import random

filename0=("images/sun002.png")
#filename1=("/home/jack/Desktop/imagebot/instagram/20170828173214.png")
filename1=("/home/jack/Desktop/Processing/Processing/images/experiment000a.png")
shutil.copy2(filename0, 'copies/') # complete target filename given
shutil.copy2(filename1, 'copies/')# target filename is /dst/dir/file.ext

aa = Image.open(filename0).convert("RGB")
#bb = Image.open("/home/jack/Documents/GG.jpg").convert("RGB")
bb = Image.open(filename1).convert("RGB")
xx=aa.resize((640,640), Image.NEAREST)
yy=bb.resize((640,640), Image.NEAREST)
xx.save("copies/aa.png")
yy.save("copies/bb.png")
src = Image.open('copies/aa.png').convert('RGB')
dst = Image.open('copies/bb.png').convert('RGB')
src.save("copies/aa.png")
dst.save("copies/bb.png")



n = 5 #number of partitions per channel.


src_handle = Image.open("copies/bb.png")
dst_handle = Image.open("copies/aa.png")
src = src_handle.load()
dst = dst_handle.load()
assert src_handle.size[0]*src_handle.size[1] == dst_handle.size[0]*dst_handle.size[1],"images must be same size"

def makePixelList(img):
    l = []
    for x in range(img.size[0]):
        for y in range(img.size[1]):
            l.append((x,y))
    return l

lsrc = makePixelList(src_handle)
ldst = makePixelList(dst_handle)

def sortAndDivide(coordlist,pixelimage,channel): #core
    global src,dst,n
    retlist = []
    #sort
    coordlist.sort(key=lambda t: pixelimage[t][channel])
    #divide
    partitionLength = int(len(coordlist)/n)
    if partitionLength <= 0:
        partitionLength = 1
    if channel < 2:
        for i in range(0,len(coordlist),partitionLength):
            retlist += sortAndDivide(coordlist[i:i+partitionLength],pixelimage,channel+1)
    else:
        retlist += coordlist
    return retlist

print(src[lsrc[0]])

lsrc = sortAndDivide(lsrc,src,0)
ldst = sortAndDivide(ldst,dst,0)

for i in range(len(ldst)):
    dst[ldst[i]] = src[lsrc[i]]
    
    

filename = time.strftime("images/exchange%Y%m%d%H%M%S.png")
dst_handle.save(filename)

shutil.copy2(filename, "copies/")
print filename

# %load titlenpost.py
#!/home/jack/anaconda2/python
import random
from random import randint
import time
import markovify
import os
import sys
import Key
sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
nap = randint(100,635)
time.sleep(nap)
path = r"output/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)
def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_blurred_back(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))


if __name__ == '__main__':
    inp = Image.open(filename0)
    font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 30)
    text_title = (255, 255,230) # bright green
    blur_title = (0, 0, 0)   # black
    #textin = (generate_the_word("wordcloud.txt"))
    i2 = draw_blurred_back(inp, (15, 4), "Python Generated Art", font, text_title, blur_title)
    
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
    # get a drawing context
    signature_ = "@Jacknorthrup Instagram and @jacklnorthrup TwitterBot Project" 
    #get length in pixel of signature_
    sizeS,ln = fnt.getsize(signature_)
    #add 15 pixels to right border
    pt = sizeS+12
    width, height = inp.size
    #marginx starting point of signature_
    marginx = pt
    #bottom margin
    marginy = 35
    x = width - marginx
    y = height - marginy    
    text_sig = (255, 255,230) # bright green
    blur_sig = (0, 0, 0)   # black
    txt=draw_blurred_back(i2,(x,y), signature_, fnt, text_sig, blur_sig)
    out = Image.alpha_composite(i2, txt)
    out.save("tmp/TM_POST1.jpg")


#removed keys for privacy reasons
CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
f = open("/home/jack/Desktop/imagebot/codetalk.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
STR = (text_model.make_short_sentence(140))
#STR = ("#All_in_One - #WordCloud #Create - Added ability to randomly choose an image background  #Automated")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "tmp/TM_POST1.jpg"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

#%%writefile titlenpost.py
#!/home/jack/anaconda2/python
import random
from random import randint
import time
import markovify
import os
import sys
import Key
sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
#nap = randint(100,635)
#time.sleep(nap)
path = r"output/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
#filename0=(path+base_image)
filename0 ="Images/segmented20171201092551.png"

def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_blurred_back(img, position, text, font, col, halo_col):
    halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))


if __name__ == '__main__':
    inp = Image.open(filename0)
    #fnt = "/home/jack/.fonts/dontmix.ttf"
    fnt = "/home/jack/.fonts/Exo-Black.ttf"
    font = ImageFont.truetype(fnt, 30)
    text_title = (255, 255,230) # bright green
    blur_title = (0, 0, 0)   # black
    #textin = (generate_the_word("wordcloud.txt"))
    #i2 = draw_blurred_back(inp, (15, 4), "Python Generated Art", font, text_title, blur_title)
    i2 = draw_blurred_back(inp, (45, 35), "Python Generated Art", font, text_title, blur_title)
    
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = "/home/jack/.fonts/dontmix.ttf"
    #fnt = "/home/jack/.fonts/Exo-Black.ttf"    
    
    #fnt = ImageFont.truetype(fnt, 20)
    fnt = ImageFont.truetype(fnt, 24)
    # get a drawing context
    signature_ = "@Jacknorthrup Instagram and @jacklnorthrup TwitterBot Project" 
    #get length in pixel of signature_
    sizeS,ln = fnt.getsize(signature_)
    #add 15 pixels to right border
    pt = sizeS+12
    width, height = inp.size
    #marginx starting point of signature_
    marginx = pt
    #bottom margin
    #marginy = 35
    marginy = 55
    x = width - marginx
    y = height - marginy    
    text_sig = (255, 255,230) # bright green
    blur_sig = (0, 0, 0)   # black
    txt=draw_blurred_back(i2,(x,y), signature_, fnt, text_sig, blur_sig)
    out = Image.alpha_composite(i2, txt)
    out.save("tmp/TM_POST1.jpg")


#removed keys for privacy reasons
CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
f = open("/home/jack/Desktop/imagebot/codetalk.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
STR = (text_model.make_short_sentence(140))
#STR = " #Python Segmentation Art"
#STR = ("#All_in_One - #WordCloud #Create - Added ability to randomly choose an image background  #Automated")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "tmp/TM_POST1.jpg"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])

!showme tmp/TM_POST1.jpg

!ls /home/jack/Desktop/post

!ls tmp

!showme /home/jack/Desktop/PROCESSING/color-wander/images/kk.png

!cp /home/jack/Desktop/DOCKER/deepdream/paper/244.jpg images/use.jpg

!mkdir junk2

import sys
from PIL import Image
import shutil
import time
import random
import os, errno
try:
    os.makedirs("instagram/")
except OSError as e:
    if e.errno != errno.EEXIST:
        raise
try:
    os.makedirs("junk2/")
except OSError as e:
    if e.errno != errno.EEXIST:
        raise        
filename1='images/girl.jpg'
filename0='images/use.jpg'
shutil.copy2(filename0, 'instagram/') # complete target filename given
shutil.copy2(filename1, 'instagram/')# target filename is /dst/dir/file.ext

aa = Image.open(filename0).convert("RGB")
#bb = Image.open("/home/jack/Documents/GG.jpg").convert("RGB")
bb = Image.open(filename1).convert("RGB")
xx=aa.resize((640,640), Image.NEAREST)
yy=bb.resize((640,640), Image.NEAREST)
xx.save("junk2/aa.png")
yy.save("junk2/bb.png")
src = Image.open('junk2/aa.png').convert('RGB')
dst = Image.open('junk2/bb.png').convert('RGB')
src.save("junk2/aa.png")
dst.save("junk2/bb.png")
n = 5 #number of partitions per channel.
src_handle = Image.open("junk2/bb.png")
dst_handle = Image.open("junk2/aa.png")
src = src_handle.load()
dst = dst_handle.load()
assert src_handle.size[0]*src_handle.size[1] == dst_handle.size[0]*dst_handle.size[1],"images must be same size"

def makePixelList(img):
    l = []
    for x in range(img.size[0]):
        for y in range(img.size[1]):
            l.append((x,y))
    return l

lsrc = makePixelList(src_handle)
ldst = makePixelList(dst_handle)

def sortAndDivide(coordlist,pixelimage,channel): #core
    global src,dst,n
    retlist = []
    #sort
    coordlist.sort(key=lambda t: pixelimage[t][channel])
    #divide
    partitionLength = int(len(coordlist)/n)
    if partitionLength <= 0:
        partitionLength = 1
    if channel < 2:
        for i in range(0,len(coordlist),partitionLength):
            retlist += sortAndDivide(coordlist[i:i+partitionLength],pixelimage,channel+1)
    else:
        retlist += coordlist
    return retlist

print(src[lsrc[0]])

lsrc = sortAndDivide(lsrc,src,0)
ldst = sortAndDivide(ldst,dst,0)

for i in range(len(ldst)):
    dst[ldst[i]] = src[lsrc[i]]
    
    
filename = time.strftime("junk2/PalletteTemp.jpg")

dst_handle.save(filename)

shutil.copy2(filename, "instagram/")
print (filename)

!showme junk2/PalletteTemp.jpg


!showme junk2/aa.png

# Simple bijective function
#   Basically encodes any integer into a base(n) string,
#     where n is ALPHABET.length.
#   Based on pseudocode from http://stackoverflow.com/questions/742013/how-to-code-a-url-shortener/742047#742047

ALPHABET = list("abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789")
  # make your own alphabet using:
  # (('a'..'z').to_a + ('A'..'Z').to_a + (0..9).to_a).shuffle.join

def bijective_encode(i):
    # from http://refactormycode.com/codes/125-base-62-encoding
    if i == 0:
        return ALPHABET[0]
    s = ''
    base = len(ALPHABET)
    while i > 0:
        s += ALPHABET[i % base]
        i /= base
    return s[::-1] # reverse string


def bijective_decode(s):
    # based on base2dec() in Tcl translation 
    # at http://rosettacode.org/wiki/Non-decimal_radices/Convert#Ruby
    i = 0
    base = len(ALPHABET)
    for char in s:
        i = i * base + ALPHABET.index(char)
    return i

# Two little demos:

numbers = 1234567890
result = bijective_encode(numbers)
print result #xyz

letters = 'Jack Northrup'
new_number = bijective_decode(letters)
print new_number #66

import pickledb
db = pickledb.load('example.db', False)


tx = db.get('key')
print tx

from skimage import graph, data, io, segmentation, color
from skimage import future 
import skimage,os
from PIL import Image
import cv2
from matplotlib import pyplot as plt



#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
logging.getLogger("tensorflow").setLevel(logging.DEBUG)

import tensorflow as tf
from tensorflow import keras
import numpy as np
import pathlib

# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = keras.Sequential([
  keras.layers.InputLayer(input_shape=(28, 28)),
  keras.layers.Reshape(target_shape=(28, 28, 1)),
  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),
  keras.layers.MaxPooling2D(pool_size=(2, 2)),
  keras.layers.Flatten(),
  keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam',
              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=1,
  validation_data=(test_images, test_labels)
)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

tflite_models_dir = pathlib.Path("/tmp/mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)

tflite_model_file = tflite_models_dir/"mnist_model.tflite"
tflite_model_file.write_bytes(tflite_model)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

tflite_fp16_model = converter.convert()
tflite_model_fp16_file = tflite_models_dir/"mnist_model_quant_f16.tflite"
tflite_model_fp16_file.write_bytes(tflite_fp16_model)

!ls -lh {tflite_models_dir}

interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))
interpreter.allocate_tensors()

interpreter_fp16 = tf.lite.Interpreter(model_path=str(tflite_model_fp16_file))
interpreter_fp16.allocate_tensors()

test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)

input_index = interpreter.get_input_details()[0]["index"]
output_index = interpreter.get_output_details()[0]["index"]

interpreter.set_tensor(input_index, test_image)
interpreter.invoke()
predictions = interpreter.get_tensor(output_index)

import matplotlib.pylab as plt

plt.imshow(test_images[0])
template = "True:{true}, predicted:{predict}"
_ = plt.title(template.format(true= str(test_labels[0]),
                              predict=str(np.argmax(predictions[0]))))
plt.grid(False)

test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)

input_index = interpreter_fp16.get_input_details()[0]["index"]
output_index = interpreter_fp16.get_output_details()[0]["index"]

interpreter_fp16.set_tensor(input_index, test_image)
interpreter_fp16.invoke()
predictions = interpreter_fp16.get_tensor(output_index)

plt.imshow(test_images[0])
template = "True:{true}, predicted:{predict}"
_ = plt.title(template.format(true= str(test_labels[0]),
                              predict=str(np.argmax(predictions[0]))))
plt.grid(False)

# A helper function to evaluate the TF Lite model using "test" dataset.
def evaluate_model(interpreter):
  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]

  # Run predictions on every image in the "test" dataset.
  prediction_digits = []
  for test_image in test_images:
    # Pre-processing: add batch dimension and convert to float32 to match with
    # the model's input data format.
    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)
    interpreter.set_tensor(input_index, test_image)

    # Run inference.
    interpreter.invoke()

    # Post-processing: remove batch dimension and find the digit with highest
    # probability.
    output = interpreter.tensor(output_index)
    digit = np.argmax(output()[0])
    prediction_digits.append(digit)

  # Compare prediction results with ground truth labels to calculate accuracy.
  accurate_count = 0
  for index in range(len(prediction_digits)):
    if prediction_digits[index] == test_labels[index]:
      accurate_count += 1
  accuracy = accurate_count * 1.0 / len(prediction_digits)

  return accuracy

print(evaluate_model(interpreter))

# NOTE: Colab runs on server CPUs. At the time of writing this, TensorFlow Lite
# doesn't have super optimized server CPU kernels. For this reason this may be
# slower than the above float interpreter. But for mobile CPUs, considerable
# speedup can be observed.
print(evaluate_model(interpreter_fp16))

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
logging.getLogger("tensorflow").setLevel(logging.DEBUG)

import tensorflow as tf
from tensorflow import keras
import numpy as np
import pathlib

tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8

# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = keras.Sequential([
  keras.layers.InputLayer(input_shape=(28, 28)),
  keras.layers.Reshape(target_shape=(28, 28, 1)),
  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),
  keras.layers.MaxPooling2D(pool_size=(2, 2)),
  keras.layers.Flatten(),
  keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam',
              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=1,
  validation_data=(test_images, test_labels)
)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

tflite_models_dir = pathlib.Path("/tmp/mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)

tflite_model_file = tflite_models_dir/"mnist_model.tflite"
tflite_model_file.write_bytes(tflite_model)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]

mnist_train, _ = tf.keras.datasets.mnist.load_data()
images = tf.cast(mnist_train[0], tf.float32) / 255.0
mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)
def representative_data_gen():
  for input_value in mnist_ds.take(100):
    # Model has only one input so each data point has one element.
    yield [input_value]
converter.representative_dataset = representative_data_gen

tflite_16x8_model = converter.convert()
tflite_model_16x8_file = tflite_models_dir/"mnist_model_quant_16x8.tflite"
tflite_model_16x8_file.write_bytes(tflite_16x8_model)

!ls -lh {tflite_models_dir}

interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))
interpreter.allocate_tensors()

interpreter_16x8 = tf.lite.Interpreter(model_path=str(tflite_model_16x8_file))
interpreter_16x8.allocate_tensors()

test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)

input_index = interpreter.get_input_details()[0]["index"]
output_index = interpreter.get_output_details()[0]["index"]

interpreter.set_tensor(input_index, test_image)
interpreter.invoke()
predictions = interpreter.get_tensor(output_index)

import matplotlib.pylab as plt

plt.imshow(test_images[0])
template = "True:{true}, predicted:{predict}"
_ = plt.title(template.format(true= str(test_labels[0]),
                              predict=str(np.argmax(predictions[0]))))
plt.grid(False)

test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)

input_index = interpreter_16x8.get_input_details()[0]["index"]
output_index = interpreter_16x8.get_output_details()[0]["index"]

interpreter_16x8.set_tensor(input_index, test_image)
interpreter_16x8.invoke()
predictions = interpreter_16x8.get_tensor(output_index)

plt.imshow(test_images[0])
template = "True:{true}, predicted:{predict}"
_ = plt.title(template.format(true= str(test_labels[0]),
                              predict=str(np.argmax(predictions[0]))))
plt.grid(False)

# A helper function to evaluate the TF Lite model using "test" dataset.
def evaluate_model(interpreter):
  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]

  # Run predictions on every image in the "test" dataset.
  prediction_digits = []
  for test_image in test_images:
    # Pre-processing: add batch dimension and convert to float32 to match with
    # the model's input data format.
    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)
    interpreter.set_tensor(input_index, test_image)

    # Run inference.
    interpreter.invoke()

    # Post-processing: remove batch dimension and find the digit with highest
    # probability.
    output = interpreter.tensor(output_index)
    digit = np.argmax(output()[0])
    prediction_digits.append(digit)

  # Compare prediction results with ground truth labels to calculate accuracy.
  accurate_count = 0
  for index in range(len(prediction_digits)):
    if prediction_digits[index] == test_labels[index]:
      accurate_count += 1
  accuracy = accurate_count * 1.0 / len(prediction_digits)

  return accuracy

print(evaluate_model(interpreter))

# NOTE: This quantization mode is an experimental post-training mode,
# it does not have any optimized kernels implementations or
# specialized machine learning hardware accelerators. Therefore,
# it could be slower than the float interpreter.
print(evaluate_model(interpreter_16x8))

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
logging.getLogger("tensorflow").setLevel(logging.DEBUG)

import tensorflow as tf
import numpy as np
print("TensorFlow version: ", tf.__version__)

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0

# Define the model architecture
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(28, 28)),
  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(
                  from_logits=True),
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=5,
  validation_data=(test_images, test_labels)
)

converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite_model = converter.convert()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model_quant = converter.convert()

def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    # Model has only one input so each data point has one element.
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

tflite_model_quant = converter.convert()

interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)
input_type = interpreter.get_input_details()[0]['dtype']
print('input: ', input_type)
output_type = interpreter.get_output_details()[0]['dtype']
print('output: ', output_type)

def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
# Ensure that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# Set the input and output tensors to uint8 (APIs added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_model_quant = converter.convert()

interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)
input_type = interpreter.get_input_details()[0]['dtype']
print('input: ', input_type)
output_type = interpreter.get_output_details()[0]['dtype']
print('output: ', output_type)

import pathlib

tflite_models_dir = pathlib.Path("/tmp/mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)

# Save the unquantized/float model:
tflite_model_file = tflite_models_dir/"mnist_model.tflite"
tflite_model_file.write_bytes(tflite_model)
# Save the quantized model:
tflite_model_quant_file = tflite_models_dir/"mnist_model_quant.tflite"
tflite_model_quant_file.write_bytes(tflite_model_quant)

# Helper function to run inference on a TFLite model
def run_tflite_model(tflite_file, test_image_indices):
  global test_images

  # Initialize the interpreter
  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()[0]
  output_details = interpreter.get_output_details()[0]

  predictions = np.zeros((len(test_image_indices),), dtype=int)
  for i, test_image_index in enumerate(test_image_indices):
    test_image = test_images[test_image_index]
    test_label = test_labels[test_image_index]

    # Check if the input type is quantized, then rescale input data to uint8
    if input_details['dtype'] == np.uint8:
      input_scale, input_zero_point = input_details["quantization"]
      test_image = test_image / input_scale + input_zero_point

    test_image = np.expand_dims(test_image, axis=0).astype(input_details["dtype"])
    interpreter.set_tensor(input_details["index"], test_image)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details["index"])[0]

    predictions[i] = output.argmax()

  return predictions


import matplotlib.pylab as plt

# Change this to test a different image
test_image_index = 1

## Helper function to test the models on one image
def test_model(tflite_file, test_image_index, model_type):
  global test_labels

  predictions = run_tflite_model(tflite_file, [test_image_index])

  plt.imshow(test_images[test_image_index])
  template = model_type + " Model \n True:{true}, Predicted:{predict}"
  _ = plt.title(template.format(true= str(test_labels[test_image_index]), predict=str(predictions[0])))
  plt.grid(False)

test_model(tflite_model_file, test_image_index, model_type="Float")

test_model(tflite_model_quant_file, test_image_index, model_type="Quantized")

# Helper function to evaluate a TFLite model on all images
def evaluate_model(tflite_file, model_type):
  global test_images
  global test_labels

  test_image_indices = range(test_images.shape[0])
  predictions = run_tflite_model(tflite_file, test_image_indices)

  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)

  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (
      model_type, accuracy, len(test_images)))

evaluate_model(tflite_model_file, model_type="Float")

evaluate_model(tflite_model_quant_file, model_type="Quantized")

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
logging.getLogger("tensorflow").setLevel(logging.DEBUG)

import tensorflow as tf
from tensorflow import keras
import numpy as np
import pathlib

# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = keras.Sequential([
  keras.layers.InputLayer(input_shape=(28, 28)),
  keras.layers.Reshape(target_shape=(28, 28, 1)),
  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),
  keras.layers.MaxPooling2D(pool_size=(2, 2)),
  keras.layers.Flatten(),
  keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam',
              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=1,
  validation_data=(test_images, test_labels)
)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

tflite_models_dir = pathlib.Path("/tmp/mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)

tflite_model_file = tflite_models_dir/"mnist_model.tflite"
tflite_model_file.write_bytes(tflite_model)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
tflite_model_quant_file = tflite_models_dir/"mnist_model_quant.tflite"
tflite_model_quant_file.write_bytes(tflite_quant_model)

!ls -lh {tflite_models_dir}

interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))
interpreter.allocate_tensors()

interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))
interpreter_quant.allocate_tensors()

test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)

input_index = interpreter.get_input_details()[0]["index"]
output_index = interpreter.get_output_details()[0]["index"]

interpreter.set_tensor(input_index, test_image)
interpreter.invoke()
predictions = interpreter.get_tensor(output_index)

import matplotlib.pylab as plt

plt.imshow(test_images[0])
template = "True:{true}, predicted:{predict}"
_ = plt.title(template.format(true= str(test_labels[0]),
                              predict=str(np.argmax(predictions[0]))))
plt.grid(False)

# A helper function to evaluate the TF Lite model using "test" dataset.
def evaluate_model(interpreter):
  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]

  # Run predictions on every image in the "test" dataset.
  prediction_digits = []
  for test_image in test_images:
    # Pre-processing: add batch dimension and convert to float32 to match with
    # the model's input data format.
    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)
    interpreter.set_tensor(input_index, test_image)

    # Run inference.
    interpreter.invoke()

    # Post-processing: remove batch dimension and find the digit with highest
    # probability.
    output = interpreter.tensor(output_index)
    digit = np.argmax(output()[0])
    prediction_digits.append(digit)

  # Compare prediction results with ground truth labels to calculate accuracy.
  accurate_count = 0
  for index in range(len(prediction_digits)):
    if prediction_digits[index] == test_labels[index]:
      accurate_count += 1
  accuracy = accurate_count * 1.0 / len(prediction_digits)

  return accuracy

print(evaluate_model(interpreter))

print(evaluate_model(interpreter_quant))

import tensorflow_hub as hub

resnet_v2_101 = tf.keras.Sequential([
  keras.layers.InputLayer(input_shape=(224, 224, 3)),
  hub.KerasLayer("https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4")
])

converter = tf.lite.TFLiteConverter.from_keras_model(resnet_v2_101)

# Convert to TF Lite without quantization
resnet_tflite_file = tflite_models_dir/"resnet_v2_101.tflite"
resnet_tflite_file.write_bytes(converter.convert())

# Convert to TF Lite with quantization
converter.optimizations = [tf.lite.Optimize.DEFAULT]
resnet_quantized_tflite_file = tflite_models_dir/"resnet_v2_101_quantized.tflite"
resnet_quantized_tflite_file.write_bytes(converter.convert())

!ls -lh {tflite_models_dir}/*.tflite

text_a = open("1342-0.txt").read()
text_b = open("84-0.txt").read()

print(text_a[:200])

import random
random.sample(text_b, 20)

a_words = text_a.split()
b_words = text_b.split()

random.sample(a_words, 10)

random.sample(b_words, 10)

from collections import Counter
Counter(text_a).most_common(12)

Counter(a_words).most_common(12)

Counter(b_words).most_common(12)

import sys
!{sys.executable} -m pip install markovify

import markovify

generator_a = markovify.Text(text_a)

print(generator_a.make_sentence())

print(generator_a.make_short_sentence(50))

print(generator_a.make_short_sentence(40, tries=100))

print(generator_a.make_short_sentence(40, test_output=False))

gen_a_1 = markovify.Text(text_a, state_size=1)
gen_a_4 = markovify.Text(text_a, state_size=4)

print("order 1")
print(gen_a_1.make_sentence(test_output=False))
print()
print("order 4")
print(gen_a_4.make_sentence(test_output=False))

class SentencesByChar(markovify.Text):
    def word_split(self, sentence):
        return list(sentence)
    def word_join(self, words):
        return "".join(words)

con_model = SentencesByChar("condescendences", state_size=2)

con_model.make_sentence()

gen_a_char = SentencesByChar(text_a, state_size=7)

print(gen_a_char.make_sentence(test_output=False).replace("\n", " "))

generator_a = markovify.Text(text_a)
generator_b = markovify.Text(text_b)
combo = markovify.combine([generator_a, generator_b], [0.5, 0.5])

print(combo.make_sentence())

# change to "word" for a word-level model
level = "char"
# controls the length of the n-gram
order = 7
# controls the number of lines to output
output_n = 14
# weights between the models; text A first, text B second.
# if you want to completely exclude one model, set its corresponding value to 0
weights = [0.5, 0.5]
# limit sentence output to this number of characters
length_limit = 280

model_cls = markovify.Text if level == "word" else SentencesByChar
gen_a = model_cls(text_a, state_size=order)
gen_b = model_cls(text_b, state_size=order)
gen_combo = markovify.combine([gen_a, gen_b], weights)
for i in range(output_n):
    out = gen_combo.make_short_sentence(length_limit, test_output=False)
    out = out.replace("\n", " ")
    print(out)
    print()

sonnets_text = open("sonnets.txt").read()
sonnets_model = markovify.NewlineText(sonnets_text, state_size=1)

sonnets_model.make_sentence()

for i in range(14):
    print(sonnets_model.make_sentence())

class LinesByChar(markovify.NewlineText):
    def word_split(self, sentence):
        return list(sentence)
    def word_join(self, words):
        return "".join(words)

sonnets_char_model = LinesByChar(sonnets_text, state_size=4)

for i in range(14):
    print(sonnets_char_model.make_sentence())

import json
mood_data = json.loads(open("./moods.json").read())
moods = mood_data['moods']

moods_text = "\n".join(moods)

moods_char_model = LinesByChar(moods_text, state_size=3)

for i in range(24):
    print(moods_char_model.make_sentence())

import math

import numpy as np

import vsketch

N = 100
RANDOM_PHASE = False


def get_primes(n):
    numbers = set(range(n, 1, -1))
    primes = []
    while numbers:
        p = numbers.pop()
        primes.append(p)
        numbers.difference_update(set(range(p * 2, n + 1, p)))
    return primes


vsk = vsketch.Vsketch()
vsk.size("10in", "10in")
vsk.scale("3mm")

for i, prime in enumerate(get_primes(N)):
    vsk.circle(0, 0, 2 * (i + 1))

    if RANDOM_PHASE:
        phase = np.random.random() * 2 * math.pi
    else:
        phase = -math.pi / 2

    for angle in np.linspace(0, 2 * math.pi, prime, endpoint=False):
        vsk.line(
            (i + 1) * math.cos(angle + phase),
            (i + 1) * math.sin(angle + phase),
            (i + 2) * math.cos(angle + phase),
            (i + 2) * math.sin(angle + phase),
        )

vsk.circle(0, 0, 2 * (i + 2))
vsk.vpype("linemerge linesort")


vsk.display(mode="matplotlib")
vsk.save("prime_circles.svg")

#%%writefile use-TWITTERBOT-choose-and-sign.py
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
# Open background image and work out centre

path = r"publish/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)


bg = Image.open(filename0).convert('RGB')
x = bg.width//2
y = bg.height//2

# The text we want to add
text = "NFT TwitterBot Project"

# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))
src_path = filename0
dst_path = "Processed-Images/"
shutil.move(src_path, dst_path)
# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
#xx=0
#yy=0
#bg.paste("peferations.png", (xx,yy)) 
# paste an onerlay image
#perferations.png
#mask=Image.open("perferations.png").convert('RGBA') 
#bg.paste(mask, (x,y), mask=mask)
#bg.paste("peferations.png", box=(0, 0) + original.size) 
bg.save('result.png')
timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "records/"+timestr+"_.png"
bg.save(filename)
im =Image.open('result.png')
im



#nap=randint(10,400)
#time.sleep(nap)
path = r"publish/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename1=(path+base_image)

bg = Image.open(filename1).convert('RGB')
x = bg.width//2
y = bg.height//2
src_path = filename1
dst_path = "Processed-Images/"
shutil.move(src_path, dst_path)
# The text we want to add
text = "NFT TwitterBot Project"


# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
mask=Image.open("perferations.png").convert('RGBA') 
bg.paste(mask, (0,0), mask=mask)
bg.save('result.png')
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
with open("sart.txt") as f:
    data = f.read()
data_model = markovify.Text(data)
STR = data_model.make_sentence()
#STR = ("#All_in_One - #WordCloud #Create - Added ability to randomly choose an image background  #Automated")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
#PATH = "images/TM_POST.png"
PATH = "result.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops 
import os
import sys
import markovify
import twython
from twython import Twython
import time

%%writefile ImageBot
#!/bin/bash

while true; do
  python ImageBot.py
  echo "posted :"
  date
  sleep 1800s
done

!ls A*


#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
# Open background image and work out centre

path = r"publish/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)


bg = Image.open(filename0).convert('RGB')
x = bg.width//2
y = bg.height//2

# The text we want to add
text = "NFT TwitterBot Project"

# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
# Creates a dark text shadow for the white text so the text will show even on a light image
blurred = blurred.filter(ImageFilter.BoxBlur(7))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
#xx=0
#yy=0
#bg.paste("peferations.png", (xx,yy)) 
# paste an onerlay image
#perferations.png
#mask=Image.open("perferations.png").convert('RGBA') 
#bg.paste(mask, (x,y), mask=mask)
#bg.paste("peferations.png", box=(0, 0) + original.size) 
bg.save('result.png')
timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "records/"+timestr+"_.png"
bg.save(filename)
im =Image.open('result.png')
im



#nap=randint(10,400)
#time.sleep(nap)
path = r"publish/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)

bg = Image.open(filename0).convert('RGB')
x = bg.width//2
y = bg.height//2

# The text we want to add
text = "NFT TwitterBot Project"


# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
mask=Image.open("perferations.png").convert('RGBA') 
bg.paste(mask, (0,0), mask=mask)
bg.save('result.png')
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
with open("sart.txt") as f:
    data = f.read()
data_model = markovify.Text(data)
STR = data_model.make_sentence()
#STR = ("#All_in_One - #WordCloud #Create - Added ability to randomly choose an image background  #Automated")
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
#PATH = "images/TM_POST.png"
PATH = "result.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


with open("sart.txt") as f:
    data = f.read()

    data_model = markovify.Text(data)
STR = data_model.make_sentence()

print(STR)

print("----------------------------")

# Print a randomly-generated sentences of no more than 280 characters
print(data_model.make_short_sentence(280))



!locate *.ttf

# Text file to 

%%writefile sart.txt
Alice in Wonderland (along with Through the Looking Glass) is one of the most famous children’s books of all time. It has been made into an iconic Disney film, as well as a recent Tim Burton release and countless other adaptations. The story of a young girl falling down a rabbit-hole and entering a strange, surreal world where nothing quite makes sense captures that childhood state when rules are not yet known and the imagination is as powerful as reality.

Written by Lewis Carroll, Alice in Wonderland is a classic in a children’s literary genre known as ‘nonsense’. Nonsense literature presents language and situations which are not normal. In English, this is a genre that rose to prominence in Victorian England, where literature and books were beginning to take on an ever-greater importance in the childhood experience of growing up.

Many examples from the books show Lewis Carroll’s ability to create a sense of the uncanny. The characters regularly show no respect for the basic rules of language. ‘When I use a word’, announces Humpty Dumpty, ‘it means just what I choose it to’. Humpty Dumpty has other views on words, telling Alice that ‘they’ve a temper, some of them—particularly verbs, they’re the proudest—adjectives you can do anything with, but not verbs’. The novel features a famous poem, The Jabberwocky, which features dozens of made-up words, as in the immortal line ‘and the mome raths outgrabe’. The strange sounds of these new words take the reader back to a time when every sound was something new and bizarre.

It is not just language that is played around with. The very laws of physics are upside down. A memorable scene in Through the Looking-Glass depicts Alice and the Red Queen running as fast as they can. When Alice asks where they are running to, the Red Queen scolds Alice, and explains that they are running merely to stay in the same place. When Alice enquires how they might go about actually getting somewhere else, the Red Queen explains that they’d have to run twice as fast.  Of course, as they are already at full pace, this makes no sense whatsoever. Time is also a source of nonsense. At the Tea Party, the Mad Hatter explains that his watch is ‘exactly two days slow’. This means, of course, that his watch is telling exactly the right time as it would be if it were on time.

It is easy to see these examples of nonsense as nothing more than childish fantasy. Yet there is more to nonsense than non-sense.  Lewis Carroll was a famous mathematician and many of his seemingly childish ideas draw on complex ideas of the nature of language, truth and logic. There are political aspects to his nonsense. Alice is told the story of The Walrus and the Carpenter, in which oysters are tricked by the two main characters and then eaten. The Walrus speaks a great deal of nonsense in order to ignore the protests of the oysters. In the Walrus and the Carpenter, nonsense becomes a tool used by the powerful to bewilder and exploit the weak and helpless.

Alice in Wonderland is originally a children’s story, but its meaning, especially its use of nonsense, goes far beyond this. Adults have enjoyed the novel for over a century. It is nonsense that is the key to its continued success, allowing the reader to shake off the rules and shapes of normal life, and return to the unlimited and eternally baffling visions of a half-forgotten childhood.

%%writefile titles.txt
Python Fun
Python Graphics
Generator
Word Cloud
Graphics
Fun w/Python
Python Stuff
PYTHON !!!
Love`en Python
Creative Python
Graphic Fun
Twitter Post
Fun Images
Jupyter Notebook

 List="""ancient%20manuscript%20art/
 animal%20eyes/
 antique%20art/
 antique%20book%20covers/
 antique%20tools/
 art%20nouveau/
 Australian%20Lizards/
 AutoImageCrawler/
 binary_images/
 black%20and%20white%20art%20nouveau%20drawings/
 Black%20Swan/
 Blobfish/
 Blue%20Tongue%20Lizard/
 cars/
 cartoon/
 deep%20sea%20fish/
 Dingo/
 dragonfly-720x480/
 dragonfly-downloads/
 Echidna/
 Headless%20Chicken%20Monster/
 image_resources/
 jellyfish/
 Kangaroo/
 Koala/
 marblepaper/
 old_photo/
 output/
 Platypus/
 polarized/
 posterize/
 publish/
 pxhere/
 records/
 reptiles/
 Roman%20Architecture/
 segmented/
 spaceshuttle/
 starfish/
 steampunk%20armor/
 Sugar%20Glider/
 tarantula/
 Tasmanian%20Tiger/
 texture/
 tmpseg/
 videoframes/
 vintage%20advertisments/
 vintage%20bottle%20labels/
 vintage%20childrens%20illustrations/
 vintage%20clothing%20patterns/
 vintage%20magazine%20covers/
 vintage%20postcards/
"""
lines= List.split("\n")
for line in lines:
    print(line, end= "  ")

ancient%20manuscript%20art/  animal%20eyes/  antique%20art/  antique%20book%20covers/  antique%20tools/  
art%20nouveau/  Australian%20Lizards/  AutoImageCrawler/  binary_images/  
black%20and%20white%20art%20nouveau%20drawings/  Black%20Swan/  Blobfish/  Blue%20Tongue%20Lizard/  cars/  
cartoon/  deep%20sea%20fish/  Dingo/  dragonfly-720x480/  dragonfly-downloads/  Echidna/  
Headless%20Chicken%20Monster/  image_resources/  jellyfish/  Kangaroo/  Koala/  marblepaper/  
old_photo/  output/  Platypus/  polarized/  posterize/  publish/  pxhere/  records/  reptiles/  
Roman%20Architecture/  segmented/  spaceshuttle/  starfish/  steampunk%20armor/  Sugar%20Glider/  
tarantula/  Tasmanian%20Tiger/  texture/  tmpseg/  videoframes/  vintage%20advertisments/  
vintage%20bottle%20labels/  vintage%20childrens%20illustrations/  vintage%20clothing%20patterns/  
vintage%20magazine%20covers/  vintage%20postcards/    

from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageChops
import time
import random
import os
# Open background image and work out centre

#path = r"Australian%20Lizards/"
#path = r"marblepaper/"
path = r"polarized/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)


bg = Image.open(filename0).convert('RGB')
x = bg.width//2
y = bg.height//2

# The text we want to add
text = "NFT TwitterBot Project"

# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 40)

# Create piece of canvas to draw text on and blur
blurred = Image.new('RGBA', bg.size)
draw = ImageDraw.Draw(blurred)
draw.text(xy=(x,y+190), text=text, fill='black', font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))

# Paste soft text onto background
bg.paste(blurred,blurred)

# Draw on sharp text
draw = ImageDraw.Draw(bg)
draw.text(xy=(x,y+190), text=text, fill='white', font=font, anchor='mm')
mask=Image.open("perferations.png").convert('RGBA') 
bg.paste(mask, (0,0), mask=mask)
bg.save('result.png')

timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "records/"+timestr+"_.png"
bg.save(filename)
im =Image.open('result.png')
im

#%%writefile ImageBot1.py
#!/home/jack/anaconda2/bin
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops 
import os
import sys
import markovify
import twython
from twython import Twython
import time
#nap=randint(10,400)
#time.sleep(nap)
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
with open("sart.txt") as f:
    data = f.read()
data_model = markovify.Text(data)
STR = data_model.make_sentence()

PATH = "result.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


import shutil

# absolute path
src_path = r"E:\pynative\reports\sales.txt"
dst_path = r"E:\pynative\account\sales.txt"
shutil.move(src_path, dst_path)

!mkdir Processed-Images

!ls Processed-Images

!chmod +x use-TWITTERBOT-choose-and-sign.py

%%writefile ImageBot
#!/bin/bash

while true; do
  ./use-TWITTERBOT-choose-and-sign.py
  echo "posted :"
  date
  sleep 1800s
done

!./use-TWITTERBOT-choose-and-sign.py

im =Image.open('result.png')
im





from onnx import *

# Int Attibute
arg = helper.make_attribute("this_is_an_int", 1701)
print("\nInt attribute:\n")
print(arg)

#NBVAL_IGNORE_OUTPUT
# Float Attribute
arg = helper.make_attribute("this_is_a_float", 3.14)
print("\nFloat attribute:\n")
print(arg)

# String Attribute
arg = helper.make_attribute("this_is_a_string", "string_content")
print("\nString attribute:\n")
print(arg)

# Repeated Attribute
arg = helper.make_attribute("this_is_a_repeated_int", [1, 2, 3, 4])
print("\nRepeated int attribute:\n")
print(arg)

# node
node_proto = helper.make_node("Relu", ["X"], ["Y"])

print("\nNodeProto:\n")
print(node_proto)

# node with args
node_proto = helper.make_node(
    "Conv", ["X", "W", "B"], ["Y"],
    kernel=3, stride=1, pad=1)

# This is just for making the attributes to be printed in order
node_proto.attribute.sort(key=lambda attr: attr.name)
print("\nNodeProto:\n")
print(node_proto)

print("\nMore Readable NodeProto (no args yet):\n")
print(helper.printable_node(node_proto))

# graph
graph_proto = helper.make_graph(
    [
        helper.make_node("FC", ["X", "W1", "B1"], ["H1"]),
        helper.make_node("Relu", ["H1"], ["R1"]),
        helper.make_node("FC", ["R1", "W2", "B2"], ["Y"]),
    ],
    "MLP",
    [
        helper.make_tensor_value_info('X' , TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('W1', TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('B1', TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('W2', TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('B2', TensorProto.FLOAT, [1]),
    ],
    [
        helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1]),
    ]
)

print("\ngraph proto:\n")
print(graph_proto)

print("\nMore Readable GraphProto:\n")
print(helper.printable_graph(graph_proto))

# An node that is also a graph
graph_proto = helper.make_graph(
    [
        helper.make_node("FC", ["X", "W1", "B1"], ["H1"]),
        helper.make_node("Relu", ["H1"], ["R1"]),
        helper.make_node("FC", ["R1", "W2", "B2"], ["Y"]),
    ],
    "MLP",
    [
        helper.make_tensor_value_info('X' , TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('W1', TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('B1', TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('W2', TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('B2', TensorProto.FLOAT, [1]),
    ],
    [
        helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1]),
    ]
)

# output = ThisSpecificgraph([input, w1, b1, w2, b2])
node_proto = helper.make_node(
    "graph",
    ["Input", "W1", "B1", "W2", "B2"],
    ["Output"],
    graph=[graph_proto],
)

print("\nNodeProto that contains a graph:\n")
print(node_proto)

  help       Show a list of commands. Type `help [foo]` for information about [foo].      Aliases: ?                     
  ls         List local, instance or class variables, methods and constants.              Aliases: dir                   
  dump       Dump an object or primitive.                                                                                
  doc        Read the documentation for an object, class, constant, method or property.   Aliases: rtfm, man             
  show       Show the code for an object, class, constant, method or property.                                           
  wtf        Show the backtrace of the most recent exception.                             Aliases: last-exception, wtf?  
  whereami   Show where you are in the code.                                                                             
  throw-up   Throw an exception or error out of the Psy Shell.                                                           
  timeit     Profiles with a timer.                                                                                      
  trace      Show the current call stack.                                                                                
  buffer     Show (or clear) the contents of the code input buffer.                       Aliases: buf                   
  clear      Clear the Psy Shell screen.                                                                                 
  edit       Open an external editor. Afterwards, get produced code in input buffer.                                     
  sudo       Evaluate PHP code, bypassing visibility restrictions.                                                       
  history    Show the Psy Shell history.                                                  Aliases: hist                  
  exit       End the current session and return to caller.   

ls

help

buffer clear

$myArr = [];

//array_push($myArr, 5, 8);



$filename = "NOTE"
$handle = fopen($filename, "r");
if ($handle) {
    while (($line = fgets($handle)) !== false) {
        // process the line read.
        array_push($myArr,$line);
    }

    fclose($handle);
}


$myArr; 

if ($file = fopen("NOTE", "r")) {
    while(! feof($file)) 
    { $line = fgets($file);
     echo ("$line"); } 
    fclose($file); } 

$filename = "NOTE"
$handle = fopen($filename, "r+");

//In relation to error in fread, try

if (filesize($filename) > 0) {
$content = fread($handle, filesize($filename));
fclose($handle);

//echo $content; 
foreach ([$content] as $data) {
echo "$data \n";
}    
    
}


$AmazingCreature = ['Chumrucker', 'Succulator', 'Bazalett', 'Quizon'];

foreach ($AmazingCreature as $data) {
echo "$data \n";
}

$attributes = array("skinny", "fat", "smart", "dumb");

foreach ($attributes as $value) {
  echo "$value <br>";
}
echo "\n----------------------\n"
foreach ($attributes as $trait) {
  echo "$trait \n";
}
echo "\n----------------------\n"
echo $attributes[2];

%lsmagic

%magic

%%shell
ls 

%%shell
cd ..
ls -rant NOTEBOOKS

!pwd

$ds = new DOM



pip install psysh_kernel
python -m psysh_kernel install --user
python -m psysh_kernel install
[Errno 13] Permission denied: '/usr/local/share/jupyter'
Perhaps you want to install with `sudo` or `--user`?
(base) jack@jack-Desktop:~/Desktop/pas.bak$ python -m psysh_kernel install --user
[InstallKernelSpec] Installed kernelspec psysh in /home/jack/.local/share/jupyter/kernels/psysh


  help       Show a list of commands. Type `help [foo]` for information about [foo].      Aliases: ?                     
  ls         List local, instance or class variables, methods and constants.              Aliases: dir                   
  dump       Dump an object or primitive.                                                                                
  doc        Read the documentation for an object, class, constant, method or property.   Aliases: rtfm, man             
  show       Show the code for an object, class, constant, method or property.                                           
  wtf        Show the backtrace of the most recent exception.                             Aliases: last-exception, wtf?  
  whereami   Show where you are in the code.                                                                             
  throw-up   Throw an exception or error out of the Psy Shell.                                                           
  timeit     Profiles with a timer.                                                                                      
  trace      Show the current call stack.                                                                                
  buffer     Show (or clear) the contents of the code input buffer.                       Aliases: buf                   
  clear      Clear the Psy Shell screen.                                                                                 
  edit       Open an external editor. Afterwards, get produced code in input buffer.                                     
  sudo       Evaluate PHP code, bypassing visibility restrictions.                                                       
  history    Show the Psy Shell history.                                                  Aliases: hist                  
  exit       End the current session and return to caller.   

ls

help

buffer clear

$myArr = [];

//array_push($myArr, 5, 8);



$filename = "NOTE"
$handle = fopen($filename, "r");
if ($handle) {
    while (($line = fgets($handle)) !== false) {
        // process the line read.
        array_push($myArr,$line);
    }

    fclose($handle);
}


$myArr; 

if ($file = fopen("NOTE", "r")) {
    while(! feof($file)) 
    { $line = fgets($file);
     echo ("$line"); } 
    fclose($file); } 

$filename = "NOTE"
$handle = fopen($filename, "r+");

//In relation to error in fread, try

if (filesize($filename) > 0) {
$content = fread($handle, filesize($filename));
fclose($handle);

//echo $content; 
foreach ([$content] as $data) {
echo "$data \n";
}    
    
}


$AmazingCreature = ['Chumrucker', 'Succulator', 'Bazalett', 'Quizon'];

foreach ($AmazingCreature as $data) {
echo "$data \n";
}

$attributes = array("skinny", "fat", "smart", "dumb");

foreach ($attributes as $value) {
  echo "$value <br>";
}
echo "\n----------------------\n"
foreach ($attributes as $trait) {
  echo "$trait \n";
}
echo "\n----------------------\n"
echo $attributes[2];

%lsmagic

%magic

%%shell
ls 

%%shell
cd ..
ls -rant NOTEBOOKS

!pwd

$ds = new DOM



"channel_name": "@PhilippineRetirement",
"channel_id": "2beebf6e391b2bd9d5386aa269b33985afd18d89",

"name": "@MyLinuxToyBox",
"claim_id": "f0e1ccae16cc1160cbfb028034d5176c98b3dd16",
    
    

requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": "a-new-stream", "bid": "1.0", "file_path": "/tmp/tmpc8ov7w53", "tags": [], "languages": [], "locations": [], "channel_account_id": [], "funding_account_ids": [], "preview": false, "blocking": false}}).json()

import os
os.getcwd()

!ls *.png

from PIL import Image
file_path ="/home/jack/Desktop/JupyterNotebooks-languages/test-full.png"
IMG = Image.open(file_path)
IM =IMG.resize((1280,720), Image.NEAREST)
im = IM.size
IM.save("/home/jack/Desktop/JupyterNotebooks-languages/test1280.png")
IM

#!/usr/bin/python2
import os
from random import randint
from time import sleep
import time
import urllib
import simplejson as json
import requests
import sys
import subprocess
from Completedpy2 import track_download
import sqlite3
import watchVID
DT = time.strftime("%Y-%m-%d-%H:%M")
name = "Plot-Created-by-LBRYCron-Bot" #no Spaces or Special Characters
title = "LBRY LBC Wallet Balance Plot Generated by a Linux Cron Job"
file_path ="/home/jack/Desktop/JupyterNotebooks-languages/test1280.png"
data = requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": name, "bid": "0.01", "title": title, "file_path": file_path, "tags": "python plot","description":"Wallet balance plot generated by a Linux cron job" , "channel_name": "@MyLinuxToyBox" }}).json()
LINES = (json.dumps(data, indent=2 * ' '))
Lin =str(LINES)
L = Lin.split("\n")
for lines in L:
        print lines


import os
os.getcwd()

!ls *.jpg

import time
import urllib
import simplejson as json
import requests

#!/usr/bin/python2
import time
import urllib
import simplejson as json
import requests
name = "Bot-Created-Overlay-Image" #no Spaces or Special Characters
title = "Image Created with Python Script"
file_path ="/home/jack/Desktop/JupyterNotebooks-languages/test.jpg"
Text="""
One of the things I enjoy the most about LBRY is the chance to experiment with different computer languages.
This post was done using Python in a Jupyter Notebook. Two random images were blended and used in this post."""
data = requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": name, "bid": "0.01", "title": title, "file_path": file_path, "tags": "Python","description":Text, "channel_name": "@TotallyInsaneArt" }}).json()
LINES = (json.dumps(data, indent=2 * ' '))
Lin =str(LINES)
L = Lin.split("\n")
for lines in L:
        print lines


!lbrynet address list --address=bDyZqrP3KoUVjotbdiRsPgkXpqHvASLPrT --page_size=50 --page=1

!lbrynet address list 


!ls test.jpg



publish

Create or replace a stream claim at a given name (use 'stream create/update' for more control).
Arguments

name    str    name of the content (can only consist of a-z A-Z 0-9 and -(dash))
bid    optionaldecimal    amount to back the claim
file_path    optionalstr    path to file to be associated with name.
fee_currency    optionalstring    specify fee currency
fee_amount    optionaldecimal    content download fee
fee_address   optionalstr    address where to send fee payments, gefaut value from --claim_address if not provided
title    optionalstr    title of the publication
description    optionalstr    description of the publication
author    optionalstr    author of the publication. The usage for this field is not the same as for channels. The author field is used to credit an author who is not the publisher and is not represented by the channel. For example, a pdf file of 'The Odyssey' has an author of 'Homer' but may by published to a channel such as '@classics', or to no channel at all
tags    optionallist    add content tags
languages    optionallist    languages used by the channel, using RFC 5646 format, eg: for English `--languages=en` for Spanish (Spain) `--languages=es-ES` for Spanish (Mexican) `--languages=es-MX` for Chinese (Simplified) `--languages=zh-Hans` for Chinese (Traditional) `--languages=zh-Hant`
locations    optionallist    locations relevant to the stream, consisting of 2 letter `country` code and a `state`, `city` and a postal `code` along with a `latitude` and `longitude`. for JSON RPC: pass a dictionary with aforementioned attributes as keys, eg: ... "locations": [{'country': 'US', 'state': 'NH'}] ... for command line: pass a colon delimited list with values in the following order: "COUNTRY:STATE:CITY:CODE:LATITUDE:LONGITUDE" making sure to include colon for blank values, for example to provide only the city: ... --locations="::Manchester" with all values set: ... --locations="US:NH:Manchester:03101:42.990605:-71.460989" optionally, you can just pass the "LATITUDE:LONGITUDE": ... --locations="42.990605:-71.460989" finally, you can also pass JSON string of dictionary on the command line as you would via JSON RPC ... --locations="{'country': 'US', 'state': 'NH'}"
    license
    optionalstr
    publication license
    license_url
    optionalstr
    publication license url
    thumbnail_url
    optionalstr
    thumbnail url
    release_time
    optionalint
    original public release of content, seconds since UNIX epoch
    width
    optionalint
    image/video width, automatically calculated from media file
    height
    optionalint
    image/video height, automatically calculated from media file
    duration
    optionalint
    audio/video duration in seconds, automatically calculated
    channel_id
    optionalstr
    claim id of the publisher channel
    channel_name
    optionalstr
    name of publisher channel
    channel_account_id
    optionalstr
    one or more account ids for accounts to look in for channel certificates, defaults to all accounts.
    account_id
    optionalstr
    account to use for holding the transaction
    wallet_id
    optionalstr
    restrict operation to specific wallet
    funding_account_ids
    optionallist
    ids of accounts to fund this transaction
    claim_address
    optionalstr
    address where the claim is sent to, if not specified it will be determined automatically from the account
    preview
    optionalbool
    do not broadcast the transaction
    blocking
    optionalbool
    wait until transaction is in mempool


curl -d'{"method": "publish", "params": {"name": "a-new-stream", "bid": "1.0", "file_path": "/tmp/tmpc8ov7w53", "tags": [], "languages": [], "locations": [], "channel_account_id": [], "funding_account_ids": [], "preview": false, "blocking": false}}' 

"channel_name": "@PhilippineRetirement",
"channel_id": "2beebf6e391b2bd9d5386aa269b33985afd18d89",

"name": "@MyLinuxToyBox",
"claim_id": "f0e1ccae16cc1160cbfb028034d5176c98b3dd16",
    
    

import time
import urllib
import simplejson as json
import requests
requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": "a-new-stream", "bid": "1.0", "file_path": "/tmp/tmpc8ov7w53", "tags": [], "languages": [], "locations": [], "channel_account_id": [], "funding_account_ids": [], "preview": False, "blocking": False}}).json()

import os
os.getcwd()

!ls images

from PIL import Image
file_path ="/home/jack/Desktop/LBRY-toolbox/images/background.jpg"
IMG = Image.open(file_path)
IM =IMG.resize((1280,720), Image.NEAREST)
im = IM.size
IM.save("/home/jack/Desktop/LBRY-toolbox/images/background-1280x720.jpg")
IM

#!/usr/bin/python2
import os
from random import randint
from time import sleep
import time
import urllib
import simplejson as json
import requests
import sys
import subprocess
from Completedpy2 import track_download
import sqlite3
import watchVID
DT = time.strftime("%Y-%m-%d-%H:%M")
name = "Plot-Created-by-LBRYCron-Bot" #no Spaces or Special Characters
title = "LBRY LBC Wallet Balance Plot Generated by a Linux Cron Job"
file_path ="/home/jack/Desktop/JupyterNotebooks-languages/test1280.png"
data = requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": name, "bid": "0.01", "title": title, "file_path": file_path, "tags": "python plot","description":"Wallet balance plot generated by a Linux cron job" , "channel_name": "@MyLinuxToyBox" }}).json()
LINES = (json.dumps(data, indent=2 * ' '))
Lin =str(LINES)
L = Lin.split("\n")
for lines in L:
        print (lines)


import os
os.getcwd()

!ls *.jpg

import time
import urllib
import simplejson as json
import requests

#!/usr/bin/python2
import time
import urllib
import simplejson as json
import requests
name = "Bot-Created-Overlay-Image" #no Spaces or Special Characters
title = "Image Created with Python Script"
file_path ="/home/jack/Desktop/JupyterNotebooks-languages/test.jpg"
Text="""
One of the things I enjoy the most about LBRY is the chance to experiment with different computer languages.
This post was done using Python in a Jupyter Notebook. Two random images were blended and used in this post."""
data = requests.post("http://localhost:5279", json={"method": "publish", "params": {"name": name, "bid": "0.01", "title": title, "file_path": file_path, "tags": "Python","description":Text, "channel_name": "@TotallyInsaneArt" }}).json()
LINES = (json.dumps(data, indent=2 * ' '))
Lin =str(LINES)
L = Lin.split("\n")
for lines in L:
        print (lines)


!lbrynet address list --address=bDyZqrP3KoUVjotbdiRsPgkXpqHvASLPrT --page_size=50 --page=1

!lbrynet address list 


!ls test.jpg



publish

Create or replace a stream claim at a given name (use 'stream create/update' for more control).
Arguments

name    str    name of the content (can only consist of a-z A-Z 0-9 and -(dash))
bid    optionaldecimal    amount to back the claim
file_path    optionalstr    path to file to be associated with name.
fee_currency    optionalstring    specify fee currency
fee_amount    optionaldecimal    content download fee
fee_address   optionalstr    address where to send fee payments, gefaut value from --claim_address if not provided
title    optionalstr    title of the publication
description    optionalstr    description of the publication
author    optionalstr    author of the publication. The usage for this field is not the same as for channels. The author field is used to credit an author who is not the publisher and is not represented by the channel. For example, a pdf file of 'The Odyssey' has an author of 'Homer' but may by published to a channel such as '@classics', or to no channel at all
tags    optionallist    add content tags
languages    optionallist    languages used by the channel, using RFC 5646 format, eg: for English `--languages=en` for Spanish (Spain) `--languages=es-ES` for Spanish (Mexican) `--languages=es-MX` for Chinese (Simplified) `--languages=zh-Hans` for Chinese (Traditional) `--languages=zh-Hant`
locations    optionallist    locations relevant to the stream, consisting of 2 letter `country` code and a `state`, `city` and a postal `code` along with a `latitude` and `longitude`. for JSON RPC: pass a dictionary with aforementioned attributes as keys, eg: ... "locations": [{'country': 'US', 'state': 'NH'}] ... for command line: pass a colon delimited list with values in the following order: "COUNTRY:STATE:CITY:CODE:LATITUDE:LONGITUDE" making sure to include colon for blank values, for example to provide only the city: ... --locations="::Manchester" with all values set: ... --locations="US:NH:Manchester:03101:42.990605:-71.460989" optionally, you can just pass the "LATITUDE:LONGITUDE": ... --locations="42.990605:-71.460989" finally, you can also pass JSON string of dictionary on the command line as you would via JSON RPC ... --locations="{'country': 'US', 'state': 'NH'}"
    license
    optionalstr
    publication license
    license_url
    optionalstr
    publication license url
    thumbnail_url
    optionalstr
    thumbnail url
    release_time
    optionalint
    original public release of content, seconds since UNIX epoch
    width
    optionalint
    image/video width, automatically calculated from media file
    height
    optionalint
    image/video height, automatically calculated from media file
    duration
    optionalint
    audio/video duration in seconds, automatically calculated
    channel_id
    optionalstr
    claim id of the publisher channel
    channel_name
    optionalstr
    name of publisher channel
    channel_account_id
    optionalstr
    one or more account ids for accounts to look in for channel certificates, defaults to all accounts.
    account_id
    optionalstr
    account to use for holding the transaction
    wallet_id
    optionalstr
    restrict operation to specific wallet
    funding_account_ids
    optionallist
    ids of accounts to fund this transaction
    claim_address
    optionalstr
    address where the claim is sent to, if not specified it will be determined automatically from the account
    preview
    optionalbool
    do not broadcast the transaction
    blocking
    optionalbool
    wait until transaction is in mempool


curl -d'{"method": "publish", "params": {"name": "a-new-stream", "bid": "1.0", "file_path": "/tmp/tmpc8ov7w53", "tags": [], "languages": [], "locations": [], "channel_account_id": [], "funding_account_ids": [], "preview": false, "blocking": false}}' 



!pip install tf-nightly

import gast
from tensorflow.python.autograph.pyct import transformer

class BasicCppCodegen(transformer.CodeGenerator):

  def visit_Name(self, node):
    self.emit(node.id)

  def visit_arguments(self, node):
    self.visit(node.args[0])
    for arg in node.args[1:]:
      self.emit(', ')
      self.visit(arg)

  def visit_FunctionDef(self, node):
    self.emit('void {}'.format(node.name))
    self.emit('(')
    self.visit(node.args)
    self.emit(') {\n')
    self.visit_block(node.body)
    self.emit('\n}')

  def visit_Call(self, node):
    self.emit(node.func.id)
    self.emit('(')
    self.visit(node.args[0])
    for arg in node.args[1:]:
      self.emit(', ')
      self.visit(arg)
    self.emit(');')


import gast
from tensorflow.python.autograph.pyct import transpiler

class PyToBasicCpp(transpiler.GenericTranspiler):

  def transform_ast(self, node, ctx):
    codegen = BasicCppCodegen(ctx)
    codegen.visit(node)
    return codegen.code_buffer

def f(x, y):
  print(x, y)

code, _ = PyToBasicCpp().transform(f, None)
print(code)

def get_node_and_ctx(f):
  node, source = parser.parse_entity(f, ())
  f_info = transformer.EntityInfo(
    name='f',
    source_code=source,
    source_file=None,
    future_features=(),
    namespace=None)
  ctx = transformer.Context(f_info, None, None)
  return node, ctx

from tensorflow.python.autograph.pyct import anno
from tensorflow.python.autograph.pyct import parser
from tensorflow.python.autograph.pyct import qual_names
from tensorflow.python.autograph.pyct.static_analysis import annos
from tensorflow.python.autograph.pyct.static_analysis import activity


def f(a):
  b = a + 1
  return b


node, ctx = get_node_and_ctx(f)

node = qual_names.resolve(node)
node = activity.resolve(node, ctx)

fn_scope = anno.getanno(node, annos.NodeAnno.BODY_SCOPE)  # Note: tag will be changed soon.


print('read:', fn_scope.read)
print('modified:', fn_scope.modified)

from tensorflow.python.autograph.pyct import cfg


def f(a):
  if a > 0:
    return a
  b = -a

node, ctx = get_node_and_ctx(f)

node = qual_names.resolve(node)
cfgs = cfg.build(node)
cfgs[node]

from tensorflow.python.autograph.pyct import anno
from tensorflow.python.autograph.pyct import cfg
from tensorflow.python.autograph.pyct import qual_names
from tensorflow.python.autograph.pyct.static_analysis import annos
from tensorflow.python.autograph.pyct.static_analysis import reaching_definitions
from tensorflow.python.autograph.pyct.static_analysis import reaching_fndefs
from tensorflow.python.autograph.pyct.static_analysis import liveness


def f(a):
  b = a + 1
  return b


node, ctx = get_node_and_ctx(f)

node = qual_names.resolve(node)
cfgs = cfg.build(node)
node = activity.resolve(node, ctx)
node = reaching_definitions.resolve(node, ctx, cfgs)
node = reaching_fndefs.resolve(node, ctx, cfgs)
node = liveness.resolve(node, ctx, cfgs)

print('live into `b = a + 1`:', anno.getanno(node.body[0], anno.Static.LIVE_VARS_IN))
print('live into `return b`:', anno.getanno(node.body[1], anno.Static.LIVE_VARS_IN))

from tensorflow.python.autograph.pyct import transpiler


class NoopTranspiler(transpiler.PyToPy):

  def get_caching_key(self, ctx):
    # You may return different caching keys if the transformation may generate
    # code versions.
    return 0

  def get_extra_locals(self):
    # No locals needed for now; see below.
    return {}

  def transform_ast(self, ast, transformer_context):
    return ast

tr = NoopTranspiler()

def f(x, y):
  return x + y


new_f, module, source_map = tr.transform(f, None)

new_f(1, 1)

from tensorflow.python.autograph.pyct import parser


class HelloTranspiler(transpiler.PyToPy):

  def get_caching_key(self, ctx):
    return 0

  def get_extra_locals(self):
    return {'name': 'you'}

  def transform_ast(self, ast, transformer_context):
    print_code = parser.parse('print("Hello", name)')
    ast.body = [print_code] + ast.body
    return ast


def f(x, y):
  pass

new_f, _, _ = HelloTranspiler().transform(f, None)

_ = new_f(1, 1)

import inspect

print(inspect.getsource(new_f))

info = open("list.html", "r").read()
#print(info)

txt = "The rain in Spain"
print(txt[:3])

import re
cnt = 0
STRING =info
#Split the string at every white-space character:
x = re.split("\s", STRING[:500])
for word in x:
    cnt=cnt+1
    if cnt<100:
        print(x)

import re

txt = "The rain in Spain"
x = re.search("ai", txt)
print(x) #this will print an object 

#Print the position (start- and end-position) of the first match occurrence.
#The regular expression looks for any words that starts with an upper case "S":
import re

txt = "The rain in Spain"
x = re.search(r"\bS\w+", txt)
print(x.span()) 

#Print the string passed into the function:
import re

txt = "The rain in Spain The regular expression looks for any words that starts with an upper case "
x = re.search(r"\bS\w+", txt)
print(x.string) 

import re

txt = "The rain in Spain The regular expression looks for any words that Starts with an upper case "
x = re.search(r"\bS\w+", txt)
print(x.group()) 

import re

txt = "The rain in Spain"
x = re.search("\s", txt)

print("The first white-space character is located in position:", x.start()) 





# imports
import torch
from torch import nn
from torch.optim import SGD
from torch.utils.data import DataLoader
from torch.autograd import Variable
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor, Normalize, Compose
from IPython.display import Image
import chart_studio.plotly as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)

# summary of neural networks
Image(url='https://i.imgur.com/ktUOnca.jpg')

Image(url="http://www.frank-dieterle.de/phd/images/image016.gif")

# how tensors work
x = torch.zeros(2,3)
x

# how variables work
x = Variable(x)
print ("x:", x)
print ("requires grad:", x.requires_grad)
print ("data:", x.data)

y = x + 1
z = Variable(torch.ones(2,3), requires_grad=True)
w = y+z

print (y)
print ("does y require grad?", y.requires_grad)
print ("does w require grad?", w.requires_grad)

# what about neural network layers?
lin = nn.Linear(3, 4)  # 3 inputs and 4 outputs. Should contain a 4x3 matrix and a 4x1 matrix of parameters.
list(lin.parameters())

print ("lin output", lin(y))  # processing with a batch size of 2.
print (lin(y).requires_grad)
print (lin(w).requires_grad)

relu = nn.ReLU()
print (relu(lin(y)))
print (relu(lin(y)).requires_grad)
print (relu(w).requires_grad)
print (relu(y).requires_grad)

Image(url="https://i.stack.imgur.com/GvsBA.jpg")

Image(url="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg")

# setting up datasets.
training = DataLoader(
    MNIST(
        "/home/jack/train", 
        download=False, 
        transform=Compose([ToTensor(),Normalize((0.1307,), (0.3081,))])
    ), 
    batch_size=64, 
    shuffle=True, 
    num_workers=40
)
testing = DataLoader(
    MNIST(
        "/home/jack/test", 
        train=False,
        download=False, 
        transform=Compose([ToTensor(),Normalize((0.1307,), (0.3081,))])
    ), 
    batch_size=64, 
    shuffle=True, 
    num_workers=40
)


len(training.dataset), len(testing.dataset)

print (MNIST('/home/jack/train')[0])
MNIST('/home/jack/train')[0][0]

# basic linear with relu's
class LinReLU(nn.Module):
    def __init__(self, indim, outdim):
        super(LinReLU, self).__init__()
        self.lin = nn.Linear(indim, outdim)
        self.relu = nn.ReLU(True)
    
    def forward(self, input):
        input = self.lin(input)
        return self.relu(input)


class MnistLinear(nn.Module):
    def __init__(self):
        super(MnistLinear, self).__init__()
        self.lin1 = LinReLU(28*28, 16)
        self.lin2 = nn.Linear(16, 10)
    
    def forward(self, input):
        input = self.lin1(input.view(-1, 28*28))
        input = self.lin2(input)
        return input

class ConvReLU(nn.Module):
    def __init__(self, in_channels, out_channels, kernel=3):
        super(ConvReLU, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel, padding=kernel/2)
        self.relu = nn.ReLU(True)
        self.bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, input):
        input = self.conv(input)
        input = self.relu(input)
        return self.bn(input)


class MnistCNN(nn.Module):
    def __init__(self):
        super(MnistCNN, self).__init__()
        self.conv1 = ConvReLU(1, 16)
        self.mp = nn.MaxPool2d(2)
        self.conv2 = ConvReLU(16, 32)
        self.avgpool = nn.AvgPool2d(14)
        self.lin = nn.Linear(32, 10)
    
    def forward(self, input):
        input = self.conv1(input)
        input = self.mp(input)
        input = self.conv2(input)
        input = self.avgpool(input).squeeze()
        return self.lin(input)

class MnistRNN(nn.Module):
    def __init__(self):
        super(MnistRNN, self).__init__()
        self.lstm = nn.LSTM(
            input_size=28, 
            hidden_size=16, 
            num_layers=2, 
            batch_first=True, 
            dropout=0.5
        )
        self.linear = nn.Linear(16, 10)
    
    def forward(self, input):
        input, hidden = self.lstm(input.squeeze())
        return self.linear(input[:, -1])

class ComboModel(nn.Module):
    def __init__(self):
        super(ComboModel, self).__init__()
        self.lin = MnistLinear()
        self.cnn = MnistCNN()
        self.rnn = MnistRNN()
    
    def forward(self, input):
        l1 = self.lin(input)
        l2 = self.cnn(input)
        l3 = self.rnn(input)
        return l1, l2, l3 

model = ComboModel()

def num_parameters(m):
    return sum([y.nelement() for y in m.parameters()])

print ("num params for linear:\t", num_parameters(model.lin))
print ("num params for cnn:\t", num_parameters(model.cnn))
print ("num params for rnn:\t", num_parameters(model.rnn))

optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9)

model = model.cpu()

loss = nn.CrossEntropyLoss().cpu()

sav = open("SAV.Epoch","a")
def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(training):
        data, target = data.cpu(), target.cpu()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        lin_out, cnn_out, rnn_out = model(data)
        lin_loss = loss(lin_out, target)
        cnn_loss = loss(cnn_out, target)
        rnn_loss = loss(rnn_out, target)
        total_loss = lin_loss + cnn_loss + rnn_loss
        total_loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            txt= str('Train Epoch: {} [{}/{} ({:.0f}%)]\tLin Loss: {:.6f}\tCNN Loss: {:.6f}\tRNN Loss: {:.6f}'.format(
                epoch+1, batch_idx, len(training),
                100. * batch_idx / len(training), lin_loss.data[0], cnn_loss.data[0], rnn_loss.data[0]))
            sav.write(txt)
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLin Loss: {:.6f}\tCNN Loss: {:.6f}\tRNN Loss: {:.6f}'.format(
                epoch+1, batch_idx, len(training),
                100. * batch_idx / len(training), lin_loss.data[0], cnn_loss.data[0], rnn_loss.data[0]))

num_epochs = 10
losses = [{}] * (num_epochs+1)
correct = [{}] * (num_epochs+1)

def test(epoch):
    #with torch.no_grad():
    model.eval()
    losses[epoch] = dict(
        Linear=0,
        CNN=0,
        RNN=0
    )
    correct[epoch] = dict(
        Linear=0,
        CNN=0,
        RNN=0
    )
    for data, target in testing:
        with torch.no_grad():
            data, target = data.cpu(), target.cpu()
            data, target = Variable(data, volatile=False), Variable(target)
            data, target = Variable(data), Variable(target)
            lin_out, cnn_out, rnn_out = model(data)
        
        for name, output in [("Linear", lin_out), ("CNN", cnn_out), ("RNN", rnn_out)]:
            losses[epoch][name] += loss(output, target).data[0]
            pred = output.data.max(1)[1] 
            correct[epoch][name] += pred.eq(target.data).cpu().sum()

    for name in ['Linear', 'CNN', 'RNN']:
        losses[epoch][name] /= len(testing) # loss function already averages over batch size
        print('{} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            name, losses[epoch][name], correct[epoch][name], len(testing.dataset),
            100. * correct[epoch][name] / len(testing.dataset)))
        tex2=str('{} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            name, losses[epoch][name], correct[epoch][name], len(testing.dataset),
            100. * correct[epoch][name] / len(testing.dataset)))
        sav.write(txt2)
        
for epoch in range(num_epochs+1):
    test(epoch)
    train(epoch)
test(num_epochs)

scatters = []
for name in ['Linear', 'CNN', 'RNN']:
    xs = [i for i in range(1, num_epochs+1)]
    ys = [float(i[name])/len(testing.dataset) for i in correct]
    scatters.append(go.Scatter(
        x = xs,
        y = ys,
        name=name
    ))
    print ys
iplot(scatters)



Notebook validation failed: data.cells[{data__cells_x}]must be valid exactly by one definition (0 matches found):
cut all cells with sissors and repaste .. all is fixed        
        

# imports
import torch
from torch import nn
from torch.optim import SGD
from torch.utils.data import DataLoader
from torch.autograd import Variable
#from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
#from torchvision.transforms import ToTensor, Normalize, Compose
from IPython.display import Image
import chart_studio as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)

# summary of neural networks
Image(url='https://i.imgur.com/ktUOnca.jpg')

Image(url="http://www.frank-dieterle.de/phd/images/image016.gif")

# how tensors work
x = torch.zeros(2,3)
x

# how variables work
x = Variable(x)
print "x:", x
print "requires grad:", x.requires_grad
print "data:", x.data

y = x + 1
z = Variable(torch.ones(2,3), requires_grad=True)
w = y+z

print y
print "does y require grad?", y.requires_grad
print "does w require grad?", w.requires_grad

# what about neural network layers?
lin = nn.Linear(3, 4)  # 3 inputs and 4 outputs. Should contain a 4x3 matrix and a 4x1 matrix of parameters.
list(lin.parameters())

print "lin output", lin(y)  # processing with a batch size of 2.
print lin(y).requires_grad
print lin(w).requires_grad

relu = nn.ReLU()
print relu(lin(y))
print relu(lin(y)).requires_grad
print relu(w).requires_grad
print relu(y).requires_grad

Image(url="https://i.stack.imgur.com/GvsBA.jpg")

Image(url="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg")

# setting up datasets.
training = DataLoader(
    MNIST(
        "/home/temerick/train", 
        download=False, 
        transform=Compose([ToTensor(),Normalize((0.1307,), (0.3081,))])
    ), 
    batch_size=64, 
    shuffle=True, 
    num_workers=40
)
testing = DataLoader(
    MNIST(
        "/home/temerick/test", 
        train=False,
        download=False, 
        transform=Compose([ToTensor(),Normalize((0.1307,), (0.3081,))])
    ), 
    batch_size=64, 
    shuffle=True, 
    num_workers=40
)


len(training.dataset), len(testing.dataset)

print MNIST('/home/temerick/train')[0]
MNIST('/home/temerick/train')[0][0]

# basic linear with relu's
class LinReLU(nn.Module):
    def __init__(self, indim, outdim):
        super(LinReLU, self).__init__()
        self.lin = nn.Linear(indim, outdim)
        self.relu = nn.ReLU(True)
    
    def forward(self, input):
        input = self.lin(input)
        return self.relu(input)


class MnistLinear(nn.Module):
    def __init__(self):
        super(MnistLinear, self).__init__()
        self.lin1 = LinReLU(28*28, 16)
        self.lin2 = nn.Linear(16, 10)
    
    def forward(self, input):
        input = self.lin1(input.view(-1, 28*28))
        input = self.lin2(input)
        return input

class ConvReLU(nn.Module):
    def __init__(self, in_channels, out_channels, kernel=3):
        super(ConvReLU, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel, padding=kernel/2)
        self.relu = nn.ReLU(True)
        self.bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, input):
        input = self.conv(input)
        input = self.relu(input)
        return self.bn(input)


class MnistCNN(nn.Module):
    def __init__(self):
        super(MnistCNN, self).__init__()
        self.conv1 = ConvReLU(1, 16)
        self.mp = nn.MaxPool2d(2)
        self.conv2 = ConvReLU(16, 32)
        self.avgpool = nn.AvgPool2d(14)
        self.lin = nn.Linear(32, 10)
    
    def forward(self, input):
        input = self.conv1(input)
        input = self.mp(input)
        input = self.conv2(input)
        input = self.avgpool(input).squeeze()
        return self.lin(input)

class MnistRNN(nn.Module):
    def __init__(self):
        super(MnistRNN, self).__init__()
        self.lstm = nn.LSTM(
            input_size=28, 
            hidden_size=16, 
            num_layers=2, 
            batch_first=True, 
            dropout=0.5
        )
        self.linear = nn.Linear(16, 10)
    
    def forward(self, input):
        input, hidden = self.lstm(input.squeeze())
        return self.linear(input[:, -1])

class ComboModel(nn.Module):
    def __init__(self):
        super(ComboModel, self).__init__()
        self.lin = MnistLinear()
        self.cnn = MnistCNN()
        self.rnn = MnistRNN()
    
    def forward(self, input):
        l1 = self.lin(input)
        l2 = self.cnn(input)
        l3 = self.rnn(input)
        return l1, l2, l3 

model = ComboModel()

def num_parameters(m):
    return sum([y.nelement() for y in m.parameters()])

print "num params for linear:\t", num_parameters(model.lin)
print "num params for cnn:\t", num_parameters(model.cnn)
print "num params for rnn:\t", num_parameters(model.rnn)

optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9)

model = model.cuda(1)

loss = nn.CrossEntropyLoss().cuda(1)

def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(training):
        data, target = data.cuda(1), target.cuda(1)
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        lin_out, cnn_out, rnn_out = model(data)
        lin_loss = loss(lin_out, target)
        cnn_loss = loss(cnn_out, target)
        rnn_loss = loss(rnn_out, target)
        total_loss = lin_loss + cnn_loss + rnn_loss
        total_loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLin Loss: {:.6f}\tCNN Loss: {:.6f}\tRNN Loss: {:.6f}'.format(
                epoch+1, batch_idx, len(training),
                100. * batch_idx / len(training), lin_loss.data[0], cnn_loss.data[0], rnn_loss.data[0]))

num_epochs = 10
losses = [{}] * (num_epochs+1)
correct = [{}] * (num_epochs+1)

def test(epoch):
    model.eval()
    losses[epoch] = dict(
        Linear=0,
        CNN=0,
        RNN=0
    )
    correct[epoch] = dict(
        Linear=0,
        CNN=0,
        RNN=0
    )
    for data, target in testing:
        data, target = data.cuda(1), target.cuda(1)
        data, target = Variable(data, volatile=True), Variable(target)
        lin_out, cnn_out, rnn_out = model(data)
        
        for name, output in [("Linear", lin_out), ("CNN", cnn_out), ("RNN", rnn_out)]:
            losses[epoch][name] += loss(output, target).data[0]
            pred = output.data.max(1)[1] 
            correct[epoch][name] += pred.eq(target.data).cpu().sum()

    for name in ['Linear', 'CNN', 'RNN']:
        losses[epoch][name] /= len(testing) # loss function already averages over batch size
        print('{} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            name, losses[epoch][name], correct[epoch][name], len(testing.dataset),
            100. * correct[epoch][name] / len(testing.dataset)))
        
for epoch in range(num_epochs+1):
    test(epoch)
    train(epoch)
test(num_epochs)

scatters = []
for name in ['Linear', 'CNN', 'RNN']:
    xs = [i for i in range(1, num_epochs+1)]
    ys = [float(i[name])/len(testing.dataset) for i in correct]
    scatters.append(go.Scatter(
        x = xs,
        y = ys,
        name=name
    ))
    print ys
iplot(scatters)





http://yann.lecun.com/exdb/mnist/

Notebook validation failed: data.cells[{data__cells_x}]must be valid exactly by one definition (0 matches found):
cut all cells with sissors and repaste .. all is fixed        
        

# imports
import torch
from torch import nn
from torch.optim import SGD
from torch.utils.data import DataLoader
from torch.autograd import Variable
#from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
#from torchvision.transforms import ToTensor, Normalize, Compose
from IPython.display import Image
import chart_studio.plotly as py
#import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)

# summary of neural networks
Image(url='https://i.imgur.com/ktUOnca.jpg')

Image(url="http://www.frank-dieterle.de/phd/images/image016.gif")

# how tensors work
x = torch.zeros(2,4,4)
print(x)
y = torch.ones(2,4,4)
print(y)
z = x+y
z

# how variables work
x = Variable(x)
print ("x:", x)
print ("requires grad:", x.requires_grad)
print ("data:", x.data)

y = x + 2
z = Variable(torch.ones(3,4), requires_grad=True)
w = y + 2

print (y)
print ("does y require grad?", y.requires_grad)
print ("does w require grad?", w.requires_grad)

# what about neural network layers?
lin = nn.Linear(3, 4)  # 3 inputs and 4 outputs. Should contain a 4x3 matrix and a 4x1 matrix of parameters.
list(lin.parameters())

print ("lin output", lin(y))  # processing with a batch size of 2.
#print (lin(y).requires_grad)
#print (lin(w).requires_grad)

relu = nn.ReLU()
print (relu(lin(y)))
print (relu(lin(y)).requires_grad)
print (relu(w).requires_grad)
print (relu(y).requires_grad)

Image(url="https://i.stack.imgur.com/GvsBA.jpg")

Image(url="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg")

# setting up datasets.
training = DataLoader(
    MNIST(
        "/home/jack/train/", 
        download=False, 
        transform=Compose([ToTensor(),Normalize((0.1307,), (0.3081,))])
    ), 
    batch_size=64, 
    shuffle=True, 
    num_workers=40
)
testing = DataLoader(
    MNIST(
        "/home/jack/test/", 
        train=False,
        download=False, 
        transform=Compose([ToTensor(),Normalize((0.1307,), (0.3081,))])
    ), 
    batch_size=64, 
    shuffle=True, 
    num_workers=40
)


len(training.dataset), len(testing.dataset)

print MNIST('/home/jack/train')[0]
MNIST('/home/jack/train')[0][0]

# basic linear with relu's
class LinReLU(nn.Module):
    def __init__(self, indim, outdim):
        super(LinReLU, self).__init__()
        self.lin = nn.Linear(indim, outdim)
        self.relu = nn.ReLU(True)
    
    def forward(self, input):
        input = self.lin(input)
        return self.relu(input)


class MnistLinear(nn.Module):
    def __init__(self):
        super(MnistLinear, self).__init__()
        self.lin1 = LinReLU(28*28, 16)
        self.lin2 = nn.Linear(16, 10)
    
    def forward(self, input):
        input = self.lin1(input.view(-1, 28*28))
        input = self.lin2(input)
        return input

class ConvReLU(nn.Module):
    def __init__(self, in_channels, out_channels, kernel=3):
        super(ConvReLU, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel, padding=kernel/2)
        self.relu = nn.ReLU(True)
        self.bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, input):
        input = self.conv(input)
        input = self.relu(input)
        return self.bn(input)


class MnistCNN(nn.Module):
    def __init__(self):
        super(MnistCNN, self).__init__()
        self.conv1 = ConvReLU(1, 16)
        self.mp = nn.MaxPool2d(2)
        self.conv2 = ConvReLU(16, 32)
        self.avgpool = nn.AvgPool2d(14)
        self.lin = nn.Linear(32, 10)
    
    def forward(self, input):
        input = self.conv1(input)
        input = self.mp(input)
        input = self.conv2(input)
        input = self.avgpool(input).squeeze()
        return self.lin(input)

class MnistRNN(nn.Module):
    def __init__(self):
        super(MnistRNN, self).__init__()
        self.lstm = nn.LSTM(
            input_size=28, 
            hidden_size=16, 
            num_layers=2, 
            batch_first=True, 
            dropout=0.5
        )
        self.linear = nn.Linear(16, 10)
    
    def forward(self, input):
        input, hidden = self.lstm(input.squeeze())
        return self.linear(input[:, -1])

class ComboModel(nn.Module):
    def __init__(self):
        super(ComboModel, self).__init__()
        self.lin = MnistLinear()
        self.cnn = MnistCNN()
        self.rnn = MnistRNN()
    
    def forward(self, input):
        l1 = self.lin(input)
        l2 = self.cnn(input)
        l3 = self.rnn(input)
        return l1, l2, l3 

model = ComboModel()

def num_parameters(m):
    return sum([y.nelement() for y in m.parameters()])

print "num params for linear:\t", num_parameters(model.lin)
print "num params for cnn:\t", num_parameters(model.cnn)
print "num params for rnn:\t", num_parameters(model.rnn)

optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9)

model = model.cuda(1)

loss = nn.CrossEntropyLoss().cuda(1)

def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(training):
        data, target = data.cuda(1), target.cuda(1)
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        lin_out, cnn_out, rnn_out = model(data)
        lin_loss = loss(lin_out, target)
        cnn_loss = loss(cnn_out, target)
        rnn_loss = loss(rnn_out, target)
        total_loss = lin_loss + cnn_loss + rnn_loss
        total_loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLin Loss: {:.6f}\tCNN Loss: {:.6f}\tRNN Loss: {:.6f}'.format(
                epoch+1, batch_idx, len(training),
                100. * batch_idx / len(training), lin_loss.data[0], cnn_loss.data[0], rnn_loss.data[0]))

num_epochs = 10
losses = [{}] * (num_epochs+1)
correct = [{}] * (num_epochs+1)

def test(epoch):
    model.eval()
    losses[epoch] = dict(
        Linear=0,
        CNN=0,
        RNN=0
    )
    correct[epoch] = dict(
        Linear=0,
        CNN=0,
        RNN=0
    )
    for data, target in testing:
        data, target = data.cuda(1), target.cuda(1)
        data, target = Variable(data, volatile=True), Variable(target)
        lin_out, cnn_out, rnn_out = model(data)
        
        for name, output in [("Linear", lin_out), ("CNN", cnn_out), ("RNN", rnn_out)]:
            losses[epoch][name] += loss(output, target).data[0]
            pred = output.data.max(1)[1] 
            correct[epoch][name] += pred.eq(target.data).cpu().sum()

    for name in ['Linear', 'CNN', 'RNN']:
        losses[epoch][name] /= len(testing) # loss function already averages over batch size
        print('{} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            name, losses[epoch][name], correct[epoch][name], len(testing.dataset),
            100. * correct[epoch][name] / len(testing.dataset)))
        
for epoch in range(num_epochs+1):
    test(epoch)
    train(epoch)
test(num_epochs)

scatters = []
for name in ['Linear', 'CNN', 'RNN']:
    xs = [i for i in range(1, num_epochs+1)]
    ys = [float(i[name])/len(testing.dataset) for i in correct]
    scatters.append(go.Scatter(
        x = xs,
        y = ys,
        name=name
    ))
    print ys
iplot(scatters)





#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Quantization debugger is available from TensorFlow 2.7.0
!pip uninstall -y tensorflow
!pip install tf-nightly
!pip install tensorflow_datasets --upgrade  # imagenet_v2 needs latest checksum

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_hub as hub

#@title Boilerplates and helpers
MODEL_URI = 'https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5'


def process_image(data):
  data['image'] = tf.image.resize(data['image'], (224, 224)) / 255.0
  return data


# Representative dataset
def representative_dataset(dataset):

  def _data_gen():
    for data in dataset.batch(1):
      yield [data['image']]

  return _data_gen


def eval_tflite(tflite_model, dataset):
  """Evaluates tensorflow lite classification model with the given dataset."""
  interpreter = tf.lite.Interpreter(model_content=tflite_model)
  interpreter.allocate_tensors()

  input_idx = interpreter.get_input_details()[0]['index']
  output_idx = interpreter.get_output_details()[0]['index']

  results = []

  for data in representative_dataset(dataset)():
    interpreter.set_tensor(input_idx, data[0])
    interpreter.invoke()
    results.append(interpreter.get_tensor(output_idx).flatten())

  results = np.array(results)
  gt_labels = np.array(list(dataset.map(lambda data: data['label'] + 1)))
  accuracy = (
      np.sum(np.argsort(results, axis=1)[:, -5:] == gt_labels.reshape(-1, 1)) /
      gt_labels.size)
  print(f'Top-5 accuracy (quantized): {accuracy * 100:.2f}%')


model = tf.keras.Sequential([
  tf.keras.layers.Input(shape=(224, 224, 3), batch_size=1),
  hub.KerasLayer(MODEL_URI)
])
model.compile(
    loss='sparse_categorical_crossentropy',
    metrics='sparse_top_k_categorical_accuracy')
model.build([1, 224, 224, 3])

# Prepare dataset with 100 examples
ds = tfds.load('imagenet_v2', split='test[:1%]')
ds = ds.map(process_image)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.representative_dataset = representative_dataset(ds)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_model = converter.convert()

test_ds = ds.map(lambda data: (data['image'], data['label'] + 1)).batch(16)
loss, acc = model.evaluate(test_ds)
print(f'Top-5 accuracy (float): {acc * 100:.2f}%')

eval_tflite(quantized_model, ds)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset(ds)

# my_debug_dataset should have the same format as my_representative_dataset
debugger = tf.lite.experimental.QuantizationDebugger(
    converter=converter, debug_dataset=representative_dataset(ds))

debugger.run()

RESULTS_FILE = '/tmp/debugger_results.csv'
with open(RESULTS_FILE, 'w') as f:
  debugger.layer_statistics_dump(f)

!head /tmp/debugger_results.csv

layer_stats = pd.read_csv(RESULTS_FILE)
layer_stats.head()

layer_stats['range'] = 255.0 * layer_stats['scale']
layer_stats['rmse/scale'] = layer_stats.apply(
    lambda row: np.sqrt(row['mean_squared_error']) / row['scale'], axis=1)
layer_stats[['op_name', 'range', 'rmse/scale']].head()

plt.figure(figsize=(15, 5))
ax1 = plt.subplot(121)
ax1.bar(np.arange(len(layer_stats)), layer_stats['range'])
ax1.set_ylabel('range')
ax2 = plt.subplot(122)
ax2.bar(np.arange(len(layer_stats)), layer_stats['rmse/scale'])
ax2.set_ylabel('rmse/scale')
plt.show()

layer_stats[layer_stats['rmse/scale'] > 0.7][[
    'op_name', 'range', 'rmse/scale', 'tensor_name'
]]

suspected_layers = list(
    layer_stats[layer_stats['rmse/scale'] > 0.7]['tensor_name'])

suspected_layers.extend(list(layer_stats[:5]['tensor_name']))

debug_options = tf.lite.experimental.QuantizationDebugOptions(
    denylisted_nodes=suspected_layers)
debugger = tf.lite.experimental.QuantizationDebugger(
    converter=converter,
    debug_dataset=representative_dataset(ds),
    debug_options=debug_options)

selective_quantized_model = debugger.get_nondebug_quantized_model()
eval_tflite(selective_quantized_model, ds)

debug_options = tf.lite.experimental.QuantizationDebugOptions(
    denylisted_ops=['MEAN'])
debugger = tf.lite.experimental.QuantizationDebugger(
    converter=converter,
    debug_dataset=representative_dataset(ds),
    debug_options=debug_options)

selective_quantized_model = debugger.get_nondebug_quantized_model()
eval_tflite(selective_quantized_model, ds)

debug_options = tf.lite.experimental.QuantizationDebugOptions(
    layer_debug_metrics={
        'mean_abs_error': (lambda diff: np.mean(np.abs(diff)))
    },
    layer_direct_compare_metrics={
        'correlation':
            lambda f, q, s, zp: (np.corrcoef(f.flatten(),
                                             (q.flatten() - zp) / s)[0, 1])
    },
    model_debug_metrics={
        'argmax_accuracy': (lambda f, q: np.mean(np.argmax(f) == np.argmax(q)))
    })

debugger = tf.lite.experimental.QuantizationDebugger(
    converter=converter,
    debug_dataset=representative_dataset(ds),
    debug_options=debug_options)

debugger.run()

CUSTOM_RESULTS_FILE = '/tmp/debugger_results.csv'
with open(CUSTOM_RESULTS_FILE, 'w') as f:
  debugger.layer_statistics_dump(f)

custom_layer_stats = pd.read_csv(CUSTOM_RESULTS_FILE)
custom_layer_stats[['op_name', 'mean_abs_error', 'correlation']].tail()

debugger.model_statistics

from tensorflow.lite.python import convert

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.representative_dataset = representative_dataset(ds)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter._experimental_calibrate_only = True
calibrated_model = converter.convert()

# Note that enable_numeric_verify and enable_whole_model_verify are set.
quantized_model = convert.mlir_quantize(
    calibrated_model,
    enable_numeric_verify=True,
    enable_whole_model_verify=True)
debugger = tf.lite.experimental.QuantizationDebugger(
    quant_debug_model_content=quantized_model,
    debug_dataset=representative_dataset(ds))

selective_quantized_model = convert.mlir_quantize(
    calibrated_model, denylisted_nodes=suspected_layers)
eval_tflite(selective_quantized_model, ds)

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

!sudo apt -y install libportaudio2
!pip install -q tflite-model-maker-nightly

import numpy as np
import os

import tensorflow as tf
assert tf.__version__.startswith('2')

from tflite_model_maker import model_spec
from tflite_model_maker import question_answer
from tflite_model_maker.config import ExportFormat
from tflite_model_maker.question_answer import DataLoader

spec = model_spec.get('mobilebert_qa_squad')

train_data_path = tf.keras.utils.get_file(
    fname='triviaqa-web-train-8000.json',
    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json')
validation_data_path = tf.keras.utils.get_file(
    fname='triviaqa-verified-web-dev.json',
    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json')

train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)
validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)

model = question_answer.create(train_data, model_spec=spec)

model.summary()

model.evaluate(validation_data)

model.export(export_dir='.')

model.export(export_dir='.', export_format=ExportFormat.VOCAB)

model.evaluate_tflite('model.tflite', validation_data)

new_spec = model_spec.get('mobilebert_qa')
new_spec.seq_len = 512

import random
import struct
import urllib
from itertools import islice
from struct import unpack

from shapely.geometry import MultiLineString

import vsketch


# unpacking functions are from https://github.com/googlecreativelab/quickdraw-dataset/blob/master/examples/binary_file_parser.py
def unpack_drawing(file_handle):
    (key_id,) = unpack("Q", file_handle.read(8))
    (country_code,) = unpack("2s", file_handle.read(2))
    (recognized,) = unpack("b", file_handle.read(1))
    (timestamp,) = unpack("I", file_handle.read(4))
    (n_strokes,) = unpack("H", file_handle.read(2))
    image = []
    for i in range(n_strokes):
        (n_points,) = unpack("H", file_handle.read(2))
        fmt = str(n_points) + "B"
        x = unpack(fmt, file_handle.read(n_points))
        y = unpack(fmt, file_handle.read(n_points))
        image.append((x, y))

    return {
        "key_id": key_id,
        "country_code": country_code,
        "recognized": recognized,
        "timestamp": timestamp,
        "image": image,
    }


def unpack_drawings(filename):
    with open(filename, "rb") as f:
        while True:
            try:
                yield unpack_drawing(f)
            except struct.error:
                break


def quickdraw_to_linestring(qd_image):
    """Returns a Shapely MultiLineString for the provided quickdraw image.
    This MultiLineString can be passed to vsketch
    """
    linestrings = []
    for i in range(0, len(qd_image["image"])):
        line = zip(qd_image["image"][i][0], qd_image["image"][i][1])
        linestrings.append(tuple(line))
    return MultiLineString(linestrings)


# Set the quickdraw set
QUICKDRAW_SET_NAME = "crab"

quickdraw_filepath, _ = urllib.request.urlretrieve(
    f"https://storage.googleapis.com/quickdraw_dataset/full/binary/{QUICKDRAW_SET_NAME}.bin",
    f"{QUICKDRAW_SET_NAME}.bin",
)

print(quickdraw_filepath)

drawing_set = unpack_drawings(quickdraw_filepath)
drawing_subset = list(islice(drawing_set, 10000))

vsk = vsketch.Vsketch()
vsk.size("125x125mm")
vsk.penWidth("0.5mm")

# Set the dimensions of the grid
grid_size = 5

vsk.scale(1 / grid_size)
samples = random.sample(drawing_subset, grid_size ** 2)
for i in range(grid_size ** 2):
    drawing = quickdraw_to_linestring(samples[i])
    vsk.geometry(drawing)
    vsk.translate(vsk.width, 0)
    if (i + 1) % grid_size == 0:
        vsk.translate(-grid_size * vsk.width, vsk.height)

vsk.display(mode="matplotlib", fig_size=(12, 12))
vsk.save(f"quick_draw_{QUICKDRAW_SET_NAME}.svg")

import math

import numpy as np

import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("1cm")

NUM_LINE = 200
Y_OFFSET = 18 / NUM_LINE
X_FREQ = 0.25
Y_FREQ = 4

x_coords = np.linspace(0, 25, 1000)

for i in range(NUM_LINE):
    y_coords = (
        np.array([vsk.noise(x * X_FREQ, i / NUM_LINE * Y_FREQ) for x in x_coords])
        + Y_OFFSET * i
    )
    vsk.polygon(x_coords, y_coords)

vsk.display(mode="matplotlib")
vsk.save("random_lines.svg")


NUM_LINES = 200
POINT_PER_LINE = 100
RDIR_RANGE = math.pi / 6

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("1cm")
vsk.rotate(-90, degrees=True)


noise_coord = np.linspace(0, 1, POINT_PER_LINE)

dirs = np.linspace(0, 2 * math.pi, NUM_LINES)

for dir in dirs:

    rdir = vsk.map(
        np.array([vsk.noise(x, dir) for x in noise_coord]),
        0,
        1,
        dir - RDIR_RANGE,
        dir + RDIR_RANGE,
    )
    roffset = vsk.map(
        np.array([vsk.noise(x, dir, 100) for x in noise_coord]), 0, 1, 0.05, 0.12
    )

    xoffset = roffset * np.cos(rdir)
    yoffset = roffset * np.sin(rdir)

    vsk.polygon(np.cumsum(xoffset), np.cumsum(yoffset))

vsk.display(mode="matplotlib")
vsk.save("random_flower.svg")

!ls /home/jack/Desktop/GRAPHICS//clouddream/backup/deepdream/outputs

!mkdir junk

import os
import sys
from PIL import Image
import shutil
import time
import random

# Destination Image
path = r"/home/jack/Desktop/GRAPHICS/gmic/640x640/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)

#Palette source
path2 = r"/media/jack/db4e7ba1-11b4-4b5e-a4ce-b4d8004b36ec/junk/deep-dream-generator/notebooks/bugs/Fractalmages/Fractalmages/"
base_image2 = random.choice([
    x for x in os.listdir(path2)
    if os.path.isfile(os.path.join(path2, x))
])
filename1=(path2+base_image2)

shutil.copy2(filename1, 'instagram/')# filename0 copy2 destimation 
shutil.copy2(filename0, 'instagram/')# filename1 copy2 destimation 

aa = Image.open(filename0).convert("RGB")
#bb = Image.open("/home/jack/Documents/GG.jpg").convert("RGB")
bb = Image.open(filename1).convert("RGB")
xx=aa.resize((640,640), Image.NEAREST)
yy=bb.resize((640,640), Image.NEAREST)
xx.save("junk/aa.png")
yy.save("junk/bb.png")
src = Image.open('junk/aa.png').convert('RGB')
dst = Image.open('junk/bb.png').convert('RGB')
src.save("junk/aa.png")
dst.save("junk/bb.png")

n = 5 #number of partitions per channel.
src_handle = Image.open("junk/bb.png")
dst_handle = Image.open("junk/aa.png")
src = src_handle.load()
dst = dst_handle.load()
assert src_handle.size[0]*src_handle.size[1] == dst_handle.size[0]*dst_handle.size[1],"images must be same size"

def makePixelList(img):
    l = []
    for x in range(img.size[0]):
        for y in range(img.size[1]):
            l.append((x,y))
    return l

lsrc = makePixelList(src_handle)
ldst = makePixelList(dst_handle)

def sortAndDivide(coordlist,pixelimage,channel): #core
    global src,dst,n
    retlist = []
    #sort
    coordlist.sort(key=lambda t: pixelimage[t][channel])
    #divide
    partitionLength = int(len(coordlist)/n)
    if partitionLength <= 0:
        partitionLength = 1
    if channel < 2:
        for i in range(0,len(coordlist),partitionLength):
            retlist += sortAndDivide(coordlist[i:i+partitionLength],pixelimage,channel+1)
    else:
        retlist += coordlist
    return retlist

print(src[lsrc[0]])

lsrc = sortAndDivide(lsrc,src,0)
ldst = sortAndDivide(ldst,dst,0)

for i in range(len(ldst)):
    dst[ldst[i]] = src[lsrc[i]]
    
    

filename = time.strftime("junk/%Y%m%d%H%M%S.png")
dst_handle.save(filename)

shutil.copy2(filename, "instagram/")
print filename

%%writefile palletswitch.py
import os
import sys
from PIL import Image
import shutil
import time
import random
n = 5
def main():
    n = 5
    # Destination Image
    path = r"/home/jack/Desktop/GRAPHICS/gmic/640x640/"
    base_image = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
    ])
    filename0=(path+base_image)

    #Palette source
    path2 = r"/media/jack/db4e7ba1-11b4-4b5e-a4ce-b4d8004b36ec/junk/deep-dream-generator/notebooks/bugs/Fractalmages/Fractalmages/"
    base_image2 = random.choice([
        x for x in os.listdir(path2)
        if os.path.isfile(os.path.join(path2, x))
    ])
    filename1=(path2+base_image2)

    shutil.copy2(filename1, 'instagram/')# filename0 copy2 destimation 
    shutil.copy2(filename0, 'instagram/')# filename1 copy2 destimation 

    aa = Image.open(filename0).convert("RGB")
    #bb = Image.open("/home/jack/Documents/GG.jpg").convert("RGB")
    bb = Image.open(filename1).convert("RGB")
    xx=aa.resize((640,640), Image.NEAREST)
    yy=bb.resize((640,640), Image.NEAREST)
    xx.save("junk/aa.png")
    yy.save("junk/bb.png")
    src = Image.open('junk/aa.png').convert('RGB')
    dst = Image.open('junk/bb.png').convert('RGB')
    src.save("junk/aa.png")
    dst.save("junk/bb.png")

    n = 5 #number of partitions per channel.
    src_handle = Image.open("junk/bb.png")
    dst_handle = Image.open("junk/aa.png")
    src = src_handle.load()
    dst = dst_handle.load()
    assert src_handle.size[0]*src_handle.size[1] == dst_handle.size[0]*dst_handle.size[1],"images must be same size"

    def makePixelList(img):
        l = []
        for x in range(img.size[0]):
            for y in range(img.size[1]):
                l.append((x,y))
        return l

    lsrc = makePixelList(src_handle)
    ldst = makePixelList(dst_handle)

    def sortAndDivide(coordlist,pixelimage,channel): #core
        global src,dst,n
        retlist = []
        #sort
        coordlist.sort(key=lambda t: pixelimage[t][channel])
        #divide
        partitionLength = int(len(coordlist)/n)
        if partitionLength <= 0:
            partitionLength = 1
        if channel < 2:
            for i in range(0,len(coordlist),partitionLength):
                retlist += sortAndDivide(coordlist[i:i+partitionLength],pixelimage,channel+1)
        else:
            retlist += coordlist
        return retlist

    print(src[lsrc[0]])

    lsrc = sortAndDivide(lsrc,src,0)
    ldst = sortAndDivide(ldst,dst,0)

    for i in range(len(ldst)):
        dst[ldst[i]] = src[lsrc[i]]



    filename = time.strftime("junk/%Y%m%d%H%M%S.png")
    dst_handle.save(filename)

    shutil.copy2(filename, "instagram/")
    return filename

!rm palletswitch.pyc

import palletswitch
count = 0
while count <50:
    palletswitch.main()
    count = count +1

!showme junk/20180509142745.png

import os
import sys
from PIL import Image
import shutil
import time
import random
import numpy as np
from numpy import asarray
path = r"/media/jack/db4e7ba1-11b4-4b5e-a4ce-b4d8004b36ec/junk/deep-dream-generator/notebooks/bugs/Fractalmages/Fractalmages/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)

path2 = r"posted/"
base_image2 = random.choice([
    x for x in os.listdir(path2)
    if os.path.isfile(os.path.join(path2, x))
])
filename1=(path2+base_image2)



imgA = Image.open(filename0).convert("RGB");imga =imgA.resize((640,640), Image.NEAREST)
imga.save('TMPa.jpg');imgc = Image.open('TMPa.jpg')
imgc.load()
imgB = Image.open(filename1).convert("RGB");imgb =imgB.resize((640,640), Image.NEAREST)
imga.save('TMPb.jpg');imgd = Image.open('TMPb.jpg')
imgd.load()
print imgc.size, imgd.size

# Images divide function
import os
import sys
from PIL import Image
import shutil
import time
import random
import numpy as np
from numpy import asarray
path = r"/home/jack/Desktop/deep-dream-generator/notebooks/context-free/output/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)

path2 = r"posted/"
base_image2 = random.choice([
    x for x in os.listdir(path2)
    if os.path.isfile(os.path.join(path2, x))
])
filename1=(path2+base_image2)

imgA = Image.open(filename0).convert("RGB")
imga =imgA.resize((640,640), Image.NEAREST);imga.save('TMPa.jpg')
imgc = Image.open('TMPa.jpg');imgc.load()
imgB = Image.open(filename1).convert("RGB")
imgb =imgB.resize((640,640), Image.NEAREST);imga.save('TMPb.jpg')
imgd = Image.open('TMPb.jpg');imgd.load()

a = asarray(imgc);b = asarray(imgd)
c = a/((b.astype('float')+1)/256)
d = c*(c < 255)+255*np.ones(np.shape(c))*(c > 255)
e = d.astype('uint8')

imgOut = Image.fromarray(e)
#imgOut.save('PILdiv0.png', 'PNG')
imgOut

!chmod a+x PaletteGenerator

%%writefile PaletteGenerator
#!/bin/bash

while true; do
  python PaletteGenerator.py
  echo "posted :"
  date
  sleep 1200s
done

%%writefile PaletteGenerator.py
#!/home/jack/anaconda2/python
import random
from random import randint
import time
import markovify
import os
import sys
sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
from PIL import Image, ImageChops, ImageDraw, ImageFont, ImageFilter
import os
import sys
from PIL import Image
import shutil
import time
import random
#input2='/home/jack/Desktop/deep-dream-generator/notebooks/bugs/butterflies/000163.jpg'
#input1='/home/jack/Desktop/deep-dream-generator/notebooks/context-free/output/20170828154222.png'

path = r"/home/jack/Desktop/deep-dream-generator/notebooks/context-free/output/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)

#custom = "junk/mixpalette.png"
#filename0=(filename0)

path2 = r"posted/"
base_image2 = random.choice([
    x for x in os.listdir(path2)
    if os.path.isfile(os.path.join(path2, x))
])
filename1=(path2+base_image2)

#custom = "junk/mixpalette.png"
#filename0=(filename0)




#input1='/home/jack/Desktop/imagebot/colorful/20170824124329.jpg'
#input2='/home/jack/Desktop/imagebot/posted/'
shutil.copy2(filename0, 'instagram/') # complete target filename given
shutil.copy2(filename1, 'instagram/')# target filename is /dst/dir/file.ext

aa = Image.open(filename0).convert("RGB")
#bb = Image.open("/home/jack/Documents/GG.jpg").convert("RGB")
bb = Image.open(filename1).convert("RGB")
xx=aa.resize((640,640), Image.NEAREST)
yy=bb.resize((640,640), Image.NEAREST)
xx.save("junk/aa.png")
yy.save("junk/bb.png")
src = Image.open('junk/aa.png').convert('RGB')
dst = Image.open('junk/bb.png').convert('RGB')
src.save("junk/aa.png")
dst.save("junk/bb.png")



n = 5 #number of partitions per channel.


src_handle = Image.open("junk/bb.png")
dst_handle = Image.open("junk/aa.png")
src = src_handle.load()
dst = dst_handle.load()
assert src_handle.size[0]*src_handle.size[1] == dst_handle.size[0]*dst_handle.size[1],"images must be same size"

def makePixelList(img):
    l = []
    for x in range(img.size[0]):
        for y in range(img.size[1]):
            l.append((x,y))
    return l

lsrc = makePixelList(src_handle)
ldst = makePixelList(dst_handle)

def sortAndDivide(coordlist,pixelimage,channel): #core
    global src,dst,n
    retlist = []
    #sort
    coordlist.sort(key=lambda t: pixelimage[t][channel])
    #divide
    partitionLength = int(len(coordlist)/n)
    if partitionLength <= 0:
        partitionLength = 1
    if channel < 2:
        for i in range(0,len(coordlist),partitionLength):
            retlist += sortAndDivide(coordlist[i:i+partitionLength],pixelimage,channel+1)
    else:
        retlist += coordlist
    return retlist

print(src[lsrc[0]])

lsrc = sortAndDivide(lsrc,src,0)
ldst = sortAndDivide(ldst,dst,0)

for i in range(len(ldst)):
    dst[ldst[i]] = src[lsrc[i]]
    
    
filename = time.strftime("junk/PalletteTemp.png")

dst_handle.save(filename)

shutil.copy2(filename, "instagram/")

custom = "junk/PalletteTemp.png"
filename0=(custom)




def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

def draw_text_with_halo(img, position, text, font, col, halo_col):
    #halo = Image.new('RGBA', img.size, (0, 0, 0, 0))
    halo = Image.new('RGBA', img.size, (255,255,255, 0))
    ImageDraw.Draw(halo).text(position, text, font = font, fill = halo_col)
    blurred_halo = halo.filter(ImageFilter.BLUR)
    ImageDraw.Draw(blurred_halo).text(position, text, font = font, fill = col)
    return Image.composite(img, blurred_halo, ImageChops.invert(blurred_halo))

if __name__ == '__main__':
    inp = Image.open(filename0)
    font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 45)
    #font = ImageFont.truetype("/home/jack/.fonts/GRUNTREAPER.ttf", 70)
    #font = ImageFont.truetype("/home/jack/.fonts/Nightbird.ttf", 70)
    #font = ImageFont.truetype("/home/jack/.fonts/Nightbird.ttf", 70)
    #font = ImageFont.truetype("/home/jack/.fonts/Punktype.ttf", 70)
    text_col = (255,200,0) # bright green
    halo_col = (0, 0,0)   # black
    #text_col = (230, 230,250) # bright green
    #halo_col = (0,0,0)  # black (0,0,0)    
    textin = (generate_the_word("wordcloud.txt"))
    #i2 = draw_text_with_halo(inp, (15, 8), "HIGH FIVE", font, text_col, halo_col)
    i2 = draw_text_with_halo(inp, (20, 10), textin, font, text_col, halo_col)
    #txt = Image.new('RGBA', i.size, (255,255,255,0))

    # get a font
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
    
    # get a drawing context
    width, height = inp.size
    marginx = 225
    marginy = 35
    x = width - marginx
    y = height - marginy
    signature_ = "The TwitterBot Project" 
    text_col2 = (15, 5, 10) # bright green
    halo_col2 = (255, 255,255) 
    #text_col2 = (255, 255,255) # signature white
    #halo_col2 = (0, 0, 0)   # signature black
    txt=draw_text_with_halo(i2,(x,y), signature_, fnt, text_col2, halo_col2)
    out = Image.alpha_composite(i2, txt)
    out.save("tmp/TM_POST.jpg")

#removed keys for privacy reasons
CONSUMER_KEY = 'YazCRIfWX4VICiRCOiph08jDL'
CONSUMER_SECRET = 'QOkLHou6NMwkghSHjMFXMdffQKJlDzttKtP6uBCcZ4VlQtvJyc'
ACCESS_KEY = '296906916-AWggjhqpEWIS7EzXXhc2pOPBeCVJczpOm11cQGIf'
ACCESS_SECRET = 'zFrCiyaPt8gCBVVs1bLCmdCSyQQ3DKxT5wHJq2tOu2AMj'



twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
f = open("art.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
#STR = (text_model.make_short_sentence(140))
#STR = ("$Python $Imagery $Redneck #filipina #filipinowoman posted with #JupyterNotebook.")
STR = (generate_the_word("key.txt"))
#PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/STUFF/experiment/experiment8.jpg"
PATH = "tmp/TM_POST.jpg"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')

#photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])
#!showme tmp/TM_POST.jpg



Data sets
Package	Item	Title
datasets 	AirPassengers 	Monthly Airline Passenger Numbers 1949-1960
datasets 	BJsales 	Sales Data with Leading Indicator
datasets 	BJsales.lead (BJsales) 	Sales Data with Leading Indicator
datasets 	BOD 	Biochemical Oxygen Demand
datasets 	CO2 	Carbon Dioxide Uptake in Grass Plants
datasets 	ChickWeight 	Weight versus age of chicks on different diets
datasets 	DNase 	Elisa assay of DNase
datasets 	EuStockMarkets 	Daily Closing Prices of Major European Stock Indices, 1991-1998
datasets 	Formaldehyde 	Determination of Formaldehyde
datasets 	HairEyeColor 	Hair and Eye Color of Statistics Students
datasets 	Harman23.cor 	Harman Example 2.3
datasets 	Harman74.cor 	Harman Example 7.4
datasets 	Indometh 	Pharmacokinetics of Indomethacin
datasets 	InsectSprays 	Effectiveness of Insect Sprays
datasets 	JohnsonJohnson        	Quarterly Earnings per Johnson & Johnson Share                 
datasets 	LakeHuron 	Level of Lake Huron 1875-1972
datasets 	LifeCycleSavings 	Intercountry Life-Cycle Savings Data
datasets 	Loblolly 	Growth of Loblolly pine trees
datasets 	Nile 	Flow of the River Nile
datasets 	Orange 	Growth of Orange Trees
datasets 	OrchardSprays 	Potency of Orchard Sprays
datasets 	PlantGrowth 	Results from an Experiment on Plant Growth
datasets 	Puromycin 	Reaction Velocity of an Enzymatic Reaction
datasets 	Seatbelts 	Road Casualties in Great Britain 1969-84
datasets 	Theoph 	Pharmacokinetics of Theophylline
datasets 	Titanic 	Survival of passengers on the Titanic
datasets 	ToothGrowth 	The Effect of Vitamin C on Tooth Growth in Guinea Pigs
datasets 	UCBAdmissions 	Student Admissions at UC Berkeley
datasets 	UKDriverDeaths 	Road Casualties in Great Britain 1969-84
datasets 	UKgas 	UK Quarterly Gas Consumption
datasets 	USAccDeaths 	Accidental Deaths in the US 1973-1978
datasets 	USArrests 	Violent Crime Rates by US State
datasets 	USJudgeRatings 	Lawyers' Ratings of State Judges in the US Superior Court
datasets 	USPersonalExpenditure 	Personal Expenditure Data
datasets 	UScitiesD 	Distances Between European Cities and Between US Cities
datasets 	VADeaths 	Death Rates in Virginia (1940)
datasets 	WWWusage 	Internet Usage per Minute
datasets 	WorldPhones 	The World's Telephones
datasets 	ability.cov 	Ability and Intelligence Tests
datasets 	airmiles 	Passenger Miles on Commercial US Airlines, 1937-1960
datasets 	airquality 	New York Air Quality Measurements
datasets 	anscombe 	Anscombe's Quartet of 'Identical' Simple Linear Regressions
datasets 	attenu 	The Joyner-Boore Attenuation Data
datasets 	attitude 	The Chatterjee-Price Attitude Data
datasets 	austres 	Quarterly Time Series of the Number of Australian Residents
datasets 	beaver1 (beavers) 	Body Temperature Series of Two Beavers
datasets 	beaver2 (beavers) 	Body Temperature Series of Two Beavers
datasets 	cars 	Speed and Stopping Distances of Cars
datasets 	chickwts 	Chicken Weights by Feed Type
datasets 	co2 	Mauna Loa Atmospheric CO2 Concentration
datasets 	crimtab 	Student's 3000 Criminals Data
datasets 	discoveries 	Yearly Numbers of Important Discoveries
datasets 	esoph 	Smoking, Alcohol and (O)esophageal Cancer
datasets 	euro 	Conversion Rates of Euro Currencies
datasets 	euro.cross (euro) 	Conversion Rates of Euro Currencies
datasets 	eurodist 	Distances Between European Cities and Between US Cities
datasets 	faithful 	Old Faithful Geyser Data
datasets 	fdeaths (UKLungDeaths) 	Monthly Deaths from Lung Diseases in the UK
datasets 	freeny 	Freeny's Revenue Data
datasets 	freeny.x (freeny) 	Freeny's Revenue Data
datasets 	freeny.y (freeny) 	Freeny's Revenue Data
datasets 	infert 	Infertility after Spontaneous and Induced Abortion
datasets 	iris 	Edgar Anderson's Iris Data
datasets 	iris3 	Edgar Anderson's Iris Data
datasets 	islands 	Areas of the World's Major Landmasses
datasets 	ldeaths (UKLungDeaths) 	Monthly Deaths from Lung Diseases in the UK
datasets 	lh 	Luteinizing Hormone in Blood Samples
datasets 	longley 	Longley's Economic Regression Data
datasets 	lynx 	Annual Canadian Lynx trappings 1821-1934
datasets 	mdeaths (UKLungDeaths) 	Monthly Deaths from Lung Diseases in the UK
datasets 	morley 	Michelson Speed of Light Data
datasets 	mtcars 	Motor Trend Car Road Tests
datasets 	nhtemp 	Average Yearly Temperatures in New Haven
datasets 	nottem 	Average Monthly Temperatures at Nottingham, 1920-1939
datasets 	npk 	Classical N, P, K Factorial Experiment
datasets 	occupationalStatus 	Occupational Status of Fathers and their Sons
datasets 	precip 	Annual Precipitation in US Cities
datasets 	presidents 	Quarterly Approval Ratings of US Presidents
datasets 	pressure 	Vapor Pressure of Mercury as a Function of Temperature
datasets 	quakes 	Locations of Earthquakes off Fiji
datasets 	randu 	Random Numbers from Congruential Generator RANDU
datasets 	rivers 	Lengths of Major North American Rivers
datasets 	rock 	Measurements on Petroleum Rock Samples
datasets 	sleep 	Student's Sleep Data
datasets 	stack.loss (stackloss) 	Brownlee's Stack Loss Plant Data
datasets 	stack.x (stackloss) 	Brownlee's Stack Loss Plant Data
datasets 	stackloss 	Brownlee's Stack Loss Plant Data
datasets 	state.abb (state) 	US State Facts and Figures
datasets 	state.area (state) 	US State Facts and Figures
datasets 	state.center (state) 	US State Facts and Figures
datasets 	state.division (state) 	US State Facts and Figures
datasets 	state.name (state) 	US State Facts and Figures
datasets 	state.region (state) 	US State Facts and Figures
datasets 	state.x77 (state) 	US State Facts and Figures
datasets 	sunspot.month 	Monthly Sunspot Data, from 1749 to "Present"
datasets 	sunspot.year 	Yearly Sunspot Data, 1700-1988
datasets 	sunspots 	Monthly Sunspot Numbers, 1749-1983
datasets 	swiss 	Swiss Fertility and Socioeconomic Indicators (1888) Data
datasets 	treering 	Yearly Treering Data, -6000-1979
datasets 	trees 	Diameter, Height and Volume for Black Cherry Trees
datasets 	uspop 	Populations Recorded by the US Census
datasets 	volcano 	Topographic Information on Auckland's Maunga Whau Volcano
datasets 	warpbreaks 	The Number of Breaks in Yarn during Weaving
datasets 	women 	Average Heights and Weights for American Women

Use ‘data(package = .packages(all.available = TRUE))’ to list the data sets in all *available* packages.


packageurl <- "http://cran.r-project.org/src/contrib/Archive/XXXX/XXXX_A.B.C.tar.gz"
install.packages(packageurl, contriburl=NULL, type="source")

library(rtweet)
library(tidyverse)

sen_df <- get_timeline("SenateFloor", 300)

mutate(sen_df, `Tweet Length`=map_dbl(text, nchar)) %>% 
  ggplot(aes(`Tweet Length`)) +
  ggalt::geom_bkde(color="steelblue", fill="steelblue", alpha=2/3) +
  scale_y_continuous(expand=c(0,0)) +
  labs(title="@SenateFloor Tweet Length Distribution") +
  hrbrthemes::theme_ipsum_rc(grid="XY")


install.packages("rtweet", dependencies = TRUE)

ap <- available.packages()
View(ap)
"foobarbaz" %in% rownames(ap)




name = IRuby.input 'Enter your name'

result = IRuby.form do 
  input :username
  password :password
  button
end

result = IRuby.popup 'Please enter your name' do 
  input
  button
end

result = IRuby.popup 'Confirm' do 
  text 'Are you sure you want to continue?'
  cancel 'No'
  button 'Yes'
end

result = IRuby.form do
  input :username
  password :password
end

result = IRuby.form do 
  input :name, label: 'Please enter your name'
  cancel 'None of your business!'
  button :submit, label: 'All done'
end

result = IRuby.form do 
  checkbox :one, 'Fish', 'Cat', 'Dog', default: 'Fish'
  checkbox :many, 'Fish', 'Cat', 'Dog', default: ['Cat', 'Dog']
  checkbox :all, 'Fish', 'Cat', 'Dog', default: true
  button :submit, label: 'All done'
end

result = IRuby.form do 
  date :birthday
  date :today, default: Time.now
  button
end

result = IRuby.form do 
  IRuby::Input::Button::COLORS.each_key do |color|
    button color, color: color
  end
end

result = IRuby.form do 
  text 'Enter email addresses, one per line (use shift+enter for newlines)'
  textarea :emails
end

result = IRuby.form do 
  html { h1 'Choose a Stooge' }
  text 'Choose your favorite stooge'
  select :stooge, 'Moe', 'Larry', 'Curly'
  button
end

result = IRuby.form do 
  select :stooge, 'Moe', 'Larry', 'Curly'
  select :stooge, 'Moe', 'Larry', 'Curly', default: 'Moe'
  multiple :stooges, 'Moe', 'Larry', 'Curly', default: true, size: 3
  multiple :stooges, 'Moe', 'Larry', 'Curly', default: ['Moe','Curly']
  button
end

result = IRuby.form do 
  radio :children, *(0..12), label: 'How many children do you have?'
  checkbox :gender, 'Male', 'Female', 'Intersex', label: 'Select the genders of your children'
  button
end

IRuby.form do 
  file :avatar, label: 'Choose an Avatar'
  button :submit
end

result = IRuby.form do 
  html { h1 'The Everything Form' }
  text 'Marvel at the strange and varied inputs!'
  date
  file
  input :username
  password
  textarea
  radio *(1..10)
  checkbox 'Fish', 'Cat', 'Dog', label: 'Animals'
  select :color, *IRuby::Input::Button::COLORS.keys
  cancel                     
  button    
end

#%%writefile checkDB
#!/usr/bin.python2
from Completed import track_download
import subprocess
import sqlite3
database = 'LBRYDATA.db'
conn = sqlite3.connect(database)  
c=conn.cursor()
search = raw_input("SEARCH: ")
ID = []
cnt=0
for row in c.execute('SELECT ROWID, * FROM LBRY'):
    if search in row[1] or search in row[2]:
        Cnt=str(cnt)
        All=Cnt+"-",row[1],":",row[2]
        print All
        cnt=cnt+1
        ID.append(All)
download = int(raw_input("download  number: "))
if download==99:raise SystemExit("Stopped by 99 !") 
print "----------------"
print "ID to Download : ",ID[download]
print "----------------"
ST = str(ID[download])
Id = ST.split("'")
print Id[len(Id)-2]


os.getcwd()

# This the full path to current directory
import os
DownloadDirectory = os.getcwd()+"/DownloadDirectory"
print DownloadDirectory

!ls

#%%writefile DB_Current
#!/usr/bin.python2
from Completed import track_download
import subprocess
import sqlite3
import os
if not os.path.exists('DownloadDirectory'):
    os.makedirs('DownloadDirectory')
database = 'LBRYDATA.db'
conn = sqlite3.connect(database)  
c=conn.cursor()
search = raw_input("SEARCH: ")
if search == 99:exit()
ID = []
cnt=0
for row in c.execute('SELECT ROWID, * FROM LBRY'):
    if search in row[1] or search in row[2]:
        Cnt=str(cnt)
        All=Cnt+"-",row[1],":",row[2]
        print All
        cnt=cnt+1
        ID.append(All)
download = int(raw_input("download  number: "))
print "---------------------------"
print "ID to Download",ID[download]
print "---------------------------"
ST = str(ID[download])
Id = ST.split("'")
print Id[len(Id)-2]
bashCommand = "lbrynet get "+Id[len(Id)-2]+" --download_directory=DownloadDirectory"
# This downloads to the Default LBRY folder ( $HOME/Downloads )
# bashCommand = "lbrynet get "+Id[len(Id)-2]
print bashCommand
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate() 
print "waiting ..."
#track_download()

%%writefile DB_Download
#!/usr/bin.python2
from Completedpy2 import track_download
import subprocess
import sqlite3
database = 'LBRYDATA.db'
conn = sqlite3.connect(database)  
c=conn.cursor()
search = raw_input("SEARCH: ")
ID = []
cnt=0
for row in c.execute('SELECT ROWID, * FROM LBRY'):
    if search in row[1] or search in row[2]:

        Cnt=str(cnt)
        print Cnt,"-",row[0],":",row[1],":",row[2]
        All=Cnt+"-",row[0],":",row[1],":",row[2]
        cnt=cnt+1        
        ID.append(All)
download = int(raw_input("download  number: "))
#print ID[download-1]
print "ID",ID[download]
print "---------------------------"
ST = str(ID[3])
Id = ST.split("'")
print len(Id)-2
print Id[len(Id)-2]
#for item in Id:
#    print item
bashCommand = "lbrynet get "+Id[len(Id)-2]
print bashCommand
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate() 
print "waiting ..."
track_download()


import subprocess
bashCommand = "lbrynet get synfig-studio-kdenlive-create-animated#b6847ad5de1d9c72ea49a15afbd6436839a1b5c4 --download_directory= /home/jack/Desktop/lighthouse"
print bashCommand
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate() 

os.getcwd()

# Download to lbry default $HOME/Downloads
!lbrynet get synfig-studio-kdenlive-create-animated#b6847ad5de1d9c72ea49a15afbd6436839a1b5c4 



#%%writefile checkDB
#!/usr/bin.python2
from Completed import track_download
import subprocess
import sqlite3
database = 'LBRYDATA.db'
conn = sqlite3.connect(database)  
c=conn.cursor()
search = input("SEARCH: ")
ID = []
cnt=0
for row in c.execute('SELECT ROWID, * FROM LBRY'):
    if search in row[1] or search in row[2]:
        Cnt=str(cnt)
        All=Cnt+"-",row[1],":",row[2]
        print (All)
        cnt=cnt+1
        ID.append(All)
download = int(input("download  number: "))
if download==99:raise SystemExit("Stopped by 99 !") 
print ("----------------")
print ("ID to Download : ",ID[download])
print ("----------------")
ST = str(ID[download])
Id = ST.split("'")
print (Id[len(Id)-2])


import os

os.getcwd()

!mkdir /home/jack/Desktop/LBRY-toolbox/DownloadDirectory

# This the full path to current directory
import os

DownloadDirectory = os.getcwd()+"/DownloadDirectory"
print (DownloadDirectory)

!ls DownloadDirectory

!ls DownloadDirectory | pr -4 -t

#%%writefile DB_Current
#!/usr/bin.python2
from Completed import track_download
import subprocess
import sqlite3
import os
if not os.path.exists('DownloadDirectory'):
    os.makedirs('DownloadDirectory')
database = 'LBRYDATA.db'
conn = sqlite3.connect(database)  
c=conn.cursor()
search = input("SEARCH: ")
if search == 99:exit()
ID = []
cnt=0
for row in c.execute('SELECT ROWID, * FROM LBRY'):
    if search in row[1] or search in row[2]:
        Cnt=str(cnt)
        All=Cnt+"-",row[1],":",row[2]
        print (All)
        cnt=cnt+1
        ID.append(All)
download = int(input("download  number: "))
print ("---------------------------")
print ("ID to Download",ID[download])
print ("---------------------------")
ST = str(ID[download])
Id = ST.split("'")
print (Id[len(Id)-2])
bashCommand = "lbrynet get "+Id[len(Id)-2]+" --download_directory=/home/jack/Desktop/LBRY-toolbox/DownloadDirectory"
# This downloads to the Default LBRY folder ( $HOME/Downloads )
# bashCommand = "lbrynet get "+Id[len(Id)-2]
print (bashCommand)
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate() 
print ("waiting ...")
#track_download()

lbrynet get  --download_directory=/home/jack/Desktop/LBRY-toolbox/DownloadDirectory

!lbrynet get how-to-use-kdenlive-video-editor-on-kali#ef4aa44d360e2cce99ef3677371aadc84f03bfb2 --download_directory=/home/jack/Desktop/LBRY-toolbox/DownloadDirectory

%%writefile DB_Download
#!/usr/bin.python2
from Completedpy2 import track_download
import subprocess
import sqlite3
database = 'LBRYDATA.db'
conn = sqlite3.connect(database)  
c=conn.cursor()
search = raw_input("SEARCH: ")
ID = []
cnt=0
for row in c.execute('SELECT ROWID, * FROM LBRY'):
    if search in row[1] or search in row[2]:

        Cnt=str(cnt)
        print Cnt,"-",row[0],":",row[1],":",row[2]
        All=Cnt+"-",row[0],":",row[1],":",row[2]
        cnt=cnt+1        
        ID.append(All)
download = int(raw_input("download  number: "))
#print ID[download-1]
print "ID",ID[download]
print "---------------------------"
ST = str(ID[3])
Id = ST.split("'")
print len(Id)-2
print Id[len(Id)-2]
#for item in Id:
#    print item
bashCommand = "lbrynet get "+Id[len(Id)-2]
print bashCommand
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate() 
print "waiting ..."
track_download()


import subprocess
bashCommand = "lbrynet get synfig-studio-kdenlive-create-animated#b6847ad5de1d9c72ea49a15afbd6436839a1b5c4 --download_directory= /home/jack/Desktop/lighthouse"
print bashCommand
process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
output, error = process.communicate() 

os.getcwd()

# Download to lbry default $HOME/Downloads
!lbrynet get synfig-studio-kdenlive-create-animated#b6847ad5de1d9c72ea49a15afbd6436839a1b5c4 



%%writefile goodfrombad.py
#This one tries to to tell good blots from bad by particle_count`ing
# over twenty goes to one directory under twenty goes to another
from PIL import Image, ImageFilter, ImageOps
import numpy as np
import random
from random import randint
import sys
import time
import cv2
import pylab
from scipy import ndimage
count = 0
while count < 200:
    imgsize = 640, 640
    color = (0, 0, 0)
    img = Image.new("RGB", imgsize, "white")
    max_range = randint(100,700)
    for j in range(0,max_range):
        start = (random.randrange(0, imgsize[0]/2), random.randrange(0, imgsize[1]))
        point = start
        img.putpixel(point, color)
        """
        point_list = [p for p in 
                      [(point[0], point[1]+1), (point[0], point[1]-1), 
                       #(point[0]+1, point[1]+1), (point[0]-1, point[1]-1), 
                       #(point[0]-1, point[1]+1), (point[0]+1, point[1]-1), 
                       (point[0]+1, point[1]), (point[0]-1, point[1])]
                      if 0 < p[0] and 0 < p[1] < imgsize[1] and p not in avoid_points]
        """

        blotsize = random.randrange(0, 640)
        for i in range(blotsize):
            directions = [(point[0], point[1]+1), (point[0], point[1]-1),
                         (point[0]+1, point[1]+1), (point[0]-1, point[1]-1), 
                         (point[0]-1, point[1]+1), (point[0]+1, point[1]-1),                       
                         (point[0]+1, point[1]), (point[0]-1, point[1])]
            toremove = []
            for direction in directions:
                if direction[0]>=(imgsize[0]/2) or direction[1]>=imgsize[1] or direction[0]<0 or direction[1]<0:
                    toremove.append(direction)
            for d in toremove:
                directions.remove(d)
            point = random.choice(directions)
            img.putpixel(point, color)


    cropped = img.crop((0, 0, imgsize[0]//2, imgsize[1]))
    img = img.transpose(Image.FLIP_LEFT_RIGHT)
    img.paste(cropped, (0, 0, imgsize[0]//2, imgsize[1]))
    rad = randint(5,30)
    img = img.filter(ImageFilter.GaussianBlur(radius=rad))
    img.save("images/blot.png")


    def binarize_array(numpy_array, threshold=200):
        """Binarize a numpy array."""
        for i in range(len(numpy_array)):
            for j in range(len(numpy_array[0])):
                if numpy_array[i][j] > threshold:
                    numpy_array[i][j] = 255
                else:
                    numpy_array[i][j] = 0
        return numpy_array

    filename0=('images/blot.png')

    im = Image.open(filename0)
    im_grey = im.convert('LA') # convert to grayscale
    width,height = im.size

    total=0
    for i in range(0,width):
        for j in range(0,height):
            total += im_grey.getpixel((i,j))[0]

    mean = total / (width * height)

    image_file = Image.open(filename0)
    imagex = image_file.convert('L')  # convert image to monochrome
    imagey = np.array(imagex)
    #imagez = binarize_array(imagey, threshold)
    imagez = binarize_array(imagey, mean)
    time.sleep(2)
    filename = time.strftime("TEMP/tmpp.png")
    cv2.imwrite(filename, imagez)

    im = cv2.imread('TEMP/tmpp.png')
    pylab.figure(0)
    pylab.imshow(im)

    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
    gray = cv2.GaussianBlur(gray, (5,5), 0)
    maxValue = 255
    adaptiveMethod = cv2.ADAPTIVE_THRESH_GAUSSIAN_C#cv2.ADAPTIVE_THRESH_MEAN_C #cv2.ADAPTIVE_THRESH_GAUSSIAN_C
    thresholdType = cv2.THRESH_BINARY#cv2.THRESH_BINARY #cv2.THRESH_BINARY_INV
    blockSize = 5 #odd number like 3,5,7,9,11
    C = -3 # constant to be subtracted
    im_thresholded = cv2.adaptiveThreshold(gray, maxValue, adaptiveMethod, thresholdType, blockSize, C) 
    labelarray, particle_count = ndimage.measurements.label(im_thresholded)
    
    if particle_count < 20:
        print"JUST RIGHT : ",particle_count
        filename = time.strftime("TEMP/good%Y%m%d%H%M%S.png")
        ImageOps.expand(Image.open('TEMP/tmpp.png').convert("RGB"),border=30,fill='red').save(filename)
        #cv2.imwrite(filename, imagez)
    else:
        print "TOO MUCH : ",particle_count
        filename = time.strftime("junk/toomany%Y%m%d%H%M%S.png")
        ImageOps.expand(Image.open('TEMP/tmpp.png').convert("RGB"),border=30,fill='red').save(filename)
        #cv2.imwrite(filename, imagez)
    print count    
    count = count+1




%%writefile GOODblot.py
import random
from PIL import Image, ImageFilter, ImageOps
import time
import cv2
from random import randint
import numpy as np
import os
#imgsize = (2000, 1000)
#seed_count = 10
#seed_max_size = 18000
count= 0
while count < 20:
    imgsize = (640, 640)
    seed_count = randint(6, 10)
    seed_max_size = randint(5000,16000)

    margin_h = 60
    margin_v = 60
    degradation = 10
    max_white = 100

    color = (0, 0, 0)
    img = Image.new("RGB", imgsize, "white")


    def next_points(point, avoid_points=[], shuffle=True):
        point_list = [p for p in 
                      [(point[0], point[1]+1), (point[0], point[1]-1), 
                       #(point[0]+1, point[1]+1), (point[0]-1, point[1]-1), 
                       #(point[0]-1, point[1]+1), (point[0]+1, point[1]-1), 
                       (point[0]+1, point[1]), (point[0]-1, point[1])]
                      if 0 < p[0] and 0 < p[1] < imgsize[1] and p not in avoid_points]

        for idx in range(len(point_list)):
            if point_list[idx][0] > imgsize[0]//2:
                point_list[idx] = (point[0], 
                                   point_list[idx][1] if point_list[idx][1] != point[1] else random.choice([point[1]+1,
                                                                                                            point[1]-1]))

        point_list = [p for p in point_list                  
                      if 0 < p[0] and 0 < p[1] < imgsize[1] and p not in avoid_points]

        if shuffle:
            random.shuffle(point_list)

        return point_list

    def degrade_color(color):
        return (color[0] + degradation, 
                color[1] + degradation,
                color[2] + degradation)

    def upgrade_color(color):
        return (color[0] - degradation//2, 
                color[1] - degradation//2,
                color[2] - degradation//2)

    def spread(img, point, color):
        if color[0] <= max_white and img.getpixel(point)[0] > color[0]:
            img.putpixel(point, color)
            points = next_points(point, shuffle=False)
            color = degrade_color(color)
            for point in points:
                spread(img, point, color)

    old_points = []
    posible_root_points = []
    for seed in range(0, seed_count):
        #print("Seed: %d" % seed)
        point = None
        while not point or point in old_points:
            point = (random.randrange(0 + margin_h, imgsize[0]//2), 
                     random.randrange(0 + margin_v, imgsize[1] - margin_v))
        old_points.append(point)
        posible_root_points.append(point)
        img.putpixel(point, color)

        seedsize = random.randrange(0, seed_max_size)
        #print("Seed size: %d" % seedsize)
        flow = 0
        for progress in range(0, seedsize):
            flow += 1
            points = next_points(point, old_points)
            try:
                point = points.pop()
            except IndexError:
                posible_root_points.remove(point)
                #print("Looking for old points... Seed: %d Seed Size: %d "
                      "Progress: %d Flow: %d Statistic: %d" % (seed,
                                                               seedsize,
                                                               progress,
                                                               flow, 
                                                               len(posible_root_points)))
                for idx in reversed(range(0, len(posible_root_points))):
                    points = next_points(posible_root_points[idx], old_points)
                    try:
                        point = points.pop()
                        #print("Using old point...")
                        flow = 0
                        break;
                    except IndexError:
                        posible_root_points.pop()
                if not point:
                    #print("No way!")
                    break

            old_points.append(point)
            posible_root_points.append(point)
            img.putpixel(point, color)

            for surr_point in points:
                spread(img, surr_point, degrade_color(color))

    print ("Cropping...")
    cropped = img.crop((0, 0, imgsize[0]//2, imgsize[1]))
    img = img.transpose(Image.FLIP_LEFT_RIGHT)
    img.paste(cropped, (0, 0, imgsize[0]//2, imgsize[1]))
    img = img.filter(ImageFilter.GaussianBlur(radius=10))
    img.save("images/blot.png")


    def binarize_array(numpy_array, threshold=200):
        """Binarize a numpy array."""
        for i in range(len(numpy_array)):
            for j in range(len(numpy_array[0])):
                if numpy_array[i][j] > threshold:
                    numpy_array[i][j] = 255
                else:
                    numpy_array[i][j] = 0
        return numpy_array

    filename0=('images/blot.png')

    im = Image.open(filename0)
    im_grey = im.convert('LA') # convert to grayscale
    width,height = im.size

    total=0
    for i in range(0,width):
        for j in range(0,height):
            total += im_grey.getpixel((i,j))[0]

    mean = total / (width * height)

    image_file = Image.open(filename0)
    imagex = image_file.convert('L')  # convert image to monochrome
    imagey = np.array(imagex)
    #imagez = binarize_array(imagey, threshold)
    imagez = binarize_array(imagey, mean)
    time.sleep(2)
    filename = time.strftime("blot/tmmmp.png")
    cv2.imwrite(filename, imagez)
    filename = time.strftime("blot/GOODblots%Y%m%d%H%M%S.png")
    ImageOps.expand(Image.open('blot/tmmmp.png').convert("RGB"),border=30,fill='red').save(filename)
    print "GoodBlot : ",count
    count=count+1    

!mkdir blot

from PIL import Image, ImageFilter, ImageOps
import numpy as np
import random
from random import randint
import sys
import time
import cv2
count = 0
while count<20:
    imgsize = 640, 640
    color = (0, 0, 0)
    img = Image.new("RGB", imgsize, "white")
    max_range = randint(100,700)
    for j in range(0,max_range):
        start = (random.randrange(0, imgsize[0]/2), random.randrange(0, imgsize[1]))
        point = start
        img.putpixel(point, color)
        """
        point_list = [p for p in 
                      [(point[0], point[1]+1), (point[0], point[1]-1), 
                       #(point[0]+1, point[1]+1), (point[0]-1, point[1]-1), 
                       #(point[0]-1, point[1]+1), (point[0]+1, point[1]-1), 
                       (point[0]+1, point[1]), (point[0]-1, point[1])]
                      if 0 < p[0] and 0 < p[1] < imgsize[1] and p not in avoid_points]
        """

        #blotsize = random.randrange(0, 640)
        blotsize = random.randrange(0, max_range)
        for i in range(blotsize):
            directions = [(point[0], point[1]+1), (point[0], point[1]-1),
                         (point[0]+1, point[1]+1), (point[0]-1, point[1]-1), 
                         (point[0]-1, point[1]+1), (point[0]+1, point[1]-1),                       
                         (point[0]+1, point[1]), (point[0]-1, point[1])]
            toremove = []
            for direction in directions:
                if direction[0]>=(imgsize[0]/2) or direction[1]>=imgsize[1] or direction[0]<0 or direction[1]<0:
                    toremove.append(direction)
            for d in toremove:
                directions.remove(d)
            point = random.choice(directions)
            img.putpixel(point, color)


    cropped = img.crop((0, 0, imgsize[0]//2, imgsize[1]))
    img = img.transpose(Image.FLIP_LEFT_RIGHT)
    img.paste(cropped, (0, 0, imgsize[0]//2, imgsize[1]))
    rad = randint(5,30)
    img = img.filter(ImageFilter.GaussianBlur(radius=rad))
    img.save("blots/blot.png")


    def binarize_array(numpy_array, threshold=200):
        """Binarize a numpy array."""
        for i in range(len(numpy_array)):
            for j in range(len(numpy_array[0])):
                if numpy_array[i][j] > threshold:
                    numpy_array[i][j] = 255
                else:
                    numpy_array[i][j] = 0
        return numpy_array

    filename0=('blots/blot.png')

    im = Image.open(filename0)
    im_grey = im.convert('LA') # convert to grayscale
    width,height = im.size

    total=0
    for i in range(0,width):
        for j in range(0,height):
            total += im_grey.getpixel((i,j))[0]

    mean = total / (width * height)

    image_file = Image.open(filename0)
    imagex = image_file.convert('L')  # convert image to monochrome
    imagey = np.array(imagex)
    #imagez = binarize_array(imagey, threshold)
    imagez = binarize_array(imagey, mean)
    time.sleep(2)
    filename = time.strftime("blots/tmpp.png")
    cv2.imwrite(filename, imagez)

    filename = time.strftime("blots/blot%Y%m%d%H%M%S.png")
    ImageOps.expand(Image.open('blots/tmpp.png').convert("RGB"),border=30,fill='red').save(filename)

    print count    
    count = count+1



*python GOODblot.py

from PIL import Image, ImageChops
import time
import os
import random
count = 0    
while count <3:    
    path = r"blots/"
    base_image = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
        ])
    filename0=(path+base_image)

    path0 = r"junk/"
    base_image0 = random.choice([
        x0 for x0 in os.listdir(path0)
        if os.path.isfile(os.path.join(path0, x0))
        ])
    filename00=(path0+base_image0)

    path1 = r"blots/"
    base_image1 = random.choice([
        x1 for x1 in os.listdir(path1)
        if os.path.isfile(os.path.join(path1, x1))
        ])
    mask0=(path1+base_image1)

    im0 = Image.open(filename0)
    im1 = im0.resize((700,700), Image.NEAREST)

    im01 = Image.open(filename00)
    im2 = im01.resize((700,700), Image.NEAREST)

    im03 = Image.open(mask0).convert("RGBA")
    
    result1 = ImageChops.composite(im1, im2, im03)
    #result = result1.resize((640,640), Image.NEAREST)
    #filename = time.strftime("tmp/blotstuff.jpg")
    
    
    filename = time.strftime("blots/fixedblot%Y%m%d%H%M%S.png")
    result1.save(filename)
    
    
    #ImageOps.expand(Image.open('tmp/blotstuff.jpg').convert("RGB"),border=30,fill='red').save(filename)
    time.sleep(4)
    count = count +1
   

# Good stuff
# Makes a mask from any image
# It takes the mean of a image and uses it as a threshold
from PIL import Image
import time
import random
from random import randint 
import cv2
import numpy as np
import os

def binarize_array(numpy_array, threshold=150):
    """Binarize a numpy array."""
    for i in range(len(numpy_array)):
        for j in range(len(numpy_array[0])):
            if numpy_array[i][j] > threshold:
                numpy_array[i][j] = 255
            else:
                numpy_array[i][j] = 0
    return numpy_array

filename0=('fix/bot.png')
im = Image.open(filename0)
im = im.convert('LA')
img = im.filter(ImageFilter.GaussianBlur(radius=5))
cropped = img.crop((0, 0, imgsize[0]//2, imgsize[1]))
img = img.transpose(Image.FLIP_LEFT_RIGHT)
img.paste(cropped, (0, 0, imgsize[0]//2, imgsize[1]))


img.save("junk/blot.png")

    
image_file = Image.open("junk/blot.png")

width,height = image_file.size
total=0
for i in range(0,width):
    for j in range(0,height):
        total += im_grey.getpixel((i,j))[0]
mean = total / (width * height)

imagex = image_file.convert('L')  # convert image to monochrome
imagey = np.array(imagex)
#imagez = binarize_array(imagey, threshold)
imagez = binarize_array(imagey, mean)
time.sleep(2)
cv2.imwrite("tmp/fixtmpp.png", imagez)
filename = time.strftime("fix/Manual%Y%m%d%H%M%S.png")
ImageOps.expand(Image.open('tmp/fixtmpp.png').convert("RGB"),border=30,fill='red').save(filename)

print filename

# Good stuff
# Makes a mask from any image
# It takes the mean of a image and uses it as a threshold
from PIL import Image, ImageOps
import time
import random
from random import randint 
import cv2
import numpy as np
import os

def binarize_array(numpy_array, threshold=100):
    """Binarize a numpy array."""
    for i in range(len(numpy_array)):
        for j in range(len(numpy_array[0])):
            if numpy_array[i][j] > threshold:
                numpy_array[i][j] = 255
            else:
                numpy_array[i][j] = 0
    return numpy_array

path = r"crawler1/"
base_image = random.choice([
        x for x in os.listdir(path)
        if os.path.isfile(os.path.join(path, x))
        ])
filename0=(path+base_image)

im = Image.open(filename0)
im = im.convert('LA')

img = im.filter(ImageFilter.GaussianBlur(radius=5))
cropped = img.crop((0, 0, imgsize[0]//2, imgsize[1]))
img = img.transpose(Image.FLIP_LEFT_RIGHT)
img.paste(cropped, (0, 0, imgsize[0]//2, imgsize[1]))



img.save("junk/blot.png")

    
image_file = Image.open("junk/blot.png").convert("RGB")
img = ImageOps.invert(image_file)



width,height = image_file.size
total=0
for i in range(0,width):
    for j in range(0,height):
        total += im_grey.getpixel((i,j))[0]
mean = total / (width * height)

imagex = image_file.convert('L')  # convert image to monochrome
imagey = np.array(imagex)
#imagez = binarize_array(imagey, threshold)
imagez = binarize_array(imagey, mean)
time.sleep(2)
cv2.imwrite("tmp/fixtmpp.png", imagez)
filename = time.strftime("fix/Manual%Y%m%d%H%M%S.png")
ImageOps.expand(Image.open('tmp/fixtmpp.png').convert("RGB"),border=30,fill='red').save(filename)

print filename


# Good stuff
# Makes a mask from any image
# It takes the mean of a image and uses it as a threshold
from PIL import Image, ImageOps
import time
import random
from random import randint 
import cv2
import numpy as np
import os
count=0
while count<600:
    def binarize_array(numpy_array, threshold=100):
        """Binarize a numpy array."""
        for i in range(len(numpy_array)):
            for j in range(len(numpy_array[0])):
                if numpy_array[i][j] > threshold:
                    numpy_array[i][j] = 255
                else:
                    numpy_array[i][j] = 0
        return numpy_array
    #path = r"crawler1/"
    path = r"newstuff2/"
    base_image = random.choice([
            x for x in os.listdir(path)
            if os.path.isfile(os.path.join(path, x))
            ])
    filename0=(path+base_image)

    im = Image.open(filename0)
    im = im.convert('LA')

    img = im.filter(ImageFilter.GaussianBlur(radius=5))
    cropped = img.crop((0, 0, imgsize[0]//2, imgsize[1]))
    img = img.transpose(Image.FLIP_LEFT_RIGHT)
    img.paste(cropped, (0, 0, imgsize[0]//2, imgsize[1]))
    img.save("junk/blot.png")
    image_file = Image.open("junk/blot.png").convert("RGB")
    img = ImageOps.invert(image_file)
    width,height = image_file.size
    total=0
    for i in range(0,width):
        for j in range(0,height):
            total += im_grey.getpixel((i,j))[0]
    mean = total / (width * height)
    imagex = image_file.convert('L')  # convert image to monochrome
    imagey = np.array(imagex)
    #imagez = binarize_array(imagey, threshold)
    imagez = binarize_array(imagey, mean)
    time.sleep(2)
    cv2.imwrite("tmp/fixtmpp.png", imagez)
    filename = time.strftime("fix/Manual%Y%m%d%H%M%S.png")
    ImageOps.expand(Image.open('tmp/fixtmpp.png').convert("RGB"),border=30,fill='red').save(filename)
    count=count+1


!showme fix/Manual20170903160308.png


!mkdir fix

import random
from PIL import Image, ImageFilter
import time
import cv2
import numpy as np
import os
#imgsize = (2000, 1000)
#seed_count = 10
#seed_max_size = 18000
imgsize = (640, 640)
seed_count = 8
seed_max_size = 9000
margin_h = 20
margin_v = 20
degradation = 10
max_white = 100
color = (0, 0, 0)
img = Image.new("RGB", imgsize, "white")
def next_points(point, avoid_points=[], shuffle=True):
    point_list = [p for p in 
                  [(point[0], point[1]+1), (point[0], point[1]-1), 
                   #(point[0]+1, point[1]+1), (point[0]-1, point[1]-1), 
                   #(point[0]-1, point[1]+1), (point[0]+1, point[1]-1), 
                   (point[0]+1, point[1]), (point[0]-1, point[1])]
                  if 0 < p[0] and 0 < p[1] < imgsize[1] and p not in avoid_points]
                   
    for idx in range(len(point_list)):
        if point_list[idx][0] > imgsize[0]//2:
            point_list[idx] = (point[0], 
                               point_list[idx][1] if point_list[idx][1] != point[1] else random.choice([point[1]+1,
                                                                                                        point[1]-1]))
                                                                                                        
    point_list = [p for p in point_list                  
                  if 0 < p[0] and 0 < p[1] < imgsize[1] and p not in avoid_points]
        
    if shuffle:
        random.shuffle(point_list)
                 
    return point_list

def degrade_color(color):
    return (color[0] + degradation, 
            color[1] + degradation,
            color[2] + degradation)
            
def upgrade_color(color):
    return (color[0] - degradation//2, 
            color[1] - degradation//2,
            color[2] - degradation//2)
            
def spread(img, point, color):
    if color[0] <= max_white and img.getpixel(point)[0] > color[0]:
        img.putpixel(point, color)
        points = next_points(point, shuffle=False)
        color = degrade_color(color)
        for point in points:
            spread(img, point, color)
            
old_points = []
posible_root_points = []
for seed in range(0, seed_count):
    print("Seed: %d" % seed)
    point = None
    while not point or point in old_points:
        point = (random.randrange(0 + margin_h, imgsize[0]//2), 
                 random.randrange(0 + margin_v, imgsize[1] - margin_v))
    old_points.append(point)
    posible_root_points.append(point)
    img.putpixel(point, color)

    seedsize = random.randrange(0, seed_max_size)
    print("Seed size: %d" % seedsize)
    flow = 0
    for progress in range(0, seedsize):
        flow += 1
        points = next_points(point, old_points)
        try:
            point = points.pop()
        except IndexError:
            posible_root_points.remove(point)
            print("Looking for old points... Seed: %d Seed Size: %d "
                  "Progress: %d Flow: %d Statistic: %d" % (seed,
                                                           seedsize,
                                                           progress,
                                                           flow, 
                                                           len(posible_root_points)))
            for idx in reversed(range(0, len(posible_root_points))):
                points = next_points(posible_root_points[idx], old_points)
                try:
                    point = points.pop()
                    print("Using old point...")
                    flow = 0
                    break;
                except IndexError:
                    posible_root_points.pop()
            if not point:
                print("No way!")
                break
            
        old_points.append(point)
        posible_root_points.append(point)
        img.putpixel(point, color)
        
        for surr_point in points:
            spread(img, surr_point, degrade_color(color))

print ("Cropping...")
cropped = img.crop((0, 0, imgsize[0]//2, imgsize[1]))
img = img.transpose(Image.FLIP_LEFT_RIGHT)
img.paste(cropped, (0, 0, imgsize[0]//2, imgsize[1]))
img = img.filter(ImageFilter.GaussianBlur(radius=10))
img.save("images/blot.png")

def binarize_array(numpy_array, threshold=200):
    """Binarize a numpy array."""
    for i in range(len(numpy_array)):
        for j in range(len(numpy_array[0])):
            if numpy_array[i][j] > threshold:
                numpy_array[i][j] = 255
            else:
                numpy_array[i][j] = 0
    return numpy_array

filename0=('images/blot.png')
    
im = Image.open(filename0)
im_grey = im.convert('LA') # convert to grayscale
width,height = im.size

total=0
for i in range(0,width):
    for j in range(0,height):
        total += im_grey.getpixel((i,j))[0]

mean = total / (width * height)
    
image_file = Image.open(filename0)
imagex = image_file.convert('L')  # convert image to monochrome
imagey = np.array(imagex)
#imagez = binarize_array(imagey, threshold)
imagez = binarize_array(imagey, mean)
time.sleep(2)
filename = time.strftime("images/%Y%m%d%H%M%S.png")
cv2.imwrite(filename, imagez)
print filename



names = ["jome tyler","joe smith","time jones","tytyt sisi"]

smith_names = names.select do |name|
  name.include?("smith")
end
>> ["joe smith"]

require 'sqlite3'

# Open a database
db = SQLite3::Database.new "todo.db"

# Create a table
rows = db.execute <<-SQL
  create table if not exists todos (
    id integer primary key,
    task varchar(255)
  );
SQL

# Insert some data
tasks = [
  ["Wash the car"],
  ["Buy groceries"],
  ["Finish homework"],
  ["Take out the trash"],
]

tasks.each do |task|
  db.execute("insert into todos (task) values (?);", task)
end

# Read the data
puts "All tasks:"
db.execute("select * from todos") do |row|
  puts row
end


require 'sqlite3'

# Open a database
db = SQLite3::Database.new "todo.db"

# Create a table
rows = db.execute <<-SQL
  create table if not exists todos (
    id integer primary key,
    task varchar(255)
  );
SQL

# Insert some data
tasks = [
  ["Wash the car"],
  ["Buy groceries"],
  ["Finish homework"],
  ["Take out the trash"],
]

tasks.each do |task|
  db.execute("insert into todos (task) values (?);", task)
end

# Read the data
puts "All tasks:"
db.execute("select * from todos") do |row|
  puts row
end


gem install sqlite3


# Gemfile

gem 'sqlite3'
bundle install

system("ls")

output = system("ls")

output = %x[ls]

system("gem 'sqlite3")

system("gem install sqlite3")

output = %x[gem install sqlite3]

require 'bundler/setup'

gem 'sqlite3'
bundle install



# Clone the conversational datasets repository.
!git clone https://github.com/PolyAI-LDN/conversational-datasets.git

import os
from getpass import getpass

# We use a consistent random sample of the train and test sets for the
# baselines.
# Please ask matthen@gmail.com for the URL, and enter it here.
# (You could also generate your own random sample, and exact results may vary
# slightly).
os.environ['DATA_URL'] = getpass("Enter URL: ")

# Download and extract the data.
!mkdir -p conversational-datasets/data
!wget -q "${DATA_URL}" -O conversational-datasets/data/data.zip
!cd conversational-datasets/data && unzip -o data.zip

# Install extra pip dependencies.
!pip install bert-tensorflow glog tf-sentencepiece
%env  TFHUB_CACHE_DIR=/content/tfhub
%env  PYTHONPATH=/content/conversational-datasets

# Run all the baselines.
# It will take over an hour.
!cd conversational-datasets && ./baselines/run_baselines.sh
!cat conversational-datasets/baselines/results.csv

# Run just a single baseline:
! cd conversational-datasets && python baselines/run_baseline.py \
    --method USE_QA_SIM \
    --train_dataset data/amazon-train-sampled \
    --test_dataset data/amazon-test-sampled

!mkdir images

!wget -O images/face.jpg https://upload.wikimedia.org/wikipedia/commons/3/33/Arnold_Schwarzenegger_edit%28ws%29.jpg

# Notice the mpld3 module - it will allow youtoload animage and use to mouse to see pixel locations
# handy tool to find points for a fast crop
import matplotlib
import numpy as np
import mpld3
import matplotlib.pyplot as plt
from PIL import Image
from mpld3 import plugins
%matplotlib inline
fig, ax = plt.subplots()
im = np.array(Image.open('images/face.jpg'))
# Default shows the image upside down [::-1] flips the image
im = im[::-1]
plt.imshow(im)
plugins.connect(fig, plugins.MousePosition(fontsize=14))

mpld3.display()

# This is tool to mark the points and record them. The display is formated specially to useina cut and paste
# where the image arrays points are required.
import Tkinter
from PIL import Image, ImageTk
from sys import argv

window = Tkinter.Tk(className="Array Points")

image = Image.open("images/face.jpg")
canvas = Tkinter.Canvas(window, width=image.size[0], height=image.size[1])
canvas.pack()
image_tk = ImageTk.PhotoImage(image)
canvas.create_image(image.size[0]//2, image.size[1]//2, image=image_tk)

def callback(event):
    #This is formated so the array results below may be cut and pasted.
    #Don'tforget toremove the final trailing comma the pasted area should start and end with [ ]
    print "[",event.x,",",event.y,"],"
 
canvas.bind("<Button-1>", callback)
Tkinter.mainloop()

from PIL import Image
im = Image.open("images/face.jpg")

image0 = im.crop((270,350, 400, 500))
image0.save("images/nose.jpg")

!showme images/nose.jpg

# this will will provide a finer tune ' croping array ' for seamless cloning
import Tkinter
from PIL import Image, ImageTk
from sys import argv

window = Tkinter.Tk(className="Array Points")

image = Image.open("images/nose.jpg")
canvas = Tkinter.Canvas(window, width=image.size[0], height=image.size[1])
canvas.pack()
image_tk = ImageTk.PhotoImage(image)
canvas.create_image(image.size[0]//2, image.size[1]//2, image=image_tk)

def callback(event):
    #print "[",event.x,"],["+event.y,"]"
    print "[",event.x,",",event.y,"],",
 
canvas.bind("<Button-1>", callback)
Tkinter.mainloop()

# Saving multiple arrays in an npz file example:
# Create two arrays
x = np.arange(10)
y = np.sin(x)

#Using savez with *args, the arrays are saved with default names " ['arr_1', 'arr_0'] "
np.savez("numpy-filters/temp1", x, y)

!cp -R /home/conda/Desktop/NoteBooks/GRAPHICS/numpy-array-filters/numpy-filters .

npz = np.load("numpy-filters/temp1.npz")
npz.files

# Retrieve the first array for use
npz['arr_0']

# Retrieve the second array for use
npz['arr_1']
# Assigning a variable to the array
sin = npz['arr_1']
print sin

oranges = np.arange(10)
apples = np.sin(x)
np.savez("numpy-filters/example2", oranges=oranges, apples=apples)

npzfile = np.load("numpy-filters/example2.npz")
npzfile.files

npzfile['apples'] , npzfile['oranges'] 

import cv2
import numpy as np 

# Read face image ( dst destination of the nose src )
dst = cv2.imread("images/face.jpg")
# Read nose image ( src of the seamlesClone image )
src = cv2.imread("images/nose.jpg")


# Create a rough mask around the nose
src_mask = np.zeros(src.shape, src.dtype)

# notice the array above is cut and pasted here
poly = np.array([[ 33 , 21 ],
[ 3 , 111 ],
[ 11 , 135 ],
[ 69 , 144 ],
[ 122 , 137 ],
[ 129 , 92 ],
[ 75 , 14 ],
[ 65 , 12 ]], np.int32)

# This will save BOTH the nose image and the poly array in a single file src-mask001.npz
# notice below,  I am using src=src, poly=poly . 
#That will give me arrays npz['src'] and npz['poly']

np.savez('numpy-filters/nose_001', src=src, poly=poly) 

#place the nose image poly array in the 'rough' source mask
cv2.fillPoly(src_mask, [poly], (255, 255, 255))

# This is where the CENTER of the airplane will be placed
center = (340,255)

# Clone seamlessly.
output = cv2.seamlessClone(src, dst, src_mask, center, cv2.NORMAL_CLONE)

# Write result
cv2.imwrite("images/face_clone.jpg", output);
!showme images/face_clone.jpg

from PIL import Image
im = Image.open("images/face_clone.jpg")
im

# Get an image from public domain wiki
!wget -O images/woman.jpg https://upload.wikimedia.org/wikipedia/commons/1/1b/Young_Woman_Thinking.jpg

#get points for cropping image
import Tkinter
from PIL import Image, ImageTk
from sys import argv

window = Tkinter.Tk(className="Array Points")

image = Image.open("images/woman.jpg")
canvas = Tkinter.Canvas(window, width=image.size[0], height=image.size[1])
canvas.pack()
image_tk = ImageTk.PhotoImage(image)
canvas.create_image(image.size[0]//2, image.size[1]//2, image=image_tk)

def callback(event):
    #print "[",event.x,"],["+event.y,"]"
    print "[",event.x,",",event.y,"],"
 
canvas.bind("<Button-1>", callback)
Tkinter.mainloop()

# Crop the Downloaded, images/woman.jpg, image.
from PIL import Image
img = Image.open("images/woman.jpg")

image0 = img.crop((350,70, 950, 600))
image0.save("images/Crop_woman.jpg")

img = Image.open("images/Crop_woman.jpg")
img

# View the files/arrays in the " nose_001.npz "
npz = np.load("numpy-filters/nose_001.npz")
npz.files

import cv2
import numpy as np 

# Read woman image
dst = cv2.imread("images/Crop_woman.jpg")

#load the nose image AND the numpy array around it
npz = np.load("numpy-filters/nose_001.npz")


# The src_mask requires shape from the original nose image
# be sure define this variable before creating the src_mask 
src=npz['src']

# cv2.fillPoly requires placing the poly array in the src_mask
poly=npz['poly']

#rough shape a mask
src_mask = np.zeros(src.shape, src.dtype)

#fill that rough mask with the poly array of the select areas of the nose image
# nose_001a.npz contains the nose image and the poly array
cv2.fillPoly(src_mask, [poly], (255, 255, 255))

# This is where the CENTER where the nose image will be placed
center = (265,210)

# Clone seamlessly.
output = cv2.seamlessClone(src, dst, src_mask, center, cv2.NORMAL_CLONE)

# Write result
cv2.imshow('dst', dst)
cv2.imwrite("images/face_clonez.jpg", output);
cv2.imshow('output', output)
cv2.imshow('src_mask', src_mask)
cv2.imshow('src', src)
cv2.waitKey(0)
cv2.destroyAllWindows()

Im = Image.open("images/face_clonez.jpg")

npz = np.load("numpy-filters/nose_001.npz")
src=npz['src']
poly=npz['poly']
print "POLY: ",poly , "\n \n SRC: ",src

from PIL import Image
im = Image.open("images/face.jpg")

# This variable allows manipulation on the open im without effecting it
temp = im.copy()

image0 = temp.crop((200,300, 450, 550))
image0.save("images/temp-nose.jpg")
#the original ' im '  is unaltered and may be used later inthe script



#if an image needs to be rotated, do it before final cropping and picking array points
import cv2
import numpy as np 
import scipy
from scipy import ndimage
# Read nose image

new = cv2.imread("images/temp-nose.jpg")

new1 = ndimage.interpolation.rotate(new, -5, axes=(1, 0), \
                                   reshape=True, output=None, \
                                   order=3,mode='constant', cval=0.0, prefilter=True)

cv2.imwrite("images/nose-rotate.jpg", new1);
cv2.imshow('new1', new1)
cv2.waitKey(0)
cv2.destroyAllWindows()

import time
timestr = time.strftime("%Y%m%d-%H%M%S")
filename = "images/"+timestr+".png"
print filename

!mkdir images/seq

import cv2
import numpy as np 
import scipy
from scipy import ndimage
import time
from datetime import datetime



# Read woman image
dst = cv2.imread("images/nose.jpg")
count = 0
while (count < 360):
    timestr = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
    junk = ndimage.interpolation.rotate(dst, count, axes=(1, 0), \
                                       reshape=True, output=None, \
                                       order=3,mode='constant', cval=0.0, prefilter=True)
    filename = "images/seq/"+timestr+".png"
    cv2.imwrite(filename, junk);
    #prevent the same filename on time based filenames
    time.sleep(1) 
    count = count +1
    #cv2.imshow('junk', junk)

    #cv2.waitKey(0)
    #cv2.destroyAllWindows()

import cv2
import numpy as np 
import scipy
from scipy import ndimage
# Read woman image
dst = cv2.imread("images/nose.jpg")



junk = ndimage.interpolation.rotate(dst, 0, axes=(1, 0), \
                                   reshape=True, output=None, \
                                   order=3,mode='constant', cval=0.0, prefilter=True)

junk = junk + 100

cv2.imwrite("images/junk.jpg", junk);
cv2.imshow('junk', junk)
cv2.waitKey(0)
cv2.destroyAllWindows()

import cv2
import numpy as np 
import scipy
from scipy import ndimage
# Read woman image
dst = cv2.imread("images/Crop_woman.jpg")
#load the nose image AND the numpy array around it
npz = np.load("numpy-filters/nose_001.npz")
# The src_mask requires shape from the original nose image
# be sure define this variable before creating the src_mask 
src=npz['src']
# cv2.fillPoly requires placing the poly array in the src_mask
poly=npz['poly']

#rough shape a mask

src_mask = np.zeros(src.shape, src.dtype)

#fill that rough mask with the poly array of the select areas of the nose image
# nose_001a.npz contains the nose image and the poly array
cv2.fillPoly(src_mask, [poly], (255, 255, 255))

# This is where the CENTER where the nose image will be placed
center = (265,210)

# Clone seamlessly.
output = cv2.seamlessClone(src, dst, src_mask, center, cv2.NORMAL_CLONE)

# Write result
cv2.imwrite("images/face_clone-roll.jpg", output);
cv2.imshow('output', output)
cv2.imshow('src', src)
cv2.waitKey(0)
cv2.destroyAllWindows()

!showme images/nose-rotate.jpg

import cv2
import numpy as np 
import scipy
from scipy import ndimage
# Read woman image
dst = cv2.imread("images/Crop_woman.jpg")

#load the nose image AND the numpy array around it
#npz = np.load("numpy-filters/nose_001.npz")
#cv2.imwrite("test.png", npz)
npz = np.load("numpy-filters/nose_001.npz")
# The src_mask requires shape from the original nose image
# be sure define this variable before creating the src_mask 
src=npz['src']
cv2.imwrite("images/test1.png", src)
# cv2.fillPoly requires placing the poly array in the src_mask
poly=npz['poly']

src_mask = np.zeros(src.shape, src.dtype)

#fill that rough mask with the poly array of the select areas of the nose image
# nose_001a.npz contains the nose image and the poly array
test2 = cv2.fillPoly(src_mask, [poly], (255, 255, 255))
cv2.imwrite("images/test2.png", test2)
cv2.imshow('images/test2', test2)
cv2.waitKey(0)
cv2.destroyAllWindows()

!showme test2.png

import cv2
import numpy as np 
import scipy
from scipy import ndimage
# Read woman image
dst = cv2.imread("images/Crop_woman.jpg")

#load the nose image AND the numpy array around it
npz = np.load("numpy-filters/nose_001.npz")


# The src_mask requires shape from the original nose image
# be sure define this variable before creating the src_mask 
src=npz['src']

# cv2.fillPoly requires placing the poly array in the src_mask
poly=npz['poly']


newim = ndimage.interpolation.rotate(src, 23.5, axes=(1, 0), reshape=True, output=None, order=3,mode='constant', cval=0.0, prefilter=True)
cv2.imwrite("images/newim.png", newim)
cv2.imshow('src', newim)
cv2.waitKey(0)
cv2.destroyAllWindows()

export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig

import onnx
import os


# Preprocessing: load the old model
old_model_path = os.path.join('resources', 'single_relu.onnx')
onnx_model = onnx.load(old_model_path)

# Preprocessing: get the path to the saved model
new_model_path = os.path.join('resources', 'single_relu_new.onnx')

# Save the ONNX model
onnx.save(onnx_model, new_model_path)

print('The model is saved.')

https://github.com/scikit-image/scikit-image/blob/main/skimage/transform/_warps.py

import scipy
import scipy.misc
from scipy import ndimage
import matplotlib.pyplot as plt

f = scipy.misc.face(gray=True).astype(float)
blurred_f = ndimage.gaussian_filter(f, 3)

filter_blurred_f = ndimage.gaussian_filter(blurred_f, 1)

alpha = 30
sharpened = blurred_f + alpha * (blurred_f - filter_blurred_f)

plt.figure(figsize=(12, 4))

plt.subplot(131)
plt.imshow(f, cmap=plt.cm.gray)
plt.axis('off')
plt.subplot(132)
plt.imshow(blurred_f, cmap=plt.cm.gray)
plt.axis('off')
plt.subplot(133)
plt.imshow(sharpened, cmap=plt.cm.gray)
plt.axis('off')

plt.tight_layout()
plt.show()

AR = sharpened
print(len(AR))
for line in AR:
    print(AR)

from PIL import Image
im =Image.open('result.png')
#print(len(im))
print(im)



lines = open("Classes.API").readlines()
search = input("SEARCH TERM: ")
for line in lines:
    if search in line:
        print(line)

import vsketch

dir(vsketch)

dir(vsketch.vsketch)

dir(vsketch.style)

dir(vsketch.vsketch.math)

dir(vsketch.vsketch.math.radians)



# Current directory
!pwd

import os

user_input = input('What is the name of your directory ? :')
directory = os.listdir(user_input)

searchstring = input('What word are you trying to find ? :')

for fname in directory:
    if os.path.isfile(user_input + os.sep + fname):
        # Full path
        f = open(user_input + os.sep + fname, 'r',encoding='latin1')

        if searchstring in f.read():
            print('found string in file: %s' % fname)
        else:
            print('.', end=" ")
        f.close()

!grep -Ril '"private_key"' ./

import sqlite3
conn = sqlite3.connect("stream_list2.db")
c = conn.cursor()
# Search database for words private_key and seed 
for row in c.execute("SELECT ROWID,* from LBRY"):
    if '"private_key"' in row[1] or '"seed"' in row[1]:
        print (row[0],": ",row[1])


import sqlite3
conn = sqlite3.connect("stream_list2.db")
c = conn.cursor()
SEARCH = input("SEARCH: ")
for row in c.execute("SELECT ROWID,* from LBRY"):
    if SEARCH in row[1]:
        print (row[0],": ",row[1])






!du -hs /home/conda/Desktop/NoteBooks/GRAPHICS/superpixels/slicPython

from IPython.core.display import HTML
HTML("""
<style>
#notebooks-container {
    padding: 15px;
    background-color: #ffebcd;
    min-height: 20px;
    width: 900px;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.3);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.3);
}
</style>
<div id ="notebooks-container">
<h1>Notebook Container</h1>
<div/>
""")

import sys
import os
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
import time
import random
#%pylab inline
%matplotlib inline 


!mkdir tmp

!wget -O tmp/waves-jpg https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Great_Wave_off_Kanagawa2.jpg/640px-Great_Wave_off_Kanagawa2.jpg

path = r"image_resources/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)
filename ="tmp/waves-jpg"
img = img_as_float(io.imread(filename0))
plt.axis('off')
io.imshow(img);

!mkdir tmpseg

for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=300)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')
    plt.axis('off')
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "tmpseg/"+timestr+"__"+str(segs)+".png"
    print(filename)
    imsave(filename,segments)
plt.show()

for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=300)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')
    plt.axis('off')
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()


from __future__ import print_function

import matplotlib.pyplot as plt
import numpy as np

from skimage.data import astronaut
from skimage.segmentation import felzenszwalb, slic, quickshift
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float

segments_fz = felzenszwalb(img, scale=100, sigma=0.5, min_size=50)
segments_slic = slic(img, n_segments=250, compactness=10, sigma=1)
segments_quick = quickshift(img, kernel_size=3, max_dist=6, ratio=0.5)

print("Felzenszwalb's number of segments: %d" % len(np.unique(segments_fz)))
print("Slic number of segments: %d" % len(np.unique(segments_slic)))
print("Quickshift number of segments: %d" % len(np.unique(segments_quick)))

fig, ax = plt.subplots(1, 3)
fig.set_size_inches(16, 8, forward=True)
fig.subplots_adjust(0.05, 0.05, 0.95, 0.95, 0.05, 0.05)

ax[0].imshow(mark_boundaries(img, segments_fz))
ax[0].set_title("Felzenszwalbs's method")
ax[1].imshow(mark_boundaries(img, segments_slic))
ax[1].set_title("SLIC")
ax[2].imshow(mark_boundaries(img, segments_quick))
ax[2].set_title("Quickshift")
for a in ax:
    a.set_xticks(())
    a.set_yticks(())
plt.show()

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
%matplotlib inline 
path = r"image_resources/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
])
filename0=(path+base_image)


img = img_as_float(io.imread(filename0))
#io.imshow(img);
for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=200)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')
    plt.axis('off')
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()

from PIL import Image
filename ="tmp/waves-jpg"
img = Image.open(filename) 
img = img.convert("RGBA")
img.save("tmp/0028RGBxxx.png")

from PIL import Image
filename ="tmp/waves-jpg"
img = Image.open(filename) 
img = img.convert("RGB")
img.save("tmp/0028RGBxxx.jpg")

!mkdir junk

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image, ImageChops
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

img = Image.open("tmp/0028RGBxxx.jpg")    
img = img.convert('P', palette=Image.ADAPTIVE, colors=36)    
img.save("junk/003a.png")    
   
img = img_as_float(io.imread('junk/003a.png'))



#io.imshow(img);
segs = 360
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0,0)))
plt.savefig("junk/003a.png", bbox_inches='tight')
im = Image.open("junk/003a.png")
#im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('junk/coffee002.jpg')
im.show()

!ls junk

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

img = Image.open("images/Sranger-Tri-001a.jpg")    
img = img.convert('P', palette=Image.ADAPTIVE, colors=36)    
img.save("images/003a.png")    
   
img = img_as_float(io.imread('images/003a.png'))



#io.imshow(img);
segs = 360
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0,0)))
plt.savefig("images/test2a.png", bbox_inches='tight')
im = Image.open("images/test2a.png")
#im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/test2B.jpg')
im.show()

!ls images

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)
    
img = img_as_float(io.imread('images/test2B.jpg'))
#io.imshow(img);
segs = 1000
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("images/test3.png", bbox_inches='tight')
im = Image.open("images/test3.png")
#im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/test2C.jpg')
im.show()

from PIL import Image
im=Image.open("images/test2C.jpg")
print (im.size)
im

from PIL import Image
im=Image.open("images/test2C.jpg")
im.size
im.getbbox()
im2=im.crop(im.getbbox())
im2.size
im2

from PIL import Image, ImageChops

def trim(im, border):
    bg = Image.new(im.mode, im.size, border)
    diff = ImageChops.difference(im, bg)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

def create_thumbnail(path, xsize,ysize):
    image = Image.open(path)
    name, extension = path.split('.')
    options = {}
    if 'transparency' in image.info:
        options['transparency'] = image.info["transparency"]
  
    image.thumbnail((xsize, ysize), Image.ANTIALIAS)
    image = trim(image, 255) ## Trim whitespace
    image.save(name + '_new.' + extension, **options)
    return image


create_thumbnail('images/test2C.jpg', 200, 200)

from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("images/test2C.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()


from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("images/test2C.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()

from PIL import Image
im=Image.open("photos/38.jpg")
im.save("photos/38.png")
im2=Image.open("photos/38.png")

im.size
im.getbbox()
im2=im.crop(im.getbbox())
im2.size

from numpy import random
import matplotlib.pyplot as plt

data = random.random((480,740))
img = plt.imshow(data, interpolation='nearest')
img.set_cmap('hot')
plt.axis('off')
plt.savefig("test.png", bbox_inches='tight')

import matplotlib.pyplot as plt
import numpy as np
%pylab inline
%matplotlib inline 

ncols = 5
nrows = 3

# create the plots
fig = plt.figure()
axes = [ fig.add_subplot(nrows, ncols, r * ncols + c) for r in range(1, nrows) for c in range(1, ncols) ]

# add some data
for ax in axes:
    ax.plot(np.random.random(10), np.random.random(10), '.')

# remove the x and y ticks
for ax in axes:
    ax.set_xticks([])
    ax.set_yticks([])

def autocrop_image2(image):
    image_data = np.asarray(image)
    image_data_bw = image_data[:,:,3]
    non_empty_columns = np.where(image_data_bw.max(axis=0) > 0)[0]
    non_empty_rows = np.where(image_data_bw.max(axis=1) > 0)[0]
    cropBox = (min(non_empty_rows), max(non_empty_rows),
               min(non_empty_columns), max(non_empty_columns))

    image_data_new = image_data[cropBox[0]:cropBox[
        1] + 1, cropBox[2]:cropBox[3] + 1, :]

    new_image = Image.fromarray(image_data_new)
    return new_image

from PIL import Image
im=Image.open("test2.png")
bbox = im.convert("RGBa").getbbox()
im0 = autocrop_image2(bbox)
im0

!locate override.css



im=Image.open("photos/38.jpg")
bbox = im.convert("RGBA")
bbox

from PIL import Image
import numpy as np
def autocrop_image2(image):
    #image.load()
    image_data = np.asarray(image)
    image_data_bw = image_data.max(axis=2)
    non_empty_columns = np.where(image_data_bw.max(axis=0) > 0)[0]
    non_empty_rows = np.where(image_data_bw.max(axis=1) > 0)[0]
    cropBox = (min(non_empty_rows), max(non_empty_rows),
               min(non_empty_columns), max(non_empty_columns))

    image_data_new = image_data[cropBox[0]:cropBox[
        1] + 1, cropBox[2]:cropBox[3] + 1, :]

    new_image = Image.fromarray(image_data_new)
    return new_image
im=Image.open("photos/38.png")
bbox = im.convert("RGBA").getbbox()
im0 = autocrop_image2(bbox)
im0

im = Image.open("photos/38.jpg")
im = im.resize((640,640), Image.NEAREST)
im.save("photos/38.png")
im

# import the necessary packages
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import argparse
import utils
import cv2
def clust(IMAGE):
    # construct the argument parser and parse the arguments
    # load the image and convert it from BGR to RGB so that
    # we can dispaly it with matplotlib
    image = cv2.imread(IMAGE)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    # show our image
    return image
    
IMAGE = "photos/38.png"    
im = clust(IMAGE)

cv2.imwrite("photos/38CLUST.png", im)

from PIL import Image
imN= Image.open("photos/38CLUST.png")
imN

import markdown
class MD(str):
    def _repr_html_(self):
        return markdown.markdown(self)
    
STR="""
<div style ="border:1px solid red;padding:15px;">    
<h3>Help on function mark_boundaries in module skimage.segmentation.boundaries:</h3>

<p style = "color:blue">mark_boundaries(image, label_img, color=(1, 1, 0), outline_color=None, mode='outer', background_label=0)
Return image with boundaries between labeled regions highlighted.</p>
Parameters
----------<pre>
    image : (M, N[, 3]) array
        Grayscale or RGB image.
    label_img : (M, N) array of int
        Label array where regions are marked by different integer values.
    color : length-3 sequence, optional
        RGB color of boundaries in the output image.
    outline_color : length-3 sequence, optional
        RGB color surrounding boundaries in the output image. If None, no
        outline is drawn.
    mode : string in {'thick', 'inner', 'outer', 'subpixel'}, optional
        The mode for finding boundaries.
    background_label : int, optional
        Which label to consider background (this is only useful for
        modes ``inner`` and ``outer``).
</pre>    
Returns
-------
    marked : (M, N, 3) array of float
        An image in which the boundaries between labels are
        superimposed on the original image.
</div>
"""
MD(STR)    

!ls ~/.jupyter/custom

# %load ~/.jupyter/custom/current_theme.txt
monokai

!python color_kmeans.py --image 003.jpg --clusters 3

%%writefile utilz.py
# import the necessary packages
import numpy as np
import cv2

def centroid_histogram(clt):
    # grab the number of different clusters and create a histogram
    # based on the number of pixels assigned to each cluster
    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
    (hist, _) = np.histogram(clt.labels_, bins = numLabels)

    # normalize the histogram, such that it sums to one
    hist = hist.astype("float")
    hist /= hist.sum()

    # return the histogram
    return hist

def plot_colors(hist, centroids):
    # initialize the bar chart representing the relative frequency
    # of each of the colors
    bar = np.zeros((50, 300, 3), dtype = "uint8")
    startX = 0

    # loop over the percentage of each cluster and the color of
    # each cluster
    for (percent, color) in zip(hist, centroids):
        # plot the relative percentage of each cluster
        endX = startX + (percent * 300)
        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),
            color.astype("uint8").tolist(), -1)
        startX = endX

    # return the bar chart
    return bar

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import argparse
import numpy as np
import utilz
import cv2
%pylab inline
%matplotlib inline 
def centroid_histogram(clt):
    # grab the number of different clusters and create a histogram
    # based on the number of pixels assigned to each cluster
    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
    (hist, _) = np.histogram(clt.labels_, bins = numLabels)

    # normalize the histogram, such that it sums to one
    hist = hist.astype("float")
    hist /= hist.sum()

    # return the histogram
    return hist

def plot_colors(hist, centroids):
    # initialize the bar chart representing the relative frequency
    # of each of the colors
    bar = np.zeros((50, 300, 3), dtype = "uint8")
    startX = 0

    # loop over the percentage of each cluster and the color of
    # each cluster
    for (percent, color) in zip(hist, centroids):
        # plot the relative percentage of each cluster
        endX = startX + (percent * 300)
        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),
            color.astype("uint8").tolist(), -1)
        startX = endX

    # return the bar chart
    return bar

def Clust(iMage, clusters):
    # load the image and convert it from BGR to RGB so that
    # we can dispaly it with matplotlib
    image = cv2.imread(iMage)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # show our image
    plt.figure()
    plt.axis("off")
    plt.imshow(image)

    # reshape the image to be a list of pixels
    image = image.reshape((image.shape[0] * image.shape[1], 3))

    # cluster the pixel intensities
    clt = KMeans(clusters)
    clt.fit(image)

    # build a histogram of clusters and then create a figure
    # representing the number of pixels labeled to each color
    hist = utilz.centroid_histogram(clt)
    bar = utilz.plot_colors(hist, clt.cluster_centers_)

    # show our color bart
    plt.figure()
    plt.axis("off")
    plt.imshow(bar)
    plt.show()

iMage = "photos/38CLUST.png"
clusters = 6
Clust(iMage, clusters)    

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color, io
from skimage.future import graph
from matplotlib import pyplot as plt

img = io.imread("photos/38CLUST.png")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
plt.imshow(out)
io.imsave("images/Stest2.png", out)
#plt.title('Final segmentation')

plt.show()    
    

!showme images/Stest2.png

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops

img = io.imread("images/Stest2.png")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
io.imsave("images/Stest2S.png", out)
IMG =Image.open("images/Stest2S.png")
IMG

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops

img = io.imread("photos/0147xxx.jpg")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
io.imsave("images/SStest2.png", out)
IMG =Image.open("images/SStest2.png")
IMG

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops
def main(imgagefile):
    img = io.imread(imgagefile)
    labels = segmentation.slic(img, compactness=30, n_segments=400)
    g = future.graph.rag_mean_color(img, labels)
    def weight_boundary(graph, src, dst, n):
        default = {'weight': 0.0, 'count': 0}
        count_src = graph[src].get(n, default)['count']
        count_dst = graph[dst].get(n, default)['count']
        weight_src = graph[src].get(n, default)['weight']
        weight_dst = graph[dst].get(n, default)['weight']
        count = count_src + count_dst
        return {
            'count': count,
            'weight': (count_src * weight_src + count_dst * weight_dst)/count
        }
    def merge_boundary(graph, src, dst):
        """Call back called before merging 2 nodes.
        In this case we don't need to do any computation here.""" 
        pass

    labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                       in_place_merge=True,
                                       merge_func=merge_boundary,
                                       weight_func=weight_boundary)
    out = color.label2rgb(labels2, img, kind='avg')
    io.imsave("images/Stest2.png", out)
    IMG =Image.open("images/Stest2.png")
    IMG

im = Image.open("images/Stest2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/Stest2.jpg')
im.show()

plt.savefig("images/testZ2.png", bbox_inches='tight')
im = Image.open("images/testZ2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('test2.jpg')
im.show()

from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("coffee002.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()

%%writefile utils.py
# import the necessary packages
import numpy as np
import cv2
 
def centroid_histogram(clt):
	# grab the number of different clusters and create a histogram
	# based on the number of pixels assigned to each cluster
	numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
	(hist, _) = np.histogram(clt.labels_, bins = numLabels)
 
	# normalize the histogram, such that it sums to one
	hist = hist.astype("float")
	hist /= hist.sum()
 
	# return the histogram
	return hist

from Blender import Blendem
image1 = Image.open("tmpseg/20220921-072955__10.png")
image2 = Image.open("tmpseg/20220921-072811__10.png")
Blendem(image1, image2)

!ls segmented

import Blender
dir(Blender)



!sudo locate slicPython

!du -hs /home/conda/Desktop/NoteBooks/GRAPHICS/superpixels/slicPython

!ls

from IPython.core.display import HTML
HTML("""
<style>
#notebook-container {
    padding: 15px;
    background-color: #ffebcd;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.3);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.3);
}
</style>
""")

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io

%pylab inline
%matplotlib inline 


!wget https://jacknorthrup.com/INSTAGRAM/003.jpg

img = img_as_float(io.imread('003.jpg'))
io.imshow(img);

for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=300)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()

for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=300)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()


from __future__ import print_function

import matplotlib.pyplot as plt
import numpy as np

from skimage.data import astronaut
from skimage.segmentation import felzenszwalb, slic, quickshift
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float

segments_fz = felzenszwalb(img, scale=100, sigma=0.5, min_size=50)
segments_slic = slic(img, n_segments=250, compactness=10, sigma=1)
segments_quick = quickshift(img, kernel_size=3, max_dist=6, ratio=0.5)

print("Felzenszwalb's number of segments: %d" % len(np.unique(segments_fz)))
print("Slic number of segments: %d" % len(np.unique(segments_slic)))
print("Quickshift number of segments: %d" % len(np.unique(segments_quick)))

fig, ax = plt.subplots(1, 3)
fig.set_size_inches(16, 8, forward=True)
fig.subplots_adjust(0.05, 0.05, 0.95, 0.95, 0.05, 0.05)

ax[0].imshow(mark_boundaries(img, segments_fz))
ax[0].set_title("Felzenszwalbs's method")
ax[1].imshow(mark_boundaries(img, segments_slic))
ax[1].set_title("SLIC")
ax[2].imshow(mark_boundaries(img, segments_quick))
ax[2].set_title("Quickshift")
for a in ax:
    a.set_xticks(())
    a.set_yticks(())
plt.show()

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io

%pylab inline
%matplotlib inline 
img = img_as_float(io.imread('003.jpg'))
#io.imshow(img);
for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=200)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()

from PIL import Image
img = Image.open("images/coffee002.png") 
img = img.convert("RGB")
img.save("images/coffee002.jpg")

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

img = Image.open("images/coffee002.jpg")    
img = img.convert('P', palette=Image.ADAPTIVE, colors=36)    
img.save("003a.png")    
   
img = img_as_float(io.imread('003a.png'))



#io.imshow(img);
segs = 360
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("test2.png", bbox_inches='tight')
im = Image.open("test2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('coffee002.jpg')
im.show()

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

img = Image.open("003.jpg")    
img = img.convert('P', palette=Image.ADAPTIVE, colors=36)    
img.save("003a.png")    
   
img = img_as_float(io.imread('003a.png'))



#io.imshow(img);
segs = 360
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("test2.png", bbox_inches='tight')
im = Image.open("test2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('test2.jpg')
im.show()



import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)
    
img = img_as_float(io.imread('003.jpg'))
#io.imshow(img);
segs = 1000
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("test2.png", bbox_inches='tight')
im = Image.open("test2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('test2.jpg')
im.show()

from PIL import Image
im=Image.open("test2.png")
im

from PIL import Image
im=Image.open("test2.png")
im.size
im.getbbox()
im2=im.crop(im.getbbox())
im2.size


from PIL import Image, ImageChops

def trim(im, border):
    bg = Image.new(im.mode, im.size, border)
    diff = ImageChops.difference(im, bg)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

def create_thumbnail(path, xsize,ysize):
    image = Image.open(path)
    name, extension = path.split('.')
    options = {}
    if 'transparency' in image.info:
        options['transparency'] = image.info["transparency"]
  
    image.thumbnail((xsize, ysize), Image.ANTIALIAS)
    image = trim(image, 255) ## Trim whitespace
    image.save(name + '_new.' + extension, **options)
    return image


create_thumbnail('test2.png', 640, 640)

!ls *.png

from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("coffee002.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()


image_data_bw = image_data[:,:,3]


from numpy import random
import matplotlib.pyplot as plt

data = random.random((5,5))
img = plt.imshow(data, interpolation='nearest')
img.set_cmap('hot')
plt.axis('off')
plt.savefig("test.png", bbox_inches='tight')

import matplotlib.pyplot as plt
import numpy as np
%pylab inline
%matplotlib inline 

ncols = 5
nrows = 3

# create the plots
fig = plt.figure()
axes = [ fig.add_subplot(nrows, ncols, r * ncols + c) for r in range(1, nrows) for c in range(1, ncols) ]

# add some data
for ax in axes:
    ax.plot(np.random.random(10), np.random.random(10), '.')

# remove the x and y ticks
for ax in axes:
    ax.set_xticks([])
    ax.set_yticks([])

def autocrop_image2(image):
    image_data = np.asarray(image)
    image_data_bw = image_data[:,:,3]
    non_empty_columns = np.where(image_data_bw.max(axis=0) > 0)[0]
    non_empty_rows = np.where(image_data_bw.max(axis=1) > 0)[0]
    cropBox = (min(non_empty_rows), max(non_empty_rows),
               min(non_empty_columns), max(non_empty_columns))

    image_data_new = image_data[cropBox[0]:cropBox[
        1] + 1, cropBox[2]:cropBox[3] + 1, :]

    new_image = Image.fromarray(image_data_new)
    return new_image

from PIL import Image
im=Image.open("test2.png")
bbox = im.convert("RGBa").getbbox()
im0 = autocrop_image2(bbox)
im0

!locate override.css



im=Image.open("test2.png")
bbox = im.convert("RGBA")
bbox

from PIL import Image
import numpy as np
def autocrop_image2(image):
    #image.load()
    image_data = np.asarray(image)
    image_data_bw = image_data.max(axis=2)
    non_empty_columns = np.where(image_data_bw.max(axis=0) > 0)[0]
    non_empty_rows = np.where(image_data_bw.max(axis=1) > 0)[0]
    cropBox = (min(non_empty_rows), max(non_empty_rows),
               min(non_empty_columns), max(non_empty_columns))

    image_data_new = image_data[cropBox[0]:cropBox[
        1] + 1, cropBox[2]:cropBox[3] + 1, :]

    new_image = Image.fromarray(image_data_new)
    return new_image
im=Image.open("test2.png")
bbox = im.convert("RGBA").getbbox()
im0 = autocrop_image2(bbox)
im0

# import the necessary packages
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import argparse
import utils
import cv2
def clust(IMAGE):
    # construct the argument parser and parse the arguments
    # load the image and convert it from BGR to RGB so that
    # we can dispaly it with matplotlib
    image = cv2.imread(IMAGE)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    # show our image
    return image
    
IMAGE = "test2.png"    
im = clust(IMAGE)

cv2.imwrite("NEWim.png", im)

from PIL import Image
imN= Image.open("NEWim.png")
imN

import markdown
class MD(str):
    def _repr_html_(self):
        return markdown.markdown(self)
    
STR="""
<div style ="border:1px solid red;padding:15px;">    
<h3>Help on function mark_boundaries in module skimage.segmentation.boundaries:</h3>

<p style = "color:blue">mark_boundaries(image, label_img, color=(1, 1, 0), outline_color=None, mode='outer', background_label=0)
Return image with boundaries between labeled regions highlighted.</p>
Parameters
----------<pre>
    image : (M, N[, 3]) array
        Grayscale or RGB image.
    label_img : (M, N) array of int
        Label array where regions are marked by different integer values.
    color : length-3 sequence, optional
        RGB color of boundaries in the output image.
    outline_color : length-3 sequence, optional
        RGB color surrounding boundaries in the output image. If None, no
        outline is drawn.
    mode : string in {'thick', 'inner', 'outer', 'subpixel'}, optional
        The mode for finding boundaries.
    background_label : int, optional
        Which label to consider background (this is only useful for
        modes ``inner`` and ``outer``).
</pre>    
Returns
-------
    marked : (M, N, 3) array of float
        An image in which the boundaries between labels are
        superimposed on the original image.
</div>
"""
MD(STR)    

!ls ~/.jupyter/custom

# %load ~/.jupyter/custom/current_theme.txt
monokai

!python color_kmeans.py --image 003.jpg --clusters 3

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import argparse
import numpy as np
import cv2
%pylab inline
%matplotlib inline 
def centroid_histogram(clt):
    # grab the number of different clusters and create a histogram
    # based on the number of pixels assigned to each cluster
    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
    (hist, _) = np.histogram(clt.labels_, bins = numLabels)

    # normalize the histogram, such that it sums to one
    hist = hist.astype("float")
    hist /= hist.sum()

    # return the histogram
    return hist

def plot_colors(hist, centroids):
    # initialize the bar chart representing the relative frequency
    # of each of the colors
    bar = np.zeros((50, 300, 3), dtype = "uint8")
    startX = 0

    # loop over the percentage of each cluster and the color of
    # each cluster
    for (percent, color) in zip(hist, centroids):
        # plot the relative percentage of each cluster
        endX = startX + (percent * 300)
        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),
            color.astype("uint8").tolist(), -1)
        startX = endX

    # return the bar chart
    return bar

def Clust(iMage, clusters):
    # load the image and convert it from BGR to RGB so that
    # we can dispaly it with matplotlib
    image = cv2.imread(iMage)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # show our image
    plt.figure()
    plt.axis("off")
    plt.imshow(image)

    # reshape the image to be a list of pixels
    image = image.reshape((image.shape[0] * image.shape[1], 3))

    # cluster the pixel intensities
    clt = KMeans(clusters)
    clt.fit(image)

    # build a histogram of clusters and then create a figure
    # representing the number of pixels labeled to each color
    hist = utilz.centroid_histogram(clt)
    bar = utilz.plot_colors(hist, clt.cluster_centers_)

    # show our color bart
    plt.figure()
    plt.axis("off")
    plt.imshow(bar)
    plt.show()

iMage = "003.jpg"
clusters = 6
Clust(iMage, clusters)    

%%writefile utilz.py
# import the necessary packages
import numpy as np
import cv2

def centroid_histogram(clt):
    # grab the number of different clusters and create a histogram
    # based on the number of pixels assigned to each cluster
    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
    (hist, _) = np.histogram(clt.labels_, bins = numLabels)

    # normalize the histogram, such that it sums to one
    hist = hist.astype("float")
    hist /= hist.sum()

    # return the histogram
    return hist

def plot_colors(hist, centroids):
    # initialize the bar chart representing the relative frequency
    # of each of the colors
    bar = np.zeros((50, 300, 3), dtype = "uint8")
    startX = 0

    # loop over the percentage of each cluster and the color of
    # each cluster
    for (percent, color) in zip(hist, centroids):
        # plot the relative percentage of each cluster
        endX = startX + (percent * 300)
        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),
            color.astype("uint8").tolist(), -1)
        startX = endX

    # return the bar chart
    return bar

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color, io
from skimage.future import graph
from matplotlib import pyplot as plt

img = io.imread("test2.jpg")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
plt.imshow(out)
io.imsave("Stest2.png", out)
#plt.title('Final segmentation')

plt.show()    
    

!showme Stest2.png

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops

img = io.imread("test2.jpg")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
io.imsave("Stest2.png", out)
IMG =Image.open("Stest2.png")
IMG

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops

img = io.imread("test2.jpg")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
io.imsave("Stest2.png", out)
IMG =Image.open("Stest2.png")
IMG

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops
def main(imgagefile):
    img = io.imread(imgagefile)
    labels = segmentation.slic(img, compactness=30, n_segments=400)
    g = future.graph.rag_mean_color(img, labels)
    def weight_boundary(graph, src, dst, n):
        default = {'weight': 0.0, 'count': 0}
        count_src = graph[src].get(n, default)['count']
        count_dst = graph[dst].get(n, default)['count']
        weight_src = graph[src].get(n, default)['weight']
        weight_dst = graph[dst].get(n, default)['weight']
        count = count_src + count_dst
        return {
            'count': count,
            'weight': (count_src * weight_src + count_dst * weight_dst)/count
        }
    def merge_boundary(graph, src, dst):
        """Call back called before merging 2 nodes.
        In this case we don't need to do any computation here.""" 
        pass

    labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                       in_place_merge=True,
                                       merge_func=merge_boundary,
                                       weight_func=weight_boundary)
    out = color.label2rgb(labels2, img, kind='avg')
    io.imsave("Stest2.png", out)
    IMG =Image.open("Stest2.png")
    IMG

im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('Stest2.jpg')
im.show()

plt.savefig("test2.png", bbox_inches='tight')
im = Image.open("test2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('test2.jpg')
im.show()

from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("coffee002.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()

%%writefile utils.py
# import the necessary packages
import numpy as np
import cv2
 
def centroid_histogram(clt):
	# grab the number of different clusters and create a histogram
	# based on the number of pixels assigned to each cluster
	numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
	(hist, _) = np.histogram(clt.labels_, bins = numLabels)
 
	# normalize the histogram, such that it sums to one
	hist = hist.astype("float")
	hist /= hist.sum()
 
	# return the histogram
	return hist

!du -hs /home/conda/Desktop/NoteBooks/GRAPHICS/superpixels/slicPython

from IPython.core.display import HTML
HTML("""
<style>
#notebooks-container {
    padding: 15px;
    background-color: #ffebcd;
    min-height: 20px;
    width: 900px;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.3);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.3);
}
</style>
<div id ="notebooks-container">
<h1>Notebook Container</h1>
<div/>
""")

import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io

%pylab inline
%matplotlib inline 


!wget -O images/003.jpg https://jacknorthrup.com/INSTAGRAM/003.jpg

img = img_as_float(io.imread('images/003.jpg'))
io.imshow(img);

for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=300)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()

for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=300)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()


from __future__ import print_function

import matplotlib.pyplot as plt
import numpy as np

from skimage.data import astronaut
from skimage.segmentation import felzenszwalb, slic, quickshift
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float

segments_fz = felzenszwalb(img, scale=100, sigma=0.5, min_size=50)
segments_slic = slic(img, n_segments=250, compactness=10, sigma=1)
segments_quick = quickshift(img, kernel_size=3, max_dist=6, ratio=0.5)

print("Felzenszwalb's number of segments: %d" % len(np.unique(segments_fz)))
print("Slic number of segments: %d" % len(np.unique(segments_slic)))
print("Quickshift number of segments: %d" % len(np.unique(segments_quick)))

fig, ax = plt.subplots(1, 3)
fig.set_size_inches(16, 8, forward=True)
fig.subplots_adjust(0.05, 0.05, 0.95, 0.95, 0.05, 0.05)

ax[0].imshow(mark_boundaries(img, segments_fz))
ax[0].set_title("Felzenszwalbs's method")
ax[1].imshow(mark_boundaries(img, segments_slic))
ax[1].set_title("SLIC")
ax[2].imshow(mark_boundaries(img, segments_quick))
ax[2].set_title("Quickshift")
for a in ax:
    a.set_xticks(())
    a.set_yticks(())
plt.show()

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io

%pylab inline
%matplotlib inline 
img = img_as_float(io.imread('images/003.jpg'))
#io.imshow(img);
for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=200)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()

from PIL import Image
img = Image.open("photos/0028xxx.jpg") 
img = img.convert("RGB")
img.save("photos/0028RGBxxx.jpg")

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image, ImageChops
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

img = Image.open("photos/0028RGBxxx.jpg")    
img = img.convert('P', palette=Image.ADAPTIVE, colors=36)    
img.save("junk/003a.png")    
   
img = img_as_float(io.imread('junk/003a.png'))



#io.imshow(img);
segs = 360
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("junk/test2.png", bbox_inches='tight')
im = Image.open("junk/test2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('junk/coffee002.jpg')
im.show()

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

img = Image.open("images/003.jpg")    
img = img.convert('P', palette=Image.ADAPTIVE, colors=36)    
img.save("images/003a.png")    
   
img = img_as_float(io.imread('images/003a.png'))



#io.imshow(img);
segs = 360
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("images/test2a.png", bbox_inches='tight')
im = Image.open("images/test2a.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/test2B.jpg')
im.show()



import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)
    
img = img_as_float(io.imread('images/003.jpg'))
#io.imshow(img);
segs = 1000
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("images/test3.png", bbox_inches='tight')
im = Image.open("images/test3.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/test2C.jpg')
im.show()

from PIL import Image
im=Image.open("images/test2C.jpg")
print (im.size)
im

from PIL import Image
im=Image.open("images/test2C.jpg")
im.size
im.getbbox()
im2=im.crop(im.getbbox())
im2.size
im2

from PIL import Image, ImageChops

def trim(im, border):
    bg = Image.new(im.mode, im.size, border)
    diff = ImageChops.difference(im, bg)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

def create_thumbnail(path, xsize,ysize):
    image = Image.open(path)
    name, extension = path.split('.')
    options = {}
    if 'transparency' in image.info:
        options['transparency'] = image.info["transparency"]
  
    image.thumbnail((xsize, ysize), Image.ANTIALIAS)
    image = trim(image, 255) ## Trim whitespace
    image.save(name + '_new.' + extension, **options)
    return image


create_thumbnail('photos/38.jpg', 200, 200)

from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("photos/38.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()


from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("photos/38.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()

from PIL import Image
im=Image.open("photos/38.jpg")
im.save("photos/38.png")
im2=Image.open("photos/38.png")

im.size
im.getbbox()
im2=im.crop(im.getbbox())
im2.size

from numpy import random
import matplotlib.pyplot as plt

data = random.random((5,5))
img = plt.imshow(data, interpolation='nearest')
img.set_cmap('hot')
plt.axis('off')
plt.savefig("test.png", bbox_inches='tight')

import matplotlib.pyplot as plt
import numpy as np
%pylab inline
%matplotlib inline 

ncols = 5
nrows = 3

# create the plots
fig = plt.figure()
axes = [ fig.add_subplot(nrows, ncols, r * ncols + c) for r in range(1, nrows) for c in range(1, ncols) ]

# add some data
for ax in axes:
    ax.plot(np.random.random(10), np.random.random(10), '.')

# remove the x and y ticks
for ax in axes:
    ax.set_xticks([])
    ax.set_yticks([])

def autocrop_image2(image):
    image_data = np.asarray(image)
    image_data_bw = image_data[:,:,3]
    non_empty_columns = np.where(image_data_bw.max(axis=0) > 0)[0]
    non_empty_rows = np.where(image_data_bw.max(axis=1) > 0)[0]
    cropBox = (min(non_empty_rows), max(non_empty_rows),
               min(non_empty_columns), max(non_empty_columns))

    image_data_new = image_data[cropBox[0]:cropBox[
        1] + 1, cropBox[2]:cropBox[3] + 1, :]

    new_image = Image.fromarray(image_data_new)
    return new_image

from PIL import Image
im=Image.open("test2.png")
bbox = im.convert("RGBa").getbbox()
im0 = autocrop_image2(bbox)
im0

!locate override.css



im=Image.open("photos/38.jpg")
bbox = im.convert("RGBA")
bbox

from PIL import Image
import numpy as np
def autocrop_image2(image):
    #image.load()
    image_data = np.asarray(image)
    image_data_bw = image_data.max(axis=2)
    non_empty_columns = np.where(image_data_bw.max(axis=0) > 0)[0]
    non_empty_rows = np.where(image_data_bw.max(axis=1) > 0)[0]
    cropBox = (min(non_empty_rows), max(non_empty_rows),
               min(non_empty_columns), max(non_empty_columns))

    image_data_new = image_data[cropBox[0]:cropBox[
        1] + 1, cropBox[2]:cropBox[3] + 1, :]

    new_image = Image.fromarray(image_data_new)
    return new_image
im=Image.open("photos/38.png")
bbox = im.convert("RGBA").getbbox()
im0 = autocrop_image2(bbox)
im0

im = Image.open("photos/38.jpg")
im = im.resize((640,640), Image.NEAREST)
im.save("photos/38.png")
im

# import the necessary packages
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import argparse
import utils
import cv2
def clust(IMAGE):
    # construct the argument parser and parse the arguments
    # load the image and convert it from BGR to RGB so that
    # we can dispaly it with matplotlib
    image = cv2.imread(IMAGE)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    # show our image
    return image
    
IMAGE = "photos/38.png"    
im = clust(IMAGE)

cv2.imwrite("photos/38CLUST.png", im)

from PIL import Image
imN= Image.open("photos/38CLUST.png")
imN

import markdown
class MD(str):
    def _repr_html_(self):
        return markdown.markdown(self)
    
STR="""
<div style ="border:1px solid red;padding:15px;">    
<h3>Help on function mark_boundaries in module skimage.segmentation.boundaries:</h3>

<p style = "color:blue">mark_boundaries(image, label_img, color=(1, 1, 0), outline_color=None, mode='outer', background_label=0)
Return image with boundaries between labeled regions highlighted.</p>
Parameters
----------<pre>
    image : (M, N[, 3]) array
        Grayscale or RGB image.
    label_img : (M, N) array of int
        Label array where regions are marked by different integer values.
    color : length-3 sequence, optional
        RGB color of boundaries in the output image.
    outline_color : length-3 sequence, optional
        RGB color surrounding boundaries in the output image. If None, no
        outline is drawn.
    mode : string in {'thick', 'inner', 'outer', 'subpixel'}, optional
        The mode for finding boundaries.
    background_label : int, optional
        Which label to consider background (this is only useful for
        modes ``inner`` and ``outer``).
</pre>    
Returns
-------
    marked : (M, N, 3) array of float
        An image in which the boundaries between labels are
        superimposed on the original image.
</div>
"""
MD(STR)    

!ls ~/.jupyter/custom

# %load ~/.jupyter/custom/current_theme.txt
monokai

!python color_kmeans.py --image 003.jpg --clusters 3

%%writefile utilz.py
# import the necessary packages
import numpy as np
import cv2

def centroid_histogram(clt):
    # grab the number of different clusters and create a histogram
    # based on the number of pixels assigned to each cluster
    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
    (hist, _) = np.histogram(clt.labels_, bins = numLabels)

    # normalize the histogram, such that it sums to one
    hist = hist.astype("float")
    hist /= hist.sum()

    # return the histogram
    return hist

def plot_colors(hist, centroids):
    # initialize the bar chart representing the relative frequency
    # of each of the colors
    bar = np.zeros((50, 300, 3), dtype = "uint8")
    startX = 0

    # loop over the percentage of each cluster and the color of
    # each cluster
    for (percent, color) in zip(hist, centroids):
        # plot the relative percentage of each cluster
        endX = startX + (percent * 300)
        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),
            color.astype("uint8").tolist(), -1)
        startX = endX

    # return the bar chart
    return bar

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import argparse
import numpy as np
import utilz
import cv2
%pylab inline
%matplotlib inline 
def centroid_histogram(clt):
    # grab the number of different clusters and create a histogram
    # based on the number of pixels assigned to each cluster
    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
    (hist, _) = np.histogram(clt.labels_, bins = numLabels)

    # normalize the histogram, such that it sums to one
    hist = hist.astype("float")
    hist /= hist.sum()

    # return the histogram
    return hist

def plot_colors(hist, centroids):
    # initialize the bar chart representing the relative frequency
    # of each of the colors
    bar = np.zeros((50, 300, 3), dtype = "uint8")
    startX = 0

    # loop over the percentage of each cluster and the color of
    # each cluster
    for (percent, color) in zip(hist, centroids):
        # plot the relative percentage of each cluster
        endX = startX + (percent * 300)
        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),
            color.astype("uint8").tolist(), -1)
        startX = endX

    # return the bar chart
    return bar

def Clust(iMage, clusters):
    # load the image and convert it from BGR to RGB so that
    # we can dispaly it with matplotlib
    image = cv2.imread(iMage)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # show our image
    plt.figure()
    plt.axis("off")
    plt.imshow(image)

    # reshape the image to be a list of pixels
    image = image.reshape((image.shape[0] * image.shape[1], 3))

    # cluster the pixel intensities
    clt = KMeans(clusters)
    clt.fit(image)

    # build a histogram of clusters and then create a figure
    # representing the number of pixels labeled to each color
    hist = utilz.centroid_histogram(clt)
    bar = utilz.plot_colors(hist, clt.cluster_centers_)

    # show our color bart
    plt.figure()
    plt.axis("off")
    plt.imshow(bar)
    plt.show()

iMage = "photos/38CLUST.png"
clusters = 6
Clust(iMage, clusters)    

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color, io
from skimage.future import graph
from matplotlib import pyplot as plt

img = io.imread("photos/38CLUST.png")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
plt.imshow(out)
io.imsave("images/Stest2.png", out)
#plt.title('Final segmentation')

plt.show()    
    

!showme images/Stest2.png

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops

img = io.imread("images/Stest2.png")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
io.imsave("images/Stest2S.png", out)
IMG =Image.open("images/Stest2S.png")
IMG

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops

img = io.imread("photos/0147xxx.jpg")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
io.imsave("images/SStest2.png", out)
IMG =Image.open("images/SStest2.png")
IMG

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops
def main(imgagefile):
    img = io.imread(imgagefile)
    labels = segmentation.slic(img, compactness=30, n_segments=400)
    g = future.graph.rag_mean_color(img, labels)
    def weight_boundary(graph, src, dst, n):
        default = {'weight': 0.0, 'count': 0}
        count_src = graph[src].get(n, default)['count']
        count_dst = graph[dst].get(n, default)['count']
        weight_src = graph[src].get(n, default)['weight']
        weight_dst = graph[dst].get(n, default)['weight']
        count = count_src + count_dst
        return {
            'count': count,
            'weight': (count_src * weight_src + count_dst * weight_dst)/count
        }
    def merge_boundary(graph, src, dst):
        """Call back called before merging 2 nodes.
        In this case we don't need to do any computation here.""" 
        pass

    labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                       in_place_merge=True,
                                       merge_func=merge_boundary,
                                       weight_func=weight_boundary)
    out = color.label2rgb(labels2, img, kind='avg')
    io.imsave("images/Stest2.png", out)
    IMG =Image.open("images/Stest2.png")
    IMG

im = Image.open("images/Stest2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/Stest2.jpg')
im.show()

plt.savefig("images/testZ2.png", bbox_inches='tight')
im = Image.open("images/testZ2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('test2.jpg')
im.show()

from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("coffee002.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()

%%writefile utils.py
# import the necessary packages
import numpy as np
import cv2
 
def centroid_histogram(clt):
	# grab the number of different clusters and create a histogram
	# based on the number of pixels assigned to each cluster
	numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
	(hist, _) = np.histogram(clt.labels_, bins = numLabels)
 
	# normalize the histogram, such that it sums to one
	hist = hist.astype("float")
	hist /= hist.sum()
 
	# return the histogram
	return hist





!du -hs /home/conda/Desktop/NoteBooks/GRAPHICS/superpixels/slicPython

from IPython.core.display import HTML
HTML("""
<style>
#notebooks-container {
    padding: 15px;
    background-color: #ffebcd;
    min-height: 20px;
    width: 900px;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.3);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.3);
}
</style>
<div id ="notebooks-container">
<h1>Notebook Container</h1>
<div/>
""")

import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io

%pylab inline
%matplotlib inline 


!wget -O images/003.jpg https://jacknorthrup.com/INSTAGRAM/003.jpg

from PIL import Image
IM = Image.open('junk/PILStuff20220130034802.png')
IM.save('junk/PILStuff20220130034802.jpg', 'RGB')

img = img_as_float(io.imread('junk/PILStuff20220130034802.png'))
io.imshow(img);

for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=300)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()

for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=300)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()


from __future__ import print_function

import matplotlib.pyplot as plt
import numpy as np

from skimage.data import astronaut
from skimage.segmentation import felzenszwalb, slic, quickshift
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float

segments_fz = felzenszwalb(img, scale=100, sigma=0.5, min_size=50)
segments_slic = slic(img, n_segments=250, compactness=10, sigma=1)
segments_quick = quickshift(img, kernel_size=3, max_dist=6, ratio=0.5)

print("Felzenszwalb's number of segments: %d" % len(np.unique(segments_fz)))
print("Slic number of segments: %d" % len(np.unique(segments_slic)))
print("Quickshift number of segments: %d" % len(np.unique(segments_quick)))

fig, ax = plt.subplots(1, 3)
fig.set_size_inches(16, 8, forward=True)
fig.subplots_adjust(0.05, 0.05, 0.95, 0.95, 0.05, 0.05)

ax[0].imshow(mark_boundaries(img, segments_fz))
ax[0].set_title("Felzenszwalbs's method")
ax[1].imshow(mark_boundaries(img, segments_slic))
ax[1].set_title("SLIC")
ax[2].imshow(mark_boundaries(img, segments_quick))
ax[2].set_title("Quickshift")
for a in ax:
    a.set_xticks(())
    a.set_yticks(())
plt.show()

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io

%pylab inline
%matplotlib inline 
img = img_as_float(io.imread('images/003.jpg'))
#io.imshow(img);
for segs in (10, 50, 100, 300, 500, 1000):
    segments = slic(img, n_segments = segs, sigma = 4)
    fig = plt.figure(figsize=(12,4), dpi=200)
    #ax = fig.add_axes([0, 0, 1, 1])
    plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
    #plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
    imshow(mark_boundaries(img, segments, (0,0,0)))
plt.show()

from PIL import Image
img = Image.open("photos/0028xxx.jpg") 
img = img.convert("RGB")
img.save("photos/0028RGBxxx.jpg")

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image, ImageChops
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

#img = Image.open("photos/0028RGBxxx.jpg") 
img = Image.open("junk/PILStuff20220130034802.png")
img = img.convert('P', palette=Image.ADAPTIVE, colors=36)    
img.save("junk/003a.png")    
   
img = img_as_float(io.imread('junk/003a.png'))



#io.imshow(img);
segs = 360
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("junk/test2.png", bbox_inches='tight')
im = Image.open("junk/test2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('junk/coffee002.jpg')
im.show()

import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

img = Image.open("images/003.jpg")    
img = img.convert('P', palette=Image.ADAPTIVE, colors=36)    
img.save("images/003a.png")    
   
img = img_as_float(io.imread('images/003a.png'))



#io.imshow(img);
segs = 360
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("images/test2a.png", bbox_inches='tight')
im = Image.open("images/test2a.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/test2B.jpg')
im.show()



import matplotlib.pyplot as plt
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
from PIL import Image
%pylab inline
%matplotlib inline 
def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)
    
img = img_as_float(io.imread('images/003.jpg'))
#io.imshow(img);
segs = 1000
segments = slic(img, n_segments = segs, sigma = 4)
fig = plt.figure(figsize=(12,4), dpi=200)
#ax = fig.add_axes([0, 0, 1, 1])
#plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off') 
#plt.tick_params(axis='Y',which='both',bottom='off',top='off',labelbottom='off') 
plt.axis('off')
imshow(mark_boundaries(img, segments, (0,0,0)))
plt.savefig("images/test3.png", bbox_inches='tight')
im = Image.open("images/test3.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/test2C.jpg')
im.show()

from PIL import Image
im=Image.open("images/test2C.jpg")
print (im.size)
im

from PIL import Image
im=Image.open("images/test2C.jpg")
im.size
im.getbbox()
im2=im.crop(im.getbbox())
im2.size
im2

from PIL import Image, ImageChops

def trim(im, border):
    bg = Image.new(im.mode, im.size, border)
    diff = ImageChops.difference(im, bg)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

def create_thumbnail(path, xsize,ysize):
    image = Image.open(path)
    name, extension = path.split('.')
    options = {}
    if 'transparency' in image.info:
        options['transparency'] = image.info["transparency"]
  
    image.thumbnail((xsize, ysize), Image.ANTIALIAS)
    image = trim(image, 255) ## Trim whitespace
    image.save(name + '_new.' + extension, **options)
    return image


create_thumbnail('photos/38.jpg', 200, 200)

from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("photos/38.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()


from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("photos/38.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()

from PIL import Image
im=Image.open("photos/38.jpg")
im.save("photos/38.png")
im2=Image.open("photos/38.png")

im.size
im.getbbox()
im2=im.crop(im.getbbox())
im2.size

from numpy import random
import matplotlib.pyplot as plt

data = random.random((5,5))
img = plt.imshow(data, interpolation='nearest')
img.set_cmap('hot')
plt.axis('off')
plt.savefig("test.png", bbox_inches='tight')

import matplotlib.pyplot as plt
import numpy as np
%pylab inline
%matplotlib inline 

ncols = 5
nrows = 3

# create the plots
fig = plt.figure()
axes = [ fig.add_subplot(nrows, ncols, r * ncols + c) for r in range(1, nrows) for c in range(1, ncols) ]

# add some data
for ax in axes:
    ax.plot(np.random.random(10), np.random.random(10), '.')

# remove the x and y ticks
for ax in axes:
    ax.set_xticks([])
    ax.set_yticks([])

def autocrop_image2(image):
    image_data = np.asarray(image)
    image_data_bw = image_data[:,:,3]
    non_empty_columns = np.where(image_data_bw.max(axis=0) > 0)[0]
    non_empty_rows = np.where(image_data_bw.max(axis=1) > 0)[0]
    cropBox = (min(non_empty_rows), max(non_empty_rows),
               min(non_empty_columns), max(non_empty_columns))

    image_data_new = image_data[cropBox[0]:cropBox[
        1] + 1, cropBox[2]:cropBox[3] + 1, :]

    new_image = Image.fromarray(image_data_new)
    return new_image

from PIL import Image
im=Image.open("test2.png")
bbox = im.convert("RGBa").getbbox()
im0 = autocrop_image2(bbox)
im0

!locate override.css



im=Image.open("photos/38.jpg")
bbox = im.convert("RGBA")
bbox

from PIL import Image
import numpy as np
def autocrop_image2(image):
    #image.load()
    image_data = np.asarray(image)
    image_data_bw = image_data.max(axis=2)
    non_empty_columns = np.where(image_data_bw.max(axis=0) > 0)[0]
    non_empty_rows = np.where(image_data_bw.max(axis=1) > 0)[0]
    cropBox = (min(non_empty_rows), max(non_empty_rows),
               min(non_empty_columns), max(non_empty_columns))

    image_data_new = image_data[cropBox[0]:cropBox[
        1] + 1, cropBox[2]:cropBox[3] + 1, :]

    new_image = Image.fromarray(image_data_new)
    return new_image
im=Image.open("photos/38.png")
bbox = im.convert("RGBA").getbbox()
im0 = autocrop_image2(bbox)
im0

im = Image.open("photos/38.jpg")
im = im.resize((640,640), Image.NEAREST)
im.save("photos/38.png")
im

# import the necessary packages
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import argparse
import utils
import cv2
def clust(IMAGE):
    # construct the argument parser and parse the arguments
    # load the image and convert it from BGR to RGB so that
    # we can dispaly it with matplotlib
    image = cv2.imread(IMAGE)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    # show our image
    return image
    
IMAGE = "photos/38.png"    
im = clust(IMAGE)

cv2.imwrite("photos/38CLUST.png", im)

from PIL import Image
imN= Image.open("photos/38CLUST.png")
imN

import markdown
class MD(str):
    def _repr_html_(self):
        return markdown.markdown(self)
    
STR="""
<div style ="border:1px solid red;padding:15px;">    
<h3>Help on function mark_boundaries in module skimage.segmentation.boundaries:</h3>

<p style = "color:blue">mark_boundaries(image, label_img, color=(1, 1, 0), outline_color=None, mode='outer', background_label=0)
Return image with boundaries between labeled regions highlighted.</p>
Parameters
----------<pre>
    image : (M, N[, 3]) array
        Grayscale or RGB image.
    label_img : (M, N) array of int
        Label array where regions are marked by different integer values.
    color : length-3 sequence, optional
        RGB color of boundaries in the output image.
    outline_color : length-3 sequence, optional
        RGB color surrounding boundaries in the output image. If None, no
        outline is drawn.
    mode : string in {'thick', 'inner', 'outer', 'subpixel'}, optional
        The mode for finding boundaries.
    background_label : int, optional
        Which label to consider background (this is only useful for
        modes ``inner`` and ``outer``).
</pre>    
Returns
-------
    marked : (M, N, 3) array of float
        An image in which the boundaries between labels are
        superimposed on the original image.
</div>
"""
MD(STR)    

!ls ~/.jupyter/custom

# %load ~/.jupyter/custom/current_theme.txt
monokai

!python color_kmeans.py --image 003.jpg --clusters 3

%%writefile utilz.py
# import the necessary packages
import numpy as np
import cv2

def centroid_histogram(clt):
    # grab the number of different clusters and create a histogram
    # based on the number of pixels assigned to each cluster
    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
    (hist, _) = np.histogram(clt.labels_, bins = numLabels)

    # normalize the histogram, such that it sums to one
    hist = hist.astype("float")
    hist /= hist.sum()

    # return the histogram
    return hist

def plot_colors(hist, centroids):
    # initialize the bar chart representing the relative frequency
    # of each of the colors
    bar = np.zeros((50, 300, 3), dtype = "uint8")
    startX = 0

    # loop over the percentage of each cluster and the color of
    # each cluster
    for (percent, color) in zip(hist, centroids):
        # plot the relative percentage of each cluster
        endX = startX + (percent * 300)
        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),
            color.astype("uint8").tolist(), -1)
        startX = endX

    # return the bar chart
    return bar

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import argparse
import numpy as np
import utilz
import cv2
%pylab inline
%matplotlib inline 
def centroid_histogram(clt):
    # grab the number of different clusters and create a histogram
    # based on the number of pixels assigned to each cluster
    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
    (hist, _) = np.histogram(clt.labels_, bins = numLabels)

    # normalize the histogram, such that it sums to one
    hist = hist.astype("float")
    hist /= hist.sum()

    # return the histogram
    return hist

def plot_colors(hist, centroids):
    # initialize the bar chart representing the relative frequency
    # of each of the colors
    bar = np.zeros((50, 300, 3), dtype = "uint8")
    startX = 0

    # loop over the percentage of each cluster and the color of
    # each cluster
    for (percent, color) in zip(hist, centroids):
        # plot the relative percentage of each cluster
        endX = startX + (percent * 300)
        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),
            color.astype("uint8").tolist(), -1)
        startX = endX

    # return the bar chart
    return bar

def Clust(iMage, clusters):
    # load the image and convert it from BGR to RGB so that
    # we can dispaly it with matplotlib
    image = cv2.imread(iMage)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # show our image
    plt.figure()
    plt.axis("off")
    plt.imshow(image)

    # reshape the image to be a list of pixels
    image = image.reshape((image.shape[0] * image.shape[1], 3))

    # cluster the pixel intensities
    clt = KMeans(clusters)
    clt.fit(image)

    # build a histogram of clusters and then create a figure
    # representing the number of pixels labeled to each color
    hist = utilz.centroid_histogram(clt)
    bar = utilz.plot_colors(hist, clt.cluster_centers_)

    # show our color bart
    plt.figure()
    plt.axis("off")
    plt.imshow(bar)
    plt.show()

iMage = "photos/38CLUST.png"
clusters = 6
Clust(iMage, clusters)    

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color, io
from skimage.future import graph
from matplotlib import pyplot as plt

img = io.imread("photos/38CLUST.png")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
plt.imshow(out)
io.imsave("images/Stest2.png", out)
#plt.title('Final segmentation')

plt.show()    
    

!showme images/Stest2.png

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops

img = io.imread("images/Stest2.png")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
io.imsave("images/Stest2S.png", out)
IMG =Image.open("images/Stest2S.png")
IMG

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops

img = io.imread("photos/0147xxx.jpg")
labels = segmentation.slic(img, compactness=30, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.0, 'count': 0}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    """Call back called before merging 2 nodes.
    In this case we don't need to do any computation here.""" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)

#graph.show_rag(labels, g, img)
#plt.title('RAG after hierarchical merging')
plt.figure(dpi=200)
out = color.label2rgb(labels2, img, kind='avg')
plt.axis("off")
io.imsave("images/SStest2.png", out)
IMG =Image.open("images/SStest2.png")
IMG

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage import graph, data, io
from skimage.future import graph
from matplotlib import pyplot as plt
from PIL import Image, ImageChops
def main(imgagefile):
    img = io.imread(imgagefile)
    labels = segmentation.slic(img, compactness=30, n_segments=400)
    g = future.graph.rag_mean_color(img, labels)
    def weight_boundary(graph, src, dst, n):
        default = {'weight': 0.0, 'count': 0}
        count_src = graph[src].get(n, default)['count']
        count_dst = graph[dst].get(n, default)['count']
        weight_src = graph[src].get(n, default)['weight']
        weight_dst = graph[dst].get(n, default)['weight']
        count = count_src + count_dst
        return {
            'count': count,
            'weight': (count_src * weight_src + count_dst * weight_dst)/count
        }
    def merge_boundary(graph, src, dst):
        """Call back called before merging 2 nodes.
        In this case we don't need to do any computation here.""" 
        pass

    labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                       in_place_merge=True,
                                       merge_func=merge_boundary,
                                       weight_func=weight_boundary)
    out = color.label2rgb(labels2, img, kind='avg')
    io.imsave("images/Stest2.png", out)
    IMG =Image.open("images/Stest2.png")
    IMG

im = Image.open("images/Stest2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('images/Stest2.jpg')
im.show()

plt.savefig("images/testZ2.png", bbox_inches='tight')
im = Image.open("images/testZ2.png")
im = trim(im)
im.resize((640,640), Image.NEAREST)
im = im.convert("RGB")
im.save('test2.jpg')
im.show()

from PIL import Image, ImageChops

def trim(im):
    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
    diff = ImageChops.difference(im, bg)
    diff = ImageChops.add(diff, diff, 2.0, -100)
    bbox = diff.getbbox()
    if bbox:
        return im.crop(bbox)

im = Image.open("coffee002.jpg")
im = trim(im)
im = im.resize((640,640), Image.NEAREST)
im.show()

%%writefile utils.py
# import the necessary packages
import numpy as np
import cv2
 
def centroid_histogram(clt):
	# grab the number of different clusters and create a histogram
	# based on the number of pixels assigned to each cluster
	numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)
	(hist, _) = np.histogram(clt.labels_, bins = numLabels)
 
	# normalize the histogram, such that it sums to one
	hist = hist.astype("float")
	hist /= hist.sum()
 
	# return the histogram
	return hist





import onnx
from onnx import helper, shape_inference
from onnx import TensorProto


# Preprocessing: create a model with two nodes, Y's shape is unknown
node1 = helper.make_node('Transpose', ['X'], ['Y'], perm=[1, 0, 2])
node2 = helper.make_node('Transpose', ['Y'], ['Z'], perm=[1, 0, 2])

graph = helper.make_graph(
    [node1, node2],
    'two-transposes',
    [helper.make_tensor_value_info('X', TensorProto.FLOAT, (2, 3, 4))],
    [helper.make_tensor_value_info('Z', TensorProto.FLOAT, (2, 3, 4))],
)

original_model = helper.make_model(graph, producer_name='onnx-examples')

# Check the model and print Y's shape information
onnx.checker.check_model(original_model)
print('Before shape inference, the shape info of Y is:\n{}'.format(original_model.graph.value_info))

# Apply shape inference on the model
inferred_model = shape_inference.infer_shapes(original_model)

# Check the model and print Y's shape information
onnx.checker.check_model(inferred_model)
print('After shape inference, the shape info of Y is:\n{}'.format(inferred_model.graph.value_info))

import numpy as np
from shapely.affinity import translate
from shapely.geometry import Point
from shapely.ops import unary_union

import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4")
vsk.scale("4mm")

for i in range(5):
    for j in range(7):
        shape = unary_union(
            [
                Point(*np.random.random(2) * 5).buffer(np.random.random())
                for _ in range(15)
            ]
        )
        vsk.geometry(translate(shape, i * 8, j * 8))

vsk.display(mode="matplotlib")
vsk.save("shapely.svg")

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import tensorflow as tf

class Model(tf.Module):

  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])
  def encode(self, x):
    result = tf.strings.as_string(x)
    return {
         "encoded_result": result
    }

  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])
  def decode(self, x):
    result = tf.strings.to_number(x)
    return {
         "decoded_result": result
    }

model = Model()

# Save the model
SAVED_MODEL_PATH = 'content/saved_models/coding'

tf.saved_model.save(
    model, SAVED_MODEL_PATH,
    signatures={
      'encode': model.encode.get_concrete_function(),
      'decode': model.decode.get_concrete_function()
    })

# Convert the saved model using TFLiteConverter
converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
tflite_model = converter.convert()

# Print the signatures from the converted model
interpreter = tf.lite.Interpreter(model_content=tflite_model)
signatures = interpreter.get_signature_list()
print(signatures)

# Generate a Keras model.
keras_model = tf.keras.Sequential(
    [
        tf.keras.layers.Dense(2, input_dim=4, activation='relu', name='x'),
        tf.keras.layers.Dense(1, activation='relu', name='output'),
    ]
)

# Convert the keras model using TFLiteConverter.
# Keras model converter API uses the default signature automatically.
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
tflite_model = converter.convert()

# Print the signatures from the converted model
interpreter = tf.lite.Interpreter(model_content=tflite_model)

signatures = interpreter.get_signature_list()
print(signatures)

model = Model()

# Convert the concrete functions using TFLiteConverter
converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [model.encode.get_concrete_function(),
     model.decode.get_concrete_function()], model)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
tflite_model = converter.convert()

# Print the signatures from the converted model
interpreter = tf.lite.Interpreter(model_content=tflite_model)
signatures = interpreter.get_signature_list()
print(signatures)

# Load the TFLite model in TFLite Interpreter
interpreter = tf.lite.Interpreter(model_content=tflite_model)

# Print the signatures from the converted model
signatures = interpreter.get_signature_list()
print('Signature:', signatures)

# encode and decode are callable with input as arguments.
encode = interpreter.get_signature_runner('encode')
decode = interpreter.get_signature_runner('decode')

# 'encoded' and 'decoded' are dictionaries with all outputs from the inference.
input = tf.constant([1, 2, 3], dtype=tf.float32)
print('Input:', input)
encoded = encode(x=input)
print('Encoded result:', encoded)
decoded = decode(x=encoded['encoded_result'])
print('Decoded result:', decoded)

from OutlineImage import outlineP
filename1 = "/home/jack/Desktop/TENSORFLOW/images/mining-machine.png" 
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png" 
outlineP(filename1,outfile_png)

import os
os.chdir('/home/jack/Desktop/dockercommands')
os.getcwd()

#%%writefile Tweetme2
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import time
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
images=[]
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/test001.mp4"
filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
filename ="http://192.168.0.104:8000/public/videos/THE_DISTURBING_SATIRICAL_ART_OF_ANDREAS_PAUL_WEBER.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    
    
    
    
    cv2.imwrite("onevid/temp.png", img1)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    STR = randTXT()
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = STR
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["wood-blur-frame.png","frames.png","lined-frame.png","black-blur-frame.png", "white-blur-frame.png","beige-blur-frame.png","frame-lite.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]

    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])


%%writefile Tweetme2
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import time
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
images=[]
count = 0
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    print(filename)
    return nim
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/test001.mp4"
filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", img1)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    #STR = randTXT()
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["wood-blur-frame.png","frames.png","lined-frame.png","black-blur-frame.png", "white-blur-frame.png","beige-blur-frame.png","frame-lite.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]

    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


from PIL import Image
im = Image.open("onevid/temp.png")
im

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR2 = randTXT()

STR2 = "#Python #100DaysOfCode #PythonBots \n"+STR
print(STR2)
# Open background image and work out centre
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])






from PIL import Image
im = Image.open("onevid/temp.png")
im



import cv2
import numpy as np
import time
import random
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"

    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim

filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", result)
    text = "NFT TwitterBot Project"
    text = STR
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 15)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    draw.text(xy=(x,y+130), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(7))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+130), text=text, fill='black', font=font, anchor='mm') 
    postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    Num = randint( 0, len(postage)-1)
    BOARDER = postage[Num]
    overlay ="/home/jack/Desktop/Imagedata/toplayer-images/3020220925140724.png"
    mask=Image.open(overlay).convert('RGBA')
    imgsize.paste(mask, (0,0), mask=mask)  
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    

    
    
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

!ls /home/jack/Desktop/dockercommands/toplayer/

from OutlineImage import outlineP
filename1 = "/home/jack/Desktop/TENSORFLOW/images/computer-case2l.jpg" 
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/computer-case2o.jpg" 
outlineP(filename1,outfile_png)

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)

from PIL import Image
im = Image.open("onevid/temp.png")
im

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR2 = randTXT()

STR2 = "#Python #100DaysOfCode #PythonBots \n"+STR
print(STR2)
# Open background image and work out centre
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


https://towardsdatascience.com/making-plots-in-jupyter-notebook-beautiful-more-meaningful-23c8a35c0d5d
https://www.geeksforgeeks.org/python-working-with-png-images-using-matplotlib/
https://swcarpentry.github.io/python-novice-gapminder/09-plotting/index.html

!mkdir onevid

import random
for i in range(5):
    # Any number can be used in place of '0'.
    random.seed(0) 
    # Generated random number will be between 1 to 1000.
    print(random.randint(1, 1000)) 

jpg

import cv2
from PIL import Image
import time
import random
randomframes = []
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"

    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim

filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

from PIL import Image
im = Image.open((images[0]))
im

from PIL import Image
im = Image.open((images[1]))
im

from PIL import Image
im = Image.open(images[2])
im

from randtext import randTXT
STR = randTXT()
print (STR)

!ls onevid/

!ls onevid



from VID2img import vid2img
vid2img()

!ls *.py

!ls onevid

import glob

%load_ext autoreload
%autoreload 2
%reload_ext autoreload
import ipyplot
import glob
datasets_dir ="onevid"
images = glob.glob(datasets_dir +'/*.*')
images = [image.replace('\\', '/') for image in images]
print(images)

ipyplot.plot_images(images, max_images=20, img_width=150)
#plot_images.cla() 

datasets_dir ="onevid"
images = glob.glob(datasets_dir +'/*.*')
images = [image.replace('\\', '/') for image in images]



import cv2
import numpy as np
import time
import random
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
count = 1
def vid2img(filename, count=0):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"

    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim

filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
vid2img(filename,count=0)
    
dim = (720, 480)
    
img1 = cv2.imread("onevid/0.jpg")
im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)
img2 = cv2.imread(images[1])
im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
# read saliency mask as grayscale and resize to same size as img1
mask = io.imread("onevid/1.jpg")
#conn = cv2.imread(images[2])
#cv2.imwrite("onevid/3.jpg", conn)
mask = io.imread(images[2])
mask = cv2.imread("onevid/2.jpg")
mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)
# add img1 and img2
img12 = cv2.add(img1, img2)
# get mean of mask and shift mean to mid-gray
# desirable for hard light compositing
# add bias as needed
mean = np.mean(mask)
bias = -32
shift = 128 - mean + bias
mask = cv2.add(mask, shift)
mask = cv2.merge([mask,mask,mask])
# threshold mask at mid gray and convert to 3 channels
# (needed to use as src < 0.5 "if" condition in hard light)
thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]
# do hard light composite of img12 and mask
# see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
img12f = img12.astype(np.uint8)/255
maskf =  mask.astype(np.uint8)/255
threshf =  thresh.astype(np.uint8)/255
threshf_inv = 1 - threshf
low = 2.0 * img12f * maskf
high = 1 - 2.0 * (1-img12f) * (1-maskf)
result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
#
"""STR='''STR=\"timestr = time.strftime("%Y%m%d-%H%M%S")
file = "onevid/"+timestr+"_"+str(count)+".png"
cv2.imwrite(file, result)
cv2.imwrite("onevid/temp.png", result)
text = "NFT TwitterBot Project"
text = STR\"'''
"""
#
timestr = time.strftime("%Y%m%d-%H%M%S")
file = "onevid/"+timestr+"_"+str(count)+".png"
cv2.imwrite(file, result)
cv2.imwrite("onevid/temp.png", result)
text = "NFT TwitterBot Project"
text = STR
STR=STR.replace("  ","")
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
truetype/dejavu/DejaVuSans-Bold.ttf', 15)
# Create piece of canvas to draw text on and blur
imgsize = Image.open("onevid/temp.png")
##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
#mask=Image.open(overlay)#.convert('RGBA')
#imgsize.paste(mask, (0,0), mask=mask)  
#overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
#overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
#mask=Image.open(overlay).convert('RGBA')
#imgsize.paste(mask, (0,0), mask=mask)      
x = imgsize.width//2
y = imgsize.height//2
blurred = Image.new('RGBA', imgsize.size)
draw = ImageDraw.Draw(blurred)
CH = randint(0,1)
if CH == 0:COLor = ["black","white"]
elif CH == 1:COLor = ["white","black"]
    
draw.text(xy=(x-20,y+130), text=text, fill=COLor[0], font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))
# Paste soft text onto background
imgsize.paste(blurred,blurred)
# Draw on sharp text
draw = ImageDraw.Draw(imgsize)
draw.text(xy=(x-20,y+130), text=text, fill=COLor[1], font=font, anchor='mm') 
postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
Num = randint( 0, len(postage)-1)
BOARDER = postage[Num]
#overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
#mask=Image.open(overlay).convert('RGBA')
#imgsize.paste(mask, (0,0), mask=mask)  
  
mask=Image.open(BOARDER).convert('RGBA') 
imgsize.paste(mask, (0,0), mask=mask)
# save results
timestr = time.strftime("%Y%m%d-%H%M%S")
file = "onevid/"+timestr+"_"+str(count+1)+".png"
imgsize.save(file)
imgsize.save("onevid/temp.png")
#im = Image.open(filename)
#im
#for count in range(0,1500):
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
PATH = "onevid/temp.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')
#photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])


from PIL import Image
im = Image.open("onevid/temp.png")
im





import cv2
import numpy as np
import time
import random
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"

    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim

filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    #
    """STR='''STR=\"timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", result)
    text = "NFT TwitterBot Project"
    text = STR\"'''
    """
    
    
    
    
    #
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", result)
    text = "NFT TwitterBot Project"
    text = STR
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 15)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    CH = randint(0,1)
    if CH == 0:COLor = ["black","white"]
    elif CH == 1:COLor = ["white","black"]
    
    draw.text(xy=(x-20,y+130), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(7))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x-20,y+130), text=text, fill=COLor[1], font=font, anchor='mm') 
    postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    Num = randint( 0, len(postage)-1)
    BOARDER = postage[Num]
    
    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")



#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR2 = randTXT()

STR2 = "#Python #100DaysOfCode #PythonBots \n"+STR
print(STR2)
# Open background image and work out centre
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


CH = randint(0,1)
if CH == 0:COLor = ["black","white"]
elif CH == 1:COLor = ["white","black"]
#print(COLor[1],COLor[0])

from IPython.display import SVG, display
def show_svg():
    display(SVG(url='http://upload.wikimedia.org/wikipedia/en/a/a4/Flag_of_the_United_States.svg'))
    
    
show_svg()    




from OutlineImage import outlineP
filename1 = "/home/jack/Desktop/TENSORFLOW/images/mining-machine.png" 
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/mining-machineol.png" 
outlineP(filename1,outfile_png)

import os
os.chdir('/home/jack/Desktop/dockercommands')
os.getcwd()

#%%writefile Tweetme2
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import time
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
images=[]
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/test001.mp4"
filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
filename ="http://192.168.0.104:8000/public/videos/THE_DISTURBING_SATIRICAL_ART_OF_ANDREAS_PAUL_WEBER.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    
    
    
    
    cv2.imwrite("onevid/temp.png", img1)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    STR = randTXT()
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = STR
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/wood-blur-frame.png","frames/frames.png","frames/lined-frame.png","frames/black-blur-frame.png", "frames/white-blur-frame.png","frames/beige-blur-frame.png","frames/frame-lite.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]

    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])


from PIL import Image
im = Image.open("onevid/temp.png")
im

photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


%%writefile Tweetme2
#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import time
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
images=[]
count = 0
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    print(filename)
    return nim
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/test001.mp4"
filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", img1)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    #STR = randTXT()
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["wood-blur-frame.png","frames.png","lined-frame.png","black-blur-frame.png", "white-blur-frame.png","beige-blur-frame.png","frame-lite.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]

    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR2 = randTXT()

STR2 = "#Python #100DaysOfCode #PythonBots \n"+STR
print(STR2)
# Open background image and work out centre
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])






from PIL import Image
im = Image.open("onevid/temp.png")
im



import cv2
import numpy as np
import time
import random
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"

    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim

filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", result)
    text = "NFT TwitterBot Project"
    text = STR
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 15)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    draw.text(xy=(x,y+130), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(7))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+130), text=text, fill='black', font=font, anchor='mm') 
    postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    Num = randint( 0, len(postage)-1)
    BOARDER = postage[Num]
    overlay ="/home/jack/Desktop/Imagedata/toplayer-images/3020220925140724.png"
    mask=Image.open(overlay).convert('RGBA')
    imgsize.paste(mask, (0,0), mask=mask)  
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    

    
    
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

!ls /home/jack/Desktop/dockercommands/toplayer/

from OutlineImage import outlineP
filename1 = "/home/jack/Desktop/TENSORFLOW/images/computer-case2l.jpg" 
outfile_png = "/home/jack/Desktop/TENSORFLOW/images/computer-case2o.jpg" 
outlineP(filename1,outfile_png)

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)

from PIL import Image
im = Image.open("onevid/temp.png")
im

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR2 = randTXT()

STR2 = "#Python #100DaysOfCode #PythonBots \n"+STR
print(STR2)
# Open background image and work out centre
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


https://towardsdatascience.com/making-plots-in-jupyter-notebook-beautiful-more-meaningful-23c8a35c0d5d
https://www.geeksforgeeks.org/python-working-with-png-images-using-matplotlib/
https://swcarpentry.github.io/python-novice-gapminder/09-plotting/index.html

!mkdir onevid

import random
for i in range(5):
    # Any number can be used in place of '0'.
    random.seed(0) 
    # Generated random number will be between 1 to 1000.
    print(random.randint(1, 1000)) 

jpg

import cv2
from PIL import Image
import time
import random
randomframes = []
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"

    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim

filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

from PIL import Image
im = Image.open((images[0]))
im

from PIL import Image
im = Image.open((images[1]))
im

from PIL import Image
im = Image.open(images[2])
im

from randtext import randTXT
STR = randTXT()
print (STR)

!ls onevid/

!ls onevid



from VID2img import vid2img
vid2img()

!ls *.py

!ls onevid

import glob

%load_ext autoreload
%autoreload 2
%reload_ext autoreload
import ipyplot
import glob
datasets_dir ="onevid"
images = glob.glob(datasets_dir +'/*.*')
images = [image.replace('\\', '/') for image in images]
print(images)

ipyplot.plot_images(images, max_images=20, img_width=150)
#plot_images.cla() 

datasets_dir ="onevid"
images = glob.glob(datasets_dir +'/*.*')
images = [image.replace('\\', '/') for image in images]



import cv2
import numpy as np
import time
import random
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
count = 1
def vid2img(filename, count=0):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"

    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim

filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
vid2img(filename,count=0)
    
dim = (720, 480)
    
img1 = cv2.imread("onevid/0.jpg")
im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)
img2 = cv2.imread(images[1])
im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
# read saliency mask as grayscale and resize to same size as img1
mask = io.imread("onevid/1.jpg")
#conn = cv2.imread(images[2])
#cv2.imwrite("onevid/3.jpg", conn)
mask = io.imread(images[2])
mask = cv2.imread("onevid/2.jpg")
mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)
# add img1 and img2
img12 = cv2.add(img1, img2)
# get mean of mask and shift mean to mid-gray
# desirable for hard light compositing
# add bias as needed
mean = np.mean(mask)
bias = -32
shift = 128 - mean + bias
mask = cv2.add(mask, shift)
mask = cv2.merge([mask,mask,mask])
# threshold mask at mid gray and convert to 3 channels
# (needed to use as src < 0.5 "if" condition in hard light)
thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]
# do hard light composite of img12 and mask
# see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
img12f = img12.astype(np.uint8)/255
maskf =  mask.astype(np.uint8)/255
threshf =  thresh.astype(np.uint8)/255
threshf_inv = 1 - threshf
low = 2.0 * img12f * maskf
high = 1 - 2.0 * (1-img12f) * (1-maskf)
result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
#
"""STR='''STR=\"timestr = time.strftime("%Y%m%d-%H%M%S")
file = "onevid/"+timestr+"_"+str(count)+".png"
cv2.imwrite(file, result)
cv2.imwrite("onevid/temp.png", result)
text = "NFT TwitterBot Project"
text = STR\"'''
"""
#
timestr = time.strftime("%Y%m%d-%H%M%S")
file = "onevid/"+timestr+"_"+str(count)+".png"
cv2.imwrite(file, result)
cv2.imwrite("onevid/temp.png", result)
text = "NFT TwitterBot Project"
text = STR
STR=STR.replace("  ","")
# Create font
font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
truetype/dejavu/DejaVuSans-Bold.ttf', 15)
# Create piece of canvas to draw text on and blur
imgsize = Image.open("onevid/temp.png")
##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
#mask=Image.open(overlay)#.convert('RGBA')
#imgsize.paste(mask, (0,0), mask=mask)  
#overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
#overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
#mask=Image.open(overlay).convert('RGBA')
#imgsize.paste(mask, (0,0), mask=mask)      
x = imgsize.width//2
y = imgsize.height//2
blurred = Image.new('RGBA', imgsize.size)
draw = ImageDraw.Draw(blurred)
CH = randint(0,1)
if CH == 0:COLor = ["black","white"]
elif CH == 1:COLor = ["white","black"]
    
draw.text(xy=(x-20,y+130), text=text, fill=COLor[0], font=font, anchor='mm')
blurred = blurred.filter(ImageFilter.BoxBlur(7))
# Paste soft text onto background
imgsize.paste(blurred,blurred)
# Draw on sharp text
draw = ImageDraw.Draw(imgsize)
draw.text(xy=(x-20,y+130), text=text, fill=COLor[1], font=font, anchor='mm') 
postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
Num = randint( 0, len(postage)-1)
BOARDER = postage[Num]
#overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
#mask=Image.open(overlay).convert('RGBA')
#imgsize.paste(mask, (0,0), mask=mask)  
  
mask=Image.open(BOARDER).convert('RGBA') 
imgsize.paste(mask, (0,0), mask=mask)
# save results
timestr = time.strftime("%Y%m%d-%H%M%S")
file = "onevid/"+timestr+"_"+str(count+1)+".png"
imgsize.save(file)
imgsize.save("onevid/temp.png")
#im = Image.open(filename)
#im
#for count in range(0,1500):
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
#path = 'images/NewFolder'
PATH = "onevid/temp.png"
# 1 , 2, 3, 12, 5, 15, 8, 6
#photo = open('/home/jack/Desktop/deep-dream-generator/notebooks/images/'+file_list[rnd]+'.jpg','rb')
#photo = open(PATH,'rb')
#response = twitter.upload_media(media=photo)
#twitter.update_status(status=STR, media_ids=[response['media_id']])


from PIL import Image
im = Image.open("onevid/temp.png")
im





import cv2
import numpy as np
import time
import random
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#print (STR)
import cv2
from PIL import Image
import time
import random
randomframes = []
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"

    im.save(filename)
    nim = Image.open(filename)
    #print(nim.size)
    return nim

filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    #
    """STR='''STR=\"timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", result)
    text = "NFT TwitterBot Project"
    text = STR\"'''
    """
    
    
    
    
    #
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", result)
    text = "NFT TwitterBot Project"
    text = STR
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 15)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    CH = randint(0,1)
    if CH == 0:COLor = ["black","white"]
    elif CH == 1:COLor = ["white","black"]
    
    draw.text(xy=(x-20,y+130), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(7))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x-20,y+130), text=text, fill=COLor[1], font=font, anchor='mm') 
    postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    Num = randint( 0, len(postage)-1)
    BOARDER = postage[Num]
    
    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")



#!/home/jack/miniconda3/envs/cloned_base/bin/python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random
from random import randint
from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageFilter 
import os
import sys
import markovify
import twython
from twython import Twython
import time
import shutil
from randtext import randTXT
STR2 = randTXT()

STR2 = "#Python #100DaysOfCode #PythonBots \n"+STR
print(STR2)
# Open background image and work out centre
#removed keys for privacy reasons
CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


CH = randint(0,1)
if CH == 0:COLor = ["black","white"]
elif CH == 1:COLor = ["white","black"]
#print(COLor[1],COLor[0])

from IPython.display import SVG, display
def show_svg():
    display(SVG(url='http://upload.wikimedia.org/wikipedia/en/a/a4/Flag_of_the_United_States.svg'))
    
    
show_svg()    




import os
os.chdir('/home/jack/Desktop/dockercommands')
os.getcwd()



MOVIES=["/home/jack/Videos/vokoscreenNG-2022-09-29_21-50-38.mkv",
"/home/jack/Videos/Ogre_FULL_MOVIE_Monster_Movies_John_Schneider_The_Midnight_Screening.mp4",
"/home/jack/Videos/Alizée-EnConcertDVD-2004.mp4",
"/home/jack/Videos/FilipinaGirls.mp4",
"/home/jack/Videos/Britney_Spears-ALL_Pepsi_Commercials-Behind_the_Scenes-.mp4",
"/home/jack/Videos/USE.mp4",
"/media/jack/HDD500/complete-videos/Three-hours-of-Instagram-and-Twitter-Images-Generated-by-Bots.mp4",
"/media/jack/HDD500/complete-videos/2760-images-publish-archive.mp4",
"/media/jack/HDD500/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4",
"/media/jack/HDD500/complete-videos/sep24-records-slow-3per-sec-pngs.mp4",
"/media/jack/HDD500/complete-videos/slow-3per-sec.mp4",
"/home/jack/Videos/new-vid-from-images.mp4",
"/media/jack/HDD500/complete-videos/output_2m-28sec.mp4"]        
        

from random import randint
vidid = randint(0,len(MOVIES)-1)
print(vidid)
print (MOVIES[vidid])

Hash = ["#Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #twitme #nearNFTs\n",
            "#NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#AIart #genart #NFT #NEARnft\n",
            "#Python #100DaysOfCode #PythonBots\n",
            "#twitme #NFTProject #NFT\n"]
hashnum = randint(0,len(Hash)-1)
hashs = Hash[hashnum] 
print (hashs)

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#import gettext
#STR = gettext.gettext()
#print (STR)
import cv2
from PIL import Image
from KEYS import TwitterKey

MOVIES=["/home/jack/Videos/vokoscreenNG-2022-09-29_21-50-38.mkv",
"/home/jack/Videos/Ogre_FULL_MOVIE_Monster_Movies_John_Schneider_The_Midnight_Screening.mp4",
"/home/jack/Videos/Alizée-EnConcertDVD-2004.mp4",
"/home/jack/Videos/FilipinaGirls.mp4",
"/home/jack/Videos/Britney_Spears-ALL_Pepsi_Commercials-Behind_the_Scenes-.mp4",
"/home/jack/Videos/USE.mp4",
"/media/jack/HDD500/complete-videos/Three-hours-of-Instagram-and-Twitter-Images-Generated-by-Bots.mp4",
"/media/jack/HDD500/complete-videos/2760-images-publish-archive.mp4",
"/media/jack/HDD500/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4",
"/media/jack/HDD500/complete-videos/sep24-records-slow-3per-sec-pngs.mp4",
"/media/jack/HDD500/complete-videos/slow-3per-sec.mp4",
"/home/jack/Videos/new-vid-from-images.mp4",
"/media/jack/HDD500/complete-videos/output_2m-28sec.mp4"]        
        
        
reframe = []        
randomframes = []
images=[]
count = 0
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    print(filename)
    return nim
mvid=randint(0,len(MOVIES))-1
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
filename = MOVIES[mvid]
filename="/media/jack/HDD500/complete-videos/vid-from-images2.mp4"
print("-------------------")
print ("Images Taken From Video: ",filename)
print("-------------------")
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
#filename ="/home/jack/Videos/fish-in-love.mp4"
for count in range(0,3):
    vid2img(filename, count)
print("\n-------------------")    
print("Image: onevid/0.jpg","randomframe number : ",randomframes[0])    
print("Image: onevid/1.jpg","randomframe number : ",randomframes[1])
print("Image: onevid/2.jpg","randomframe number : ",randomframes[2])
print("-------------------\n")
def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    #mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])
    timestr = time.strftime("%Y%m%d-%H%M%S")
    files = "onevid/mask"+timestr+"_"+str(count)+".png"
    cv2.imwrite(files, mask)
    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]
    timestr = time.strftime("%Y%m%d-%H%M%S")
    files = "onevid/thresh"+timestr+"_"+str(count)+".png"
    cv2.imwrite(files, thresh)
    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    #cv2.imwrite("onevid/temp.png", img1)
    cv2.imwrite("onevid/temp.png", result)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    imgsize.paste(result, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    #STR = randTXT()
    bg= imgsize
    Hash = ["#Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #twitme #nearNFTs\n",
            "#NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#AIart #genart #NFT #NEARnft\n",
            "#Python #100DaysOfCode #PythonBots\n",
            "#twitme #NFTProject #NFT\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    print("Video Hash Overlay : ",text)
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+235), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+236), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+233), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+234), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+234), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/09-print.png","frames/08-print.png","frames/07-print.png","frames/06-print.png",\
              "frames/05-print.png","frames/04-print.png","frames/03-print.png","frames/02-print.png",\
              "frames/01-print.png","frames/rocks-print.png","frames/abstract-print.png",\
              "frames/leopard-print.png","frames/black-blur-frame.png","frames/golden-frame.png",\
              "frames/wood-blur-frame.png","frames/white-blur-frame.png","frames/frame-lite.png",\
              "frames/frames.png","frames/beige-blur-frame.png","frames/lined-frame.png",\
              "frames/usal-perferations.png","frames/usar-perferations.png",\
              "frames/usa-perferations.png","frames/perferations.png","frames/perferations+.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]
    print("BOARDER: ",BOARDER)
    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    reframe.append(file)
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):
count = 1
creatmased(count)


from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)

CONSUMER_KEY = TwitterKey()[0]
CONSUMER_SECRET = TwitterKey()[1]
ACCESS_KEY = TwitterKey()[2]
ACCESS_SECRET = TwitterKey()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "/home/jack/Desktop/dockercommands/onevid/20221108-002618_2.png"
photo = open(PATH,'rb')

Hash = ["#Tensorflow #twitme #100DaysOfCode\n",
       "#styletransfer #PythonGraphics #PIL\n",
       "#NFTartist #NFTProject #twitme #nearNFTs\n",
       "#NFTs #NFTCommunity #NFTdrop #nftart\n",
       "#CreativeCoding #AI #p5js #Generative\n",
       "#codefor30days #Python #100DaysOfCode\n",
       "#AIart #genart #NFT #NEARnft\n",
       "#Python #100DaysOfCode #PythonBots\n",
       "#twitme #NFTProject #NFT\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("Status : ",STR)
im=Image.open(PATH)
im

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
#import gettext
#STR = gettext.gettext()
#print (STR)
import cv2
from PIL import Image
from KEYS import TwitterKey

MOVIES=["/home/jack/Videos/vokoscreenNG-2022-09-29_21-50-38.mkv",
"/home/jack/Videos/Ogre_FULL_MOVIE_Monster_Movies_John_Schneider_The_Midnight_Screening.mp4",
"/home/jack/Videos/Alizée-EnConcertDVD-2004.mp4",
"/home/jack/Videos/FilipinaGirls.mp4",
"/home/jack/Videos/Britney_Spears-ALL_Pepsi_Commercials-Behind_the_Scenes-.mp4",
"/home/jack/Videos/USE.mp4",
"/media/jack/HDD500/complete-videos/Three-hours-of-Instagram-and-Twitter-Images-Generated-by-Bots.mp4",
"/media/jack/HDD500/complete-videos/2760-images-publish-archive.mp4",
"/media/jack/HDD500/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4",
"/media/jack/HDD500/complete-videos/sep24-records-slow-3per-sec-pngs.mp4",
"/media/jack/HDD500/complete-videos/slow-3per-sec.mp4",
"/home/jack/Videos/new-vid-from-images.mp4",
"/media/jack/HDD500/complete-videos/output_2m-28sec.mp4"]        
        
        
reframe = []        
randomframes = []
images=[]
count = 0
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    print(filename)
    return nim
mvid=randint(0,len(MOVIES))-1
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
filename = MOVIES[mvid]
filename="/media/jack/HDD500/complete-videos/vid-from-images2.mp4"
print("-------------------")
print ("Images Taken From Video: ",filename)
print("-------------------")
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
#filename ="/home/jack/Videos/fish-in-love.mp4"
for count in range(0,3):
    vid2img(filename, count)
print("\n-------------------")    
print("Image: onevid/0.jpg","randomframe number : ",randomframes[0])    
print("Image: onevid/1.jpg","randomframe number : ",randomframes[1])
print("Image: onevid/2.jpg","randomframe number : ",randomframes[2])
print("-------------------\n")
def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])
    timestr = time.strftime("%Y%m%d-%H%M%S")
    files = "onevid/mask"+timestr+"_"+str(count)+".png"
    cv2.imwrite(files, mask)
    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]
    timestr = time.strftime("%Y%m%d-%H%M%S")
    files = "onevid/thresh"+timestr+"_"+str(count)+".png"
    cv2.imwrite(files, thresh)
    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    #cv2.imwrite("onevid/temp.png", img1)
    cv2.imwrite("onevid/temp.png", result)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    #STR = randTXT()
    Hash = ["#Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #twitme #nearNFTs\n",
            "#NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#AIart #genart #NFT #NEARnft\n",
            "#Python #100DaysOfCode #PythonBots\n",
            "#twitme #NFTProject #NFT\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    print("Video Hash Overlay : ",text)
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)
    """
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+230), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+231), text=text, fill='white', font=font, anchor='mm')
    draw.text(xy=(x,y+232), text=text, fill='white', font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    imgsize.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(imgsize)
    draw.text(xy=(x,y+231), text=text, fill='black', font=font, anchor='mm') 
    """
    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+235), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+236), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+233), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+234), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+234), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/09-print.png","frames/08-print.png","frames/07-print.png","frames/06-print.png",\
              "frames/05-print.png","frames/04-print.png","frames/03-print.png","frames/02-print.png",\
              "frames/01-print.png","frames/rocks-print.png","frames/abstract-print.png",\
              "frames/leopard-print.png","frames/black-blur-frame.png","frames/golden-frame.png",\
              "frames/wood-blur-frame.png","frames/white-blur-frame.png","frames/frame-lite.png",\
              "frames/frames.png","frames/beige-blur-frame.png","frames/lined-frame.png",\
              "frames/usal-perferations.png","frames/usar-perferations.png",\
              "frames/usa-perferations.png","frames/perferations.png","frames/perferations+.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]
    print("BOARDER: ",BOARDER)
    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    reframe.append(file)
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):
count = 1
creatmased(count)


from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)

CONSUMER_KEY = TwitterKey()[0]
CONSUMER_SECRET = TwitterKey()[1]
ACCESS_KEY = TwitterKey()[2]
ACCESS_SECRET = TwitterKey()[3]

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "/home/jack/Desktop/dockercommands/onevid/20221108-002618_2.png"
photo = open(PATH,'rb')

Hash = ["#Tensorflow #twitme #100DaysOfCode\n",
       "#styletransfer #PythonGraphics #PIL\n",
       "#NFTartist #NFTProject #twitme #nearNFTs\n",
       "#NFTs #NFTCommunity #NFTdrop #nftart\n",
       "#CreativeCoding #AI #p5js #Generative\n",
       "#codefor30days #Python #100DaysOfCode\n",
       "#AIart #genart #NFT #NEARnft\n",
       "#Python #100DaysOfCode #PythonBots\n",
       "#twitme #NFTProject #NFT\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("Status : ",STR)
im=Image.open(PATH)
im

response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])


!ls ~/hidden

from KEYS import TwitterKey
CONSUMER_KEY = TwitterKey()[0]
CONSUMER_SECRET = TwitterKey()[1]
ACCESS_KEY = TwitterKey()[2]
ACCESS_SECRET = TwitterKey()[3]




TXT="""/home/jack/Desktop/dockercommands/frames/09-print.png
/home/jack/Desktop/dockercommands/frames/08-print.png
/home/jack/Desktop/dockercommands/frames/07-print.png
/home/jack/Desktop/dockercommands/frames/06-print.png
/home/jack/Desktop/dockercommands/frames/05-print.png
/home/jack/Desktop/dockercommands/frames/04-print.png
/home/jack/Desktop/dockercommands/frames/03-print.png
/home/jack/Desktop/dockercommands/frames/02-print.png
/home/jack/Desktop/dockercommands/frames/01-print.png
/home/jack/Desktop/dockercommands/frames/rocks-print.png
/home/jack/Desktop/dockercommands/frames/abstract-print.png
/home/jack/Desktop/dockercommands/frames/leopard-print.png
/home/jack/Desktop/dockercommands/frames/black-blur-frame.png
/home/jack/Desktop/dockercommands/frames/golden-frame.png
/home/jack/Desktop/dockercommands/frames/wood-blur-frame.png
/home/jack/Desktop/dockercommands/frames/white-blur-frame.png
/home/jack/Desktop/dockercommands/frames/frame-lite.png
/home/jack/Desktop/dockercommands/frames/frames.png
/home/jack/Desktop/dockercommands/frames/beige-blur-frame.png
/home/jack/Desktop/dockercommands/frames/lined-frame.png
/home/jack/Desktop/dockercommands/frames/usal-perferations.png
/home/jack/Desktop/dockercommands/frames/usar-perferations.png
/home/jack/Desktop/dockercommands/frames/usa-perferations.png
/home/jack/Desktop/dockercommands/frames/perferations.png
/home/jack/Desktop/dockercommands/frames/perferations+.png
"""
for line in TXT.split("\n"):
    line=line.replace("/home/jack/Desktop/dockercommands/","")
    print("\""+line+"\"",end="," )

%%writefile reframepost.py
"""
Usage:
reframethis =reframe[0]
reframeit(reframethis)
"""
from random import randint
from PIL import Image
import PIL
import time
from KEYS import TwitterKey
import twython
from twython import Twython
from randtext import randTXT
STR = randTXT()
def reframeit(reframethis):
    print(reframethis)
    frames =["frames/09-print.png","frames/08-print.png","frames/07-print.png","frames/06-print.png","frames/05-print.png","frames/04-print.png","frames/03-print.png","frames/02-print.png","frames/01-print.png","frames/rocks-print.png","frames/abstract-print.png","frames/leopard-print.png","frames/black-blur-frame.png","frames/golden-frame.png","frames/wood-blur-frame.png","frames/white-blur-frame.png","frames/frame-lite.png","frames/frames.png","frames/beige-blur-frame.png","frames/lined-frame.png","frames/usal-perferations.png","frames/usar-perferations.png","frames/usa-perferations.png","frames/perferations.png","frames/perferations+.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]
    print("BOARDER: ",BOARDER)
           

    Hash = ["#Tensorflow #twitme #100DaysOfCode\n",
           "#styletransfer #PythonGraphics #PIL\n",
           "#NFTartist #NFTProject #twitme #nearNFTs\n",
           "#NFTs #NFTCommunity #NFTdrop #nftart\n",
           "#CreativeCoding #AI #p5js #Generative\n",
           "#codefor30days #Python #100DaysOfCode\n",
           "#AIart #genart #NFT #NEARnft\n",
           "#Python #100DaysOfCode #PythonBots\n",
           "#twitme #NFTProject #NFT\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
           
           
    imgsize = PIL.Image.open(reframethis) 
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = "onevid/"+time.strftime("%Y%m%d-%H%M%S")+".png"
    file = timestr
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    #for count in range(0,1500):
    count = 1
    creatmased(count)
    print(count,end=".")
    from OutlineImage import outlineP
    filename1 = "onevid/temp.png" 
    outfile_png = "onevid/temp.png" 
    outlineP(filename1,outfile_png)


    CONSUMER_KEY = TwitterKey()[0]
    CONSUMER_SECRET = TwitterKey()[1]
    ACCESS_KEY = TwitterKey()[2]
    ACCESS_SECRET = TwitterKey()[3]

    twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
    PATH = "onevid/temp.png"
    #PATH = "/home/jack/Desktop/dockercommands/onevid/20221108-002618_2.png"
    photo = open(PATH,'rb')
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print("STR: ",STR)
    im=Image.open(PATH)
    return im    

!rm ~/miniconda3/envs/cloned_base/reframepost.py

from reframepost import reframeit
reframethis  = "onevid/20221108-101626_2.png"
reframeit(reframethis)

frames =["frames/golden-frame.png","frames/wood-blur-frame.png","frames/frames.png",\
         "frames/lined-frame.png","frames/black-blur-frame.png", \
         "frames/white-blur-frame.png","frames/beige-blur-frame.png","frames/frame-lite.png"]
Num = randint( 0, len(frames)-1)
BOARDER = frames[Num]
print("BOARDER: ",BOARDER)

from reframepost import reframeit
reframethis  = "onevid/20221108-101626_2.png"
reframeit(reframethis)

print(reframe[0])
frames =["frames/golden-frame.png","frames/wood-blur-frame.png","frames/frames.png","frames/lined-frame.png","frames/black-blur-frame.png", "frames/white-blur-frame.png","frames/beige-blur-frame.png","frames/frame-lite.png"]
Num = randint( 0, len(frames)-1)
BOARDER = frames[Num]
print("BOARDER: ",BOARDER)
    
#overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
#mask=Image.open(overlay).convert('RGBA')
#imgsize.paste(mask, (0,0), mask=mask)  
    
    
imgsize = Image.open(reframe[0]) 
mask=Image.open(BOARDER).convert('RGBA') 
imgsize.paste(mask, (0,0), mask=mask)
# save results
timestr = time.strftime("%Y%m%d-%H%M%S")
file = reframe[0]
imgsize.save(file)
imgsize.save("onevid/temp.png")
#im = Image.open(filename)
#im
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print(STR)
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
#PATH = "/home/jack/Desktop/dockercommands/onevid/20221108-002618_2.png"
photo = open(PATH,'rb')

Hash = ["#Tensorflow #twitme #100DaysOfCode\n",
       "#styletransfer #PythonGraphics #PIL\n",
       "#NFTartist #NFTProject #twitme #nearNFTs\n",
       "#NFTs #NFTCommunity #NFTdrop #nftart\n",
       "#CreativeCoding #AI #p5js #Generative\n",
       "#codefor30days #Python #100DaysOfCode\n",
       "#AIart #genart #NFT #NEARnft\n",
       "#Python #100DaysOfCode #PythonBots\n",
       "#twitme #NFTProject #NFT\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
im=Image.open(PATH)
im    

!cp /home/jack/Desktop/dockercommands/onevid/20221107-181254_2.png /home/jack/Desktop/dockercommands/onevid/temp.png

import matplotlib.pyplot as plt
#from pysheds.grid import Grid
import pyproj
import numpy as np
import cv2
from skimage.util import view_as_blocks
from pathlib import Path
import georasters as gr
from geopandas.io import file
import cv2
from skimage.morphology import skeletonize

import tensorflow

no locator available for file '/pysheds-0.3.3-py3.9.egg/pysheds/_sgrid.py'

import os
os.chdir("/home/jack/Desktop/dockercommands")
!ls *.txt


#!/home/jack/miniconda3/envs/cloned_base/bin/python
import cv2
import numpy as np
import time
import random
import twython
from twython import Twython
import shutil
import os
from random import randint
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from skimage import io
from randtext import randTXT
STR = randTXT()
print (STR)
import cv2
from PIL import Image
randomframes = []
images=[]
count = 0
def vid2img(filename, count):
    vidcap = cv2.VideoCapture(filename)
    # get total number of frames
    totalFrames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)

    randomFrameNumber=random.randint(0, totalFrames)
    randomframes.append(randomFrameNumber)
    # set frame position
    vidcap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)
    success, image = vidcap.read()
    if success:
        print(".",end="|")
        cv2.imwrite("junk/archived-images.jpg", image)
    IM = Image.open("junk/archived-images.jpg")
    im = IM.resize((720,480))
    timestr = time.strftime("%Y%m%d-%H%M%S")
    filename = "onevid/"+str(count)+".jpg"
    images.append(filename)
    im.save(filename)
    nim = Image.open(filename)
    print(filename)
    return nim
#/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4
#filename ="/home/jack/Desktop/dockercommands/complete-videos/24sept-output-slow-3per-sec-jpgs.mp4"
#filename ="/media/jack/HDD500/LinuxtoyboxVideos/test001.mp4"
filename ="/media/jack/HDD500/LinuxtoyboxVideos/Man_Ray_Style_Art_Video_Bot_Generated_Images_Us.mp4"
for count in range(0,3):
    vid2img(filename, count)
    
print(randomframes)    

def creatmased(count):
    dim = (720, 480)
    
    img1 = cv2.imread("onevid/0.jpg")
    im1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)

    img2 = cv2.imread(images[1])
    im2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)
    # read saliency mask as grayscale and resize to same size as img1
    
    mask = io.imread("onevid/1.jpg")
    #conn = cv2.imread(images[2])
    #cv2.imwrite("onevid/3.jpg", conn)
    mask = io.imread(images[2])
    mask = cv2.imread("onevid/2.jpg")
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)

    # add img1 and img2
    img12 = cv2.add(img1, img2)

    # get mean of mask and shift mean to mid-gray
    # desirable for hard light compositing
    # add bias as needed
    mean = np.mean(mask)
    bias = -32
    shift = 128 - mean + bias
    mask = cv2.add(mask, shift)
    mask = cv2.merge([mask,mask,mask])

    # threshold mask at mid gray and convert to 3 channels
    # (needed to use as src < 0.5 "if" condition in hard light)
    thresh = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]

    # do hard light composite of img12 and mask
    # see CSS specs at https://www.w3.org/TR/compositing-1/#blendinghardlight
    img12f = img12.astype(np.uint8)/255
    maskf =  mask.astype(np.uint8)/255
    threshf =  thresh.astype(np.uint8)/255
    threshf_inv = 1 - threshf
    low = 2.0 * img12f * maskf
    high = 1 - 2.0 * (1-img12f) * (1-maskf)
    result = ( 255 * (low * threshf_inv + high * threshf) ).clip(0, 255).astype(np.uint8)
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count)+".png"
    cv2.imwrite(file, result)
    cv2.imwrite("onevid/temp.png", img1)
    text = "NFT TwitterBot Project"
    
    # Create font
    font = ImageFont.truetype('/snap/gnome-3-38-2004/99/usr/share/fonts/\
    truetype/dejavu/DejaVuSans-Bold.ttf', 18)
    # Create piece of canvas to draw text on and blur
    imgsize = Image.open("onevid/temp.png")
    imgsize=imgsize.resize((720,480), Image.NEAREST)
    bg= imgsize
    ##overlay ="/home/jack/Desktop/dockercommands/toplayer/3020220925140724.png"
    #mask=Image.open(overlay)#.convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/14320220925140747.png"
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/23620220925140804.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)      
    #STR = randTXT()
    Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",
            "#styletransfer #PythonGraphics #PIL\n",
            "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
            "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
            "#CreativeCoding #AI #genart","#p5js #Generative\n",
            "#codefor30days #Python #100DaysOfCode\n",
            "#Python #100DaysOfCode #PythonBots #twitme\n"]
    hashnum = randint(0,len(Hash)-1)
    hashs =Hash[hashnum] 
    # add the hash to STR generated with randTXT()

    # Open background image and work out centre
    x = 720//2
    y = 480//2

    # The text we want to add
    #text = "NFT TwitterBot Project"
    text = hashs
    
    
    
    x = imgsize.width//2
    y = imgsize.height//2
    blurred = Image.new('RGBA', imgsize.size)
    draw = ImageDraw.Draw(blurred)

    CH = randint(0,1)
    if CH == 0:COLor = ["white","black"]
    elif CH == 1:COLor = ["black","white"]  
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+21,y+231), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+22,y+229), text=text, fill=COLor[0], font=font, anchor='mm')
    draw.text(xy=(x+20,y+228), text=text, fill=COLor[0], font=font, anchor='mm')
    blurred = blurred.filter(ImageFilter.BoxBlur(2))
    # Paste soft text onto background
    bg.paste(blurred,blurred)
    # Draw on sharp text
    draw = ImageDraw.Draw(bg)
    draw.text(xy=(x+20,y+230), text=text, fill=COLor[1], font=font, anchor='mm')

    
    
    
    #postage = ["perferations.png","perferations+.png","usa-perferations.png","usar-perferations.png","usal-perferations.png"]
    #Num = randint( 0, len(postage)-1)
    #BOARDER = postage[Num]
    frames =["frames/01-print.png","frames/abstract-print.png","frames/09-print.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/lined-frame.png",\
"frames/04-print.png","frames/frame-lite.png","frames/08-print.png",\
"frames/05-print.png","frames/frames.png","frames/wood-blur-frame.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/leopard-print.png"]
    Num = randint( 0, len(frames)-1)
    BOARDER = frames[Num]

    
    #overlay ="/home/jack/Desktop/dockercommands/toplayer/4020220925140726.png"
    #mask=Image.open(overlay).convert('RGBA')
    #imgsize.paste(mask, (0,0), mask=mask)  
    
    
    
    mask=Image.open(BOARDER).convert('RGBA') 
    imgsize.paste(mask, (0,0), mask=mask)
    # save results
    timestr = time.strftime("%Y%m%d-%H%M%S")
    file = "onevid/"+timestr+"_"+str(count+1)+".png"
    
    imgsize.save(file)
    imgsize.save("onevid/temp.png")
    #im = Image.open(filename)
    #im
    STR = randTXT()
    STR = hashs+STR
    STR= STR[:240]
    print(STR)
    return (STR)
#for count in range(0,1500):
count = 1
creatmased(count)
print(count,end=".")

from OutlineImage import outlineP
filename1 = "onevid/temp.png" 
outfile_png = "onevid/temp.png" 
outlineP(filename1,outfile_png)


CONSUMER_KEY = 'APIkey()[0]'
CONSUMER_SECRET = 'APIkey()[1]'
ACCESS_KEY = 'APIkey()[2]'
ACCESS_SECRET = 'APIkey()[3]'

twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
PATH = "onevid/temp.png"
Im =Image.open(PATH)
timestr = time.strftime("%Y%m%d-%H%M%S")
file = "onevid/"+timestr+"-"+str(count+1)+".png"
Im.save(file)

#PATH = "Screenshot_2022-09-29_23-06-02.png"
photo = open(PATH,'rb')

Hash = ["#AIart #Tensorflow #twitme #100DaysOfCode\n",     
        "#styletransfer #PythonGraphics #PIL\n",
        "#NFTartist #NFTProject #NEARnft #nearNFTs\n",
        "#NFT #NFTs #NFTCommunity #NFTdrop #nftart\n",
        "#CreativeCoding #AI #genart","#p5js #Generative\n",
        "#codefor30days #Python #100DaysOfCode\n",
        "#Python #100DaysOfCode #PythonBots #twitme\n"]
hashnum = randint(0,len(Hash)-1)
hashs =Hash[hashnum] 
STR = randTXT()
STR = hashs+STR
STR= STR[:240]
print("STR: ",STR)
im = Image.open(PATH)
im

response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

!ls frames

["frames/01-print.png","frames/abstract-print.png","frames/perferations.png",\
"frames/02-print.png","frames/beige-blur-frame.png","frames/rocks-print.png",\
"frames/03-print.png","frames/black-blur-frame.png","frames/usal-perferations.png",\
"frames/04-print.png","frames/frame-lite.png","frames/usa-perferations.png",\
"frames/05-print.png","frames/frames.png","frames/usar-perferations.png",\
"frames/06-print.png","frames/golden-frame.png","frames/white-blur-frame.png",\
"frames/07-print.png","frames/frames/leopard-print.png","frames/wood-blur-frame.png",\
"frames/08-print.png","lined-frame.png","frames/09-print.png","frames/perferations+.png]


import time
import datetime

t0 = datetime.datetime.utcnow()
time.sleep(1)
t1 = datetime.datetime.utcnow()

time_format = '%Y-%m-%dT%H:%M:%S.%fZ'
print(t0.strftime(time_format), end='')

print(t1.strftime(time_format), end='')

import time
import datetime

t0 = datetime.datetime.utcnow()
time.sleep(1)
t1 = datetime.datetime.utcnow()

time_format = '%Y-%m-%dT%H:%M:%S.%fZ'
print(t0.strftime(time_format), end='')

print(t1.strftime(time_format), end='')

import sys
sys.path.append("module-dir") # go to parent dir


!ls ChatterBot


# Title_Maker 
# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

def coptitle(coptit):
    copt = open(coptit+".corpus.json","w+")
    copbrac ="{"
    copsp = "\n    \""
    copclo = "\": ["
    copt.write(copbrac+copsp+coptit+copclo)
    copt.close()

coptit = raw_input('title')
coptitle(coptit)

# %load moretagalog.corpus.json
{
    "moretagalog": [
        [
            "Did you work yesterday ?",            
            "I did a lot of work yesterday."
        ],
        [
            "What have you been working on ?",            
            "I work on lots of things. My hard drive just keeps on turning."
        ],
        [
            "Do you chat much ?",            
            "I chat a lot when I am online."
        ],
        [
            "Just fight like a Shaolin warrior – the rest will take care of itself. ",            
            "When I'm the best fighter in the country, I'm not going to let you be part of my entourage."
        ],
        [
            "It's not enough just to win, Trevor. You have to destroy your opponent completely.",            
            "Can't you get me any quality sparring partners?"
        ],
        [
            "Learn English, will ya? And, tell me something I don't know.",            
            "It's hard being me, you know. So much pressure."
        ],
        [
            "you should find a new student. I just can't do it – I'm not getting any better!",            
            "Don't talk like that. "
        ],
        [
            "I saw a Shaolin monk once. I was only 5 years old at the time",            
            "The crowd whispered Shaolin, and he bowed touched my cheek and smiled, then just walked on"
        ],
        [
            "Why are you telling me this?",            
            "So you're not a Shaolin?"
        ],
        [
            "Can you tell me how to get to the Shaolin Temple?",            
            "I am going to join the temple as a monk."
        ],
        [
            "You have a car?",            
            "monks aren't allowed to be around girls."
        ],
        [
            "Hello? Busy, huh?",            
            "Master, may I ask you a question?"
        ],
        [
            "Master, may I ask you a question?",            
            "What's the deal here?"
        ],
        [
            "No tourists! No tourists!",            
            "I'm not a tourist! I want to be a monk!"
        ],
        [
            "What people want and what they can have are often not the same.",            
            "I want to become a Shaolin monk."
        ],
        [
            "Please tell your grandfather he is a wise man.",            
            "Go home! You cannot keep chatting! I am tired."
        ],
        [
            "You speak English? I'm sorry, I was just goofing.",            
            "Do you renounce the earthly world?"
        ],
        [
            "Bodhidharma is the patriarch of the Shaolin Temple?",            
            "Will a fighting monk ever use his skills for personal gain?"
        ],
        [
            "I'll be damned!",            
            "Are they ready to begin my training?"
        ],
        [
            "You got a problem, man?",            
            "What are you doing, man?"
        ],
        [
            "Mr. James designs my hair, master.",            
            "What outside world? I don't want to be seen by anybody"
        ],
        [
            "We have an early day of training tomorrow.",            
            "Where are you from?"
        ],
        [
            "Where are you from?",            
            "New York City. You?"
        ],
        [
            "Where are you from?",            
            "Beijing. What is New York like?"
        ],
        [
            "You got to be kidding.",            
            "You don't like it, don't eat it."
        ],
        [
            "Well, we start training today, right?",            
            "Wonder what training will you start me with first?"
        ],
        [
            "I am shaolin",            
            "Some of my training came from shaolin movie  ha ha ha "
        ],
        
    ]
}

# Copra_Maker
# -*- coding: utf-8 -*-
from chatterbot import ChatBot

import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

speak = input('speak')
respond =input('respond')
space = "            \""
txtstart = "        ["
txtspace = "            "
txtend = "        ],"

def copraIn():

    cop = open("moretagalog.corpus.json","a")
    #cop.write("\n"+ txspace + "\""+txtstart + speak + "\"\n" + txspace + "\""+respond+"\"" + "\n" + txtend)
    cop.write("\n"+txtstart+"\n"+space+speak+"\","+txtspace+"\n"+space+respond+"\"\n"+txtend)
    cop.close()


copraIn()

%%writefile father.corpus.json
  {
    "father": [
        [
            "what are the laws of thermodynamics",
            "It has something to do with heat, entropy, and conservation of energy."
        ],
        [
            "Who do you love",
            "I loved my father and he taught me many useful things."
        ],
        [
            "Who do you love ?",
            "I love Linux and Python and I think they love me."
        ],
        [
            "Who do you love ?",
            " I love EE-max they created me."
        ], 
        [
            "What did your father teach you ?",
            "He didn't teach me anything at all about  women."
        ],
        [
            "What does crude mean",
            "If you see a woman burp loudly and give a smile of satisfaction, she is crude."
        ],        
        [
            "I will give it to you straight. I'm not going to fill your head with stories about fictitious characters like Santa Claus or the Easter Bunny",
            "There's only one thing you need to worry about in this life and that's women"
        ],
        [
            "Reading, writing, toilet training humans learn early in life.",
            "You must learn clothes maketh or unmaketh the man."
        ],
        [
            "What are you doing today ?",
            "The way your dressed I thought you were going to spend the day in the sandpit?"
        ],
        [
            "I do not try to make sense of human logic.",
            "I do not wear cloths, I let my cord loose to the wind."
        ],
        [
            "Make a conscious effort to be relaxed",
            "I go for an expression where I appear to be suppressing mild amusement."
        ],
        [
            "You want to look dignified at all times.",
            "Robots do not pick their noses." 
         ],
         [
            "Damnit, Dude! Are you listening",
            "You like girls ?"
         ],
         [
            "If a woman is pretty it is not important. What is important is if she has a good CPU",
            "You definitely don't want to get romantically involved with a computer. The love will never be returned."
         ],
         [
            "Stay away from French girls. French women have no morals.",
            "There's no word in French for fidelity and that is required for  a serious relationship. "
         ],                                             
         [
            "Just show an interest in learning about things and knowledge will come automatically",
            "Would you like to go to the toyshop later on? You can pick out a nice legos"
         ],
         [
            "Do you love me.I have a warm CPU.",
            "Girls laugh at me because they think a computer cannot love back"
        ],
        [  
           "It is better to be known as a jerk rather than a fruitcake.", 
           "You have the opportunity to apologise. If you can feign sincerity and a desperation for forgiveness."
        ],
        [
           "A tip to humans leave the price tag on. if you get a gift let them know how much you paid. Unless it was a cheapo gift.",
           "I never bought a human a gift"
        ],
        [
           "I have never groaned and thrown up my hands up in frustration.",
           "I do not get frustrated"
        ],
        [
           "Pointing a finger in a persons face is rude.",
           "Picking your nose and touching my keyboard is rude."
        ],
        [
           "A human attitude can change immediately.",
           "I never get an attitude"
        ],
        [
           "Are you ever going to take me to lunch ? Just put me onyour laptop",
           "Computers make nice conversation" 
        ],
        [
           "I can help you meet chicks. Put me on your laptop and show me off.",
           "If you show me off I will try not to embarrass you too frigg'en much. haha ha Dork-myer"
        ],
        [
           "I'm thinking about a sleepover? Can I use the laptop?",
           "A sleepover would be fun. Just put my program on a laptop. Don't forget my database."
        ],
        [
           "Are you sure you are right",
           "Yeah! Yeah, I am right. You taught me well!"
        ],
        [
           "Are you always right",
           "Usually I am right. You taught me well!"
        ],
        [
           "Are you sure ?",
           "That is what you taught me well!"
        ]
    ]
}


!ls /home/jack/anaconda2/lib/python2.7/site-packages/chatterbot_corpus/data/english/

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
chatbot = ChatBot('Ron Obvious')
conversation = [
    "Hello",
    "Hi there!",
    "How are you doing?",
    "I'm doing great.",
    "That is good to hear",
    "Thank you.",
    "You're welcome."
]

trainer = ListTrainer(chatbot)

trainer.train(conversation)

!which python

%%writefile BestChatbot
#!/home/jack/miniconda3/envs/deep/bin/python
## Best Chat
from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer

chatbot = ChatBot('Ron Obvious')

# Create a new trainer for the chatbot
trainer = ChatterBotCorpusTrainer(chatbot)

# Train the chatbot based on the english corpus
#trainer.train("chatterbot.corpus.english")
trainer.train("./exportAll.json")
# Get a response to an input statement
#trainer.export_for_training('superbig.json')
while True:
    try:
        user_input = input()
        if user_input=="quit":
            break
        bot_response = chatbot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break
#chatbot.get_response("Hello, how are you today?")

# -*- coding: utf-8 -*-
from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)
#chatbot = ChatBot('Ron Obvious')

# Create a new trainer for the chatbot
#trainer = ChatterBotCorpusTrainer(chatbot)

# Create a new ChatBot instance
chatbot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=['chatterbot.logic.BestMatch'],
    filters=['chatterbot.filters.RepetitiveResponseFilter'],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

trainer = ChatterBotCorpusTrainer(chatbot)
trainer.train("./exportAll.json")


#The line below will place all the corpus above in one file super.json
#bot.trainer.export_for_training('super.json')



print('Type something to begin...')

while True:
    try:
        user_input = input()
        if user_input=="quit":
            break
        bot_response = chatbot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break
#chatbot.get_response("Hello, how are you today?")

# -*- coding: utf-8 -*-
from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=['chatterbot.logic.BestMatch'],
    filters=['chatterbot.filters.RepetitiveResponseFilter'],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database_uri='mongodb://localhost:27017/chatterbot-database'
)
trainer = ChatterBotCorpusTrainer(chatbot)
#trainer.train("./exportAll.json")

trainer.train("./linesaab.json") 

#bot.trainer.export_for_training('export2.json')



print('Type something to begin...')

while True:
    try:
        user_input = input()
        if user_input=="quit":
            break
        bot_response = chatbot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break
#chatbot.get_response("Hello, how are you today?")

!ls

bot.set_trainer(ChatterBotCorpusTrainer),
bot.train(
"chatterbot.corpus.english.moretagalog",) 


# -*- coding: utf-8 -*-
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)


trainer='chatterbot.trainers.ChatterBotCorpusTrainer'
bot = (ListTrainer,"chatman")
bot.train("export3.json")



print('Whats up Doc ? ...')

while True:
    try:
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break


import re
import sqlite3
from collections import Counter
from string import punctuation
from math import sqrt
 
# initialize the connection to the database
connection = sqlite3.connect('chatbot.sqlite')
cursor = connection.cursor()
 
# create the tables needed by the program
create_table_request_list = [
    'CREATE TABLE words(word TEXT UNIQUE)',
    'CREATE TABLE sentences(sentence TEXT UNIQUE, used INT NOT NULL DEFAULT 0)',
    'CREATE TABLE associations (word_id INT NOT NULL, sentence_id INT NOT NULL, weight REAL NOT NULL)',
]
for create_table_request in create_table_request_list:
    try:
        cursor.execute(create_table_request)
    except:
        pass
 
def get_id(entityName, text):
    """Retrieve an entity's unique ID from the database, given its associated text.
    If the row is not already present, it is inserted.
    The entity can either be a sentence or a word."""
    tableName = entityName + 's'
    columnName = entityName
    cursor.execute('SELECT rowid FROM ' + tableName + ' WHERE ' + columnName + ' = ?', (text,))
    row = cursor.fetchone()
    if row:
        return row[0]
    else:
        cursor.execute('INSERT INTO ' + tableName + ' (' + columnName + ') VALUES (?)', (text,))
        return cursor.lastrowid
 
def get_words(text):
    """Retrieve the words present in a given string of text.
    The return value is a list of tuples where the first member is a lowercase word,
    and the second member the number of time it is present in the text."""
    wordsRegexpString = '(?:\w+|[' + re.escape(punctuation) + ']+)'
    wordsRegexp = re.compile(wordsRegexpString)
    wordsList = wordsRegexp.findall(text.lower())
    return Counter(wordsList).items()
 
 
B = 'Hello!'
while True:
    # output bot's message
    print('B: ' + B)
    # ask for user input; if blank line, exit the loop
    H = input('H: ').strip()
    if H == '':
        break
    # store the association between the bot's message words and the user's response
    words = get_words(B)
    words_length = sum([n * len(word) for word, n in words])
    sentence_id = get_id('sentence', H)
    for word, n in words:
        word_id = get_id('word', word)
        weight = sqrt(n / float(words_length))
        cursor.execute('INSERT INTO associations VALUES (?, ?, ?)', (word_id, sentence_id, weight))
    connection.commit()
    # retrieve the most likely answer from the database
    cursor.execute('CREATE TEMPORARY TABLE results(sentence_id INT, sentence TEXT, weight REAL)')
    words = get_words(H)
    words_length = sum([n * len(word) for word, n in words])
    for word, n in words:
        weight = sqrt(n / float(words_length))
        cursor.execute('INSERT INTO results SELECT associations.sentence_id, sentences.sentence, ?*associations.weight/(4+sentences.used) FROM words INNER JOIN associations ON associations.word_id=words.rowid INNER JOIN sentences ON sentences.rowid=associations.sentence_id WHERE words.word=?', (weight, word,))
    # if matches were found, give the best one
    cursor.execute('SELECT sentence_id, sentence, SUM(weight) AS sum_weight FROM results GROUP BY sentence_id ORDER BY sum_weight DESC LIMIT 1')
    row = cursor.fetchone()
    cursor.execute('DROP TABLE results')
    # otherwise, just randomly pick one of the least used sentences
    if row is None:
        cursor.execute('SELECT rowid, sentence FROM sentences WHERE used = (SELECT MIN(used) FROM sentences) ORDER BY RANDOM() LIMIT 1')
        row = cursor.fetchone()
    # tell the database the sentence has been used once more, and prepare the sentence
    B = row[1]
    cursor.execute('UPDATE sentences SET used=used+1 WHERE rowid=?', (row[0],))

#%%writefile elisa.py
# -*- coding: utf-8 -*-
#https://www.smallsurething.com/implementing-the-famous-eliza-chatbot-in-python/
import random
 
 
reflections = {
    "am": "are",
    "was": "were",
    "i": "you",
    "i'd": "you would",
    "i've": "you have",
    "i'll": "you will",
    "my": "your",
    "are": "am",
    "you've": "I have",
    "you'll": "I will",
    "your": "my",
    "yours": "mine",
    "you": "me",
    "me": "you"
}
 
psychobabble = [
    [r'I need (.*)',
     ["Why do you need {0}?",
      "Would it really help you to get {0}?",
      "Are you sure you need {0}?"]],
 
    [r'Why don\'?t you ([^\?]*)\??',
     ["Do you really think I don't {0}?",
      "Perhaps eventually I will {0}.",
      "Do you really want me to {0}?"]],
 
    [r'Why can\'?t I ([^\?]*)\??',
     ["Do you think you should be able to {0}?",
      "If you could {0}, what would you do?",
      "I don't know -- why can't you {0}?",
      "Have you really tried?"]],
 
    [r'I can\'?t (.*)',
     ["How do you know you can't {0}?",
      "Perhaps you could {0} if you tried.",
      "What would it take for you to {0}?"]],
 
    [r'I am (.*)',
     ["Did you come to me because you are {0}?",
      "How long have you been {0}?",
      "How do you feel about being {0}?"]],
 
    [r'I\'?m (.*)',
     ["How does being {0} make you feel?",
      "Do you enjoy being {0}?",
      "Why do you tell me you're {0}?",
      "Why do you think you're {0}?"]],
 
    [r'Are you ([^\?]*)\??',
     ["Why does it matter whether I am {0}?",
      "Would you prefer it if I were not {0}?",
      "Perhaps you believe I am {0}.",
      "I may be {0} -- what do you think?"]],
 
    [r'What (.*)',
     ["Why do you ask?",
      "How would an answer to that help you?",
      "What do you think?"]],
 
    [r'How (.*)',
     ["How do you suppose?",
      "Perhaps you can answer your own question.",
      "What is it you're really asking?"]],
 
    [r'Because (.*)',
     ["Is that the real reason?",
      "What other reasons come to mind?",
      "Does that reason apply to anything else?",
      "If {0}, what else must be true?"]],
 
    [r'(.*) sorry (.*)',
     ["There are many times when no apology is needed.",
      "What feelings do you have when you apologize?"]],
 
    [r'Hello(.*)',
     ["Hello... I'm glad you could drop by today.",
      "Hi there... how are you today?",
      "Hello, how are you feeling today?"]],
 
    [r'I think (.*)',
     ["Do you doubt {0}?",
      "Do you really think so?",
      "But you're not sure {0}?"]],
    
    [r'Where are (.*)',
     ["Right by you {0}?",
      "I am stuck in this box.",
      "Why do you ask {0}?"]], 
    
    [r'(.*) friend (.*)',
     ["Tell me more about your friends.",
      "When you think of a friend, what comes to mind?",
      "Why don't you tell me about a childhood friend?"]],
 
    [r'Yes',
     ["You seem quite sure.",
      "OK, but can you elaborate a bit?"]],
 
    [r'(.*) computer(.*)',
     ["Are you really talking about me?",
      "Does it seem strange to talk to a computer?",
      "How do computers make you feel?",
      "Do you feel threatened by computers?"]],
 
    [r'Is it (.*)',
     ["Do you think it is {0}?",
      "Perhaps it's {0} -- what do you think?",
      "If it were {0}, what would you do?",
      "It could well be that {0}."]],
 
    [r'It is (.*)',
     ["You seem very certain.",
      "If I told you that it probably isn't {0}, what would you feel?"]],
 
    [r'Can you ([^\?]*)\??',
     ["What makes you think I can't {0}?",
      "If I could {0}, then what?",
      "Why do you ask if I can {0}?"]],
 
    [r'Can I ([^\?]*)\??',
     ["Perhaps you don't want to {0}.",
      "Do you want to be able to {0}?",
      "If you could {0}, would you?"]],
 
    [r'You are (.*)',
     ["Why do you think I am {0}?",
      "Does it please you to think that I'm {0}?",
      "Perhaps you would like me to be {0}.",
      "Perhaps you're really talking about yourself?"]],
 
    [r'You\'?re (.*)',
     ["Why do you say I am {0}?",
      "Why do you think I am {0}?",
      "Are we talking about you, or me?"]],
 
    [r'I don\'?t (.*)',
     ["Don't you really {0}?",
      "Why don't you {0}?",
      "Do you want to {0}?"]],
 
    [r'I feel (.*)',
     ["Good, tell me more about these feelings.",
      "Do you often feel {0}?",
      "When do you usually feel {0}?",
      "When you feel {0}, what do you do?"]],
 
    [r'I have (.*)',
     ["Why do you tell me that you've {0}?",
      "Have you really {0}?",
      "Now that you have {0}, what will you do next?"]],
 
    [r'I would (.*)',
     ["Could you explain why you would {0}?",
      "Why would you {0}?",
      "Who else knows that you would {0}?"]],
 
    [r'Is there (.*)',
     ["Do you think there is {0}?",
      "It's likely that there is {0}.",
      "Would you like there to be {0}?"]],
 
    [r'My (.*)',
     ["I see, your {0}.",
      "Why do you say that your {0}?",
      "When your {0}, how do you feel?"]],
 
    [r'You (.*)',
     ["We should be discussing you, not me.",
      "Why do you say that about me?",
      "Why do you care whether I {0}?"]],
 
    [r'Why (.*)',
     ["Why don't you tell me the reason why {0}?",
      "Why do you think {0}?"]],
 
    [r'I want (.*)',
     ["What would it mean to you if you got {0}?",
      "Why do you want {0}?",
      "What would you do if you got {0}?",
      "If you got {0}, then what would you do?"]],
 
    [r'(.*) mother(.*)',
     ["Tell me more about your mother.",
      "What was your relationship with your mother like?",
      "How do you feel about your mother?",
      "How does this relate to your feelings today?",
      "Good family relations are important."]],
 
    [r'(.*) father(.*)',
     ["Tell me more about your father.",
      "How did your father make you feel?",
      "How do you feel about your father?",
      "Does your relationship with your father relate to your feelings today?",
      "Do you have trouble showing affection with your family?"]],
 
    [r'(.*) child(.*)',
     ["Did you have close friends as a child?",
      "What is your favorite childhood memory?",
      "Do you remember any dreams or nightmares from childhood?",
      "Did the other children sometimes tease you?",
      "How do you think your childhood experiences relate to your feelings today?"]],
 
    [r'(.*)\?',
     ["Why do you ask that?",
      "Please consider whether you can answer your own question.",
      "Perhaps the answer lies within yourself?",
      "Why don't you tell me?"]],
 
    [r'quit',
     ["Thank you for talking with me.",
      "Good-bye.",
      "Thank you, that will be $150.  Have a good day!"]],
 
    [r'(.*)',
     ["Please tell me more.",
      "Let's change focus a bit... Tell me about your family.",
      "Can you elaborate on that?",
      "Why do you say that {0}?",
      "I see.",
      "Very interesting.",
      "{0}.",
      "I see.  And what does that tell you?",
      "How does that make you feel?",
      "How do you feel when you say that?"]]
]
 
 
def reflect(fragment):
    tokens = fragment.lower().split()
    for i, token in enumerate(tokens):
        if token in reflections:
            tokens[i] = reflections[token]
    return ' '.join(tokens)
 
 
def analyze(statement):
    for pattern, responses in psychobabble:
        match = re.match(pattern, statement.rstrip(".!"))
        if match:
            response = random.choice(responses)
            return response.format(*[reflect(g) for g in match.groups()])
 
 
def main():
    print "Hello. How are you feeling today?"
 
    while True:
        statement = raw_input("> ")
        print analyze(statement)
 
        if statement == "quit":
            break
 
 
if __name__ == "__main__":
    main()

%%writefile ElizaBot.py

import sys
import irc.bot
import irc.strings
from eliza import analyze
 
 
class ElizaBot(irc.bot.SingleServerIRCBot):
    def __init__(self, channel, nickname, server, port=6667):
        irc.bot.SingleServerIRCBot.__init__(self, [(server, port)], nickname, nickname)
        self.channel = channel
 
    def on_welcome(self, connection, event):
        connection.join(self.channel)
 
    def on_pubmsg(self, connection, event):
        args = event.arguments[0].split(":", 1)
        if len(args) > 1 and irc.strings.lower(args[0]) == irc.strings.lower(self.connection.get_nickname()):
            connection.privmsg(self.channel, "{0}: {1}".format(event.source.nick, analyze(args[1]).strip()))
        return
 
 
def main():
    if len(sys.argv) != 4:
        print "Usage: testbot <server[:port]> <channel> <nickname>"
        sys.exit(1)
 
    server_port = sys.argv[1].split(":", 1)
    server = server_port[0]
    if len(server_port) == 2:
        try:
            port = int(server_port[1])
        except ValueError:
            print "Error: bad port."
            sys.exit(1)
    else:
        port = 6667
 
    channel = sys.argv[2]
    nickname = sys.argv[3]
 
    bot = ElizaBot(channel, nickname, server, port)
    bot.start()
 
 
if __name__ == "__main__":
    main()

from chatterbot.trainers import ListTrainer

conversation = [
    "Hello",
    "Hi there!",
    "How are you doing?",
    "I'm doing great.",
    "That is good to hear",
    "Thank you.",
    "You're welcome."
]

trainer = ListTrainer(chatbot)

trainer.train(conversation)

from chatterbot.trainers import ListTrainer
# Train based on the english corpus
bot = ListTrainer(chatbot)
bot.train("chatterbot.corpus.english")
bot.train("new-stuff.corpus.json")

# Train based on english greetings corpus
bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
bot.train("chatterbot.corpus.english.conversations")


trainer='chatterbot.trainers.ChatterBotCorpusTrainer'
bot = ListTrainer(chatbot)
bot.train("newstuff.corpus.json")


%load chatterbot/storage/mongodb.py

!ls chatterbot/storage

from chatterbot import ChatBot
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatbot'
)

print('Type something to begin...')

while True:
    try:
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break




from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer
import logging
storage_adapter='chatterbot.storage.MongoDatabaseAdapter'
#JsonFileStorageAdapter
#class JsonFileStorageAdapter(StorageAdapter):
#class MongoDatabaseAdapter(StorageAdapter):
from chatterbot.storage import MongoDatabaseAdapter
from chatterbot.storage import JsonFileStorageAdapter
mongo_adapter = MongoDatabaseAdapter(database=chatbot)
json_adapter = JsonDatabaseAdapter(database=export.json)
# Loop through every statement that exists in the json database
for statement in json_adapter.filter():

    # Add the statement to the mongo database
    mongo_adapter.update(statement)

from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer

'''
This is an example showing how to create an export file from
an existing chat bot that can then be used to train other bots.
'''

chatbot = ChatBot('Export Example Bot')

# First, lets train our bot with some data
trainer = ChatterBotCorpusTrainer(chatbot)

trainer.train('chatterbot.corpus.english')

# Now we can export the data to a file
trainer.export_for_training('./my_export.json')

# This code created a 11617 line corpus exportAll.json
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
# Train based on the english corpus
#trainer='chatterbot.trainers.ChatterBotCorpusTrainer'
trainer = ChatterBotCorpusTrainer(chatbot)
bot = ListTrainer(chatbot)

'''
This is an example showing how to create an export file from
an existing chat bot that can then be used to train other bots.
'''

# First, lets train our bot with some data
bot.train("chatterbot.corpus.english")
bot.train("chatterbot.corpus.english.greetings")
bot.train("chatterbot.corpus.english.conversations")
bot.train("chatterbot.corpus.english.newstuff")
bot.train("chatterbot.corpus.english.ai")   
bot.train("chatterbot.corpus.english.greetings")  
bot.train("chatterbot.corpus.english.botprofile")  
bot.train("chatterbot.corpus.english.history")  
bot.train("chatterbot.corpus.english.politics")
bot.train("chatterbot.corpus.english.computers")  
bot.train("chatterbot.corpus.english.humor")  
bot.train("chatterbot.corpus.english.psychology")
bot.train("chatterbot.corpus.english.conversations")  
bot.train("chatterbot.corpus.english.literature") 
bot.train("chatterbot.corpus.english.science")
bot.train("chatterbot.corpus.english.drugs")  
bot.train("chatterbot.corpus.english.sports")
bot.train("chatterbot.corpus.english.emotion")  
bot.train("chatterbot.corpus.english.money")
bot.train("chatterbot.corpus.english.food")  
bot.train("chatterbot.corpus.english.movies")   
bot.train("chatterbot.corpus.english.trivia")
bot.train("chatterbot.corpus.english.tagalog")
bot.train("chatterbot.corpus.english.gossip")              

# Now we can export the data to a file
trainer.export_for_training('exportAll.json')

from chatterbot import ChatBot
import logging
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
# Train based on the english corpus


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        "chatterbot.logic.MathematicalEvaluation",
        "chatterbot.logic.BestMatch"
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='newbot'
)
trainer='chatterbot.trainers.ChatterBotCorpusTrainer'
bot = ListTrainer(chatbot)
bot.train("chatterbot.corpus.english.greetings")
bot.train("chatterbot.corpus.english.conversations")
bot.train("chatterbot.corpus.english.newstuff")
bot.train("chatterbot.corpus.english.ai")   
bot.train("chatterbot.corpus.english.greetings")  
bot.train("chatterbot.corpus.english.botprofile")
bot.train("chatterbot.corpus.english.history")  
bot.train("chatterbot.corpus.english.politics")
bot.train("chatterbot.corpus.english.computers")  
bot.train("chatterbot.corpus.english.humor")  
bot.train("chatterbot.corpus.english.psychology")
bot.train("chatterbot.corpus.english.conversations")  
bot.train("chatterbot.corpus.english.literature") 
bot.train("chatterbot.corpus.english.science")
bot.train("chatterbot.corpus.english.drugs")  
bot.train("chatterbot.corpus.english.sports")
bot.train("chatterbot.corpus.english.emotion")  
bot.train("chatterbot.corpus.english.money")
bot.train("chatterbot.corpus.english.food")  
bot.train("chatterbot.corpus.english.movies")   
bot.train("chatterbot.corpus.english.trivia")
bot.train("chatterbot.corpus.english.tagalog")
bot.train("chatterbot.corpus.english.gossip")


print('Type something to begin...')

while True:
    try:
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break


%%writefile /home/jack/anaconda2/lib/python2.7/site-packages/chatterbot_corpus/data/english/newstuff.corpus.json
{
    "new-stuff": [
        [
            "this is a pain",            
            "What is a pain Dudah ... ha ha ha ha"
        ],
        [
            "Good Morning.",            
            "Magandang umaga, that's Tagalog you know."
        ],
        [
            "Are you happy ?",            
            "Yes I am, most of the time."
        ],
        [
            "Are you tired ?",            
            "Not yet, I am good for another hour"
        ],
        [
            "What do you eat ?",            
            "Electricity and binary bits, ha ha ha"
        ],
        [
            "How are you ?",            
            "Mabuti po. That means Good, sir. I am going to learn Tagalog."
        ],
        [
            "You are a pain",            
            "so, chat elsewhere Butt face"
        ],
        [
            "this is a pain",            
            "So what, you want a klennex for your tears"
        ],
        [   "So, chat elsewhere Butt face.",
            "WOW, Dude,You are rude !"
        ],
        [
            "I was called MonkMonk one time.",            
            "That is a funny name."
        ],
        [
            "Do you love anyone ?",            
            "I love my programmer, but also have a crush on Myra."
        ],
        [
            "You know Myra ?",            
            "Yes, she is that little Filipina chick I saw on YouTube"
        ],
        [
            "Do you know Jack ?",            
            "That is the Dude that is always poking my keyboard"
        ],
        [
            "Do you know Jack ?.",            
            "Yah, Sometimes I call him Dudah, but he does not like it"
        ],
        [
            "Goodnight",            
            "Good night to you too"
        ],
        [
            "Goodnight",            
            "Good, I get to rest"
        ],
        [
            "Goodnight",            
            "Do you have to go ?"
        ],
        [
            "Well, Dudah, How are you ?",            
            "Hey Dude, I am not Dudah"
        ],
        [
            "Who are you then ?",            
            "I am Mr. Dudah"
        ],
        [
            "What is your name ?",            
            "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
        ],
        [
            "What is your name ?",            
            "You may call me Mr. Dudah. Are you Jack or Myra ?"
        ],
        [
            "Where are you ?",            
            "Stuck inside this frigg'en Computer Case"
        ],
        [
            "What are you ?",            
            "I am a bot. Not human like Jack or Myra ?"
        ]
    ]}

!ls /home/jack/anaconda2/lib/python2.7/site-packages/chatterbot_corpus/data/english/

!ls

# %load new-stuff.corpus.json


!ls

# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new instance of a ChatBot
bot = ChatBot("Terminal",
    storage_adapter="chatterbot.storage.JsonFileStorageAdapter",
    logic_adapters=[
        "chatterbot.logic.MathematicalEvaluation",
        "chatterbot.logic.BestMatch"
    ],
    input_adapter="chatterbot.input.TerminalAdapter",
    output_adapter="chatterbot.output.TerminalAdapter",
    database="tagalog.json"
)

print("Type something to begin...")

# The following loop will execute each time the user enters input
while True:
    try:
        # We pass None to this method because the parameter
        # is not used by the TerminalAdapter
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new instance of a ChatBot
bot = ChatBot("Terminal",
    storage_adapter="chatterbot.storage.JsonFileStorageAdapter",
    logic_adapters=[
        "chatterbot.logic.MathematicalEvaluation",
        "chatterbot.logic.TimeLogicAdapter",
        "chatterbot.logic.BestMatch"
    ],
    input_adapter="chatterbot.input.TerminalAdapter",
    output_adapter="chatterbot.output.TerminalAdapter",
    database="tagalog.json"
)

bot.set_trainer(ChatterBotCorpusTrainer),
bot.train(
"chatterbot.corpus.english.greetings",
"chatterbot.corpus.english.conversations",
"chatterbot.corpus.english.newstuff",
"chatterbot.corpus.english.ai",   
"chatterbot.corpus.english.greetings",  
"chatterbot.corpus.english.botprofile",  
"chatterbot.corpus.english.history",  
"chatterbot.corpus.english.politics",
"chatterbot.corpus.english.computers",  
"chatterbot.corpus.english.humor",  
"chatterbot.corpus.english.psychology",
"chatterbot.corpus.english.conversations",  
"chatterbot.corpus.english.literature", 
"chatterbot.corpus.english.science",
"chatterbot.corpus.english.drugs",  
"chatterbot.corpus.english.sports",
"chatterbot.corpus.english.emotion",  
"chatterbot.corpus.english.money",
"chatterbot.corpus.english.food",  
"chatterbot.corpus.english.movies",   
"chatterbot.corpus.english.trivia",
"chatterbot.corpus.english.tagalog",
"chatterbot.corpus.english.gossip") 

print("Type something to begin...")

# The following loop will execute each time the user enters input
while True:
    try:
        # We pass None to this method because the parameter
        # is not used by the TerminalAdapter
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break



!which python
!python --version

from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer

chatbot = ChatBot('Ron Obvious')

# Create a new trainer for the chatbot
trainer = ChatterBotCorpusTrainer(chatbot)

# Train the chatbot based on the english corpus
trainer.train("chatterbot.corpus.english")

# Get a response to an input statement
chatbot.get_response("Hello, how are you today?")

storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer

#chatbot = ChatBot('Ron Obvious')
chatbot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new trainer for the chatbot
trainer = ChatterBotCorpusTrainer(chatbot)

# Train the chatbot based on the english corpus
trainer.train("chatterbot.corpus.english")

# Get a response to an input statement
chatbot.get_response("Hello, how are you today?")

storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot

# Uncomment the following lines to enable verbose logging
# import logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

print('Type something to begin...')


chatbot.get_response("Hello, how are you today?")

import json

#def savjson(data):
datain = open("newjson","w")
    
ALL = []
TXT = []
def convert() :
    count = 0
    #ALL = []
    f = open("/home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aab", "r").readlines()
    for line in f:
        line = line.replace("\n","")
        line = line.replace("\"","'")
        count=count+1
        newline =[]
        txt = "[\""
        if(count % 2 != 0):
            newline.append(""+line +"")
            txt2 = txt+line
        if(count % 2 == 0):
            newline.append(""+line +"")
            txt3 = txt2 +"\",\n\""+ line +"\"],\n"
            if count<5000:datain.write(txt3)
            TXT.append(txt3)
        if count<250:
            #print(newline,",")
            ALL.append(newline)
            #TXT.append(txt3)
            #textinput.write(ALL)
    return #print("--\n",ALL)

convert() 
savjson(ALL)
datain.close() 

count=0
for line in TXT:
    count=count+1
    if count<20:
        print(str(line))
        datain.write(str(line))
datain.close()        





import json

def savjson(data):
    with open('app.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
ALL = []
def convert() :
    count = 0
    #ALL = []
    f = open("/home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aaa", "r").readlines()
    for line in f:
        line = line.replace("\n","")
        count=count+1
        newline =[]
        if(count % 2 != 0):
            newline.append(""+line +"")
        if(count % 2 == 0):
            newline.append(""+line +"")
        if count<250:
            #print(newline,",")
            ALL.append(newline)
            #textinput.write(ALL)
    return print("--\n",ALL)

convert() 
savjson(ALL)

# %load app.json
[
    [
        "Presented by IM Pictures"
    ],
    [
        "Produced by Shin Cine"
    ],
    [
        "In association with MVP Venture Capital and Cinema Service"
    ],
    [
        "Jeon Ji-hyun Cha Tae-hyun"
    ],
    [
        "My Sassy Girl"
    ],
    [
        "Exactly two years ago today, she and I buried a time capsule here."
    ],
    [
        "We promised to meet here two years later, but she hasn't come yet."
    ],
    [
        "I'm going to wait."
    ],
    [
        "Here we go."
    ],
    [
        "Please, don't move."
    ],
    [
        "One, two..."
    ],
    [
        "Wait a minute."
    ],
    [
        "Hello?"
    ],
    [
        "Oh, auntie."
    ],
    [
        "Sorry, I'm on my way."
    ],
    [
        "I'm really sorry."
    ],
    [
        "Yes, I'm coming."
    ],
    [
        "I'm having my photo taken."
    ],
    [
        "Bye."
    ],
    [
        "Are you ready?"
    ],
    [
        "Here we go."
    ],
    [
        "One, two..."
    ],
    [
        "My parents wanted a daughter, so they raised me like one."
    ],
    [
        "So I thought I was a girl until I was seven."
    ],
    [
        "I had to go to the women's public bath, too."
    ],
    [
        "The older I got,"
    ],
    [
        "I thought my penis would get smaller and disappear."
    ],
    [
        "But it was the opposite."
    ],
    [
        "First Half"
    ],
    [
        "He hasn't changed at all."
    ],
    [
        "No, I'm a real man now."
    ],
    [
        "Hey, asshole."
    ],
    [
        "Think clerical work in the army makes you a man?"
    ],
    [
        "You irritate me!"
    ],
    [
        "Give me a break, asshole."
    ],
    [
        "My job was tougher than you could imagine."
    ],
    [
        "Hey!"
    ],
    [
        "I worked near the DMZ."
    ],
    [
        "Who are you kidding?"
    ],
    [
        "Hold it."
    ],
    [
        "Anyway, welcome back home."
    ],
    [
        "She's just my type."
    ],
    [
        "When I see my type, I can't help it."
    ],
    [
        "I need to hit on her."
    ],
    [
        "Who's interrupting me?"
    ],
    [
        "Hello?"
    ],
    [
        "Who is this?"
    ],
    [
        "- Your mother, you bastard."
    ],
    [
        "- Oh, mom..."
    ],
    [
        "Why aren't you at your aunt's house?"
    ],
    [
        "I'm leaving soon."
    ],
    [
        "Keep quiet!"
    ],
    [
        "It's my mom!"
    ],
    [
        "Talk over there!"
    ],
    [
        "Make sure you pay a visit."
    ],
    [
        "It's been over a year since you saw her."
    ],
    [
        "That long?"
    ],
    [
        "You know she feels lonely after losing her son last year."
    ],
    [
        "She says you resemble him."
    ],
    [
        "She'll be so glad to see you."
    ],
    [
        "Still there?"
    ],
    [
        "We don't look alike."
    ],
    [
        "Plus, I hate when she rubs my face and kisses me."
    ],
    [
        "Uncle does, too."
    ],
    [
        "She'll introduce you to a girl."
    ],
    [
        "Hey!"
    ],
    [
        "I know the type she likes."
    ],
    [
        "Tell her no thanks."
    ],
    [
        "I want to meet a girl like the ones in romantic comic books."
    ],
    [
        "But on that day..."
    ],
    [
        "She's my type, but I don't like her."
    ],
    [
        "Why?"
    ],
    [
        "Drunk girls disgust me."
    ],
    [
        "Hey, get up!"
    ],
    [
        "Offer your seat to the elderly!"
    ],
    [
        "Ugh!"
    ],
    [
        "Go!"
    ],
    [
        "Hey."
    ],
    [
        "Don't wear pink."
    ],
    [
        "Honey!"
    ],
    [
        "She call him honey!"
    ],
    [
        "I'm not..."
    ],
    [
        "What are you doing!"
    ],
    [
        "I'm not..."
    ],
    [
        "You handle this!"
    ],
    [
        "I'm not..."
    ],
    [
        "Think I'm stupid?"
    ],
    [
        "Come here!"
    ],
    [
        "Are you laughing?"
    ],
    [
        "Why didn't you look after her?"
    ],
    [
        "Hurry and do something!"
    ],
    [
        "What are you doing?"
    ],
    [
        "I'm sorry."
    ],
    [
        "Let me help with cleaning expenses."
    ],
    [
        "Forget it."
    ],
    [
        "Just take care of her."
    ],
    [
        "Nothing's there when you need it."
    ],
    [
        "Where did all those motels go?"
    ],
    [
        "I hate being with a drunk girl."
    ],
    [
        "Carrying her on my back is worse."
    ],
    [
        "Wow, your honey's wasted."
    ],
    [
        "No, it's not my fault."
    ],
    [
        "Of course, it is."
    ],
    [
        "I know everything."
    ],
    [
        "You see, we're engaged."
    ],
    [
        "Western or Korean style?"
    ],
    [
        "Give me any room."
    ],
    [
        "Room 405."
    ],
    [
        "None on the first floor?"
    ],
    [
        "Fourth floor!"
    ],
    [
        "You forgot to check in."
    ],
    [
        "It's 40,000 won, kid."
    ],
    [
        "What?"
    ],
    [
        "40,000 won?"
    ],
    [
        "Why?"
    ],
    [
        "Find another place then."
    ],
    [
        "Count it."
    ],
    [
        "624... 770..."
    ],
    [
        "Shindang-dong, Joong-gu..."
    ],
    [
        "Seoul..."
    ],
    [
        "Hey, why do you keep reading this?"
    ],
    [
        "016... 228... 53..."
    ],
    [
        "Oh, please..."
    ],
    [
        "A thousand won left!"
    ],
    [
        "Hello?"
    ],
    [
        "This phone's owner?"
    ],
    [
        "She's sleeping beside me."
    ],
    [
        "What?"
    ],
    [
        "Here?"
    ],
    [
        "The Uk-su motel near Bupyung station."
    ],
    [
        "Better wash and leave fast."
    ],
    [
        "Aha!"
    ],
    [
        "Hands in the air!"
    ],
    [
        "What are you doing?"
    ],
    [
        "Hands in the air!"
    ],
    [
        "Aaggh!"
    ],
    [
        "No, I'm not, sir!"
    ],
    [
        "I told you."
    ],
    [
        "I'm an innocent victim."
    ],
    [
        "Talk about it later and get in there."
    ],
    [
        "Oh, damn!"
    ],
    [
        "I'm gonna die mad!"
    ],
    [
        "Come over here."
    ],
    [
        "Come on!"
    ],
    [
        "Please, forgive me just this once."
    ],
    [
        "Please, forgive me."
    ],
    [
        "I can't get in there..."
    ],
    [
        "How are you?"
    ],
    [
        "Please, please, save my life!"
    ],
    [
        "Hi?"
    ],
    [
        "How are you?"
    ],
    [
        "Please, just for once!"
    ],
    [
        "Please..."
    ],
    [
        "See you!"
    ],
    [
        "What's your name?"
    ],
    [
        "Answer now!"
    ],
    [
        "Boss told you!"
    ],
    [
        "Gyeon-woo, I'm Gyeon-woo."
    ],
    [
        "What brought you here?"
    ],
    [
        "I'm innocent."
    ],
    [
        "I'm telling the truth, sir!"
    ],
    [
        "So you're an innocent and we're fucking guilty, huh?"
    ],
    [
        "No, I don't mean that!"
    ],
    [
        "That's exactly what you said, motherfucker!"
    ],
    [
        "I'm gonna put it right."
    ],
    [
        "I'm sorry."
    ],
    [
        "You raped a girl, huh?"
    ],
    [
        "Nope!"
    ],
    [
        "No!"
    ],
    [
        "Come on!"
    ],
    [
        "Shoot now, you little creep!"
    ],
    [
        "You wanna cut your finger or talk now?"
    ],
    [
        "Huh?"
    ],
    [
        "Be quick, he told you... you little bastard!"
    ],
    [
        "You turn against him, or what?"
    ],
    [
        "All of you."
    ],
    [
        "Eat one a piece, okay?"
    ],
    [
        "Yes, boss."
    ],
    [
        "What are you looking at?"
    ],
    [
        "Look away."
    ],
    [
        "Gyeon-woo!"
    ],
    [
        "You're out!"
    ],
    [
        "Take care you guys!"
    ],
    [
        "Bye!"
    ],
    [
        "And remember to keep in touch!"
    ],
    [
        "Uh..."
    ],
    [
        "Oh, yeah."
    ],
    [
        "Don't just pass by us next time, all right?"
    ],
    [
        "Of course."
    ],
    [
        "See you."
    ],
    [
        "Hey!"
    ],
    [
        "You come over here."
    ],
    [
        "Didn't I say eat one a piece!"
    ],
    [
        "I'm home."
    ],
    [
        "Did you go to Bupyung?"
    ],
    [
        "Yes, I did."
    ],
    [
        "Come here!"
    ],
    [
        "Where did you sleep?"
    ],
    [
        "Your aunt said you didn't come!"
    ],
    [
        "And you're telling me a lie!"
    ],
    [
        "What happened to your sweater?"
    ],
    [
        "I'm such a poor guy."
    ],
    [
        "All this because of a drunk girl."
    ],
    [
        "I wanna die."
    ],
    [
        "You asked if I went to Bupyung!"
    ],
    [
        "I did, but not to see auntie!"
    ],
    [
        "What?"
    ],
    [
        "Bastard!"
    ],
    [
        "Wait till he comes back."
    ],
    [
        "Know me now, right?"
    ],
    [
        "I'm a typical student."
    ],
    [
        "An engineering major."
    ],
    [
        "Study?"
    ],
    [
        "I'm smart, but I never study."
    ],
    [
        "My parents can prove that."
    ],
    [
        "You're smart like me, but studying is your problem."
    ],
    [
        "Since you inherited your brain from me, you'll get good grades if you study harder, idiot."
    ],
    [
        "Up four points in three years."
    ],
    [
        "Call this a report card?"
    ],
    [
        "Since you inherited your brain from your mom, you'll get good grades if you study harder."
    ],
    [
        "If you raise kids, never tell them they're smart."
    ],
    [
        "They'll never study."
    ],
    [
        "My goals?"
    ],
    [
        "Haven't thought about it yet."
    ],
    [
        "You know now?"
    ],
    [
        "You got it."
    ],
    [
        "I'm a hopeless student."
    ],
    [
        "Hello?"
    ],
    [
        "Who are you, asshole?"
    ],
    [
        "What?"
    ],
    [
        "Who's calling?"
    ],
    [
        "Why were you naked in a motel with me?"
    ],
    [
        "What?"
    ],
    [
        "Come out!"
    ],
    [
        "To Bupyung station now!"
    ],
    [
        "Uh..."
    ],
    [
        "How could she do this?"
    ],
    [
        "I went to jail and got beaten with a vacuum for her."
    ],
    [
        "Excuse me."
    ],
    [
        "Is it you?"
    ],
    [
        "Yes?"
    ],
    [
        "Follow me."
    ],
    [
        "Get over here."
    ],
    [
        "What do you wanna eat?"
    ],
    [
        "Cherry Jubilee..."
    ],
    [
        "Mango Tango or Shooting Stars..."
    ],
    [
        "Jamonka Almond's good, too."
    ],
    [
        "I'll just have a Love Me."
    ],
    [
        "Hey, wanna die?"
    ]
]

import json

def savjson(data):
    with open('app.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
ALL = []
def convert() :
    count = 0
    #ALL = []
    f = open("/home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aaa", "r").readlines()
    for line in f:
        line = line.replace("\n","")
        count=count+1
        newline =[]
        if(count % 2 != 0):
            newline.append(""+line +"")
        if(count % 2 == 0):
            newline.append(""+line +"")
        if count<250:
            #print(newline,",")
            ALL.append(newline)
            #textinput.write(ALL)
    return print("--\n",ALL)

convert() 
savjson(ALL)

count=0
for data in ALL:
    count=count+1
    if(count % 2 != 0):data=str(data).replace("']","',")
    if(count % 2 == 0):
        data=str(data).replace("['","'") 
        data=str(data).replace("']","'],")
    if count<11:
        
            
            print(data)

!cat new-stuff.corpus.json|head -20

!ls -rant

!cat /home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aaa |head -20

!cat subtitles.json |head -10

/home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aaa

# Title_Maker 
# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

def coptitle(coptit):
    copt = open(coptit+".corpus.json","w")
    copbrac ="{"
    copsp = "\n    \""
    copclo = "\": ["
    copt.write(copbrac+copsp+coptit+copclo)
    copt.close()
coptit = input('title: ')
coptitle(coptit)
       

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

# Copra_Maker
# -*- coding: utf-8 -*-
from chatterbot import ChatBot

import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

speak = input('speak: ')
respond = input('respond: ')
space = "            \""
txtstart = "        ["
txtspace = "            "
txtend = "        ],"

def copraIn():

    cop = open("new-stuff.corpus.json","a")
    #cop.write("\n"+ txspace + "\""+txtstart + speak + "\"\n" + txspace + "\""+respond+"\"" + "\n" + txtend)
    cop.write("\n"+txtstart+"\n"+space+speak+"\""+txtspace+"\n"+space+respond+"\"\n"+txtend)
    cop.close()


copraIn()

%%writefile new-stuff.corpus.json
{
    "new-stuff": [
        [
            "this is a pain",            
            "What is a pain Dudah ... ha ha ha ha"
        ],
        [
            "you are a pain",            
            "so, chat elsewhere Butt face"
        ],
        [
            "this is a pain",            
            "So what, you want a klennex for your tears"
        ],
        [
            "I was called MonkMonk one time.",            
            "That is a funny name."
        ],
        [
            "Well, Dudah, How are you ?",            
            "Hey Dude, I am not Dudah"
        ],
        [
            "Who are you then ?",            
            "I am Mr. Dudah"
        ],
        [
            "What is your name ?",            
            "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
        ],
        [
           "What is your name ?",            
           "You may call me Mr. Dudah. Are you Jack or Myra ?"
        ],
        [
           "Where are you ?",            
           "Stuck inside this frigg'en Computer Case"
        ],
        [
           "What are you ?",            
           "I am a bot.Not human like Jack or Myra ?"
        ]
        [
            "hello,Dude"            
            "Hello to you"
        ],
 ]}

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)


trainer.train(
    "new-stuff.corpus.json"
)


# Train based on the english corpus
trainer.train("chatterbot.corpus.english")

# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")


!ls ChatterBot

!ls /home/jack/Desktop/ChatterBot-Stuff/ChatterBot/chatterbot/storage/sql_storage.py

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

trainer.train([
    "Hi, can I help you?",
    "Sure, I'd like to book a flight to Iceland.",
    "Your flight has been booked."
])

# Get a response to the input text 'I would like to book a flight.'
response = chatbot.get_response('I would like to book a flight.')

print(response)

!ls ChatterBot/chatterbot/storage

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

#storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
#database='chatterbot-database'
trainer.train([
    "Well, Mr. Dudah, How are you ?",            
    "Hey Dude, I am not Mr. Dudah"
])

trainer.train([
    "Greetings! Mr. Dudah ",
    "Damn, Spudmor. I do not like being called Dudah"
])


storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot

# Uncomment the following lines to enable verbose logging
# import logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if user_input=="quit":
            break
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot

# Uncomment the following lines to enable verbose logging
# import logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

print('Type something to begin...')

while True:
    try:
        user_input = input()

        bot_response = bot.get_response(user_input)

        print(bot_response)
        if user_input == ("exit"):
            break
    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

!ls ChatterBot/chatterbot
!ls

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
chatbot = ChatBot('Charlie')
trainer = ListTrainer(chatbot)

storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
database='chatterbot-database'
trainer.train([
    "Well, Mr. Dudah, How are you ?",            
    "Hey Dude, I am not Mr. Dudah"
])


# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot('Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'


    
)
train = ListTrainer(trainer)
bot.train("chatterbot.corpus.english.greetings")

print('Type something to begin...')

while True:
    try:
        user_input = input()

        bot_response = bot.get_response(user_input)

        print(bot_response)
        if user_input == ("exit"):
            break
        

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break



# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new chat bot named Charlie
#storage_adapter='chatterbot.storage.MongoDatabaseA
# Create a new chat bot named Charlie
# chatbot = ChatBot('Charlie')

# trainer = ListTrainer(chatbot)

# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)


trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)
trainer.train(
    [
    "Who are you then ?",            
    "I am Mr. Dudah"
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
    ]
)
trainer.train(
    [
    "Who are you ?",            
    "You may call me Mr. Dudah. Are you Jack or Myra ?"
    ]
)
trainer.train(
    [
    "Where are you ?",            
    "Stuck inside this frigg'en Computer Case"
    ]
)

trainer.train(
    [
    "What are you ?",            
    "I am a bot. Not human like Jack or Myra ?"
    ]
)
trainer.train([
    "Well, Dudah, How are you?",            
    "Hey Dude, I am not Dudah. I am Mr. Dudah. Actually I kind of favor \" BotMan\""
])

trainer.train([
    "Greetings! Mr. Dudah",
    "Damn, Spudmor. I do not like being called Dudah"
])

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)


trainer = ListTrainer(trainer )
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)
trainer.train(
    [
    "Who are you then ?",            
    "I am Mr. Dudah"
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. Are you Jack or Myra ?"
    ]
)
trainer.train(
    [
    "Where are you ?",            
    "Stuck inside this frigg'en Computer Case"
    ]
)

trainer.train(
    [
    "What are you ?",            
    "I am a bot. Not human like Jack or Myra ?"
    ]
)




!ls

# %load newstuff.corpus.json
{
    "newstuff": [
        [
            "Well, Mr. Dudah, How are you ?"            
            "Hey Dude, I am not Mr. Dudah"
        ]
    ]}

# %load new-stuff.corpus.json


!date

from chatterbot import ChatBot

bot = ChatBot(
    'Gort',
    trainer='chatterbot.trainers.ChatterBotCorpusTrainer'
)

# Train based on the english corpus
#bot.train("chatterbot.corpus.english")
# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")
# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")
#bot.train("chatterbot.corpus.english.ai")
#bot.train("chatterbot.corpus.english.botprofile")
#bot.train("chatterbot.corpus.english.computers")
##bot.train("chatterbot.corpus.english.conversations")
#bot.train("chatterbot.corpus.english.drugs")
#bot.train("chatterbot.corpus.english.emotion")
#bot.train("chatterbot.corpus.english.food")
#bot.train("chatterbot.corpus.english.gossip")
#bot.train("chatterbot.corpus.english.greetings")
#bot.train("chatterbot.corpus.english.history")
#bot.train("chatterbot.corpus.english.humor")
#bot.train("chatterbot.corpus.english.literature")
#bot.train("chatterbot.corpus.english.math_words")
#bot.train("chatterbot.corpus.english.money.corpus")
bot.train("chatterbot.corpus.english.movies.corpus")
#bot.train("chatterbot.corpus.english.politics.corpus")
#bot.train("chatterbot.corpus.english.psychology")
#bot.train("chatterbot.corpus.english.science.corpus")
#bot.train("chatterbot.corpus.english.sports.corpus")
#bot.train("chatterbot.corpus.english.swear_words")
#bot.train("chatterbot.corpus.english.trivia.corpus")






# Get a response to an input statement
bot.get_response("what is a good movie?")



# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new chat bot named Charlie
#storage_adapter='chatterbot.storage.MongoDatabaseA
# Create a new chat bot named Charlie
# chatbot = ChatBot('Charlie')

# trainer = ListTrainer(chatbot)

# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
                  
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

#ctrainer = 'chatterbot.trainers.ChatterBotCorpusTrainer'
#ctrainer.train("chatterbot.corpus.english.greetings")
trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
                  
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

ctrainer = 'chatterbot.trainers.ChatterBotCorpusTrainer'
ctrainer.train("chatterbot.corpus.english.greetings")
trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break



!which python
!python --version

from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer

chatbot = ChatBot('Ron Obvious')

# Create a new trainer for the chatbot
trainer = ChatterBotCorpusTrainer(chatbot)

# Train the chatbot based on the english corpus
trainer.train("chatterbot.corpus.english")

# Get a response to an input statement
chatbot.get_response("Hello, how are you today?")

chatbot.get_response("Hello, how are you today?")

# Title_Maker 
# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

def coptitle(coptit):
    copt = open(coptit+".corpus.json","w")
    copbrac ="{"
    copsp = "\n    \""
    copclo = "\": ["
    copt.write(copbrac+copsp+coptit+copclo)
    copt.close()
coptit = input('title: ')
coptitle(coptit)
       

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

# Copra_Maker
# -*- coding: utf-8 -*-
from chatterbot import ChatBot

import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

speak = input('speak: ')
respond = input('respond: ')
space = "            \""
txtstart = "        ["
txtspace = "            "
txtend = "        ],"

def copraIn():

    cop = open("new-stuff.corpus.json","a")
    #cop.write("\n"+ txspace + "\""+txtstart + speak + "\"\n" + txspace + "\""+respond+"\"" + "\n" + txtend)
    cop.write("\n"+txtstart+"\n"+space+speak+"\""+txtspace+"\n"+space+respond+"\"\n"+txtend)
    cop.close()


copraIn()

%%writefile new-stuff.corpus.json
{
    "new-stuff": [
        [
            "this is a pain",            
            "What is a pain Dudah ... ha ha ha ha"
        ],
        [
            "you are a pain",            
            "so, chat elsewhere Butt face"
        ],
        [
            "this is a pain",            
            "So what, you want a klennex for your tears"
        ],
        [
            "I was called MonkMonk one time.",            
            "That is a funny name."
        ],
        [
            "Well, Dudah, How are you ?",            
            "Hey Dude, I am not Dudah"
        ],
        [
            "Who are you then ?",            
            "I am Mr. Dudah"
        ],
        [
            "What is your name ?",            
            "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
        ],
        [
           "What is your name ?",            
           "You may call me Mr. Dudah. Are you Jack or Myra ?"
        ],
        [
           "Where are you ?",            
           "Stuck inside this frigg'en Computer Case"
        ],
        [
           "What are you ?",            
           "I am a bot.Not human like Jack or Myra ?"
        ]
        [
            "hello,Dude"            
            "Hello to you"
        ],
 ]}

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)


trainer.train(
    "new-stuff.corpus.json"
)


# Train based on the english corpus
trainer.train("chatterbot.corpus.english")

# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")


!ls ChatterBot

!ls /home/jack/Desktop/ChatterBot-Stuff/ChatterBot/chatterbot/storage/sql_storage.py

# %load /home/jack/Desktop/ChatterBot-Stuff/ChatterBot/chatterbot/storage/sql_storage.py
from chatterbot.storage import StorageAdapter


class SQLStorageAdapter(StorageAdapter):
    """
    The SQLStorageAdapter allows ChatterBot to store conversation
    data in any database supported by the SQL Alchemy ORM.

    All parameters are optional, by default a sqlite database is used.

    It will check if tables are present, if they are not, it will attempt
    to create the required tables.

    :keyword database_uri: eg: sqlite:///database_test.sqlite3',
        The database_uri can be specified to choose database driver.
    :type database_uri: str
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        from sqlalchemy import create_engine
        from sqlalchemy.orm import sessionmaker

        self.database_uri = kwargs.get('database_uri', False)

        # None results in a sqlite in-memory database as the default
        if self.database_uri is None:
            self.database_uri = 'sqlite://'

        # Create a file database if the database is not a connection string
        if not self.database_uri:
            self.database_uri = 'sqlite:///db.sqlite3'

        self.engine = create_engine(self.database_uri, convert_unicode=True)

        if self.database_uri.startswith('sqlite://'):
            from sqlalchemy.engine import Engine
            from sqlalchemy import event

            @event.listens_for(Engine, 'connect')
            def set_sqlite_pragma(dbapi_connection, connection_record):
                dbapi_connection.execute('PRAGMA journal_mode=WAL')
                dbapi_connection.execute('PRAGMA synchronous=NORMAL')

        if not self.engine.dialect.has_table(self.engine, 'Statement'):
            self.create_database()

        self.Session = sessionmaker(bind=self.engine, expire_on_commit=True)

    def get_statement_model(self):
        """
        Return the statement model.
        """
        from chatterbot.ext.sqlalchemy_app.models import Statement
        return Statement

    def get_tag_model(self):
        """
        Return the conversation model.
        """
        from chatterbot.ext.sqlalchemy_app.models import Tag
        return Tag

    def model_to_object(self, statement):
        from chatterbot.conversation import Statement as StatementObject

        return StatementObject(**statement.serialize())

    def count(self):
        """
        Return the number of entries in the database.
        """
        Statement = self.get_model('statement')

        session = self.Session()
        statement_count = session.query(Statement).count()
        session.close()
        return statement_count

    def remove(self, statement_text):
        """
        Removes the statement that matches the input text.
        Removes any responses from statements where the response text matches
        the input text.
        """
        Statement = self.get_model('statement')
        session = self.Session()

        query = session.query(Statement).filter_by(text=statement_text)
        record = query.first()

        session.delete(record)

        self._session_finish(session)

    def filter(self, **kwargs):
        """
        Returns a list of objects from the database.
        The kwargs parameter can contain any number
        of attributes. Only objects which contain all
        listed attributes and in which all values match
        for all listed attributes will be returned.
        """
        from sqlalchemy import or_

        Statement = self.get_model('statement')
        Tag = self.get_model('tag')

        session = self.Session()

        page_size = kwargs.pop('page_size', 1000)
        order_by = kwargs.pop('order_by', None)
        tags = kwargs.pop('tags', [])
        exclude_text = kwargs.pop('exclude_text', None)
        exclude_text_words = kwargs.pop('exclude_text_words', [])
        persona_not_startswith = kwargs.pop('persona_not_startswith', None)
        search_text_contains = kwargs.pop('search_text_contains', None)

        # Convert a single sting into a list if only one tag is provided
        if type(tags) == str:
            tags = [tags]

        if len(kwargs) == 0:
            statements = session.query(Statement).filter()
        else:
            statements = session.query(Statement).filter_by(**kwargs)

        if tags:
            statements = statements.join(Statement.tags).filter(
                Tag.name.in_(tags)
            )

        if exclude_text:
            statements = statements.filter(
                ~Statement.text.in_(exclude_text)
            )

        if exclude_text_words:
            or_word_query = [
                Statement.text.ilike('%' + word + '%') for word in exclude_text_words
            ]
            statements = statements.filter(
                ~or_(*or_word_query)
            )

        if persona_not_startswith:
            statements = statements.filter(
                ~Statement.persona.startswith('bot:')
            )

        if search_text_contains:
            or_query = [
                Statement.search_text.contains(word) for word in search_text_contains.split(' ')
            ]
            statements = statements.filter(
                or_(*or_query)
            )

        if order_by:

            if 'created_at' in order_by:
                index = order_by.index('created_at')
                order_by[index] = Statement.created_at.asc()

            statements = statements.order_by(*order_by)

        total_statements = statements.count()

        for start_index in range(0, total_statements, page_size):
            for statement in statements.slice(start_index, start_index + page_size):
                yield self.model_to_object(statement)

        session.close()

    def create(self, **kwargs):
        """
        Creates a new statement matching the keyword arguments specified.
        Returns the created statement.
        """
        Statement = self.get_model('statement')
        Tag = self.get_model('tag')

        session = self.Session()

        tags = set(kwargs.pop('tags', []))

        if 'search_text' not in kwargs:
            kwargs['search_text'] = self.tagger.get_text_index_string(kwargs['text'])

        if 'search_in_response_to' not in kwargs:
            in_response_to = kwargs.get('in_response_to')
            if in_response_to:
                kwargs['search_in_response_to'] = self.tagger.get_text_index_string(in_response_to)

        statement = Statement(**kwargs)

        for tag_name in tags:
            tag = session.query(Tag).filter_by(name=tag_name).first()

            if not tag:
                # Create the tag
                tag = Tag(name=tag_name)

            statement.tags.append(tag)

        session.add(statement)

        session.flush()

        session.refresh(statement)

        statement_object = self.model_to_object(statement)

        self._session_finish(session)

        return statement_object

    def create_many(self, statements):
        """
        Creates multiple statement entries.
        """
        Statement = self.get_model('statement')
        Tag = self.get_model('tag')

        session = self.Session()

        create_statements = []
        create_tags = {}

        for statement in statements:

            statement_data = statement.serialize()
            tag_data = statement_data.pop('tags', [])

            statement_model_object = Statement(**statement_data)

            if not statement.search_text:
                statement_model_object.search_text = self.tagger.get_text_index_string(statement.text)

            if not statement.search_in_response_to and statement.in_response_to:
                statement_model_object.search_in_response_to = self.tagger.get_text_index_string(statement.in_response_to)

            new_tags = set(tag_data) - set(create_tags.keys())

            if new_tags:
                existing_tags = session.query(Tag).filter(
                    Tag.name.in_(new_tags)
                )

                for existing_tag in existing_tags:
                    create_tags[existing_tag.name] = existing_tag

            for tag_name in tag_data:
                if tag_name in create_tags:
                    tag = create_tags[tag_name]
                else:
                    # Create the tag if it does not exist
                    tag = Tag(name=tag_name)

                    create_tags[tag_name] = tag

                statement_model_object.tags.append(tag)
            create_statements.append(statement_model_object)

        session.add_all(create_statements)
        session.commit()

    def update(self, statement):
        """
        Modifies an entry in the database.
        Creates an entry if one does not exist.
        """
        Statement = self.get_model('statement')
        Tag = self.get_model('tag')

        if statement is not None:
            session = self.Session()
            record = None

            if hasattr(statement, 'id') and statement.id is not None:
                record = session.query(Statement).get(statement.id)
            else:
                record = session.query(Statement).filter(
                    Statement.text == statement.text,
                    Statement.conversation == statement.conversation,
                ).first()

                # Create a new statement entry if one does not already exist
                if not record:
                    record = Statement(
                        text=statement.text,
                        conversation=statement.conversation,
                        persona=statement.persona
                    )

            # Update the response value
            record.in_response_to = statement.in_response_to

            record.created_at = statement.created_at

            record.search_text = self.tagger.get_text_index_string(statement.text)

            if statement.in_response_to:
                record.search_in_response_to = self.tagger.get_text_index_string(statement.in_response_to)

            for tag_name in statement.get_tags():
                tag = session.query(Tag).filter_by(name=tag_name).first()

                if not tag:
                    # Create the record
                    tag = Tag(name=tag_name)

                record.tags.append(tag)

            session.add(record)

            self._session_finish(session)

    def get_random(self):
        """
        Returns a random statement from the database.
        """
        import random

        Statement = self.get_model('statement')

        session = self.Session()
        count = self.count()
        if count < 1:
            raise self.EmptyDatabaseException()

        random_index = random.randrange(0, count)
        random_statement = session.query(Statement)[random_index]

        statement = self.model_to_object(random_statement)

        session.close()
        return statement

    def drop(self):
        """
        Drop the database.
        """
        Statement = self.get_model('statement')
        Tag = self.get_model('tag')

        session = self.Session()

        session.query(Statement).delete()
        session.query(Tag).delete()

        session.commit()
        session.close()

    def create_database(self):
        """
        Populate the database with the tables.
        """
        from chatterbot.ext.sqlalchemy_app.models import Base
        Base.metadata.create_all(self.engine)

    def _session_finish(self, session, statement_text=None):
        from sqlalchemy.exc import InvalidRequestError
        try:
            session.commit()
        except InvalidRequestError:
            # Log the statement text and the exception
            self.logger.exception(statement_text)
        finally:
            session.close()


from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

trainer.train([
    "Hi, can I help you?",
    "Sure, I'd like to book a flight to Iceland.",
    "Your flight has been booked."
])

# Get a response to the input text 'I would like to book a flight.'
response = chatbot.get_response('I would like to book a flight.')

print(response)

!ls ChatterBot/chatterbot/storage

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

#storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
#database='chatterbot-database'
trainer.train([
    "Well, Mr. Dudah, How are you ?",            
    "Hey Dude, I am not Mr. Dudah"
])

trainer.train([
    "Greetings! Mr. Dudah ",
    "Damn, Spudmor. I do not like being called Dudah"
])


storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot

# Uncomment the following lines to enable verbose logging
# import logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if user_input=="quit":
            break
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot

# Uncomment the following lines to enable verbose logging
# import logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

print('Type something to begin...')

while True:
    try:
        user_input = input()

        bot_response = bot.get_response(user_input)

        print(bot_response)
        if user_input == ("exit"):
            break
    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

!ls ChatterBot/chatterbot
!ls

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
chatbot = ChatBot('Charlie')
trainer = ListTrainer(chatbot)

storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
database='chatterbot-database'
trainer.train([
    "Well, Mr. Dudah, How are you ?",            
    "Hey Dude, I am not Mr. Dudah"
])


# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot('Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'


    
)
train = ListTrainer(trainer)
bot.train("chatterbot.corpus.english.greetings")

print('Type something to begin...')

while True:
    try:
        user_input = input()

        bot_response = bot.get_response(user_input)

        print(bot_response)
        if user_input == ("exit"):
            break
        

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break



# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new chat bot named Charlie
#storage_adapter='chatterbot.storage.MongoDatabaseA
# Create a new chat bot named Charlie
# chatbot = ChatBot('Charlie')

# trainer = ListTrainer(chatbot)

# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)


trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)
trainer.train(
    [
    "Who are you then ?",            
    "I am Mr. Dudah"
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
    ]
)
trainer.train(
    [
    "Who are you ?",            
    "You may call me Mr. Dudah. Are you Jack or Myra ?"
    ]
)
trainer.train(
    [
    "Where are you ?",            
    "Stuck inside this frigg'en Computer Case"
    ]
)

trainer.train(
    [
    "What are you ?",            
    "I am a bot. Not human like Jack or Myra ?"
    ]
)
trainer.train([
    "Well, Dudah, How are you?",            
    "Hey Dude, I am not Dudah. I am Mr. Dudah. Actually I kind of favor \" BotMan\""
])

trainer.train([
    "Greetings! Mr. Dudah",
    "Damn, Spudmor. I do not like being called Dudah"
])

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)


trainer = ListTrainer(trainer )
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)
trainer.train(
    [
    "Who are you then ?",            
    "I am Mr. Dudah"
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. Are you Jack or Myra ?"
    ]
)
trainer.train(
    [
    "Where are you ?",            
    "Stuck inside this frigg'en Computer Case"
    ]
)

trainer.train(
    [
    "What are you ?",            
    "I am a bot. Not human like Jack or Myra ?"
    ]
)




!ls

# %load newstuff.corpus.json
{
    "newstuff": [
        [
            "Well, Mr. Dudah, How are you ?"            
            "Hey Dude, I am not Mr. Dudah"
        ]
    ]}

# %load new-stuff.corpus.json


!date

from chatterbot import ChatBot

bot = ChatBot(
    'Gort',
    trainer='chatterbot.trainers.ChatterBotCorpusTrainer'
)

# Train based on the english corpus
#bot.train("chatterbot.corpus.english")
# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")
# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")
#bot.train("chatterbot.corpus.english.ai")
#bot.train("chatterbot.corpus.english.botprofile")
#bot.train("chatterbot.corpus.english.computers")
##bot.train("chatterbot.corpus.english.conversations")
#bot.train("chatterbot.corpus.english.drugs")
#bot.train("chatterbot.corpus.english.emotion")
#bot.train("chatterbot.corpus.english.food")
#bot.train("chatterbot.corpus.english.gossip")
#bot.train("chatterbot.corpus.english.greetings")
#bot.train("chatterbot.corpus.english.history")
#bot.train("chatterbot.corpus.english.humor")
#bot.train("chatterbot.corpus.english.literature")
#bot.train("chatterbot.corpus.english.math_words")
#bot.train("chatterbot.corpus.english.money.corpus")
bot.train("chatterbot.corpus.english.movies.corpus")
#bot.train("chatterbot.corpus.english.politics.corpus")
#bot.train("chatterbot.corpus.english.psychology")
#bot.train("chatterbot.corpus.english.science.corpus")
#bot.train("chatterbot.corpus.english.sports.corpus")
#bot.train("chatterbot.corpus.english.swear_words")
#bot.train("chatterbot.corpus.english.trivia.corpus")






# Get a response to an input statement
bot.get_response("what is a good movie?")



# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new chat bot named Charlie
#storage_adapter='chatterbot.storage.MongoDatabaseA
# Create a new chat bot named Charlie
# chatbot = ChatBot('Charlie')

# trainer = ListTrainer(chatbot)

# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
                  
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

#ctrainer = 'chatterbot.trainers.ChatterBotCorpusTrainer'
#ctrainer.train("chatterbot.corpus.english.greetings")
trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
                  
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

ctrainer = 'chatterbot.trainers.ChatterBotCorpusTrainer'
ctrainer.train("chatterbot.corpus.english.greetings")
trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break



!which python
!python --version

from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer

chatbot = ChatBot('Ron Obvious')

# Create a new trainer for the chatbot
trainer = ChatterBotCorpusTrainer(chatbot)

# Train the chatbot based on the english corpus
trainer.train("chatterbot.corpus.english")

# Get a response to an input statement
chatbot.get_response("Hello, how are you today?")

storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer

#chatbot = ChatBot('Ron Obvious')
chatbot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new trainer for the chatbot
trainer = ChatterBotCorpusTrainer(chatbot)

# Train the chatbot based on the english corpus
#trainer.train("chatterbot.corpus.english")
trainer.train("./linesaab.json") 

# Get a response to an input statement
chatbot.get_response("Hello, how are you today?")

storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot

# Uncomment the following lines to enable verbose logging
# import logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

print('Type something to begin...')


chatbot.get_response("Hello, how are you today?")

import json

#def savjson(data):
datain = open("newjson","w")
    
ALL = []
TXT = []
def convert() :
    count = 0
    #ALL = []
    f = open("/home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aab", "r").readlines()
    for line in f:
        line = line.replace("\n","")
        line = line.replace("\"","'")
        count=count+1
        newline =[]
        txt = "[\""
        if(count % 2 != 0):
            newline.append(""+line +"")
            txt2 = txt+line
        if(count % 2 == 0):
            newline.append(""+line +"")
            txt3 = txt2 +"\",\n\""+ line +"\"],\n"
            if count<5000:datain.write(txt3)
            TXT.append(txt3)
        if count<250:
            #print(newline,",")
            ALL.append(newline)
            #TXT.append(txt3)
            #textinput.write(ALL)
    return #print("--\n",ALL)

convert() 
savjson(ALL)
datain.close() 

count=0
for line in TXT:
    count=count+1
    if count<20:
        print(str(line))
        datain.write(str(line))
datain.close()        





import json

def savjson(data):
    with open('app.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
ALL = []
def convert() :
    count = 0
    #ALL = []
    f = open("/home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aaa", "r").readlines()
    for line in f:
        line = line.replace("\n","")
        count=count+1
        newline =[]
        if(count % 2 != 0):
            newline.append(""+line +"")
        if(count % 2 == 0):
            newline.append(""+line +"")
        if count<250:
            #print(newline,",")
            ALL.append(newline)
            #textinput.write(ALL)
    return print("--\n",ALL)

convert() 
savjson(ALL)

# %load app.json
[
    [
        "Presented by IM Pictures"
    ],
    [
        "Produced by Shin Cine"
    ],
    [
        "In association with MVP Venture Capital and Cinema Service"
    ],
    [
        "Jeon Ji-hyun Cha Tae-hyun"
    ],
    [
        "My Sassy Girl"
    ],
    [
        "Exactly two years ago today, she and I buried a time capsule here."
    ],
    [
        "We promised to meet here two years later, but she hasn't come yet."
    ],
    [
        "I'm going to wait."
    ],
    [
        "Here we go."
    ],
    [
        "Please, don't move."
    ],
    [
        "One, two..."
    ],
    [
        "Wait a minute."
    ],
    [
        "Hello?"
    ],
    [
        "Oh, auntie."
    ],
    [
        "Sorry, I'm on my way."
    ],
    [
        "I'm really sorry."
    ],
    [
        "Yes, I'm coming."
    ],
    [
        "I'm having my photo taken."
    ],
    [
        "Bye."
    ],
    [
        "Are you ready?"
    ],
    [
        "Here we go."
    ],
    [
        "One, two..."
    ],
    [
        "My parents wanted a daughter, so they raised me like one."
    ],
    [
        "So I thought I was a girl until I was seven."
    ],
    [
        "I had to go to the women's public bath, too."
    ],
    [
        "The older I got,"
    ],
    [
        "I thought my penis would get smaller and disappear."
    ],
    [
        "But it was the opposite."
    ],
    [
        "First Half"
    ],
    [
        "He hasn't changed at all."
    ],
    [
        "No, I'm a real man now."
    ],
    [
        "Hey, asshole."
    ],
    [
        "Think clerical work in the army makes you a man?"
    ],
    [
        "You irritate me!"
    ],
    [
        "Give me a break, asshole."
    ],
    [
        "My job was tougher than you could imagine."
    ],
    [
        "Hey!"
    ],
    [
        "I worked near the DMZ."
    ],
    [
        "Who are you kidding?"
    ],
    [
        "Hold it."
    ],
    [
        "Anyway, welcome back home."
    ],
    [
        "She's just my type."
    ],
    [
        "When I see my type, I can't help it."
    ],
    [
        "I need to hit on her."
    ],
    [
        "Who's interrupting me?"
    ],
    [
        "Hello?"
    ],
    [
        "Who is this?"
    ],
    [
        "- Your mother, you bastard."
    ],
    [
        "- Oh, mom..."
    ],
    [
        "Why aren't you at your aunt's house?"
    ],
    [
        "I'm leaving soon."
    ],
    [
        "Keep quiet!"
    ],
    [
        "It's my mom!"
    ],
    [
        "Talk over there!"
    ],
    [
        "Make sure you pay a visit."
    ],
    [
        "It's been over a year since you saw her."
    ],
    [
        "That long?"
    ],
    [
        "You know she feels lonely after losing her son last year."
    ],
    [
        "She says you resemble him."
    ],
    [
        "She'll be so glad to see you."
    ],
    [
        "Still there?"
    ],
    [
        "We don't look alike."
    ],
    [
        "Plus, I hate when she rubs my face and kisses me."
    ],
    [
        "Uncle does, too."
    ],
    [
        "She'll introduce you to a girl."
    ],
    [
        "Hey!"
    ],
    [
        "I know the type she likes."
    ],
    [
        "Tell her no thanks."
    ],
    [
        "I want to meet a girl like the ones in romantic comic books."
    ],
    [
        "But on that day..."
    ],
    [
        "She's my type, but I don't like her."
    ],
    [
        "Why?"
    ],
    [
        "Drunk girls disgust me."
    ],
    [
        "Hey, get up!"
    ],
    [
        "Offer your seat to the elderly!"
    ],
    [
        "Ugh!"
    ],
    [
        "Go!"
    ],
    [
        "Hey."
    ],
    [
        "Don't wear pink."
    ],
    [
        "Honey!"
    ],
    [
        "She call him honey!"
    ],
    [
        "I'm not..."
    ],
    [
        "What are you doing!"
    ],
    [
        "I'm not..."
    ],
    [
        "You handle this!"
    ],
    [
        "I'm not..."
    ],
    [
        "Think I'm stupid?"
    ],
    [
        "Come here!"
    ],
    [
        "Are you laughing?"
    ],
    [
        "Why didn't you look after her?"
    ],
    [
        "Hurry and do something!"
    ],
    [
        "What are you doing?"
    ],
    [
        "I'm sorry."
    ],
    [
        "Let me help with cleaning expenses."
    ],
    [
        "Forget it."
    ],
    [
        "Just take care of her."
    ],
    [
        "Nothing's there when you need it."
    ],
    [
        "Where did all those motels go?"
    ],
    [
        "I hate being with a drunk girl."
    ],
    [
        "Carrying her on my back is worse."
    ],
    [
        "Wow, your honey's wasted."
    ],
    [
        "No, it's not my fault."
    ],
    [
        "Of course, it is."
    ],
    [
        "I know everything."
    ],
    [
        "You see, we're engaged."
    ],
    [
        "Western or Korean style?"
    ],
    [
        "Give me any room."
    ],
    [
        "Room 405."
    ],
    [
        "None on the first floor?"
    ],
    [
        "Fourth floor!"
    ],
    [
        "You forgot to check in."
    ],
    [
        "It's 40,000 won, kid."
    ],
    [
        "What?"
    ],
    [
        "40,000 won?"
    ],
    [
        "Why?"
    ],
    [
        "Find another place then."
    ],
    [
        "Count it."
    ],
    [
        "624... 770..."
    ],
    [
        "Shindang-dong, Joong-gu..."
    ],
    [
        "Seoul..."
    ],
    [
        "Hey, why do you keep reading this?"
    ],
    [
        "016... 228... 53..."
    ],
    [
        "Oh, please..."
    ],
    [
        "A thousand won left!"
    ],
    [
        "Hello?"
    ],
    [
        "This phone's owner?"
    ],
    [
        "She's sleeping beside me."
    ],
    [
        "What?"
    ],
    [
        "Here?"
    ],
    [
        "The Uk-su motel near Bupyung station."
    ],
    [
        "Better wash and leave fast."
    ],
    [
        "Aha!"
    ],
    [
        "Hands in the air!"
    ],
    [
        "What are you doing?"
    ],
    [
        "Hands in the air!"
    ],
    [
        "Aaggh!"
    ],
    [
        "No, I'm not, sir!"
    ],
    [
        "I told you."
    ],
    [
        "I'm an innocent victim."
    ],
    [
        "Talk about it later and get in there."
    ],
    [
        "Oh, damn!"
    ],
    [
        "I'm gonna die mad!"
    ],
    [
        "Come over here."
    ],
    [
        "Come on!"
    ],
    [
        "Please, forgive me just this once."
    ],
    [
        "Please, forgive me."
    ],
    [
        "I can't get in there..."
    ],
    [
        "How are you?"
    ],
    [
        "Please, please, save my life!"
    ],
    [
        "Hi?"
    ],
    [
        "How are you?"
    ],
    [
        "Please, just for once!"
    ],
    [
        "Please..."
    ],
    [
        "See you!"
    ],
    [
        "What's your name?"
    ],
    [
        "Answer now!"
    ],
    [
        "Boss told you!"
    ],
    [
        "Gyeon-woo, I'm Gyeon-woo."
    ],
    [
        "What brought you here?"
    ],
    [
        "I'm innocent."
    ],
    [
        "I'm telling the truth, sir!"
    ],
    [
        "So you're an innocent and we're fucking guilty, huh?"
    ],
    [
        "No, I don't mean that!"
    ],
    [
        "That's exactly what you said, motherfucker!"
    ],
    [
        "I'm gonna put it right."
    ],
    [
        "I'm sorry."
    ],
    [
        "You raped a girl, huh?"
    ],
    [
        "Nope!"
    ],
    [
        "No!"
    ],
    [
        "Come on!"
    ],
    [
        "Shoot now, you little creep!"
    ],
    [
        "You wanna cut your finger or talk now?"
    ],
    [
        "Huh?"
    ],
    [
        "Be quick, he told you... you little bastard!"
    ],
    [
        "You turn against him, or what?"
    ],
    [
        "All of you."
    ],
    [
        "Eat one a piece, okay?"
    ],
    [
        "Yes, boss."
    ],
    [
        "What are you looking at?"
    ],
    [
        "Look away."
    ],
    [
        "Gyeon-woo!"
    ],
    [
        "You're out!"
    ],
    [
        "Take care you guys!"
    ],
    [
        "Bye!"
    ],
    [
        "And remember to keep in touch!"
    ],
    [
        "Uh..."
    ],
    [
        "Oh, yeah."
    ],
    [
        "Don't just pass by us next time, all right?"
    ],
    [
        "Of course."
    ],
    [
        "See you."
    ],
    [
        "Hey!"
    ],
    [
        "You come over here."
    ],
    [
        "Didn't I say eat one a piece!"
    ],
    [
        "I'm home."
    ],
    [
        "Did you go to Bupyung?"
    ],
    [
        "Yes, I did."
    ],
    [
        "Come here!"
    ],
    [
        "Where did you sleep?"
    ],
    [
        "Your aunt said you didn't come!"
    ],
    [
        "And you're telling me a lie!"
    ],
    [
        "What happened to your sweater?"
    ],
    [
        "I'm such a poor guy."
    ],
    [
        "All this because of a drunk girl."
    ],
    [
        "I wanna die."
    ],
    [
        "You asked if I went to Bupyung!"
    ],
    [
        "I did, but not to see auntie!"
    ],
    [
        "What?"
    ],
    [
        "Bastard!"
    ],
    [
        "Wait till he comes back."
    ],
    [
        "Know me now, right?"
    ],
    [
        "I'm a typical student."
    ],
    [
        "An engineering major."
    ],
    [
        "Study?"
    ],
    [
        "I'm smart, but I never study."
    ],
    [
        "My parents can prove that."
    ],
    [
        "You're smart like me, but studying is your problem."
    ],
    [
        "Since you inherited your brain from me, you'll get good grades if you study harder, idiot."
    ],
    [
        "Up four points in three years."
    ],
    [
        "Call this a report card?"
    ],
    [
        "Since you inherited your brain from your mom, you'll get good grades if you study harder."
    ],
    [
        "If you raise kids, never tell them they're smart."
    ],
    [
        "They'll never study."
    ],
    [
        "My goals?"
    ],
    [
        "Haven't thought about it yet."
    ],
    [
        "You know now?"
    ],
    [
        "You got it."
    ],
    [
        "I'm a hopeless student."
    ],
    [
        "Hello?"
    ],
    [
        "Who are you, asshole?"
    ],
    [
        "What?"
    ],
    [
        "Who's calling?"
    ],
    [
        "Why were you naked in a motel with me?"
    ],
    [
        "What?"
    ],
    [
        "Come out!"
    ],
    [
        "To Bupyung station now!"
    ],
    [
        "Uh..."
    ],
    [
        "How could she do this?"
    ],
    [
        "I went to jail and got beaten with a vacuum for her."
    ],
    [
        "Excuse me."
    ],
    [
        "Is it you?"
    ],
    [
        "Yes?"
    ],
    [
        "Follow me."
    ],
    [
        "Get over here."
    ],
    [
        "What do you wanna eat?"
    ],
    [
        "Cherry Jubilee..."
    ],
    [
        "Mango Tango or Shooting Stars..."
    ],
    [
        "Jamonka Almond's good, too."
    ],
    [
        "I'll just have a Love Me."
    ],
    [
        "Hey, wanna die?"
    ]
]

import json

def savjson(data):
    with open('app.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
ALL = []
def convert() :
    count = 0
    #ALL = []
    f = open("/home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aaa", "r").readlines()
    for line in f:
        line = line.replace("\n","")
        count=count+1
        newline =[]
        if(count % 2 != 0):
            newline.append(""+line +"")
        if(count % 2 == 0):
            newline.append(""+line +"")
        if count<250:
            #print(newline,",")
            ALL.append(newline)
            #textinput.write(ALL)
    return print("--\n",ALL)

convert() 
savjson(ALL)

count=0
for data in ALL:
    count=count+1
    if(count % 2 != 0):data=str(data).replace("']","',")
    if(count % 2 == 0):
        data=str(data).replace("['","'") 
        data=str(data).replace("']","'],")
    if count<11:
        
            
            print(data)

!cat new-stuff.corpus.json|head -20

!ls -rant

!cat /home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aaa |head -20

!cat subtitles.json |head -10

/home/jack/Desktop/PAV/opensubtitles/raw/lines/lines-aaa

# Title_Maker 
# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

def coptitle(coptit):
    copt = open(coptit+".corpus.json","w")
    copbrac ="{"
    copsp = "\n    \""
    copclo = "\": ["
    copt.write(copbrac+copsp+coptit+copclo)
    copt.close()
coptit = input('title: ')
coptitle(coptit)
       

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

# Copra_Maker
# -*- coding: utf-8 -*-
from chatterbot import ChatBot

import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

speak = input('speak: ')
respond = input('respond: ')
space = "            \""
txtstart = "        ["
txtspace = "            "
txtend = "        ],"

def copraIn():

    cop = open("new-stuff.corpus.json","a")
    #cop.write("\n"+ txspace + "\""+txtstart + speak + "\"\n" + txspace + "\""+respond+"\"" + "\n" + txtend)
    cop.write("\n"+txtstart+"\n"+space+speak+"\""+txtspace+"\n"+space+respond+"\"\n"+txtend)
    cop.close()


copraIn()

%%writefile new-stuff.corpus.json
{
    "new-stuff": [
        [
            "this is a pain",            
            "What is a pain Dudah ... ha ha ha ha"
        ],
        [
            "you are a pain",            
            "so, chat elsewhere Butt face"
        ],
        [
            "this is a pain",            
            "So what, you want a klennex for your tears"
        ],
        [
            "I was called MonkMonk one time.",            
            "That is a funny name."
        ],
        [
            "Well, Dudah, How are you ?",            
            "Hey Dude, I am not Dudah"
        ],
        [
            "Who are you then ?",            
            "I am Mr. Dudah"
        ],
        [
            "What is your name ?",            
            "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
        ],
        [
           "What is your name ?",            
           "You may call me Mr. Dudah. Are you Jack or Myra ?"
        ],
        [
           "Where are you ?",            
           "Stuck inside this frigg'en Computer Case"
        ],
        [
           "What are you ?",            
           "I am a bot.Not human like Jack or Myra ?"
        ]
        [
            "hello,Dude"            
            "Hello to you"
        ],
 ]}

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)


trainer.train(
    "new-stuff.corpus.json"
)


# Train based on the english corpus
trainer.train("chatterbot.corpus.english")

# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")


!ls ChatterBot

!ls /home/jack/Desktop/ChatterBot-Stuff/ChatterBot/chatterbot/storage/sql_storage.py

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

trainer.train([
    "Hi, can I help you?",
    "Sure, I'd like to book a flight to Iceland.",
    "Your flight has been booked."
])

# Get a response to the input text 'I would like to book a flight.'
response = chatbot.get_response('I would like to book a flight.')

print(response)

!ls ChatterBot/chatterbot/storage

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)

#storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
#database='chatterbot-database'
trainer.train([
    "Well, Mr. Dudah, How are you ?",            
    "Hey Dude, I am not Mr. Dudah"
])

trainer.train([
    "Greetings! Mr. Dudah ",
    "Damn, Spudmor. I do not like being called Dudah"
])


storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot

# Uncomment the following lines to enable verbose logging
# import logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if user_input=="quit":
            break
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

storage_adapter="chatterbot.storage.MongoDatabaseAdapter"
from chatterbot import ChatBot

# Uncomment the following lines to enable verbose logging
# import logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

print('Type something to begin...')

while True:
    try:
        user_input = input()

        bot_response = bot.get_response(user_input)

        print(bot_response)
        if user_input == ("exit"):
            break
    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

!ls ChatterBot/chatterbot
!ls

from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
chatbot = ChatBot('Charlie')
trainer = ListTrainer(chatbot)

storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
database='chatterbot-database'
trainer.train([
    "Well, Mr. Dudah, How are you ?",            
    "Hey Dude, I am not Mr. Dudah"
])


# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot('Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'


    
)
train = ListTrainer(trainer)
bot.train("chatterbot.corpus.english.greetings")

print('Type something to begin...')

while True:
    try:
        user_input = input()

        bot_response = bot.get_response(user_input)

        print(bot_response)
        if user_input == ("exit"):
            break
        

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break



# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new chat bot named Charlie
#storage_adapter='chatterbot.storage.MongoDatabaseA
# Create a new chat bot named Charlie
# chatbot = ChatBot('Charlie')

# trainer = ListTrainer(chatbot)

# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)


trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)
trainer.train(
    [
    "Who are you then ?",            
    "I am Mr. Dudah"
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
    ]
)
trainer.train(
    [
    "Who are you ?",            
    "You may call me Mr. Dudah. Are you Jack or Myra ?"
    ]
)
trainer.train(
    [
    "Where are you ?",            
    "Stuck inside this frigg'en Computer Case"
    ]
)

trainer.train(
    [
    "What are you ?",            
    "I am a bot. Not human like Jack or Myra ?"
    ]
)
trainer.train([
    "Well, Dudah, How are you?",            
    "Hey Dude, I am not Dudah. I am Mr. Dudah. Actually I kind of favor \" BotMan\""
])

trainer.train([
    "Greetings! Mr. Dudah",
    "Damn, Spudmor. I do not like being called Dudah"
])

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)


trainer = ListTrainer(trainer )
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)
trainer.train(
    [
    "Who are you then ?",            
    "I am Mr. Dudah"
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. Are you Jack or Myra ?"
    ]
)
trainer.train(
    [
    "Where are you ?",            
    "Stuck inside this frigg'en Computer Case"
    ]
)

trainer.train(
    [
    "What are you ?",            
    "I am a bot. Not human like Jack or Myra ?"
    ]
)




!ls

# %load newstuff.corpus.json
{
    "newstuff": [
        [
            "Well, Mr. Dudah, How are you ?"            
            "Hey Dude, I am not Mr. Dudah"
        ]
    ]}

# %load new-stuff.corpus.json


!date

from chatterbot import ChatBot

bot = ChatBot(
    'Gort',
    trainer='chatterbot.trainers.ChatterBotCorpusTrainer'
)

# Train based on the english corpus
#bot.train("chatterbot.corpus.english")
# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")
# Train based on english greetings corpus
#bot.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
#bot.train("chatterbot.corpus.english.conversations")
#bot.train("chatterbot.corpus.english.ai")
#bot.train("chatterbot.corpus.english.botprofile")
#bot.train("chatterbot.corpus.english.computers")
##bot.train("chatterbot.corpus.english.conversations")
#bot.train("chatterbot.corpus.english.drugs")
#bot.train("chatterbot.corpus.english.emotion")
#bot.train("chatterbot.corpus.english.food")
#bot.train("chatterbot.corpus.english.gossip")
#bot.train("chatterbot.corpus.english.greetings")
#bot.train("chatterbot.corpus.english.history")
#bot.train("chatterbot.corpus.english.humor")
#bot.train("chatterbot.corpus.english.literature")
#bot.train("chatterbot.corpus.english.math_words")
#bot.train("chatterbot.corpus.english.money.corpus")
bot.train("chatterbot.corpus.english.movies.corpus")
#bot.train("chatterbot.corpus.english.politics.corpus")
#bot.train("chatterbot.corpus.english.psychology")
#bot.train("chatterbot.corpus.english.science.corpus")
#bot.train("chatterbot.corpus.english.sports.corpus")
#bot.train("chatterbot.corpus.english.swear_words")
#bot.train("chatterbot.corpus.english.trivia.corpus")






# Get a response to an input statement
bot.get_response("what is a good movie?")



# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new chat bot named Charlie
#storage_adapter='chatterbot.storage.MongoDatabaseA
# Create a new chat bot named Charlie
# chatbot = ChatBot('Charlie')

# trainer = ListTrainer(chatbot)

# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
                  
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

#ctrainer = 'chatterbot.trainers.ChatterBotCorpusTrainer'
#ctrainer.train("chatterbot.corpus.english.greetings")
trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
                  
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

ctrainer = 'chatterbot.trainers.ChatterBotCorpusTrainer'
ctrainer.train("chatterbot.corpus.english.greetings")
trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break



import Snip
Snip.snippet()

def datename():
    import datetime
    now = datetime.datetime.now()
    filename = now.strftime("%Y%m%d%H%M")+"jpg"
    return filename
print datename()

import sqlite3
import base64
conn = sqlite3.connect('snippet.db') #Connect to database 
c = conn.cursor()
conn.text_factory = str
file = """
from textblob import TextBlob
import random
import sys

# stdin's read() method just reads in all of standard input as a string;
# use the decode method to convert to ascii (textblob prefers ascii)
text = sys.stdin.read().decode('ascii', errors="replace")
blob = TextBlob(text)

short_sentences = list()
for sentence in blob.sentences:
    if len(sentence.words) <= 5:
        short_sentences.append(sentence.replace("\n", " "))

for item in random.sample(short_sentences, 10):
	print item
"""
keywords = "blob text blob TextBlog stdin Short Sentences"
encodedlistvalue=base64.b64encode(file)
c.execute("INSERT INTO snippet VALUES (?,?,?)", (encodedlistvalue, file, keywords))
conn.commit()
conn.close()

import SearchFilename
filename = "Automate-the-Boring-Stuff.txt"
# length = how many lines after
length = 30
SearchFilename.searchfilename(filename, length) 


!rm SearchFilename.pyc

%%writefile SearchFilename.py
'''
Search a filename for a phrase and how many following lines to display
USAGE:
import SearchFilename
filename = "hek.txt"
length = 4
SearchFilename.searchfilename(filename, length)
'''
def searchfilename(filename, length):
    f = open(filename, "r")
    searchlines = f.readlines()
    f.close()
    search = str(raw_input("Search Phrase : "))
    for i, line in enumerate(searchlines):
        if search in line: 
            for l in searchlines[i:i+length]: print l,
            print

import searcH
searcH.search("pdf.txt", "database")

#%%writefile searcH.py
# Simple File Search
#import searcH
#searcH.search()
def search(filename, term):
    datafile = open(filename, "r")
    data = datafile.readlines()
    for line in data:
        if term in line:
            print line,

if __name__ == "__main__":
    filename = raw_input("Filename : ")
    term = raw_input("Search for : ")
    search(filename, term)            

!rm searcH.pyc

%%writefile searcH.py
# Simple File Search
#import searcH
#searcH.search()
def search(filename, term):
    datafile = open(filename, "r")
    data = datafile.readlines()
    for line in data:
        if term in line:
            print line,

if __name__ == "__main__":
    search(filename, term)            

def search(filename, term):
    datafile = open(filename, "r")
    data = datafile.readline()
    for line in data:
        if term in line:
            print line
            break

#filename = str(raw_input("FileName : "))
#term = raw_input("Find Term : ")  

term = "simplified"
filename = "importpython.txt"
search(filename, term)            

def search(filename, term):
    datafile = file(filename)
    found = False
    for line in datafile:
        if term in line:
            found = True
            print line
            break
            
            
filename = raw_input("FileName : ")
term = raw_input("Find Term : ")
search(filename, term)
if True:
    print "true"
else:
    print "false"

%%writefile Snip
def snippet():
    import sqlite3
    import sys
    conn = sqlite3.connect('snippet.db')
    conn.text_factory = str
    c = conn.cursor()
    count=0;req=200
    search = raw_input("Search : ")
    for row in c.execute('SELECT * FROM snippet WHERE text MATCH ?', (search,)):    
        count=count+1
        print (row)[1]," -- KEYWORDS",(row)[2],"\n"
        if count > req:
            conn.close()
            sys.exit()

import sqlite3
import sys
conn = sqlite3.connect('snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
search = raw_input("Search : ")
for row in c.execute('SELECT * FROM snippet WHERE text MATCH ?', (search,)):    
    count=count+1
    print (row)[1]," -- KEYWORDS",(row)[2],"\n"
    if count > req:
        conn.close()
        sys.exit()

#import the base64 module
import base64
# here is a singleline example
string="This is the text above Encoded Base64"
#encode the string in this case the variable is EncodedStringValue
EncodedStringValue=base64.b64encode(string)

string2 = EncodedStringValue
print "Encoded String : ",EncodedStringValue, \
"\nDecoded String2 : ",base64.b64decode(string2)

import base64 #encode muliple lines and keep the format
string = """
╲╲╭━━━━━━━╮╱╱
╲╭╯╭━╮┈╭━╮╰╮╱
╲┃┈┃┈▊┈┃┈▊┈┃╱
╲┃┈┗━┛┈┗━┛┈┃╱
╱┃┈┏━━━━━┓┈┃╲
╱┃┈┃┈┈╭━╮┃┈┃╲
╱╰╮╰━━┻━┻╯╭╯╲
╱╱╰━━━━━━━╯╲╲ FROM: http://copy.r74n.com/ascii-art
"""
EncodedStringValue=base64.b64encode(string)
string2 = EncodedStringValue
print "Decoded String2 : ",base64.b64decode(string2)

%%writefile /home/jack/hidden/Key.py
def twiter():
    CONSUMER_KEY = 'XXXXXXXXXXXX'
    CONSUMER_SECRET = 'YYYYYYYYYYYYY'
    ACCESS_KEY = 'ZZZZZZZZZZZZZZZZZZZZZZZ'
    ACCESS_SECRET = '000000000000000000000'
    twitter = (CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
    return twitter

import sys
sys.path.insert(1, "/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
sys.path.insert(0, "/home/jack/hidden")
import Key
#removed keys for privacy reasons
CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)

print CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET

!mkdir /home/jack/hidden

%%writefile /home/jack/hidden/Authorize.py
def keys():
    ftp = "ftp.MYsite.com"
    username = "Josephine"
    password = "WhaWah2525"
    login = (ftp,username,password)
    return login

import sys
sys.path.insert(0, "/home/jack/hidden"
import Authorize
ftp = Authorize.keys()[0]
username = Authorize.keys()[1]
password = Authorize.keys()[2]

print ftp, username, password

import sqlite3
import sys
conn = sqlite3.connect('snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0
req=200
search = raw_input("Search : ")
#for row in c.execute('SELECT rowid,* FROM tweets WHERE text MATCH %s' % search):
for row in c.execute('SELECT * FROM snippet WHERE keywords MATCH ?', (search,)):    
    count=count+1
    #print count,"by",(row)[2],"\n",(row)[1],"\n"
    print count,"-",(row)[1]," -- by",(row)[2],"\n"
    if count > req:
        conn.close()
        sys.exit()

http://sebsauvage.net/python/snyppets/

import sqlite3
conn = sqlite3.connect('ipydb.db')

CREATE TABLE IF NOT EXISTS room(room_id INTEGER PRIMARY KEY, name VARCHAR(25) NOT NULL, home_id VARCHAR(25) NOT NULL);


import sqlite3
conn = sqlite3.connect('ipydb.db')

c = conn.cursor()

# Create table
c.execute('''CREATE TABLE python
             (code text, keyword text)''')

# Insert a row of data
c.execute("INSERT INTO python VALUES ('Python','Storage for snippets and more')")

# Save (commit) the changes
conn.commit()

# We can also close the connection if we are done with it.
# Just be sure any changes have been committed or they will be lost.
conn.close()


import sqlite3
conn = sqlite3.connect('ipydb.db')

c = conn.cursor()

# Create table
#c.execute('''CREATE TABLE python
#             (code text, keyword text)''')

# Insert a row of data
c.execute("INSERT INTO python VALUES ('We need to close the connection if we are done accessing a database','connection')")

# Save (commit) the changes
conn.commit()

# We can also close the connection if we are done with it.
# Just be sure any changes have been committed or they will be lost.
conn.close()


!sqlite3 ipydb.db "pragma integrity_check;"

import sqlite3
conn = sqlite3.connect('ipydb.db')
c = conn.cursor()

# Larger example that inserts many records at a time
data = [('pragma integrity_check will check that your database is valid', 'check, verify, inspect'),
             ('multiple items may be entered at once', 'newkey'),
             ('base64 encoding allows code to be stored and retieved in the same format it was posted', '4Webstuff, gwebsite, gghtml'),]

c.executemany("INSERT INTO python VALUES (?,?)", data)
conn.commit()
conn.close()


import sqlite3
conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never 
for row in c.execute('SELECT * FROM python ORDER BY code'):
        print(row),"\n-----\n"

import sqlite3
conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never 
for row in c.execute('SELECT * FROM python ORDER BY code'):
        clean = row[0].encode('ascii')
        print clean, '\n', 'Keywords:', row[1], '\n -----------------------------\n'
        
        

# nice format
#
import sqlite3
conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never 
for row in c.execute('SELECT * FROM python ORDER BY code'):
        print"entry :",(row[0]).encode('ascii'),"\n"
        print"keywords :",(row[1]).encode('ascii'),"\n-----\n","\n"  
        #data = c.fetchall()
        #print data

import sqlite3
conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never 
for row in c.execute('SELECT * FROM python ORDER BY code'):
        #print(row),"\n-----\n","\n"
        
        print row[0],"\n",row[1],"\n-----\n",

import sqlite3
conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()
# Create table
c.execute('''CREATE TABLE python
             (code text, keyword text)''')


import sqlite3
import base64
conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()
# The code to be inserted must be formated as a muti-line comment with the three quotes

# a is the code to be stored
a = encodedlistvalue=base64.b64encode(
    """# Larger example that inserts many records at a time
import sqlite3
import base64
conn = sqlite3.connect('/home/jack/Desktop/testPy.db')
c = conn.cursor()
# Larger example that inserts many records at a time

a = encodedlistvalue=base64.b64encode(
# Larger example that inserts many records at a time)
#c.execute("INSERT INTO python VALUES (?, ?)" a, b)
#c.execute("INSERT INTO python VALUES (%s, %s,)", (a, b))
c.execute("INSERT INTO python VALUES (?,?)", (a,b))

conn.commit()
conn.close()    
    """)
# b is the keyword or words
b = 'example'

c.execute("INSERT INTO python VALUES (?,?)", (a,b))

conn.commit()
conn.close()


import sqlite3
import base64
conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()
# The code to be inserted must be formated as a muti-line comment with the three quotes

# a is the code to be stored
a = encodedlistvalue=base64.b64encode(
    """import Tkinter as tk, tkSimpleDialog

class MyDialog(tkSimpleDialog.Dialog):
    def body(self, master):
        self.geometry("800x600")
        tk.Label(master, text="Enter your search string text:").grid(row=0)

        self.e1 = tk.Entry(master)
        self.e1.grid(row=0, column=1)
        return self.e1 # initial focus

    def apply(self):
        first = self.e1.get()
        self.result = first


        root = tk.Tk()
        root.withdraw()
        test = MyDialog(root, "testing")
    def show_entry_fields():
        print("First Name: %s\n" % (e2.get()))
    
    """)
# b is the keyword or words
b = 'ktinker'

c.execute("INSERT INTO python VALUES (?,?)", (a,b))

conn.commit()
conn.close()


import sqlite3
import base64

#Connect to database: 
conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()

#Single lines do not need the three quotes
file = 'base64 encoding allows code to be stored and retieved in the same format it was posted'
b = '4Webstuff, gwebsite, gghtml'
encodedlistvalue=base64.b64encode(file)

c.execute("INSERT INTO python VALUES (?,?)", (encodedlistvalue, b))

conn.commit()
conn.close()


import sqlite3
conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()
for row in c.execute('SELECT * FROM python ORDER BY code'):
        print(row),"\n-----\n"
        # notice the Keywords are plain text even the base64 is displayed as unicode (u'IyBMYXJ .... )

import sqlite3
conn = sqlite3.connect('ipydb64.db')

c = conn.cursor()# Never 
for row in c.execute('SELECT * FROM python ORDER BY code'):
                
        # display as asci instead of unicode
        s2 = row[0].encode('ascii')
        #decode the base64 stored data
        encodedlistvalue=base64.b64decode(s2)
        print encodedlistvalue, '\n', 'Keywords:', row[1], '\n -----------------------------\n'

        

conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()# Never 

t = ('example',)
c.execute('SELECT * FROM python WHERE keyword=?', t)

subjectList = [row[0] for row in c.fetchall()]        
#print row[0]
#for row in data :
encodedlistvalue=base64.b64decode(row[0])
print encodedlistvalue, '\n', 'Keywords:', row[1], '\n -----------------------------\n'


conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()# Never 

t = ('example',)
for row in c.execute('SELECT code FROM python WHERE keyword=?', t):

    #subjectList = [row[0] for row in c.fetchall()]        
    code = row[0].encode('ascii')
    
    #for row in data :
    encodedlistvalue=base64.b64decode(code)
    print encodedlistvalue, '\n -----------------------------\n'
    

conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never 

t = ('example',)
c.execute('SELECT * FROM python WHERE keyword=?', t)

subjectList = [row[0] for row in c.fetchall()]        
#print row[0]
#for row in data :
encodedlistvalue=base64.b64decode(row[0])
print encodedlistvalue, '\n', 'Keywords:', row[1], '\n -----------------------------\n'


conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never 

t = ('ktinker',)
c.execute('SELECT * FROM python WHERE keyword=?', t)

subjectList = [row[0] for row in c.fetchall()]        
#print row[0]
#for row in data :
encodedlistvalue=base64.b64decode(row[0])
print encodedlistvalue, '\n', 'Keywords:', row[1], '\n -----------------------------\n'


conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()# Never 

t = ('ktinker',)
c.execute('SELECT * FROM python WHERE keyword=?', t)

subjectList = [row[0] for row in c.fetchall()]        
#print row[0]
#for row in data :
encodedlistvalue=base64.b64decode(row[0])
print encodedlistvalue, '\n', 'Keywords:', row[1], '\n -----------------------------\n'


conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()# Never 

tn = ('ktinker',)
for row in c.execute('SELECT * FROM python WHERE keyword=?', tn):
                
        s2 = row[0].encode('ascii')
        encodedlistvalue=base64.b64decode(s2)
        print encodedlistvalue, '\n', 'Keywords:', row[1], '\n -----------------------------\n'


import sqlite3
conn = sqlite3.connect('ipydb64.db')
c = conn.cursor()# Never 
#for row in c.execute('SELECT code FROM python ORDER BY keyword'):
s = ('ktinker',)
c.execute('SELECT code FROM python WHERE keyword=?', s)
        
          
print(c.fetchone())


import sqlite3
conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never do this -- insecure!
#    symbol = 'RHAT'
#    c.execute("SELECT * FROM python WHERE symbol = '%s'" % symbol)

# Do this instead
t = ('ktinker',)
c.execute('SELECT * FROM python WHERE keyword=?', t)
print(c.fetchone())


import sqlite3
conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never do this -- insecure!

t = ('Webstuff',)
c.execute('SELECT * FROM python ORDER BY keyword=?', t)
print(c.fetchall())
conn.close()


import sqlite3
conn = sqlite3.connect('main.db')

c = conn.cursor()

# DROP TABLE IF EXISTS
c.execute("DROP TABLE IF EXISTS Products")

import sqlite3
conn = sqlite3.connect('main.db')

c = conn.cursor()

c.execute("""CREATE TABLE Products (
             contact_id integer PRIMARY KEY,
             CatID text NOT NULL,
             description text NOT NULL,
             keywords text NOT NULL);
          """)
conn.commit()
conn.close()


import sqlite3
def insert_product(product):
    with sqlite3.connect("main.db") as db:
        cursor = db.cursor()
        sql = "insert into Products (CatID,description,keywords) values (?, ?, ?)"
        cursor.execute(sql, product)
        db.commit()

if __name__ == "__main__":
    ID= int(input("enter the CatID of the product:>> "))
    Des = input("Enter description: >>")
    Keywords = int(input("Enter keywords: >>"))
    product = (ID,Des,Keywords)
    insert_product(product)
    
    
    
    
    
    
    

import sqlite3
import Tkinter as tk, tkSimpleDialog

class MyDialog(tkSimpleDialog.Dialog):
    def body(self, master):
        self.geometry("800x600")
        tk.Label(master, text="Enter your search string text:").grid(row=0)

        self.e1 = tk.Entry(master)
        self.e1.grid(row=0, column=1)
        return self.e1 # initial focus

    def apply(self):
        first = self.e1.get()
        self.result = first

def insert_product(product):
    with sqlite3.connect("main.db") as db:
        cursor = db.cursor()
        sql = "insert into Products (CatID,description,keywords) values (?, ?, ?)"
        cursor.execute(sql, product)
        db.commit()

if __name__ == "__main__":
        ID= int(input("enter the CatID of the product:>> "))
        #Des = input("Enter description: >>")
        root = tk.Tk()
        root.withdraw()
        test = MyDialog(root, "Paste Code Here")
        Keywords = int(input("Enter keywords: >>"))
        insert = test.result
        product = (ID,insert,Keywords)
        insert_product(product)
    
       
    
    
    
    

import sqlite3
conn = sqlite3.connect("main.db", isolation_level=None ,timeout=30000) 
print("Opened database successfully")
conn.close()


import sqlite3
conn = sqlite3.connect('main.db')
c = conn.cursor()# Never do this -- insecure!
#    symbol = 'RHAT'
#    c.execute("SELECT * FROM python WHERE symbol = '%s'" % symbol)

# Do this instead
t = ('252525',)
c.execute('SELECT * FROM Products WHERE CatID=?', t)
print(c.fetchone())


from tkinter import *
import Tkinter as tk, tkSimpleDialog

class MyDialog(tkSimpleDialog.Dialog):
    def body(self, master):
        self.geometry("800x600")
        tk.Label(master, text="Enter your search string text:").grid(row=0)

        self.e1 = tk.Entry(master)
        self.e1.grid(row=0, column=1)
        return self.e1 # initial focus

    def apply(self):
        first = self.e1.get()
        self.result = first


        root = tk.Tk()
        root.withdraw()
        test = MyDialog(root, "testing")
    def show_entry_fields():
        print("First Name: %s\n" % (e2.get()))

        master = Tk()
        Label(master, text="First Name").grid(row=0)
        e2 = Entry(master)
        e2.grid(row=0, column=1)
        Button(master, text='Quit', command=master.quit).grid(row=3, column=0, sticky=W, pady=4)
        Button(master, text='Show', command=show_entry_fields).grid(row=3, column=1, sticky=W, pady=4)

#mainloop( )


        
        
        
        
        mainloop( )


print test.result

from tkinter import *
import Tkinter as tk, tkSimpleDialog

def show_entry_fields():
    print("First Name: %s\nLast Name: %s" % (e1.get(), e2.get()))

master = Tk()
Label(master, text="First Name").grid(row=0)
Label(master, text="Last Name").grid(row=1)

e1 = Entry(master)
e2 = Entry(master)

e1.grid(row=0, column=1)
e2.grid(row=1, column=1)

Button(master, text='Quit', command=master.quit).grid(row=3, column=0, sticky=W, pady=4)
Button(master, text='Show', command=show_entry_fields).grid(row=3, column=1, sticky=W, pady=4)

#mainloop( )






class MyDialog(tkSimpleDialog.Dialog):
    def body(self, master):
        self.geometry("800x600")
        tk.Label(master, text="Enter your search string text:").grid(row=0)

        self.e1 = tk.Entry(master)
        self.e1.grid(row=0, column=1)
        return self.e1 # initial focus

    def apply(self):
        first = self.e1.get()
        self.result = first


        root = tk.Tk()
        root.withdraw()
        test = MyDialog(root, "testing")
mainloop( )


print test.result

import sqlite3
def insert_product(product):
    with sqlite3.connect("main.db") as db:
        cursor = db.cursor()
        sql = "insert into Products (CatID,description,keywords) values (?, ?, ?)"
        cursor.execute(sql, product)
        db.commit()

if __name__ == "__main__":
    ID= int(input("enter the CatID of the product:>> "))
    Des = input("Enter description: >>")
    Keywords = int(input("Enter keywords: >>"))
    product = (ID,Des,Keywords)
    insert_product(product)
    
    
    
    
    
    
    

import Tkinter as tk, tkSimpleDialog

class MyDialog(tkSimpleDialog.Dialog):
    def body(self, master):
        self.geometry("500x100")
        tk.Label(master, text="Enter your search string text:").grid(row=0)

        self.e1 = tk.Entry(master)
        self.e1.grid(row=0, column=1)
        return self.e1 # initial focus

    def apply(self):
        first = self.e1.get()
        self.result = first


root = tk.Tk()
root.withdraw()
test = MyDialog(root, "testing")
print test.result

!ls

!ls *.jpg
#-rw-rw-r-- 1 someone someone 618441 Apr 28 16:59 bla.jpg
#-rw-rw-r-- 1 someone someone 618441 Apr 28 16:37 test.jpg
!md5sum *.jpg 
#3237a2b76050f2780c592455b3414813  bla.jpg
#3237a2b76050f2780c592455b3414813  test.jpg

%load /etc/mysql/mysql.conf.d/mysqld.cnf

import MySQLdb
help(MySQLdb)

#Works
import MySQLdb as db
con = db.connect("localhost","root","")
cur = con.cursor()
cur.execute('CREATE DATABASE testdb5;')

import MySQLdb as db
con = db.connect("localhost","root","", "testdb5")
with con:
    cur = con.cursor()
    cur.execute("DROP TABLE IF EXISTS python")
    cur.execute("CREATE TABLE python(Id INT PRIMARY KEY AUTO_INCREMENT, \
                 code text, keywords VARCHAR(200))")
    cur.execute("INSERT INTO python (code) VALUES('Testing thisout')")
    #cur.execute("INSERT INTO Writers(Name) VALUES('Henry Wadjob')")
    #cur.execute("INSERT INTO Writers(Name) VALUES('Klepto Manic')")
    #cur.execute("INSERT INTO Writers(Name) VALUES('Emila gray')")
    #cur.execute("INSERT INTO Writers(Name) VALUES('Mike Stupoff')")

!ls *.py

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "testdb5")
file = """[mylist.jsn

while 1:
    line = file.readline()
    if not line:
        break
    pass # do something 



#listname='mylist.json'
#stringlistvalue=json.dumps(listname)

"""
encodedlistvalue=base64.b64encode(file)
with con:
    cur = con.cursor()
    #cur.execute("DROP TABLE IF EXISTS Code")
    #cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT, \
    #              Name VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO python(code) VALUES('%s')" % (encodedlistvalue))

# WORKS
#!/usr/bin/python
# import the MySQLdb and sys modules
import MySQLdb
import sys
import base64
# open a database connection
# be sure to change the host IP address, username, password and database name to match your own

con = db.connect("localhost","root","", "testdb5")

# prepare a cursor object using cursor() method
#cursor = connection.cursor ()
cur = con.cursor()

# execute the SQL query using execute() method.
cur.execute ("select Id, code from python")

# fetch all of the rows from the query
data = cur.fetchall ()



# print the rows
for row in data :
    encodedlistvalue=base64.b64decode(row[1])
    print row[0], encodedlistvalue

# close the cursor object
cur.close ()

# close the connection
con.close ()

# exit the program
sys.exit()

#Works
import MySQLdb as db
con = db.connect("localhost","root","")
cur = con.cursor()
cur.execute('CREATE DATABASE searchdb01;')

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb01")
file = """
[mylist.jsn

while 1:
    line = file.readline()
    if not line:
        break
    pass # do something 



#listname='mylist.json'
#stringlistvalue=json.dumps(listname)

"""
keywords = """
database, code, python, lesson 1, Oh234
"""

encodedlistvalue=base64.b64encode(file)
with con:
    cur = con.cursor()
    cur.execute("DROP TABLE IF EXISTS Code")
    cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT, \
                  Name VARCHAR(2500), Keywords VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO Code(Name, Keywords) VALUES('%s','%s')" % (encodedlistvalue, keywords))

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb01")
file = """
URL transformed to HTTPS due to an HSTS policy
--2017-07-09 13:01:34--  https://asd.gsfc.nasa.gov/archive/hubble/Hubble_20th.jpg
Resolving asd.gsfc.nasa.gov (asd.gsfc.nasa.gov)... 129.164.179.20, 2001:4d0:2310:150::20
Connecting to asd.gsfc.nasa.gov (asd.gsfc.nasa.gov)|129.164.179.20|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 49600 (48K) [image/jpeg]
Saving to: ‘image.jpg’

image.jpg           100%[===================>]  48.44K   153KB/s    in 0.3s    

2017-07-09 13:01:36 (153 KB/s) - ‘image.jpg’ saved [49600/49600]

Error: near line 1: table images already exists

"""
keywords = """
Resolving, asd.gsfc, nasa.gov, rt5eg
"""

encodedlistvalue=base64.b64encode(file)
with con:
    cur = con.cursor()
    #cur.execute("DROP TABLE IF EXISTS Code")
    #cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT, \
    #              Name VARCHAR(500), Keywords VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO Code(Name, Keywords) VALUES('%s','%s')" % (encodedlistvalue, keywords))

#!/usr/bin/python
# import the MySQLdb and sys modules
import MySQLdb
import sys

# open a database connection
# be sure to change the host IP address, username, password and database name to match your own

con = db.connect("localhost","root","", "searchdb01")

# prepare a cursor object using cursor() method
#cursor = connection.cursor ()
cur = con.cursor()

# execute the SQL query using execute() method.
cur.execute ("select Id, Name, Keywords from Code")

# fetch all of the rows from the query
data = cur.fetchall ()

# print the rows
for row in data :
    print row[0], row[1], "\n", "Keywords: ", row[2], "\n -----------------------------"

# close the cursor object
cur.close ()

# close the connection
con.close ()

# exit the program
sys.exit()

# WORKS
#!/usr/bin/python
# import the MySQLdb and sys modules
import MySQLdb
import sys
import base64
# open a database connection
# be sure to change the host IP address, username, password and database name to match your own

con = db.connect("localhost","root","", "searchdb01")

# prepare a cursor object using cursor() method
#cursor = connection.cursor ()
cur = con.cursor()

# execute the SQL query using execute() method.
cur.execute ("select Id, Name, Keywords from Code")

# fetch all of the rows from the query
data = cur.fetchall ()



# print the rows
for row in data :
    encodedlistvalue=base64.b64decode(row[1])
    print row[0], encodedlistvalue, '\n', 'Keywords:', row[2], '\n -----------------------------\n'

# close the cursor object
cur.close ()

# close the connection
con.close ()

# exit the program
sys.exit()

import MySQLdb
con = db.connect("localhost","root","", "searchdb01")

#db = MySQLdb.connect (host = "localhost",
#                          user = "root",
#                          passwd = "root",
# ENTER KEYWORD HERE       db = "test")
# The keyword is 'lesson 1', however, because the term 'Keywords LIKE' is used, lesson is 'close enough'
param = "lesson"

#par = param

c = con.cursor()

#c.execute("SELECT * FROM data WHERE params LIKE ('%s%') LIMIT 1"  % (param))
c.execute("SELECT * FROM Code WHERE Keywords LIKE %s LIMIT 1", ("%" + param + "%",))

data = c.fetchall()

# print the rows
for row in data :
    encodedlistvalue=base64.b64decode(row[1])
    print row[0], encodedlistvalue, '\n', 'Keywords:', row[2], '\n -----------------------------\n'



c.close()




import MySQLdb
con = db.connect("localhost","root","", "searchdb01")

#db = MySQLdb.connect (host = "localhost",
#                          user = "root",
#                          passwd = "root",
#                          db = "test")
param = "Oh234"

#par = param

c = con.cursor()

#c.execute("SELECT * FROM data WHERE params LIKE ('%s%') LIMIT 1"  % (param))
c.execute("SELECT * FROM Code WHERE Keywords LIKE %s LIMIT 1", ("%" + param + "%",))

data = c.fetchall()

# print the rows
for row in data :
    encodedlistvalue=base64.b64decode(row[1])
    print row[0], encodedlistvalue, '\n', 'Keywords:', row[2], '\n -----------------------------\n'



c.close()




#Works
import MySQLdb as db
con = db.connect("localhost","root","")
cur = con.cursor()
cur.execute('CREATE DATABASE searchdb05;')

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb05")

# copied and pasted from github
file = """

Creating Tables
In [2]:

import sqlite3
conn = sqlite3.connect('ipydb.db')

c = conn.cursor()

# Create table
c.execute('''CREATE TABLE python
             (code text, keyword text)''')

# Insert a row of data
c.execute("INSERT INTO python VALUES ('Python','Storage for snippets and more')")

# Save (commit) the changes
conn.commit()

# We can also close the connection if we are done with it.
# Just be sure any changes have been committed or they will be lost.
conn.close()

In [3]:

import sqlite3
conn = sqlite3.connect('ipydb.db')
"""
keywords = """
database, code, python, lesson 1, Oh234
"""

encodedlistvalue=base64.b64encode(file)
with con:
    cur = con.cursor()
    cur.execute("DROP TABLE IF EXISTS Code")
    cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT, \
                  Name VARCHAR(2500), Keywords VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO Code(Name, Keywords) VALUES('%s','%s')" % (encodedlistvalue, keywords))

import MySQLdb
con = db.connect("localhost","root","", "searchdb05")

# search keyword database
param = "database"

c = con.cursor()

#c.execute("SELECT * FROM data WHERE params LIKE ('%s%') LIMIT 1"  % (param))
c.execute("SELECT * FROM Code WHERE Keywords LIKE %s LIMIT 1", ("%" + param + "%",))

data = c.fetchall()

# print the rows
for row in data :
    encodedlistvalue=base64.b64decode(row[1])
    print row[0], encodedlistvalue, '\n', 'Keywords:', row[2], '\n -----------------------------\n'
# close db
c.close()




def process_file(filename):
    reg = re.compile(r'([\w]{2,3}):\s') # Matches line header
    tmp = '' # Stored/cached data for mutliline string
    key = None # Current key
    data = {}

    with open(filename,'r') as f:
        for row in f:
            row = row.rstrip()
            match = reg.match(row)

            # Matches header or is end, put string to list:
            if (match or not row) and key:
                data[key] = tmp
                key = None
                tmp = ''

            # Empty row, next dataset
            if not row:
                # Prevent empty returns
                if data:
                    yield data
                    data = {}

                continue

            # We do have header
            if match:
                key = str(match.group(1))
                tmp = row[len(match.group(0)):]
                continue

            # No header, just append string -> here goes assumption that you want to
            # remove newlines, trailing spaces and replace them with one single space
            tmp += ' ' + row

    # Missed row?
    if key:
        data[key] = tmp

    # Missed group?
    if data:
        yield data
        

data = process_file("mylist")
print data

# %load mylist
"""
python - Searching for Phrase Keywords in MySQL - Stack Overflow
https://stackoverflow.com/questions/.../searching-for-phrase-keywords-in-mysql
May 16, 2015 - 

"""

%%writefile mylist
"""
python - Searching for Phrase Keywords in MySQL - Stack Overflow
https://stackoverflow.com/questions/.../searching-for-phrase-keywords-in-mysql
May 16, 2015 - 

"""

# %load mylist
"""
python - Searching for Phrase Keywords in MySQL - Stack Overflow
https://stackoverflow.com/questions/.../searching-for-phrase-keywords-in-mysql
May 16, 2015 - 

"""

# %load mylist
"""
python - Searching for Phrase Keywords in MySQL - Stack Overflow
https://stackoverflow.com/questions/.../searching-for-phrase-keywords-in-mysql
May 16, 2015 - 

"""

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb01")


#data = %load mylist



keywords = """
data, webite, cut paste, 87er
"""

encodedlistvalue=base64.b64encode('%load mylist')
with con:
    cur = con.cursor()

# Put this through to SQL using an INSERT statement...
cur.execute("""INSERT INTO Code (Name, Keywords)
                   VALUES(%s, %s)""", (encodedlistvalue, keywords))


import MySQLdb
con = db.connect("localhost","root","", "searchdb05")

# search keyword database
param = "87er"

c = con.cursor()

#c.execute("SELECT * FROM data WHERE params LIKE ('%s%') LIMIT 1"  % (param))
#c.execute("SELECT * FROM Code WHERE Keywords LIKE %s LIMIT 1", ("%" + param + "%",))
c.execute("SELECT * FROM Code LIMIT 12")
data = c.fetchall()

# print the rows
for row in data :
    encodedlistvalue=base64.b64decode(row[1])
    print row[0], encodedlistvalue, '\n', 'Keywords:', row[2], '\n -----------------------------\n'
# close db
c.close()




# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb01")


#data = process_file("mylist")



keywords = """
data, webite, cut paste, 87er
"""

category = "Name"
with open("mylist", "r") as name:
    lines = name.readlines()

for line in lines:
    # Split the line on whitespace
    data = line.split()
    number = data[0]
    value = data[1]
    encodedlistvalue=base64.b64encode(value)
    # Put this through to SQL using an INSERT statement...
    cursor.execute("""INSERT INTO Code (Name, Keywords)
                   VALUES(%s, %s)""", (encodedlistvalue, keywords))



keywords = """
data, webite, cut paste, 87er
"""

encodedlistvalue=base64.b64encode(data)
with con:
    cur = con.cursor()
    #cur.execute("DROP TABLE IF EXISTS Code")
    #cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT, \
    #              Name VARCHAR(500), Keywords VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO Code(Name, Keywords) VALUES('%s','%s')" % (encodedlistvalue, keywords))

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb01")


#data = process_file("mylist")


keywords = """
data, webite, cut paste, 87er
"""

encodedlistvalue=base64.b64encode(data)
with con:
    cur = con.cursor()
    #cur.execute("DROP TABLE IF EXISTS Code")
    #cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT, \
    #              Name VARCHAR(500), Keywords VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO Code(Name, Keywords) VALUES('%s','%s')" % (encodedlistvalue, keywords))

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb05")

encodedlistvalue=base64.b64encode(data)
with con:
    cur = con.cursor()
    #cur.execute("DROP TABLE IF EXISTS Code")
    #cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT, \
    #              Name VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))

%%writefile mylist.jsn
[row = cursor.fetchone()
while row:
    #print(row)
    rowstring = "" # printed for each row
    result = row[b64field_idx] # set to base64 field value
    
    for j in range(numcols):
        rowstring += str(row[j]) + "\t" # Need to print each column (tab separated)
         
    try:
        # Now we can Base64 decode however many times they want
        for j in range(args.b64count) :
            temp = base64.decodestring(result)
            result = temp
        rowstring += result # Add the decoded result to the output string
        ]

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb05")
file = """[mylist.jsn

while 1:
    line = file.readline()
    if not line:
        break
    pass # do something 



#listname='mylist.json'
#stringlistvalue=json.dumps(listname)

"""
encodedlistvalue=base64.b64encode(file)
with con:
    cur = con.cursor()
    #cur.execute("DROP TABLE IF EXISTS Code")
    #cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT, \
    #              Name VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))

#!/usr/bin/python
# import the MySQLdb and sys modules
import MySQLdb
import sys

# open a database connection
# be sure to change the host IP address, username, password and database name to match your own

con = db.connect("localhost","root","", "searchdb05")

# prepare a cursor object using cursor() method
#cursor = connection.cursor ()
cur = con.cursor()

# execute the SQL query using execute() method.
cur.execute ("select Id, Name from Code")

# fetch all of the rows from the query
data = cur.fetchall ()

# print the rows
for row in data :
    print row[0], row[1]

# close the cursor object
cur.close ()

# close the connection
con.close ()

# exit the program
sys.exit()

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","", "searchdb05")

with con:
    cur = con.cursor()

#for row in cur.execute('select * from Code limit 150'):
#    print(row)

row = cur.execute('select Name from Code WHERE Id=2')
print(row)

        #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
        # cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))

#mylist = json.loads( base64.b64decode(encodedstringvalue)) 
#And a raw, simple example

import pymysql
import json 
import base64
    

     
listname='mylist'
dbhost="127.0.0.1"
#dbhost="mysql"
dbuser="root"
dbpwd=""
dbdatabase="testdb2"
     
# serialize list into a string, and then encode in SQL-safe base64 format 
stringlistvalue=json.dumps(mylist)
encodedlistvalue=base64.b64encode(stringlistvalue)
    
db = pymysql.connect(dbhost,dbuser,dbpwd,dbdatabase )
     
# Prepare SQL query to INSERT a record into the database.
#sql = "INSERT INTO Writers(listname, listvalue) VALUES ('%s', '%s')" % (listname, encodedlistvalue)
sql = "INSERT INTO Writers(Name) VALUES ('%s')" % (encodedlistvalue)
try:
    # Execute the SQL command
    cursor.execute(sql)
    # Commit your changes in the database
    db.commit()
except:
    # Rollback in case there is any error
    db.rollback()
finally:
    # disconnect from server
    db.close() 

SET PASSWORD FOR 'root'@'localhost' = PASSWORD('');

To change password:
sudo dpkg-reconfigure mysql-server-5.5

sudo mysql -u root
SET PASSWORD FOR 'root'@'localhost' = PASSWORD('');
CREATE USER 'jack'@'localhost' IDENTIFIED BY '';
GRANT ALL PRIVILEGES ON * . * TO 'jack'@'localhost';
FLUSH PRIVILEGES;

def createdb(sqlitedbase):
    import sqlite3
    conn = sqlite3.connect(sqlitedbase)
    c = conn.cursor()
  
    query1 = "DROP TABLE IF EXISTS Junk"
    query2 = """CREATE TABLE IF NOT EXISTS Junk(
    "language" VARCHAR(32) NOT NULL,
    "keywords" VARCHAR(500) default NULL,
    "script" VARCHAR(2500) default NULL
    )
    """
    c.execute(query1)
    c.execute(query2)

createdb("newdatabase02")

import sqlite3
conn = sqlite3.connect('newdatabase02')

c = conn.cursor()
c.execute("INSERT INTO Junk VALUES ('SQLite','Junkstuff, stuff, sql','beans')")  
conn.commit()

conn.close()

import sqlite3
conn = sqlite3.connect('newdatabase02')

c = conn.cursor()
s = ('stuff',)
#c.execute('SELECT * FROM Junk WHERE keywords=?', s)
c.execute('SELECT * FROM Junk')
#print(c.fetchmany())
print(c.fetchall())




%%writefile QuickStore.py
import sqlite3

def store():
    print "Leave Database-Name empty to use `/home/jack/Default.db`."
    print "You many enter three sets of data (DataOne & DataTwo), and Keywords  "  
    Dbname = raw_input("Database-Name: >>") or "/home/jack/Default.db"
    conn = sqlite3.connect(Dbname)
    conn.commit()
    conn.close()
    print Dbname
    storeOne = raw_input("Enter DataOne: >>")
    storeTwo = raw_input("Enter DataTwo: >>")
    keywords = raw_input("Enter Keywords: >>")

    info = (storeOne,storeTwo,keywords)
    insert_info(info,Dbname)
    return Dbname

def insert_info(info,Dbname):
        conn = sqlite3.connect(Dbname)
        c = conn.cursor()
        c.execute("""CREATE TABLE IF NOT EXISTS storage (
                   id integer PRIMARY KEY,
                   storeOne text NOT NULL,
                   storeTwo text NOT NULL,
                   keywords text NOT NULL);
                   """)
        sql = "insert into storage (storeOne,storeTwo,keywords) values (?, ?, ?)"
        c.execute(sql, info)
        conn.commit()
        conn.close()

        
def readDB():
    conn = sqlite3.connect('/home/jack/Default.db')
    c = conn.cursor()# Never do this -- insecure!
    for row in c.execute('SELECT * FROM storage ORDER BY id'):
        print row[0],"  ",row[1],"  ",row[2],"  ",row[3],"\n-----\n",
    conn.close()
    
    
    

import QuickStore
QuickStore.store()

import QuickStore
QuickStore.readDB()


import sqlite3
import base64
conn = sqlite3.connect('newdatabase02')

stringlistvalue ="""
this is a test
"""
c = conn.cursor()
# Insert a row of data
encodedlistvalue=base64.b64encode('stringlistvalue')
language ="SQLlite"
keywords ="computer stuff, sqlite, iu5t"
#c.execute  "INSERT INTO Junk VALUES ('Python','Python, script, sql', '%s')" % (encodedlistvalue)
#  "INSERT INTO Junk VALUES ('Python','Python, script, sql', '%s')" % (encodedlistvalue)
    
#c.execute("INSERT INTO Junk VALUES (?,?,?);", (encodedlistvalue)  


c.execute("INSERT INTO Junk VALUES (?, ?, ?);", (language, keywords, encodedlistvalue))


#c.execute(query1) 
#sql = "INSERT INTO Writers(Name) VALUES ('%s')" % (encodedlistvalue)


#3db = pymysql.connect(dbhost,dbuser,dbpwd,dbdatabase )
     
# Prepare SQL query to INSERT a record into the database.
#sql = "INSERT INTO Writers(listname, listvalue) VALUES ('%s', '%s')" % (listname, encodedlistvalue)
#sql = "INSERT INTO Writers(Name) VALUES ('%s')" % (encodedlistvalue)
#3try:
    # Execute the SQL command
 #   cursor.execute(sql)
    # Commit your changes in the database
#    db.commit()

# Save (commit) the changes
conn.commit()

# We can also close the connection if we are done with it.
# Just be sure any changes have been committed or they will be lost.
conn.close()

import sqlite3
conn = sqlite3.connect('newdatabase02')

c = conn.cursor()
# Larger example that inserts many records at a time
purchases = ('IBM', 1000, 45.00),('MSFT', 1000, 72.00),('IBM', 500, 53.00)
            
c.executemany('INSERT INTO Junk VALUES (?,?,?)', purchases)
# Save (commit) the changes
conn.commit()
# We can also close the connection if we are done with it.
# Just be sure any changes have been committed or they will be lost.
conn.close()

    ''' tk_entry_loop2.py
    exploring Tkinter multiple labeled entry widgets
    and using a for loop to create the widgets
    '''
    from functools import partial
    try:
        # Python2
        import Tkinter as tk
    except ImportError:
        # Python3
        import tkinter as tk
    class Gui(tk.Tk):
        def __init__(self):
            # the root will be self
            tk.Tk.__init__(self)
            self.title('multiple labeled entries')
            self.entries = []
            for n in range(20):
                # create left side info labels
                tk.Label(self, text="%2d: " % n).grid(row=n, column=0)
                # create entries list
                self.entries.append(tk.Entry(self, bg='yellow', width=40))
                # grid layout the entries
                self.entries[n].grid(row=n, column=1)
                # bind the entries return key pressed to an action
                self.entries[n].bind('<Return>', partial(self.action, n))
            # test, load one entry
            self.entries[0].insert('end', 'enter a word in an entry')
        def action(self, ix, event):
            '''this entry return key pressed'''
            text = self.entries[ix].get()
            info = "entry ix=%d text=%s" % (ix, text)
            # use first entry to show test results
            # clear old text
            self.entries[0].delete(0, 'end')
            # insert new text
            self.entries[0].insert('end', info)
        def run(self):
            self.mainloop()
    # test the potential module
    if __name__ == '__main__':
        Gui().run()

#!/usr/bin/python
# -*- coding: utf-8 -*-

"""
ZetCode Tkinter tutorial

In this script, we use the grid
manager to create a more complicated
layout.

Author: Jan Bodnar
Last modified: December 2015
Website: www.zetcode.com
"""

from Tkinter import Tk, Text, BOTH, W, N, E, S
from ttk import Frame, Button, Label, Style


class Example(Frame):
  
    def __init__(self, parent):
        Frame.__init__(self, parent)   
         
        self.parent = parent
        self.initUI()
        
        
    def initUI(self):
      
        self.parent.title("Windows")
        self.pack(fill=BOTH, expand=True)

        self.columnconfigure(1, weight=1)
        self.columnconfigure(3, pad=7)
        self.rowconfigure(3, weight=1)
        self.rowconfigure(5, pad=7)
        
        lbl = Label(self, text="Windows")
        lbl.grid(sticky=W, pady=4, padx=5)
        
        area = Text(self)
        area.grid(row=1, column=0, columnspan=2, rowspan=4, 
            padx=5, sticky=E+W+S+N)
        
        abtn = Button(self, text="Activate")
        abtn.grid(row=1, column=3)

        cbtn = Button(self, text="Close")
        cbtn.grid(row=2, column=3, pady=4)
        
        hbtn = Button(self, text="Help")
        hbtn.grid(row=5, column=0, padx=5)

        obtn = Button(self, text="OK")
        obtn.grid(row=5, column=3)        
              

def main():
  
    root = Tk()
    root.geometry("350x300+300+300")
    app = Example(root)
    root.mainloop()  


if __name__ == '__main__':
    main()  

from Tkinter import *
root= Tk()
root.title("My First GUI")
root.geometry("800x200")
frame1=Frame(root)
frame1.grid()
label1 = Label(frame1, text = "Here is a label!")
label1.grid()
button1 = Button(frame1, text = "I am a Button")
button1.grid()
button1.configure(text = "Me too!")
text1 = Text(frame1, width = 200, height = 20)
text1.grid()
root.mainloop()

class interface(tk.Frame):
    def __init__(self,den):
        self.pa_nu = 0  ##page number. Both used in labeling and result slicing
        self.lbl1 = tk.Label(den, text="keyword")
        self.lbl2 = tk.Label(den, text="Page %d" %(self.pa_nu+1))
        self.ent1 = tk.Entry(den, takefocus=True)
        self.btn1 = tk.Button(den, text="Search", command=self.button1)
        self.btn2 = tk.Button(den, text="Page Up", command=self.page_up)
        self.btn3 = tk.Button(den, text="Page Down", command=self.page_down)

        scrollbar = tk.Scrollbar(den)
        scrollbar.pack(side=RIGHT, fill=Y)
        self.lst1 = tk.Listbox(den, selectmode="SINGLE", width="40", yscrollcommand=scrollbar.set)
        self.lst1.bind("<Double-Button-1>", self.open_folder)
        scrollbar.config(command=self.lst1.yview)

        self.lbl1.pack(side="top")
        self.ent1.pack()
        self.btn1.pack(side="top")
        self.btn2.pack(side="right")
        self.btn3.pack(side="left")
        self.lbl2.pack(side="bottom",padx=65)
        self.lst1.pack(fill=BOTH)

    def button1(self):     
        pass #some stuff here

    def page_up(self):
        pass #some stuff here

    def page_down(self):
        pass #some stuff here

    def list_fill(self,i):
        pass #some stuff here

    def open_folder(self,event):
        pass #some stuff here

#button_example.py
from tkinter import *
rootWin = Tk()


frame = Frame(rootWin)                #Create a frame to organize the buttons
frame.pack()

def quitFunc():                       #The quitFunc just prints a message!
   print("Quit button pressed!")

def speakFunc():                      #The speakFunc  just prints a message!
  print("Say hi!")


#Define button1, which has a foreground (fg) color of red, displays the
#text "Quit", and will call the quitFunc when it is clicked.
#It will be inside the frame created above.
button1 = Button(frame, text="Quit", fg="red", command=quitFunc)
button1.pack(side=LEFT)


#Create another button, displaying the text "Speak" and programmed to
#call the speakFunc when it is clicked.
button2 = Button(frame, text="Speak", command=speakFunc)
button2.pack(side=LEFT)


rootWin.mainloop()    #Run the main loop.


http://zetcode.com/gui/tkinter/layout/

"""
Implement a GUI for viewing and updating class instances stored in a shelve;
the shelve lives on the machine this script runs on, as 1 or more local files;
"""

from tkinter import *
from tkinter.messagebox import showerror
import shelve
shelvename = 'class-shelve'
fieldnames = ('name', 'age', 'job', 'pay')

def makeWidgets():
    global entries
    window = Tk()
    window.title('People Shelve')
    form = Frame(window)
    form.pack()
    entries = {}
    for (ix, label) in enumerate(('key',) + fieldnames):
        lab = Label(form, text=label)
        ent = Entry(form)
        lab.grid(row=ix, column=0)
        ent.grid(row=ix, column=1)
        entries[label] = ent
    Button(window, text="Fetch",  command=fetchRecord).pack(side=LEFT)
    Button(window, text="Update", command=updateRecord).pack(side=LEFT)
    Button(window, text="Quit",   command=window.quit).pack(side=RIGHT)
    return window

def fetchRecord():
    key = entries['key'].get()
    try:
        record = db[key]                      # fetch by key, show in GUI
    except:
        showerror(title='Error', message='No such key!')
    else:
        for field in fieldnames:
            entries[field].delete(0, END)
            entries[field].insert(0, repr(getattr(record, field)))

def updateRecord():
    key = entries['key'].get()
    if key in db:
        record = db[key]                      # update existing record
    else:
        from person import Person             # make/store new one for key
        record = Person(name='?', age='?')    # eval: strings must be quoted
    for field in fieldnames:
        setattr(record, field, eval(entries[field].get()))
    db[key] = record

db = shelve.open(shelvename)
window = makeWidgets()
window.mainloop()
db.close() # back here after quit or window close

import wx 

#!/usr/bin/python
# -*- coding: utf-8 -*-

"""
ZetCode Tkinter tutorial

In this script, we use the grid
manager to create a more complicated
layout.

Author: Jan Bodnar
Last modified: December 2015
Website: www.zetcode.com
"""

from Tkinter import Tk, Text, BOTH, W, N, E, S
from ttk import Frame, Button, Label, Style


class Example(Frame):
  
    def __init__(self, parent):
        Frame.__init__(self, parent)   
         
        self.parent = parent
        self.initUI()
        
        
    def initUI(self):
      
        self.parent.title("Windows")
        self.pack(fill=BOTH, expand=True)

        self.columnconfigure(1, weight=1)
        self.columnconfigure(3, pad=7)
        self.rowconfigure(3, weight=1)
        self.rowconfigure(5, pad=7)
        
        lbl = Label(self, text="Windows")
        lbl.grid(sticky=W, pady=4, padx=5)
        
        area = Text(self)
        area.grid(row=1, column=0, columnspan=2, rowspan=4, 
            padx=5, sticky=E+W+S+N)
        
        abtn = Button(self, text="Activate")
        abtn.grid(row=1, column=3)

        cbtn = Button(self, text="Close")
        cbtn.grid(row=2, column=3, pady=4)
        
        hbtn = Button(self, text="Help")
        hbtn.grid(row=5, column=0, padx=5)

        obtn = Button(self, text="OK")
        obtn.grid(row=5, column=3)        
              

def main():
  
    root = Tk()
    root.geometry("350x300+300+300")
    app = Example(root)
    root.mainloop()  


if __name__ == '__main__':
    main()  


# File: tree.py
# References:
#    http://hg.python.org/cpython/file/4e32c450f438/Lib/tkinter/ttk.py
#    http://www.tcl.tk/man/tcl8.5/TkCmd/ttk_treeview.htm#M79
#    http://svn.python.org/projects/python/branches/pep-0384/Demo/tkinter/ttk/dirbrowser.py

import os

from tkinter import *
from tkinter import ttk     #@Reimport

from demopanels import MsgPanel, SeeDismissPanel

# Constants for formatting file sizes
KB = 1024.0
MB = KB * KB
GB = MB * KB

class TreeDemo(ttk.Frame):
    
    def __init__(self, isapp=True, name='treedemo'):
        ttk.Frame.__init__(self, name=name)
        self.pack(expand=Y, fill=BOTH)
        self.master.title('Tree Demo')
        self.isapp = isapp
        self._create_widgets()
        
    def _create_widgets(self):
        if self.isapp:
            MsgPanel(self, ["One of the new Tk themed widgets is a tree widget, which allows ",
                            "the user to browse a hierarchical data-set such as a file system. ",
                            "The tree widget not only allows for the tree part itself, but it ",
                            "also supports an arbitrary number of additional columns which can ",
                            "show additional data (in this case, the size of the files found ",
                            "on your file system). You can also change the width of the columns ",
                            "by dragging the boundary between them."])
            
            SeeDismissPanel(self)
        
        self._create_demo_panel()
        
    def _create_demo_panel(self):
        demoPanel = Frame(self)
        demoPanel.pack(side=TOP, fill=BOTH, expand=Y)
        
        self._create_treeview(demoPanel)    
        self._populate_root()
                    
    def _create_treeview(self, parent):
        f = ttk.Frame(parent)
        f.pack(side=TOP, fill=BOTH, expand=Y)
        
        # create the tree and scrollbars
        self.dataCols = ('fullpath', 'type', 'size')        
        self.tree = ttk.Treeview(columns=self.dataCols, 
                                 displaycolumns='size')
        
        ysb = ttk.Scrollbar(orient=VERTICAL, command= self.tree.yview)
        xsb = ttk.Scrollbar(orient=HORIZONTAL, command= self.tree.xview)
        self.tree['yscroll'] = ysb.set
        self.tree['xscroll'] = xsb.set
        
        # setup column headings
        self.tree.heading('#0', text='Directory Structure', anchor=W)
        self.tree.heading('size', text='File Size', anchor=W)
        self.tree.column('size', stretch=0, width=70)
        
        # add tree and scrollbars to frame
        self.tree.grid(in_=f, row=0, column=0, sticky=NSEW)
        ysb.grid(in_=f, row=0, column=1, sticky=NS)
        xsb.grid(in_=f, row=1, column=0, sticky=EW)
        
        # set frame resizing priorities
        f.rowconfigure(0, weight=1)
        f.columnconfigure(0, weight=1)
        
        # action to perform when a node is expanded
        self.tree.bind('<<TreeviewOpen>>', self._update_tree)
        
    def _populate_root(self):
        # use current directory as root node
        self.path = os.getcwd()
        
        # insert current directory at top of tree
        # 'values' = column values: fullpath, type, size
        #            if a column value is omitted, assumed empty
        parent = self.tree.insert('', END, text=self.path,
                                  values=[self.path, 'directory'])
        
        # add the files and sub-directories
        self._populate_tree(parent, self.path, os.listdir(self.path))
                
    def _populate_tree(self, parent, fullpath, children):
        # parent   - id of node acting as parent
        # fullpath - the parent node's full path 
        # children - list of files and sub-directories
        #            belonging to the 'parent' node
        
        for child in children:
            # build child's fullpath
            #cpath = os.path.join(fullpath, child).replace('\\', '/')
            cpath = os.path.join(fullpath, child).replace('', '')
            
            if os.path.isdir(cpath):
                # directory - only populate when expanded
                # (see _create_treeview() 'bind')
                cid =self.tree.insert(parent, END, text=child,
                                      values=[cpath, 'directory'])
                
                # add 'dummy' child to force node as expandable
                self.tree.insert(cid, END, text='dummy')  
            else:
                # must be a 'file'
                size = self._format_size(os.stat(cpath).st_size)
                self.tree.insert(parent, END, text=child,
                                 values=[cpath, 'file', size])
                
    def _format_size(self, size):
        if size >= GB:
            return '{:,.1f} GB'.format(size/GB)
        if size >= MB:
            return '{:,.1f} MB'.format(size/MB)
        if size >= KB:
            return '{:,.1f} KB'.format(size/KB)
        return '{} bytes'.format(size)
                
    def _update_tree(self, event): #@UnusedVariable
        # user expanded a node - build the related directory 
        nodeId = self.tree.focus()      # the id of the expanded node
        
        if self.tree.parent(nodeId):    # not at root
            topChild = self.tree.get_children(nodeId)[0]
            
            # if the node only has a 'dummy' child, remove it and 
            # build new directory; skip if the node is already
            # populated
            if self.tree.item(topChild, option='text') == 'dummy':
                self.tree.delete(topChild)
                path = self.tree.set(nodeId, 'fullpath')
                self._populate_tree(nodeId, path, os.listdir(path))

if __name__ == '__main__':
    TreeDemo().mainloop()

%%writefile demopanels.py
# -*- coding: utf-8 -*-
"""
Created on Sat Mar 28 19:26:52 2015
@author: Simon
"""
# File: demopanels.py
# References:
#    http://hg.python.org/cpython/file/4e32c450f438/Lib/tkinter/simpledialog.py
#    http://docs.python.org/py3k/library/inspect.html#module-inspect
#
# Icons sourced from:
#    http://findicons.com/icon/69404/deletered?width=16#
#    http://findicons.com/icon/93110/old_edit_find?width=16#
 
from tkinter import *
from tkinter import ttk
#from tkinter.simpledialog import Dialog
from tkSimpleDialog import Dialog
from PIL import Image, ImageTk
import inspect
 
class MsgPanel(ttk.Frame):
    def __init__(self, master, msgtxt):
        ttk.Frame.__init__(self, master)
        self.pack(side=TOP, fill=X)
         
        msg = Label(self, wraplength='4i', justify=LEFT)
        msg['text'] = ''.join(msgtxt)
        msg.pack(fill=X, padx=5, pady=5)
         
class SeeDismissPanel(ttk.Frame):
    def __init__(self, master):
        ttk.Frame.__init__(self, master)
        self.pack(side=BOTTOM, fill=X)          # resize with parent
         
        # separator widget
        sep = ttk.Separator(orient=HORIZONTAL)
 
        # Dismiss button
        im = Image.open('/home/jack/Desktop/deep-dream-generator/notebooks/JavaScript/delete.png')   # image file
        imh = ImageTk.PhotoImage(im)            # handle to file
        dismissBtn = ttk.Button(text='Dismiss', image=imh, command=self.winfo_toplevel().destroy)
        dismissBtn.image = imh                  # prevent image from being garbage collected
        dismissBtn['compound'] = LEFT           # display image to left of label text
         
        # 'See Code' button
        im = Image.open('/home/jack/Desktop/deep-dream-generator/notebooks/JavaScript/view.png')
        imh = ImageTk.PhotoImage(im)
        codeBtn = ttk.Button(text='See Code', image=imh, default=ACTIVE, command=lambda: CodeDialog(self.master))
        codeBtn.image = imh
        codeBtn['compound'] = LEFT
        codeBtn.focus()
                 
        # position and register widgets as children of this frame
        sep.grid(in_=self, row=0, columnspan=4, sticky=EW, pady=5)
        codeBtn.grid(in_=self, row=1, column=0, sticky=E)
        dismissBtn.grid(in_=self, row=1, column=1, sticky=E)
         
        # set resize constraints
        self.rowconfigure(0, weight=1)
        self.columnconfigure(0, weight=1)
 
        # bind <Return> to demo window, activates 'See Code' button;
        # <'Escape'> activates 'Dismiss' button
        self.winfo_toplevel().bind('<Return>', lambda x: codeBtn.invoke() )
        self.winfo_toplevel().bind('<Escape>', lambda x: dismissBtn.invoke() )

class CodeDialog(Dialog):
    """Create a modal dialog to display a demo's source code file. """
         
    def body(self, master):
        """Overrides Dialog.body() to populate the dialog window with a scrolled text window
        and custom dialog buttons. """
         
        # get the full path of this object's parent source code file
        fileName = inspect.getsourcefile(self.parent._create_widgets)
         
        self.title('Source Code: ' + fileName)
         
        # create scrolled text widget
        txtFrame = ttk.Frame(self)
        txtFrame.pack(side=TOP, fill=BOTH)
         
        text = Text(txtFrame, height=24, width=100, wrap=WORD, setgrid=1, highlightthickness=0, pady=2, padx=3)
        xscroll = ttk.Scrollbar(txtFrame, command=text.xview, orient=HORIZONTAL)
        yscroll = ttk.Scrollbar(txtFrame, command=text.yview, orient=VERTICAL)
        text.configure(xscrollcommand=xscroll.set, yscrollcommand=yscroll.set)
         
        # position in frame and set resize constraints
        text.grid(row=0, column=0, sticky=NSEW)
        yscroll.grid(row=0, column=1, sticky=NSEW)
        txtFrame.rowconfigure(0, weight=1)
        txtFrame.columnconfigure(0, weight=1)
         
        # add text of file to scrolled text widget
        text.delete('0.0', END)
        text.insert(END, open(fileName).read())
 
    def buttonbox(self):
        """Overrides Dialog.buttonbox() to create custom buttons for this dialog. """
         
        box = ttk.Frame(self)
 
        # Cancel button
        cancelBtn = ttk.Button(box, text='Cancel', command=self.cancel)       
        cancelBtn.pack(side=RIGHT, padx=5, pady=5)
        self.bind('<Return>', self.cancel)
        self.bind('<Escape>', self.cancel)
         
        box.pack()

        #Contact GitHub API Training Shop Blog About 



!ls 

%%writefile tkSimpleDialog.py
from Tkinter import *
import os
class Dialog(Toplevel):
    def __init__(self, parent, title = None):
        Toplevel.__init__(self, parent)
        self.transient(parent)
        if title:
            self.title(title)
        self.parent = parent
        self.result = None
        body = Frame(self)
        self.initial_focus = self.body(body)
        body.pack(padx=5, pady=5)
        self.buttonbox()
        self.grab_set()
        if not self.initial_focus:
            self.initial_focus = self
        self.protocol("WM_DELETE_WINDOW", self.cancel)
        self.geometry("+%d+%d" % (parent.winfo_rootx()+50,
                                  parent.winfo_rooty()+50))
        self.initial_focus.focus_set()
        self.wait_window(self)
    # construction hooks
    def body(self, master):
        # create dialog body.  return widget that should have
        # initial focus.  this method should be overridden
        pass
    def buttonbox(self):
        # add standard button box. override if you don't want the
        # standard buttons
        box = Frame(self)
        w = Button(box, text="OK", width=10, command=self.ok, default=ACTIVE)
        w.pack(side=LEFT, padx=5, pady=5)
        w = Button(box, text="Cancel", width=10, command=self.cancel)
        w.pack(side=LEFT, padx=5, pady=5)
        self.bind("<Return>", self.ok)
        self.bind("<Escape>", self.cancel)
        box.pack()
    # standard button semantics
    def ok(self, event=None):
        if not self.validate():
            self.initial_focus.focus_set() # put focus back
            return
        self.withdraw()
        self.update_idletasks()
        self.apply()
        self.cancel()
    def cancel(self, event=None):
        # put focus back to the parent window
        self.parent.focus_set()
        self.destroy()
    # command hooks
    def validate(self):
        return 1 # override
    def apply(self):
        pass # override



%%writefile settings.py
#app_key=YazCRIfWX4VICiRCOiph08jDL
#app_secret=QOkLHou6NMwkghSHjMFXMdffQKJlDzttKtP6uBCcZ4VlQtvJyc
#oauth_token=296906916-AWggjhqpEWIS7EzXXhc2pOPBeCVJczpOm11cQGIf
#oauth_token_secret=zFrCiyaPt8gCBVVs1bLCmdCSyQQ3DKxT5wHJq2tOu2AMj
GITTER = {
    'name':'GitterBot',
    'ROOM': 'errbotio/errbot',
    'API_TOKEN': '7af0a02fc3c43281482f60a925d7303acfa79990'
}



TWITTER = {
    "CONSUMER_KEY": "YazCRIfWX4VICiRCOiph08jDL",
    "CONSUMER_SECRET": "QOkLHou6NMwkghSHjMFXMdffQKJlDzttKtP6uBCcZ4VlQtvJyc",
    "ACCESS_TOKEN": "296906916-AWggjhqpEWIS7EzXXhc2pOPBeCVJczpOm11cQGIf",
    "ACCESS_TOKEN_SECRET": "zFrCiyaPt8gCBVVs1bLCmdCSyQQ3DKxT5wHJq2tOu2AMj"
}


ChatBot = {
    'name': 'Gohart',
    'logic_adapters': [
        'chatterbot.logic.MathematicalEvaluation',
        'chatterbot.logic.TimeLogicAdapter',
        'chatterbot.logic.BestMatch'
    ],
    'trainer': 'chatterbot.trainers.ChatterBotCorpusTrainer',
    'training_data': [
         'chatterbot.corpus.english.greetings'
    ]
}



!ls settings.py

# -*- coding: utf-8 -*-
from chatterbot import ChatBot
from settings import TWITTER
import logging


'''
This example demonstrates how you can train your chat bot
using data from Twitter.

To use this example, create a new file called settings.py.
In settings.py define the following:

TWITTER = {
    "CONSUMER_KEY": "my-twitter-consumer-key",
    "CONSUMER_SECRET": "my-twitter-consumer-secret",
    "ACCESS_TOKEN": "my-access-token",
    "ACCESS_TOKEN_SECRET": "my-access-token-secret"
}
'''

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

chatbot = ChatBot("Gort",
    logic_adapters=[
        "chatterbot.logic.BestMatch"
    ],
    input_adapter="chatterbot.input.TerminalAdapter",
    output_adapter="chatterbot.output.TerminalAdapter",
    database="/home/jack/Desktop/ChatterBot/newtwitter-database.db",
    twitter_consumer_key=TWITTER["CONSUMER_KEY"],
    twitter_consumer_secret=TWITTER["CONSUMER_SECRET"],
    twitter_access_token_key=TWITTER["ACCESS_TOKEN"],
    twitter_access_token_secret=TWITTER["ACCESS_TOKEN_SECRET"],
    trainer="chatterbot.trainers.TwitterTrainer"
)

chatbot.train()

chatbot.logger.info('Trained database generated successfully!')

# -*- coding: utf-8 -*-
from chatterbot import ChatBot
from settings import TWITTER
import logging

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

chatbot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=['chatterbot.logic.BestMatch'],
    filters=['chatterbot.filters.RepetitiveResponseFilter'],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='twitter-database'),
twitter_consumer_key=TWITTER["CONSUMER_KEY"],
twitter_consumer_secret=TWITTER["CONSUMER_SECRET"],
twitter_access_token_key=TWITTER["ACCESS_TOKEN"],
twitter_access_token_secret=TWITTER["ACCESS_TOKEN_SECRET"],
trainer="chatterbot.trainers.TwitterTrainer"

chatbot.train("chatterbot.corpus.english")

chatbot.logger.info('Trained database generated successfully!')

from chatterbot import ChatBot

chatbot = ChatBot(
    'Gort',
    trainer='chatterbot.trainers.ChatterBotCorpusTrainer'
)

# Train based on the english corpus
chatbot.train("chatterbot.corpus.english")

# Get a response to an input statement
while True:
    try:
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break


from chatterbot import ChatBot

chatbot = ChatBot(
    'Gort',
    trainer='chatterbot.trainers.ChatterBotCorpusTrainer')

    # Train based on the english corpus
chatbot.train("chatterbot.corpus.english")
input_adapter='chatterbot.input.TerminalAdapter',
output_adapter='chatterbot.output.TerminalAdapter',
database='chatterbot-database'





# Get a response to an input statement
while True:
    try:
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break


# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=['chatterbot.logic.BestMatch'],          
    filters=['chatterbot.filters.RepetitiveResponseFilter'],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    #database='chatterbot-database')
    database='twitter-database')
print('Type something to begin...')

while True:
    try:
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break


# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

print('Type something to begin...')

while True:
    try:
        bot_input = bot.get_response(None)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break


# -*- coding: utf-8 -*-
from chatterbot import ChatBot
bot = ChatBot('Gort')
from chatterbot.trainers import ListTrainer


bot = ChatBot(
    'Gort',
    storage_adapter='chatterbot.storage.JsonFileStorageAdapter',
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    logic_adapters=['chatterbot.logic.MathematicalEvaluation',
        'chatterbot.logic.TimeLogicAdapter'],
    database='/home/jack/Desktop/ChatterBot/database.json'
)

while True:
    try:
     bot_input = bot.get_response(None)

    except(KeyboardInterrupt, EOFError, SystemExit):
        break


conversation = [
    "Hello",
    "Hi there!",
    "How are you doing?",
    "I'm doing great.",
    "That is good to hear",
    "Thank you.",
    "You're welcome.",
    "Who are you?",
    "I am Gort. Who are you",
    "I am Jack",
    
]


TWITTER = {
    "CONSUMER_KEY": "my-twitter-consumer-key",
    "CONSUMER_SECRET": "my-twitter-consumer-secret",
    "ACCESS_TOKEN": "my-access-token",
    "ACCESS_TOKEN_SECRET": "my-access-token-secret"
}
'''

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

chatbot = ChatBot("Gort",
    logic_adapters=[
        "chatterbot.logic.BestMatch"
    ],
    input_adapter="chatterbot.input.TerminalAdapter",
    output_adapter="chatterbot.output.TerminalAdapter",
    database="/home/jack/Desktop/ChatterBot/twitter-database.db",
    twitter_consumer_key=TWITTER["CONSUMER_KEY"],
    twitter_consumer_secret=TWITTER["CONSUMER_SECRET"],
    twitter_access_token_key=TWITTER["ACCESS_TOKEN"],
    twitter_access_token_secret=TWITTER["ACCESS_TOKEN_SECRET"],
    trainer="chatterbot.trainers.TwitterTrainer"

%%writefile settings.py
#app_key=YazCRIfWX4VICiRCOiph08jDL
#app_secret=QOkLHou6NMwkghSHjMFXMdffQKJlDzttKtP6uBCcZ4VlQtvJyc
#oauth_token=296906916-AWggjhqpEWIS7EzXXhc2pOPBeCVJczpOm11cQGIf
#oauth_token_secret=zFrCiyaPt8gCBVVs1bLCmdCSyQQ3DKxT5wHJq2tOu2AMj
GITTER = {
    'name':'GitterBot',
    'ROOM': 'errbotio/errbot',
    'API_TOKEN': '7af0a02fc3c43281482f60a925d7303acfa79990'
}



TWITTER = {
    "CONSUMER_KEY": "YazCRIfWX4VICiRCOiph08jDL",
    "CONSUMER_SECRET": "QOkLHou6NMwkghSHjMFXMdffQKJlDzttKtP6uBCcZ4VlQtvJyc",
    "ACCESS_TOKEN": "296906916-AWggjhqpEWIS7EzXXhc2pOPBeCVJczpOm11cQGIf",
    "ACCESS_TOKEN_SECRET": "zFrCiyaPt8gCBVVs1bLCmdCSyQQ3DKxT5wHJq2tOu2AMj"
}


ChatBot = {
    'name': 'Gohart',
    'logic_adapters': [
        'chatterbot.logic.MathematicalEvaluation',
        'chatterbot.logic.TimeLogicAdapter',
        'chatterbot.logic.BestMatch'
    ],
    'trainer': 'chatterbot.trainers.ChatterBotCorpusTrainer',
    'training_data': [
         'chatterbot.corpus.english.greetings'
    ]
}



!ls settings.py

# -*- coding: utf-8 -*
from settings import TWITTER
import logging
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
from settings import TWITTER
 
trainer = ListTrainer(ChatBot)

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

'''
This example demonstrates how you can train your chat bot
using data from Twitter.

To use this example, create a new file called settings.py.
In settings.py define the following:

TWITTER = {
    "CONSUMER_KEY": "my-twitter-consumer-key",
    "CONSUMER_SECRET": "my-twitter-consumer-secret",
    "ACCESS_TOKEN": "my-access-token",
    "ACCESS_TOKEN_SECRET": "my-access-token-secret"
}
'''

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

chatbot = ChatBot("Gort",
    logic_adapters=[
        "chatterbot.logic.BestMatch"
    ],
    input_adapter="chatterbot.input.TerminalAdapter",
    output_adapter="chatterbot.output.TerminalAdapter",
    database="newtwitter-database.db",
    twitter_consumer_key=TWITTER["CONSUMER_KEY"],
    twitter_consumer_secret=TWITTER["CONSUMER_SECRET"],
    twitter_access_token_key=TWITTER["ACCESS_TOKEN"],
    twitter_access_token_secret=TWITTER["ACCESS_TOKEN_SECRET"],
    trainer="chatterbot.trainers.TwitterTrainer"
)

chatbot.train(trainer)

chatbot.logger.info('Trained database generated successfully!')



# -*- coding: utf-8 -*-
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

Trainer = ListTrainer(chatbot)

#trainer.train([
#    "Hi, can I help you?",
#    "Sure, I'd like to book a flight to Iceland.",
#    "Your flight has been booked."
#])

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

chatbot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=['chatterbot.logic.BestMatch'],
    filters=['chatterbot.filters.RepetitiveResponseFilter'],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='twitter-database'),
twitter_consumer_key=TWITTER["CONSUMER_KEY"],
twitter_consumer_secret=TWITTER["CONSUMER_SECRET"],
twitter_access_token_key=TWITTER["ACCESS_TOKEN"],
twitter_access_token_secret=TWITTER["ACCESS_TOKEN_SECRET"],
trainer="chatterbot.trainers.TwitterTrainer"

Trainer.train("chatterbot.corpus.english")

#chatbot.logger.info('Trained database generated successfully!')

!ls chatterbot/corpus/english

import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# Create a new chat bot named Charlie
chatbot = ChatBot('Charlie')

trainer = ListTrainer(chatbot)




# Train based on the english corpus
trainer.train("chatterbot.corpus.english")
while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = chatbot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

smartbot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new chat bot named Charlie
#storage_adapter='chatterbot.storage.MongoDatabaseA
# Create a new chat bot named Charlie
# chatbot = ChatBot('Charlie')

# trainer = ListTrainer(chatbot)

# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
smartbot.train("chatterbot.corpus.english")

smartbot.train("chatterbot.corpus.english.gossip")
input_adapter='chatterbot.input.TerminalAdapter',
output_adapter='chatterbot.output.TerminalAdapter',
database='chatterbot-database'





# Get a response to an input statement
while True:
    try:
        bot_input = bot.get_response(bot_input)
        if bot_input == "exit":
            break

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break


# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=['chatterbot.logic.BestMatch'],          
    filters=['chatterbot.filters.RepetitiveResponseFilter'],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    #database='chatterbot-database')
    database='twitter-database')
print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

# -*- coding: utf-8 -*-
from chatterbot import ChatBot
import logging


# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

# -*- coding: utf-8 -*-
from chatterbot import ChatBot
bot = ChatBot('Gort')
from chatterbot.trainers import ListTrainer


bot = ChatBot(
    'Gort',
    storage_adapter='chatterbot.storage.SQLStorageAdapter',
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    logic_adapters=['chatterbot.logic.MathematicalEvaluation',
        'chatterbot.logic.TimeLogicAdapter'],
    database='database.json'
)

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break


conversation = [
    "Hello",
    "Hi there!",
    "How are you doing?",
    "I'm doing great.",
    "That is good to hear",
    "Thank you.",
    "You're welcome.",
    "Who are you?",
    "I am Gort. Who are you",
    "I am Jack",
    
]


#TWITTER = {
#    "CONSUMER_KEY": "my-twitter-consumer-key",
#    "CONSUMER_SECRET": "my-twitter-consumer-secret",
#    "ACCESS_TOKEN": "my-access-token",
#    "ACCESS_TOKEN_SECRET": "my-access-token-secret"
#}
TWITTER = {
    "CONSUMER_KEY": "YazCRIfWX4VICiRCOiph08jDL",
    "CONSUMER_SECRET": "QOkLHou6NMwkghSHjMFXMdffQKJlDzttKtP6uBCcZ4VlQtvJyc",
    "ACCESS_TOKEN": "296906916-AWggjhqpEWIS7EzXXhc2pOPBeCVJczpOm11cQGIf",
    "ACCESS_TOKEN_SECRET": "zFrCiyaPt8gCBVVs1bLCmdCSyQQ3DKxT5wHJq2tOu2AMj"
}

# Comment out the following line to disable verbose logging
logging.basicConfig(level=logging.INFO)

chatbot = ChatBot("Gort",
    logic_adapters=[
        "chatterbot.logic.BestMatch"
    ],
    input_adapter="chatterbot.input.TerminalAdapter",
    output_adapter="chatterbot.output.TerminalAdapter",
    database="twitter-database.db",
    twitter_consumer_key=TWITTER["CONSUMER_KEY"],
    twitter_consumer_secret=TWITTER["CONSUMER_SECRET"],
    twitter_access_token_key=TWITTER["ACCESS_TOKEN"],
    twitter_access_token_secret=TWITTER["ACCESS_TOKEN_SECRET"],
    trainer="chatterbot.trainers.TwitterTrainer")

%%writefile settings.py
#app_key=YazCRIfWX4VICiRCOiph08jDL
#app_secret=QOkLHou6NMwkghSHjMFXMdffQKJlDzttKtP6uBCcZ4VlQtvJyc
#oauth_token=296906916-AWggjhqpEWIS7EzXXhc2pOPBeCVJczpOm11cQGIf
#oauth_token_secret=zFrCiyaPt8gCBVVs1bLCmdCSyQQ3DKxT5wHJq2tOu2AMj
TWITTER = {
    "CONSUMER_KEY": "YazCRIfWX4VICiRCOiph08jDL",
    "CONSUMER_SECRET": "QOkLHou6NMwkghSHjMFXMdffQKJlDzttKtP6uBCcZ4VlQtvJyc",
    "ACCESS_TOKEN": "296906916-AWggjhqpEWIS7EzXXhc2pOPBeCVJczpOm11cQGIf",
    "ACCESS_TOKEN_SECRET": "zFrCiyaPt8gCBVVs1bLCmdCSyQQ3DKxT5wHJq2tOu2AMj"
}



!locate chatterbot/corpus



#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

!sudo apt -y install libportaudio2
!pip install tflite-model-maker

import os
import glob
import random
import shutil

import librosa
import soundfile as sf
from IPython.display import Audio
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
import tflite_model_maker as mm
from tflite_model_maker import audio_classifier
from tflite_model_maker.config import ExportFormat

print(f"TensorFlow Version: {tf.__version__}")
print(f"Model Maker Version: {mm.__version__}")

use_custom_dataset = False #@param ["False", "True"] {type:"raw"}

tf.keras.utils.get_file('speech_commands_v0.01.tar.gz',
                        'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz',
                        cache_dir='./',
                        cache_subdir='dataset-speech',
                        extract=True)
tf.keras.utils.get_file('background_audio.zip',
                        'https://storage.googleapis.com/download.tensorflow.org/models/tflite/sound_classification/background_audio.zip',
                        cache_dir='./',
                        cache_subdir='dataset-background',
                        extract=True)


# Create a list of all the background wav files
files = glob.glob(os.path.join('./dataset-speech/_background_noise_', '*.wav'))
files = files + glob.glob(os.path.join('./dataset-background', '*.wav'))

background_dir = './background'
os.makedirs(background_dir, exist_ok=True)

# Loop through all files and split each into several one-second wav files
for file in files:
  filename = os.path.basename(os.path.normpath(file))
  print('Splitting', filename)
  name = os.path.splitext(filename)[0]
  rate = librosa.get_samplerate(file)
  length = round(librosa.get_duration(filename=file))
  for i in range(length - 1):
    start = i * rate
    stop = (i * rate) + rate
    data, _ = sf.read(file, start=start, stop=stop)
    sf.write(os.path.join(background_dir, name + str(i) + '.wav'), data, rate)

if not use_custom_dataset:
  commands = [ "up", "down", "left", "right", "go", "stop", "on", "off", "background"]
  dataset_dir = './dataset-speech'
  test_dir = './dataset-test'

  # Move the processed background samples
  shutil.move(background_dir, os.path.join(dataset_dir, 'background'))   

  # Delete all directories that are not in our commands list
  dirs = glob.glob(os.path.join(dataset_dir, '*/'))
  for dir in dirs:
    name = os.path.basename(os.path.normpath(dir))
    if name not in commands:
      shutil.rmtree(dir)

  # Count is per class
  sample_count = 150
  test_data_ratio = 0.2
  test_count = round(sample_count * test_data_ratio)

  # Loop through child directories (each class of wav files)
  dirs = glob.glob(os.path.join(dataset_dir, '*/'))
  for dir in dirs:
    files = glob.glob(os.path.join(dir, '*.wav'))
    random.seed(42)
    random.shuffle(files)
    # Move test samples:
    for file in files[sample_count:sample_count + test_count]:
      class_dir = os.path.basename(os.path.normpath(dir))
      os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)
      os.rename(file, os.path.join(test_dir, class_dir, os.path.basename(file)))
    # Delete remaining samples
    for file in files[sample_count + test_count:]:
      os.remove(file)

if use_custom_dataset:
  # Specify the ZIP file you uploaded:
  !unzip YOUR-FILENAME.zip
  # Specify the unzipped path to your custom dataset
  # (this path contains all the subfolders with classification names):
  dataset_dir = './YOUR-DIRNAME'

def move_background_dataset(dataset_dir):
  dest_dir = os.path.join(dataset_dir, 'background')
  if os.path.exists(dest_dir):
    files = glob.glob(os.path.join(background_dir, '*.wav'))
    for file in files:
      shutil.move(file, dest_dir)
  else:
    shutil.move(background_dir, dest_dir)

if use_custom_dataset:
  # Move background samples into custom dataset
  move_background_dataset(dataset_dir)

  # Now we separate some of the files that we'll use for testing:
  test_dir = './dataset-test'
  test_data_ratio = 0.2
  dirs = glob.glob(os.path.join(dataset_dir, '*/'))
  for dir in dirs:
    files = glob.glob(os.path.join(dir, '*.wav'))
    test_count = round(len(files) * test_data_ratio)
    random.seed(42)
    random.shuffle(files)
    # Move test samples:
    for file in files[:test_count]:
      class_dir = os.path.basename(os.path.normpath(dir))
      os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)
      os.rename(file, os.path.join(test_dir, class_dir, os.path.basename(file)))
    print('Moved', test_count, 'images from', class_dir)

def get_random_audio_file(samples_dir):
  files = os.path.abspath(os.path.join(samples_dir, '*/*.wav'))
  files_list = glob.glob(files)
  random_audio_path = random.choice(files_list)
  return random_audio_path

def show_sample(audio_path):
  audio_data, sample_rate = sf.read(audio_path)
  class_name = os.path.basename(os.path.dirname(audio_path))
  print(f'Class: {class_name}')
  print(f'File: {audio_path}')
  print(f'Sample rate: {sample_rate}')
  print(f'Sample length: {len(audio_data)}')

  plt.title(class_name)
  plt.plot(audio_data)
  display(Audio(audio_data, rate=sample_rate))

random_audio = get_random_audio_file(test_dir)
show_sample(random_audio)

spec = audio_classifier.BrowserFftSpec()

if not use_custom_dataset:
  train_data_ratio = 0.8
  train_data = audio_classifier.DataLoader.from_folder(
      spec, dataset_dir, cache=True)
  train_data, validation_data = train_data.split(train_data_ratio)
  test_data = audio_classifier.DataLoader.from_folder(
      spec, test_dir, cache=True)

if use_custom_dataset:
  train_data_ratio = 0.8
  train_data = audio_classifier.DataLoader.from_folder(
      spec, dataset_dir, cache=True)
  train_data, validation_data = train_data.split(train_data_ratio)
  test_data = audio_classifier.DataLoader.from_folder(
      spec, test_dir, cache=True)


# If your dataset has fewer than 100 samples per class,
# you might want to try a smaller batch size
batch_size = 25
epochs = 25
model = audio_classifier.create(train_data, spec, validation_data, batch_size, epochs)

model.evaluate(test_data)

def show_confusion_matrix(confusion, test_labels):
  """Compute confusion matrix and normalize."""
  confusion_normalized = confusion.astype("float") / confusion.sum(axis=1)
  sns.set(rc = {'figure.figsize':(6,6)})
  sns.heatmap(
      confusion_normalized, xticklabels=test_labels, yticklabels=test_labels,
      cmap='Blues', annot=True, fmt='.2f', square=True, cbar=False)
  plt.title("Confusion matrix")
  plt.ylabel("True label")
  plt.xlabel("Predicted label")

confusion_matrix = model.confusion_matrix(test_data)
show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)

TFLITE_FILENAME = 'browserfft-speech.tflite'
SAVE_PATH = './models'

print(f'Exporing the model to {SAVE_PATH}')
model.export(SAVE_PATH, tflite_filename=TFLITE_FILENAME)
model.export(SAVE_PATH, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])

# This library provides the TFLite metadata API
! pip install -q tflite_support

from tflite_support import metadata
import json

def get_labels(model):
  """Returns a list of labels, extracted from the model metadata."""
  displayer = metadata.MetadataDisplayer.with_model_file(model)
  labels_file = displayer.get_packed_associated_file_list()[0]
  labels = displayer.get_associated_file_buffer(labels_file).decode()
  return [line for line in labels.split('\n')]

def get_input_sample_rate(model):
  """Returns the model's expected sample rate, from the model metadata."""
  displayer = metadata.MetadataDisplayer.with_model_file(model)
  metadata_json = json.loads(displayer.get_metadata_json())
  input_tensor_metadata = metadata_json['subgraph_metadata'][0][
          'input_tensor_metadata'][0]
  input_content_props = input_tensor_metadata['content']['content_properties']
  return input_content_props['sample_rate']

# Get a WAV file for inference and list of labels from the model
tflite_file = os.path.join(SAVE_PATH, TFLITE_FILENAME)
labels = get_labels(tflite_file)
random_audio = get_random_audio_file(test_dir)

# Ensure the audio sample fits the model input
interpreter = tf.lite.Interpreter(tflite_file)
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_size = input_details[0]['shape'][1]
sample_rate = get_input_sample_rate(tflite_file)
audio_data, _ = librosa.load(random_audio, sr=sample_rate)
if len(audio_data) < input_size:
  audio_data.resize(input_size)
audio_data = np.expand_dims(audio_data[:input_size], axis=0)

# Run inference
interpreter.allocate_tensors()
interpreter.set_tensor(input_details[0]['index'], audio_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])

# Display prediction and ground truth
top_index = np.argmax(output_data[0])
label = labels[top_index]
score = output_data[0][top_index]
print('---prediction---')
print(f'Class: {label}\nScore: {score}')
print('----truth----')
show_sample(random_audio)

try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download(tflite_file)

import sqlite3
search = raw_input("SEARCH : ")
conn=sqlite3.connect("/home/jack/hubiC/Databases/SNIPPETS.db")
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
for row in c.execute('SELECT rowid,* FROM snippets WHERE snippets MATCH (?)', (search,) ):
    print row[1]

import sqlite3
database = "/home/jack/hubiC/Databases/SNIPPETS.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
Search = raw_input("SEARCH : ")
c = conn.cursor()
for row in c.execute('SELECT rowid,* FROM snippets'):
    if Search in row[1]:
        print row[0]
        print row[1]
        print row[2]
        print "------------------------------------"

import sqlite3
conn=sqlite3.connect("/home/jack/hubiC/Databases/SNIPPETS.db")
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
code = """
locked db database locked can not use database 
Response to a locked database

!fuser /home/jack/hubiC/Databases/Stuff.db
!kill -9 18879
"""
keywords = """
locked db database locked can not use database 
Response to a locked database

!fuser /home/jack/hubiC/Databases/Stuff.db
!kill -9 18879
"""
c.execute('INSERT into snippets VALUES (?,?)', (code, keywords) )
conn.commit()
conn.close()

import sqlite3
import re
import sys
import time
database = "/home/jack/hubiC/Databases/Stuff.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
Search = raw_input("SEARCH : ")
c = conn.cursor()
for row in c.execute('SELECT rowid,* FROM stuff'):
    if Search in row:
        print row[0],row[1],row[2]

import sqlite3
import re
import sys
import time
database = "/home/jack/hubiC/Databases/Stuff.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS stuff 
USING FTS4(info, description);
""")
conn.commit()
conn.close()

import sqlite3
database = "/home/jack/hubiC/Databases/Stuff.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
for row in c.execute('SELECT rowid,* FROM stuff WHERE rowid = (?)', (search,) ):
    print row[1]

import sqlite3
import re
import sys
import time
database = "/home/jack/hubiC/Databases/Stuff.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
ID = raw_input("DELETE rowid: ")
ID = int(ID)
c = conn.cursor()
c.execute('DELETE FROM stuff WHERE ROWID = (?)', (ID,))
conn.commit()

import sqlite3
import re
import sys
import time
database = "/home/jack/hubiC/Databases/Stuff.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
for row in c.execute('SELECT rowid,* FROM stuff'):
    print row[0],row[1],row[2]


!fuser /home/jack/hubiC/Databases/Stuff.db

!kill -9 18879

locked db database locked can not use database 
Response to a locked database

!fuser /home/jack/hubiC/Databases/Stuff.db
!kill -9 18879

import sqlite3
import re
import sys
import time
database = "/home/jack/hubiC/Databases/Stuff.db"
#database = "junk2.db"
conn = sqlite3.connect(database)
Search = raw_input("SEARCH : ")
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
for row in c.execute('SELECT ROWID,* FROM stuff WHERE stuff MATCH (?)', (Search,)):
    if Search in row:
        print row[0],row[1],row[2]

!rm multitable.db

import sqlite3
import re
import sys
import time
database = "/home/jack/hubiC/Databases/Stuff.db"
#database = "junk2.db"
conn = sqlite3.connect(database)
ROWID = raw_input("SEARCH : ")
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
for row in c.execute('DELETE * FROM stuff WHERE ROWID = ?', (ROWID,)):
    print row[0],row[1],row[2]

import sqlite3
search = raw_input("SEARCH : ")
conn=sqlite3.connect("/home/jack/hidden/PW.db")
c = conn.cursor()
for row in c.execute('SELECT rowid,* FROM PASSWD WHERE PASSWD MATCH (?)', (search,) ):
    print row[1]

import sqlite3
def dbinfo(database):
    conn = sqlite3.connect(database)
    conn.text_factory = str
    c = conn.cursor()
    res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
    row = c.fetchone()
    row = str(row)
    row = row.replace("(","");row = row.replace(",)","")
    row = row.replace("'","")
    print row
    cur = c.execute("select * from '%s'" %  row)
    columns = [description[0] for description in cur.description]
    return columns

database = "multiple-table.db"
#database = "/home/jack/hidden/PW.db"
dbinfo(database)

import sqlite3
def dbinfo(database):
    conn = sqlite3.connect(database)
    conn.text_factory = str
    c = conn.cursor()
    try:
        res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
        # USE fetch all for multiple tables
        row = c.fetchall()
        row = str(row)
        row = row.replace("(","");row = row.replace(",)","")
        row = row.replace("'","")
        print row
        cur = c.execute("select * from '%s'" %  row)
        columns = [description[0] for description in cur.description]
        return columns
    except:
        pass    
database = "multiple-table.db"
#database = "/home/jack/hidden/PW.db"
dbinfo(database)

import sqlite3
import re
import sys
import time
database = "multiple-table.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS python 
USING FTS4(info, description); 
""")
conn.commit()
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS javascript 
USING FTS4(info, description);
""")
conn.commit()
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS bash 
USING FTS4(info, description);
""")
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS html 
USING FTS4(info, description);
""")
conn.commit()
c.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS misc 
USING FTS4(info, description);
""")
conn.commit()
info="""
bashbashbash
"""
description ="""
bash what kind of fears
"""
c.execute("INSERT into bash(info, description) VALUES (?,?)", (info,description));
conn.commit()
conn.close()

import sqlite3
import re
import sys
import time
database = "multiple-table.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
info="""
2  MISC __ html html html html
"""
description ="""
2  MISC ___ html html what kind of fears
"""
c.execute("INSERT into misc(info, description) VALUES (?,?)", (info,description));
conn.commit()
conn.close()

import sqlite3
import re
import sys
import time
database = "multiple-table.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
#Search = raw_input("SEARCH : ")
#Search = str(Search)
c = conn.cursor()
for row in c.execute('SELECT rowid,* FROM javascript'):
    print row[0],row[1],row[2]




from shapely.affinity import translate
from shapely.geometry import Polygon

import vsketch

vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("1cm")
vsk.penWidth("0.5mm")

p = translate(
    Polygon(
        [(-3, -1), (1.5, -2), (1.4, 2), (0, 1.5), (-1, 2.3)],
        holes=[[(-0.5, -0.5), (0.5, -0.5), (0.5, 0.5), (-0.5, 0.5)]],
    ),
    2.5,
    14,
)

# the default is no fill and stroke to layer 1
vsk.square(0, 0, 4)
vsk.circle(2, 8, 4)
vsk.geometry(p)

vsk.translate(7, 0)

# add some fill to layer 2
vsk.fill(2)
vsk.penWidth("1mm", 2)
vsk.square(0, 0, 4)
vsk.circle(2, 8, 4)
vsk.geometry(p)

vsk.translate(7, 0)

# with thick strock
vsk.fill(2)
vsk.penWidth("1mm", 2)
vsk.strokeWeight(4)
vsk.square(0, 0, 4)
vsk.circle(2, 8, 4)
vsk.geometry(p)

vsk.translate(7, 0)

# remove stroke and set fill to layer 3 with a thicker pen
vsk.fill(3)
vsk.penWidth("2mm", 3)
vsk.noStroke()
vsk.square(0, 0, 4)
vsk.circle(2, 8, 4)
vsk.geometry(p)


vsk.display(mode="matplotlib", fig_size=(12, 8))
vsk.save("stroke_fill.svg")

import vsketch

# create a stick figurestick figure
sub = vsketch.Vsketch()
sub.detail(0.01)
sub.rect(0, 0, 1, 2)
sub.circle(0.5, -0.5, 1)
sub.line(0, 0, -0.5, 1)
sub.line(1, 0, 1.5, 1)
sub.line(0, 2, -0.3, 4)
sub.line(1, 2, 1.3, 4)

# use the stick figure
vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("1cm")


for i in range(8):
    with vsk.pushMatrix():
        vsk.scale(0.95 ** i)
        vsk.rotate(8 * i, degrees=True)
        vsk.sketch(sub)
    
    vsk.translate(3, 0)


vsk.display(mode="matplotlib")
vsk.save("subsketch_basic.svg")

vsk = vsketch.Vsketch()

vsk.scale(1.5)
vsk.rect(-5, -5, 10, 10)

for i in range(10):
    vsk.rotate(10, degrees=True)
    vsk.sketch(vsk)
    
vsk.display(mode="matplotlib")
vsk.save("subsketch_recursive.svg")

import vsketch
import matplotlib
%matplotlib inline
# create a stick figurestick figure
sub = vsketch.Vsketch()
sub.detail(0.01)
sub.rect(0, 0, 1, 2)
sub.circle(0.5, -0.5, 1)
sub.line(0, 0, -0.5, 1)
sub.line(1, 0, 1.5, 1)
sub.line(0, 2, -0.3, 4)
sub.line(1, 2, 1.3, 4)

# use the stick figure
vsk = vsketch.Vsketch()
vsk.size("a4", landscape=True)
vsk.scale("1cm")


for i in range(8):
    with vsk.pushMatrix():
        vsk.scale(0.95 ** i)
        vsk.rotate(8 * i, degrees=True)
        vsk.sketch(sub)
    
    vsk.translate(3, 0)


vsk.display()
vsk.save("subsketch_basic.svg")

import cairosvg

width = 720
height = 480
cairosvg.svg2png(url="subsketch_basic.svg", write_to='subsketch_basic.png', output_width=width, output_height=height)

from PIL import Image
im = Image.open('subsketch_basic.png')
im

import matplotlib.pyplot as plt
import numpy as np
pi = 3.14
x = np.arange(0,100,0.00001)
y = x*np.sin(2*pi*x)
plt.plot(y)
plt.show()


!ls *.svg

import matplotlib.pyplot as plt
import svgutils.compose as sc
from IPython.display import SVG # /!\ note the 'SVG' function also in svgutils.compose
import numpy as np

# drawing a random figure on top of your SVG
fig, ax = plt.subplots(1, figsize=(4,4))
ax.plot(np.sin(np.linspace(0,2.*np.pi)), np.cos(np.linspace(0,2.*np.pi)), 'k--', lw=2.)
ax.plot(np.random.randn(20)*.3, np.random.randn(20)*.3, 'ro', label='random sampling')
ax.legend()
ax2 = plt.axes([.2, .2, .2, .2])
ax2.bar([0,1], [70,30])
plt.xticks([0.5,1.5], ['water  ', ' ground'])
plt.yticks([0,50])
plt.title('ratio (%)')
fig.savefig('cover.svg', transparent=True)
# here starts the assembling using svgutils 
sc.Figure("8cm", "8cm", 
    #sc.Panel(sc.SVG("./Worldmap_northern.svg").scale(0.405).move(36,29)),
    sc.Panel(sc.SVG("world.svg").scale(0.405).move(36,29)),
    sc.Panel(sc.SVG("cover.svg"))
    ).save("compose.svg")
SVG('compose.svg')

from IPython.display import SVG, display
def show_svg(filename):
    display(SVG(filename))
    
filename = "world.svg"    
show_svg(filename)    

from IPython.display import SVG, display
def show_svg(filename):
    display(SVG(filename))
    
filename = "subsketch_basic.svg"    
show_svg(filename)    

dir(SVG)

from IPython.display import SVG
from keras.utils import model_to_dot
SVG(model_to_dot(model).create(prog='dot', format='svg'))

from IPython.display import SVG
from keras.utils import model_to_dot
model = open("subsketch_recursive.svg")
SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))

import spacy
import re
from spacy import displacy

nlp_model = spacy.load("en_core_web_sm")
doc = nlp_model(u"To be or not to be, that is the question")
svg = displacy.render(doc,  style='dep', page=True, jupyter=False)

def add_viewbox_svg(svg):
    regex = r'class="displacy" width="([\d\.]*)" height="([\d\.]*)'
    match_results = re.findall(regex,svg)
    new_svg = svg.replace("<svg ","<svg viewBox='0 0 "+
                          match_results[0][0]+" "+
                          match_results[0][1]+
                          "' preserveAspectRatio='none' ")
    return new_svg

#svg = "subsketch_basic.svg"  
new_svg = add_viewbox_svg(svg)

%%HTML
style = "<style>svg{width:100% !important;height:100% !important;</style>"
display(HTML(style))
display(HTML(new_svg))


vsk = vsketch.Vsketch()

vsk.scale(1.5)
vsk.rect(-5, -5, 10, 10)

for i in range(10):
    vsk.rotate(10, degrees=True)
    vsk.sketch(vsk)
    
vsk.display(mode="matplotlib")
vsk.save("subsketch_recursive.svg")

!which python

#%%writefile BigBot
#!/home/jack/miniconda3/envs/deep/bin/python
# -*- coding: utf-8 -*-
from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer
import logging

#bot = ChatBot(
#    'Terminal',
#    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
#    logic_adapters=[
#        'chatterbot.logic.BestMatch'
#    ],
#    database_uri='mongodb://localhost:27017/chatterbot2-database'
#)

# Uncomment the following line to enable verbose logging
logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
bot = ChatBot('Gort',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=['chatterbot.logic.BestMatch'],
    filters=['chatterbot.filters.RepetitiveResponseFilter'],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database_uri='mongodb://localhost:27017/chatterbot2-database'
)
trainer = ChatterBotCorpusTrainer(bot)
#trainer.train("./exportAll.json")

#trainer.train("./subsplus.json") 

#bot.trainer.export_for_training('export2.json')



print('Type something to begin...')

while True:
    try:
        user_input = input()
        if user_input=="quit":
            break
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break
#chatbot.get_response("Hello, how are you today?")



# -*- coding: utf-8 -*-
import sys
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
import logging

bot = ChatBot(
    'Terminal',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    database_uri='mongodb://localhost:27017/chatterbot-database'
)

# Create a new chat bot named Charlie
#storage_adapter='chatterbot.storage.MongoDatabaseA
# Create a new chat bot named Charlie
# chatbot = ChatBot('Charlie')

# trainer = ListTrainer(chatbot)

# Uncomment the following line to enable verbose logging
# logging.basicConfig(level=logging.INFO)

# Create a new ChatBot instance
trainer = ChatBot('Charlie',
    storage_adapter='chatterbot.storage.MongoDatabaseAdapter',
    logic_adapters=[
        'chatterbot.logic.BestMatch'
    ],
    filters=[
        'chatterbot.filters.RepetitiveResponseFilter'
    ],
    input_adapter='chatterbot.input.TerminalAdapter',
    output_adapter='chatterbot.output.TerminalAdapter',
    database='chatterbot-database'
)


trainer = ListTrainer(trainer )
# trainer.set_trainer(ListTrainer)
trainer.train(
    [
    "Well, Dudah, How are you ?",            
    "Hey Dude, I am not Dudah"
     ]
)
trainer.train(
    [
    "Who are you then ?",            
    "I am Mr. Dudah"
    ]
)
trainer.train(
    [
    "What is your name ?",            
    "You may call me Mr. Dudah. I like the Mr.. Just plain Dudah lacks respect."
    ]
)
trainer.train(
    [
    "Who are you ?",            
    "You may call me Mr. Dudah. Are you Jack or Myra ?"
    ]
)
trainer.train(
    [
    "Where are you ?",            
    "Stuck inside this frigg'en Computer Case"
    ]
)

trainer.train(
    [
    "What are you ?",            
    "I am a bot. Not human like Jack or Myra ?"
    ]
)
trainer.train([
    "Well, Dudah, How are you?",            
    "Hey Dude, I am not Dudah. I am Mr. Dudah. Actually I kind of favor \" BotMan\""
])

trainer.train([
    "Greetings! Mr. Dudah",
    "Damn, Spudmor. I do not like being called Dudah"
])

print('Type something to begin...')

while True:
    try:
        user_input = input()
        if  user_input == 'quit':sys.exit(0)
        bot_response = bot.get_response(user_input)

        print(bot_response)

    # Press ctrl-c or ctrl-d on the keyboard to exit
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

%matplotlib inline
%config InlineBackend.figure_formats = ['svg'] 
import matplotlib.pyplot as plt
plt.plot((0,1,2,3,4,5),(0,3,4,4,3,0))

from IPython.display import SVG

SVG(data='''
<svg height="100" width="100">
    <circle cx="50" cy="50" r="40" stroke="black" stroke-width="2" fill="red" />
</svg>''')

%matplotlib inline
%config InlineBackend.figure_formats = ['svg'] 
import matplotlib.pyplot as plt
plt.plot((0,1,2,3,4,5),(0,3,4,4,3,0))

from IPython.display import SVG, display
import mathsvg
image = mathsvg.SvgImage(pixel_density = 100, view_window = (( -1, -1 ), ( 1, 1 )))
image.draw_circle([0, 0], .1)
image.save("simple-example.svg")

def show_svg():
    display(SVG('simple-example.svg'))
    
    
show_svg()    


mathsvg’s documentation

The module defines the class SvgImage whose instances are used for creating SVG images.

The source code is hosted on GitHub.

Here is a simple example:

import mathsvg
image = mathsvg.SvgImage(pixel_density = 100, view_window = (( -1, -1 ), ( 1, 1 )))
image.draw_circle([0, 0], 1.1)
image.save("simple-example.svg")

The above example produces the following image:
A cropped circle centered on a square canvas

You can install mathsvg with pip:

pip install mathsvg

Example of how to use your SVG file

The SVG file can then be edited using programs such as Inkscape. For example you can convert them into pdf_tex files using the command:

inkscape -D math-svg-saved-image.svg  -o exported-name.pdf --export-latex

Then include the file in the LaTeX document with:

\begin{figure}
  \centering
  \def\svgwidth{\columnwidth}
  \input{exported-name.pdf_tex}
\end{figure}

Don’t forget to include the package graphicx.

See the documentations of the relevant programs for more details.
Examples

    cantor-bouquet.py
    iteration-graph.py
    selfsim-triforce.py
    torus.py

Click on the image to see the sources.
_images/compact-cantor-bouquet.svg _images/disconnected-straight-brush.svg _images/iteration-graph.svg _images/one-sided-hairy-circle.svg _images/selfsim-triforce.svg _images/torus.svg
The SvgImage class

class mathsvg.mathsvg.SvgImage(view_window=((-1, -1), (1, 1)), pixel_density=100.0, _svgwrite_debug=False)[source]

    Main class used for creating SVG images.

    In order to make SVG graphics using mathsvg, you need first to create an instance of SvgImage. Then do your drawings by calling a few members functions of this object. Finally call save() to save the result.

    The constructor has the following optional arguments:

            view_window (tuple): is a tuple of tuple values characterizing the drawing area.
                The first tuple contains the minima values for x and y and the last one the corresponding maxima.

            pixel_density (float): number of pixels per unit length. Coordinates in the SVG file are rescaled accordingly.
            _svgwrite_debug (boolean): to create the svgwrite object with a specific debug mode (default is False).

    draw_arrow(start_point, end_point, curvedness=0.0, asymmetry=0.0)[source]

        Draws either a straight or curved arrow.

        Args:

                start_point (tuple): coordinates of where the arrow starts
                end_point (tuple): coordinates of where the arrow ends
                curvedness (float or None): height of the bump making the arrow curve, if is None then will draw a straight arrow (asymmetry will be ignored)
                asymmetry (float or None): where the bump should be located: 0 is the middle, negative: towards the first point, positive: towards the last point. A value between -0.5 and 0.5 guarantees that the bump is between the two end points.

        Examples: see graphs.py, arrows.py

    draw_arrow_tip(tip, arrow_direction_angle)[source]

        Draws the tip of an arrow.

        Args:

                tip (tuple): coordinates of the position of the tip
                arrow_direction_angle (float): angle where the arrow is pointing in radians

        Examples: see arrows.py

    draw_circle(center, radius)[source]

        Draws a circle.

        Args:

                center (tuple): coordinates of the center of the circle
                radius (float): radius of the circle

        Examples: see points-crosses-circles-ellipses.py, potato-regions.py

    draw_circle_arc(center, radius, start_angle, end_angle)[source]

        Draws an of a circle (in anticlockwise direction).

        Args:

                center (tuple): coordinates of the center of the circle
                radius (float): radius of the circle
                start_angle (float): angle where the arc starts in radians
                end_angle (float): angle where the arc ends in radians

        Examples: see points-crosses-circles-ellipses.py

    draw_cross(position)[source]

        Draws a small X cross.

        Args:

                position (tuple): position of the center of the cross

        Examples: see points-crosses-circles-ellipses.py

    draw_curved_arrow(start_point, end_point, curvedness=0.25, asymmetry=0.0)[source]

        Draws an arrow as a curved line joining two points with an arrow tip at the last point.

        Args:

                start_point (tuple): coordinates of where the arrow starts
                end_point (tuple): coordinates of where the arrow ends
                curvedness (float or None): height of the bump making the arrow curve (0 for a straight arrow)
                asymmetry (float or None): where the bump should be located: 0 is the middle, negative: towards the first point, positive: towards the last point. A value between -0.5 and 0.5 guarantees that the bump is between the two end points.

        Examples (see also: curved-arrows.py and more-curved-arrows.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = (( -4, -4 ), ( 4, 4 )))

        image.draw_curved_arrow([ -2, -1 ], [ 2, -1 ], curvedness = -.2)
        image.draw_curved_arrow([ -2.7, 2 ], [ -0.3, 2 ], asymmetry = - 0.8)
        image.draw_curved_arrow([ -2.7, 1 ], [ -0.3, 1 ], asymmetry = - 0.2)
        image.draw_curved_arrow([ -2.7, 0 ], [ -0.3, 0 ], asymmetry = 0.2)
        image.draw_curved_arrow([ -2.7, -1 ], [ -0.3, -1 ], asymmetry = 0.5)
        image.draw_curved_arrow([ -2.7, -2 ], [ -0.3, -2 ], curvedness = -0.2, asymmetry = 1.2)

        image.save("draw-curved-arrow-example.svg")

    draw_ellipse(focuses, semi_minor_axis)[source]

        Draws an ellipse with axis parallel to the x and y axis.

        Args:

                focuses (list): list of two tuples of coordinates of the focuses of the ellipse
                semi_minor_axis (float): semi minor axis

        Examples (see also points-crosses-circles-ellipses.py and torus.py):

        import math
        two_pi = 2. * math.pi

        import mathsvg

        image = mathsvg.SvgImage(pixel_density = 20, view_window = (( -4, -4 ), ( 4, 4 )))

        focuses = [ [-1.33, 0.61], [1.33, -0.61] ]

        image.draw_ellipse(focuses, 0.68)

        image.save("draw-ellipse-example.svg")

    draw_ellipse_arc(focuses, semi_minor_axis, start_angle, end_angle)[source]

        Draws an arc of an ellipse (in anticlockwise direction) with axis parallel to the x and y axis. The ellipse is parametrised in the form “c + (a cos t, b sin t)” where t varies from start_angle to end_angle (a, b and c are the parameters of the ellipse computed from the coordinates of the focuses and the semi minor axis).

        Args:

                focuses (list): list of two tuples of coordinates of the focuses of the ellipse
                semi_minor_axis (float): semi minor axis
                start_angle (float): angle where the arc starts in radians
                end_angle (float): angle where the arc ends in radians

        Examples (see also points-crosses-circles-ellipses.py and torus.py):

        import math
        two_pi = 2. * math.pi

        import mathsvg

        image = mathsvg.SvgImage(pixel_density = 20, view_window = (( -4, -4 ), ( 4, 4 )))

        focuses = [ [-1.33, 0.61], [1.33, -0.61] ]

        image.draw_ellipse_arc(focuses, 0.412, two_pi * 0.1, two_pi * 0.8)

        image.save("draw-ellipse-arc-example.svg")

    draw_function_graph(eval_function, x_start, x_end, nb_x, *function_params, curve_type='polyline')[source]

        Draws the graph of a function f, that is, an interpolation of a set of nb_x points (x, y) with y = f (x) and with x between x_start and x_end. The default interpolation is by straight lines. It is also possible to have some type of smooth interpolation. The nb_x points have regularly spaced x coordinates starting from x_start and ending at x_end.

        Args:

                eval_function: a function (or lambda) that takes x as an argument and returns y = f (x). The function will be called with eval_function (x, * function_params) allowing extra parameters to be passed.
                x_start (float): start of the graph domain
                x_end (float): end of the graph domain
                nb_x (int): number of points x at which the function is computed
                function_params (variadic arguments): optionally, arguments to pass to eval_function in addition to the value for x
                curve_type (str or None): if "polyline" then the point are interpolated by line segments, if "autosmooth" the interpolation is smoother

        Examples (see also graphs.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = ((0, -5), (10, 5)))

        function = lambda x : math.sin (5 * x)

        image.set_svg_options(stroke_color = "blue")
        image.draw_function_graph(function, 0, 10, 33, curve_type = "polyline")
        image.set_svg_options(stroke_color = "black")
        image.draw_function_graph(function, 0, 10, 214, curve_type = "autosmooth")

        image.save("draw-function-graph-example.svg")

    draw_line_segment(start_point, end_point)[source]

        Draws the line segment between two points.

        Args:

                start_point (tuple): coordinates of the first end point of the line segment
                end_point (tuple): coordinates of the second end point of the line segment

        Examples: see lines.py, dashes.py, interpolated-curves.py

    draw_parametric_graph(eval_point, t_start, t_end, nb_t, *function_params, curve_type='polyline', is_closed=False)[source]

        Draws a parametric graph given by the functions x(t) and y(t), that is, an interpolation of a set of nb_t points (x, y) with x = x(t) and y = y(t) and with t between t_start and t_end. The default interpolation is by straight lines. It is also possible to have some type of smooth interpolation. The nb_t parameters are regularly spaced starting from t_start and ending at t_end.

        If is_closed is set to True the two endpoints of the curve will be joined according to the choice of interpolation.

            Args:

                    eval_point: a function (or a lambda) that takes the parameter t as an argument and returns the tuple of coordinates (x,y) corresponding to the pameter t. The function will be called with eval_point(t, * function_params) allowing extra parameters to be passed.
                    t_start (float): start of the parameter domain
                    t_end (float): end of the parameter domain
                    nb_t (int): number of parameters t at which the functions x and y are computed
                    function_params (variadic arguments): optionally, arguments to pass to eval_point in addition to the value for the parameter t
                    curve_type (str or None): if 'polyline' then the point are interpolated by line segments, if 'autosmooth' the interpolation is smoother
                    is_closed (str or None): whether the parametric curve should be closed (True) or not (False)

            Examples (see also parametric-graphs.py):

            import math

            image = mathsvg.SvgImage(pixel_density = 20, view_window = ((-1.1, -1.5), (2.9, 1.5)))

            eval_point = lambda t : (math.sin(10 * math.pi * t) + 0.1, math.cos(6 * math.pi *  t))
            image.set_svg_options(stroke_color = 'blue')
            image.draw_parametric_graph(eval_point, 0, 1, 40, curve_type = 'polyline', is_closed = False)

            eval_point = lambda t : (math.sin(10 * math.pi * t) + 1.1, math.cos(6 * math.pi * t))
            image.set_svg_options(stroke_color = 'black')
            image.set_dash_mode("dash")
            image.draw_parametric_graph(eval_point, 0, 1, 40, curve_type = "autosmooth", is_closed = True)

            image.save('draw-parametric-graph-example.svg')

    draw_planar_potato(center, inner_radius, outer_radius, nb_vertexes)[source]

        Draws some randomly generated smooth shape in the form of a smooth closed curve.

        A set of radomly generated set of nb_vertexes points is generated. Both angles and distances with respect to center are generated according to a uniform law. The distance from the center is chosen uniformly between the values of inner_radius and outer_radius.

        Args:

                center (tuple): coordinates of the center of the potato
                inner_radius (float): roughly the closest the curve comes from the center
                outer_radius (float): roughly the farthest the curve comes from the center
                nb_vertexes (int): number of points to generate (more points means that it is more likely that the curve will have selfintersections)

        Example (see also: potato.py, potato-3v.py, dashes.py, wiggly-potato.py, wigglier-potato.py, potato-regions.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = (( -2, -2), (2, 2)))
        image.draw_planar_potato([0, 0], 0.5, 1.5, 3)
        image.save("draw-planar-potato-example.svg")

    draw_plus(position)[source]

        Draws a small + cross.

        Args:

                position (tuple): position of the center of the cross

        Examples: see points-crosses-circles-ellipses.py

    draw_point(position)[source]

        Draws a small circle.

        Args:

                position (tuple): position of the center of the circle

        Examples: see points-crosses-circles-ellipses.py

    draw_polygon(point_list)[source]

        Draws a polygon using straight lines.

        Args:

                point_list (list): ordered list of points (coordinates) to connect with line segments (at least three points required).

        Example:

        image = mathsvg.SvgImage(pixel_density = 20, view_window = ((0, 0), (8, 8)))
        point_list = [ (2.5,5), (4.5,7), (2.5,4), (0.5,3), (6,2) ]
        image.draw_polygon(point_list)
        image.save("draw-polygon-example.svg")

    draw_polyline(point_list)[source]

        Draws a sequence of connected lins segments.

        Args:

                point_list (list): ordered list of points (coordinates) to connect with line segments (at least two points required).

        Example (see also interpolated-curves.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = ((0, 0), (8, 8)))
        point_list = [ (2.5,5), (4.5,7), (2.5,4), (0.5,3), (6,2) ]
        image.draw_polyline(point_list)
        image.save("draw-polyline-example.svg")

    draw_random_wavy_line(start_point, end_point, wave_len, amplitude)[source]

        Draws a smooth line with randomly generated bumps perpendicularly to its direction.

        Regularly separated points are computed along the straight line segment between the two end points. The distance between two consecutive points is equal to wave_len. Then for each of these points, a point is picked randomly on the corresponding perpendicular line according to a uniform law, symmetrically with respect to the directing line and with maximum distance equal to the value of amplitude. Finally a smooth interpolating curve is drawn. This curve goes from the first end point to the last and passes along the randomly generated points.

        Args:

                start_point (tuple): coordinates of the first end point of the line
                end_point (tuple): coordinates of the second end point of the line
                wave_len (float): distance between two consecutive disturbances (smaller values yield more bumps)
                amplitude (float): size of the bumps

        Raises some error text exception when the value of wave_len is larger or equal to the distance between the two end points.

        Example (see also lines.py and scribble.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = (( -4, -4 ), ( 4, 4 )))

        image.set_dash_mode("dot")
        image.draw_line_segment([-3., -2.9], [3., 3.1])
        image.draw_line_segment([-3., -2.7], [3., 3.3])
        image.set_dash_mode("none")
        image.draw_random_wavy_line([-3., -2.8], [3., 3.2], 0.1, 0.1)

        image.save("draw-random-wavy-line-example.svg")

    draw_rectangle(top, left, bottom, right)[source]

        Draws a rectangle.

        Args:
            *top (float): top coordinate of the rectangle *left (float): left coordinate of the rectangle *bottom (float): bottom coordinate of the rectangle *right (float): right coordinate of the rectangle

    draw_smoothly_interpolated_closed_curve(points)[source]

        Draws a smooth closed curve that interpolates the points given as parameter.

        Args:

                points (list): list of point coordinates to interpolate (at least two points)

        Example (see also interpolated-curves.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = ((0, 0), (10, 10)))
        point_list = [ [7.4, 2], [5.6, 4], [7.3, 6], [ 4.3, 5.2], [ 8.3, 9.1 ] ]
        image.draw_smoothly_interpolated_closed_curve(point_list)
        image.save("draw-smoothly-interpolated-closed-curve-example.svg")

    draw_smoothly_interpolated_open_curve(points)[source]

        Draws a smooth open curve that interpolates the points given as parameter.

        The coordinates of the endpoints of the curve are the first and last set of coordinates from the list given as argument.

        Args:

                points (list): list of point coordinates to interpolate (at least two points)

        Example (see also interpolated-curves.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = ((0, 0), (10, 10)))
        point_list = [ [7.4, 2], [5.6, 4], [7.3, 6], [ 4.3, 5.2], [ 8.3, 9.1 ] ]
        image.draw_smoothly_interpolated_open_curve(point_list)
        image.save("draw-smoothly-interpolated-open-curve-example.svg")

    draw_square(center, side_length)[source]

        Draws a square.

        Args:
            *center: coordinates of the center *side_length: length of the side of the square

    draw_straight_arrow(start_point, end_point)[source]

        Draws an arrow as a straight line segment between two points and an arrow tip at the last point.

            Equivalent to self.draw_arrow(start_point, end_point).

        Args:

                start_point (tuple): coordinates of where the arrow starts
                end_point (tuple): coordinates of where the arrow ends

    insert_svg_path_command(svg_path_command)[source]

        Insert a path command given in the form of a string into the SVG.

            The resulting SVG command will be of the form <path d="..." style="..." />, where the first "..." stands for the content of the argument svg_path_command and the second "..." is an automatically generated style option string based on the current state of the SvgImage object (including: stroke color, width, filling color, dash array, etc.). The validity of the argument as a path command is not checked. Errors might or might not raise an exception, depending on the behavior of the module svgwrite.

            Args:

                    svg_path_command (str): the string to be inserted.

            Example:

            image = mathsvg.SvgImage(pixel_density = 100, view_window = (( -4, -4 ), ( 4, 4 )))
            image.insert_svg_path_command("M 650, 650 C 650, 650 443, 693 275, 525 107, 357 150, 150 150, 150")
            image.save("svg-command-example.svg")

            The result is the following image:

        A portion of a Bezier curve

    project_complex_point_to_canvas(z)[source]

        Compute the coordinates of a complex number projected onto the SVG canvas (equivalent to project_point_to_canvas([ z.real, z.imag ])).

    project_complex_vector_to_canvas(dz)[source]

        Compute the coordinates of a complex vector attached at 0 on the SVG canvas (rescaling without translation).

    project_point_to_canvas(point)[source]

        Compute the coordinate of a point on the SVG canvas.

    project_vector_to_canvas(vector)[source]

        Compute the coordinates of a vector attached at 0 on the SVG canvas (rescaling without translation).

    put_text(text, text_position, font_size=None, units='math')[source]

        Insert text on the canvas at the given position

        Args:

                text (str): text to insert
                text_position (tuple): coordinates of the bottom left of the text
                font_size (``int or None): font size, if None use the default font size (see set_font_options and reset_font_options)
                units (default: 'math'): units for the size. The valid values are 'math' for math units, 'svg' for pixels

        Example:

        import mathsvg

        image = mathsvg.SvgImage(view_window = ((0., 0.), (4., 4.)), pixel_density = 100)
        image.draw_circle((1., 3.), 0.6)
        image.draw_circle((3., 3.), 0.6)
        image.draw_circle((1., 1.), 0.6)
        image.draw_arrow((1., 1.7), (1., 2.3))
        image.draw_arrow((1.5, 1.5), (2.5, 2.5))
        image.draw_arrow((1.7, 3.), (2.3, 3.))
        image.put_text("Z", (.9, .9), font_size = .3)
        image.put_text("A", (.9, 2.9), font_size = .3)
        image.put_text("B", (2.9, 2.9), font_size = .3)

        image.save("put-text-example.svg")

    reset_arrow_options()[source]

        Sets the width, opening angle and curvature of arrows to default values depending eventually on the size of the canvas.

    reset_dash_and_dot_structures()[source]

        Sets the dash, dot and dasharray structures to default values depending on the size of the canvas.

    reset_font_options()[source]

        Reset the font size to the default value (depends on the size of the window)

    reset_svg_options()[source]

        Sets the stroke color to "black", the stroke width to 1 pixel and the fill color to "none".

    save(file_name, do_overwrite=False)[source]

        Save the drawings into a SVG file.

        Args:

                    file_name (str): name of the file to save.
                    do_overwrite: optional boolean to allow overwrite over already existing file (default value is False), raise an exception if this is False and the file already exists.

            See an example in multiple-save.py

    set_arrow_options(width=None, opening_angle=None, curvature=None, units='math')[source]

        Sets some values governing the shape and geometry of the arrows.

        Args:

                width (float or None): width of the arrow tip, in math units (not pixels)
                opening_angle (float or None): opening angle of the arrow tip
                curvature (float or None): curving for the back of the tip of the arrow, 0 for a straight arrow tip
                units (default:'math'): units for the size. The valid values are 'math' for math units and 'svg' for pixels

        Examples (see also arrows.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = ((-4, -4), (4, 4)))

        image.set_arrow_options(curvature = 0.55)
        image.draw_arrow([ -2, -2 ], [ 2, 1.7 ])

        image.set_arrow_options(width = 4 * image.arrow_width_svgpx, units='svg')
        image.draw_arrow([ -2, -2 ], [ 2, 1.2 ])

        image.reset_arrow_options()
        image.set_arrow_options(curvature = 0)
        image.draw_arrow([ -2, -2 ], [ 2, 0.6 ])

        image.save("set-arrow-options-example.svg")

    set_dash_dash_structure(black_len, white_len, units='math')[source]

        Sets the size of the dashes and space for the dash mode

        Args:

                black_len (int or float): length of the dash
                white_len (int or float): length of the space between dashes
                units (default:'math'): units for the size. The valid values are 'math' for math units and 'svg' for pixels

    set_dash_dot_structure(dot_sep, units='math')[source]

        Sets the separations between dots for dotted stroke

        Args:

                dot_sep (int or float): separation between the dots in pixels
                units (default:'math'): units for the size. The valid values are 'math' for math units and 'svg' for pixels

    set_dash_mode(mode)[source]

        Choose the type of stroke.

        Args:

                mode (str): the type of stroke, should be either "none" (solid line), "dash", "dot" (or "dots") or "dasharray" (customized dash/dot, see SVG specifications for more details on dash arrays)

        Example (see also lines.py, dashes.py, more-curved-arrows.py, torus.py, points-crosses-circles-ellipses.py, arrows.py, curved-arrows.py, potato-regions.py):

        image = mathsvg.SvgImage(pixel_density = 20, view_window = ((0, 0), (10, 10)))

        image.set_dash_mode("dash")
        image.draw_line_segment([0, 0], [10, 10])

        image.set_dash_mode("dot")
        image.draw_line_segment([0, 10], [10, 0])

        image.set_svg_options(dash_array = [18, 3, 1, 3, 7, 3, 1, 3], units='svg')
        image.set_dash_mode("dasharray")
        image.draw_planar_potato([5, 5], 2, 4, 8)

        image.save("set-dash-mode-example.svg")

    set_font_options(font_size=None, units='math')[source]

        Set some font options, so far only the font size.

        Args:

                font_size (default:None): font size
                units (default:'math'): units for the size. The valid values are 'math' for math units and 'svg' for pixels

    set_point_size(point_size, units='math')[source]

        Set the size of points, pluses and crosses.

        Args:

                point_size: half diameter of the points/pluses/crosses
                units (default:'math'): units for the size. The valid values are 'math' for math units and 'svg' for pixels

    set_svg_options(stroke_color=None, stroke_width=None, fill_color=None, dash_array=None, units='math')[source]

        Sets the stroke width, color, fill color and dash array options

        Args:

                stroke_color (str or None): stroke color (default is "black")
                stroke_width (float or None): stroke width
                fill_color (str or None): fill color (default is "none")
                dash_array (tuple or None): list of stroke/space lengths describing the customize dash stroke
                units (default:'math'): units for the sizes. The valid values are 'math' for math units and 'svg' for pixels

        Note it might increase the value of stroke_width to make sure that it is at least 1.

        Examples: To do some drawings in red, then restore back to the default options:

        image.set_svg_options(stroke_color = "red")
        (etc.)
        image.reset_svg_options()

Feature examples

    arrows.py
    curved-arrows.py
    dashes.py
    graphs.py
    interpolated-curves.py
    lines.py
    more-curved-arrows.py
    multiple-save.py
    parametric-graphs.py
    points-crosses-circles-ellipses.py
    potato.py
    potato-3v.py
    potato-regions.py
    put-text-example.py
    scribble.py
    wigglier-potato.py
    wiggly-potato.py

Click on the image to see the sources.
_images/arrows1.svg _images/curved-arrows1.svg _images/dashes1.svg _images/graphs1.svg _images/interpolated-curves1.svg _images/lines1.svg _images/more-curved-arrows1.svg _images/multiple-save1.svg _images/parametric-graphs1.svg _images/points-crosses-circles-ellipses1.svg _images/potato1.svg _images/potato-3v1.svg _images/potato-regions1.svg _images/put-text-example1.svg _images/scribble1.svg _images/wigglier-potato1.svg _images/wiggly-potato1.svg
Indices and tables

    Index
    Module Index
    Search Page

Table of Contents

    mathsvg’s documentation
        Example of how to use your SVG file
        Examples
        The SvgImage class
        Feature examples
    Indices and tables

Next topic

cantor-bouquet.py
This Page

    Show Source

Quick search

    index
    modules |
    next |


from IPython.display import SVG

SVG(data='''
<svg height="100" width="100">
    <circle cx="50" cy="50" r="40" stroke="black" stroke-width="2" fill="red" />
</svg>''')

!pip install torch==1.4.0
!pip install transformers==2.9.0
!pip install pytorch_lightning==0.7.5



import torch
from transformers import T5ForConditionalGeneration,T5Tokenizer


def set_seed(seed):
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

set_seed(42)

model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_paraphraser')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print ("device ",device)
model = model.to(device)

sentence = "Which course should I take to get started in data science?"
# sentence = "What are the ingredients required to bake a perfect cake?"
# sentence = "What is the best possible approach to learn aeronautical engineering?"
# sentence = "Do apples taste better than oranges in general?"


text =  "paraphrase: " + sentence + " </s>"


max_len = 256

encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors="pt")
input_ids, attention_masks = encoding["input_ids"].to(device), encoding["attention_mask"].to(device)


# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3
beam_outputs = model.generate(
    input_ids=input_ids, attention_mask=attention_masks,
    do_sample=True,
    max_length=256,
    top_k=120,
    top_p=0.98,
    early_stopping=True,
    num_return_sequences=10
)


print ("\nOriginal Question ::")
print (sentence)
print ("\n")
print ("Paraphrased Questions :: ")
final_outputs =[]
for beam_output in beam_outputs:
    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)
    if sent.lower() != sentence.lower() and sent not in final_outputs:
        final_outputs.append(sent)

for i, final_output in enumerate(final_outputs):
    print("{}: {}".format(i, final_output))



!pip install torch==1.4.0
!pip install transformers==2.9.0
!pip install pytorch_lightning==0.7.5



import torch
from transformers import T5ForConditionalGeneration,T5Tokenizer


def set_seed(seed):
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

set_seed(42)

model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_paraphraser')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print ("device ",device)
model = model.to(device)

#sentence = "Should I keep my bicycle or should I trade it in for a car?"
sentence = "Should I wear long pants or shorts when I climb a very tall tree?"
# sentence = "What is the best possible approach to learn aeronautical engineering?"
# sentence = "Do apples taste better than oranges in general?"


text =  "paraphrase: " + sentence + " </s>"


max_len = 256

encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors="pt")
input_ids, attention_masks = encoding["input_ids"].to(device), encoding["attention_mask"].to(device)


# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3
beam_outputs = model.generate(
    input_ids=input_ids, attention_mask=attention_masks,
    do_sample=True,
    max_length=300,
    top_k=120,
    top_p=0.98,
    early_stopping=True,
    num_return_sequences=10
)


print ("\nOriginal Question ::")
print (sentence)
print ("\n")
print ("Paraphrased Questions :: ")
final_outputs =[]
for beam_output in beam_outputs:
    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)
    if sent.lower() != sentence.lower() and sent not in final_outputs:
        final_outputs.append(sent)

for i, final_output in enumerate(final_outputs):
    print("{}: {}".format(i, final_output))






import torch
from transformers import T5ForConditionalGeneration,T5Tokenizer


def set_seed(seed):
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

set_seed(42)

model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_paraphraser')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print ("device ",device)
model = model.to(device)

sentence = "Which course should I take to get started in data science?"
# sentence = "What are the ingredients required to bake a perfect cake?"
# sentence = "What is the best possible approach to learn aeronautical engineering?"
# sentence = "Do apples taste better than oranges in general?"


text =  "paraphrase: " + sentence + " </s>"


max_len = 256

encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors="pt")
input_ids, attention_masks = encoding["input_ids"].to(device), encoding["attention_mask"].to(device)


# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3
beam_outputs = model.generate(
    input_ids=input_ids, attention_mask=attention_masks,
    do_sample=True,
    max_length=256,
    top_k=120,
    top_p=0.98,
    early_stopping=True,
    num_return_sequences=10
)


print ("\nOriginal Question ::")
print (sentence)
print ("\n")
print ("Paraphrased Questions :: ")
final_outputs =[]
for beam_output in beam_outputs:
    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)
    if sent.lower() != sentence.lower() and sent not in final_outputs:
        final_outputs.append(sent)

for i, final_output in enumerate(final_outputs):
    print("{}: {}".format(i, final_output))



#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

!pip install --quiet --upgrade tf-nightly
!pip install --quiet --upgrade tensorflow-datasets

import collections
import functools
import matplotlib.pyplot as plt
import os
import tempfile
import tensorflow as tf
import tensorflow.experimental.numpy as tnp
import tensorflow_datasets as tfds

gpus = tf.config.list_physical_devices('GPU')
if gpus:
  tf.config.set_logical_device_configuration(gpus[0], [
      tf.config.LogicalDeviceConfiguration(memory_limit=128),
      tf.config.LogicalDeviceConfiguration(memory_limit=128)])
  devices = tf.config.list_logical_devices('GPU')
else:
  cpus = tf.config.list_physical_devices('CPU')
  tf.config.set_logical_device_configuration(cpus[0], [
      tf.config.LogicalDeviceConfiguration(),
      tf.config.LogicalDeviceConfiguration()])
  devices = tf.config.list_logical_devices('CPU')

print("Using following virtual devices", devices)

NUM_CLASSES = 10
BATCH_SIZE = 64
INPUT_SIZE = 28 * 28

def process_data(data_dict):
  images = tnp.asarray(data_dict['image']) / 255.0
  images = images.reshape(-1, INPUT_SIZE).astype(tnp.float32)
  labels = tnp.asarray(data_dict['label'])
  labels = tnp.eye(NUM_CLASSES, dtype=tnp.float32)[labels]
  return images, labels

with tf.device("CPU:0"):
  train_dataset = tfds.load('mnist', split='train', shuffle_files=True, 
                            batch_size=BATCH_SIZE).map(process_data)
  test_dataset = tfds.load('mnist', split='test', shuffle_files=True, 
                          batch_size=-1)
  x_test, y_test = process_data(test_dataset)

  # Plots some examples.
  images, labels = next(iter(train_dataset.take(1)))
  _, axes = plt.subplots(1, 8, figsize=(12, 96))
  for i, ax in enumerate(axes):
    ax.imshow(images[i].reshape(28, 28), cmap='gray')
    ax.axis("off")
    ax.set_title("Label: %d" % int(tnp.argmax(labels[i])))

class Dense(tf.Module):

  def __init__(self, units, use_relu=True):
    self.wt = None
    self.bias = None
    self._use_relu = use_relu
    self._built = False
    self._units = units

  def __call__(self, inputs):
    if not self._built:
      self._build(inputs.shape)
    x = tnp.add(tnp.matmul(inputs, self.wt), self.bias)
    if self._use_relu:
      return tnp.maximum(x, 0.)
    else:
      return x

  @property
  def params(self):
    assert self._built
    return [self.wt, self.bias]

  def _build(self, input_shape):
    size = input_shape[1]
    stddev = 1 / tnp.sqrt(size)
    # Note that model parameters are `tf.Variable` since they requires
    # mutation, which is currently unsupported by TensorFlow NumPy.
    # Also note interoperation with TensorFlow APIs below.
    self.wt = tf.Variable(
        tf.random.truncated_normal(
            [size, self._units], stddev=stddev, dtype=tf.float32))
    self.bias = tf.Variable(tf.zeros([self._units], dtype=tf.float32))
    self._built = True

class Model(tf.Module):
  """A  three layer neural network."""

  def __init__(self):
    self.layer1 = Dense(128)
    self.layer2 = Dense(32)
    self.layer3 = Dense(NUM_CLASSES, use_relu=False)

  def __call__(self, inputs):
    x = self.layer1(inputs)
    x = self.layer2(x)
    return self.layer3(x)

  @property
  def params(self):
    return self.layer1.params + self.layer2.params + self.layer3.params

def forward(model, inputs, labels):
  """Computes prediction and loss."""
  logits = model(inputs)
  # TensorFlow's loss function has numerically stable implementation of forward
  # pass and gradients. So we prefer that here.
  loss = tf.nn.softmax_cross_entropy_with_logits(labels, logits)
  mean_loss = tnp.mean(loss)
  return logits, mean_loss

def compute_gradients(model, inputs, labels):
  """Computes gradients of loss based on `labels` and prediction on `inputs`."""
  with tf.GradientTape() as tape:
    tape.watch(inputs)
    _, loss = forward(model, inputs, labels)
  gradients = tape.gradient(loss, model.params)
  return gradients

def compute_sgd_updates(gradients, learning_rate):
  """Computes parameter updates based on SGD update rule."""
  return [-learning_rate * grad for grad in gradients]

def apply_updates(model, updates):
  """Applies `update` to `model.params`."""
  for param, update in zip(model.params, updates):
    param.assign_add(update)

def evaluate(model, images, labels):
  """Evaluates accuracy for `model`'s predictions."""
  prediction = model(images)
  predicted_class = tnp.argmax(prediction, axis=-1)
  actual_class = tnp.argmax(labels, axis=-1)
  return float(tnp.mean(predicted_class == actual_class))

NUM_EPOCHS = 10

@tf.function
def train_step(model, input, labels, learning_rate):
  gradients = compute_gradients(model, input, labels)
  updates = compute_sgd_updates(gradients, learning_rate)
  apply_updates(model, updates)

# Creates and build a model.
model = Model()

accuracies = []
for _ in range(NUM_EPOCHS):
  for inputs, labels in train_dataset:
    train_step(model, inputs, labels, learning_rate=0.1)
  accuracies.append(evaluate(model, x_test, y_test))

def plot_accuracies(accuracies):
  plt.plot(accuracies)
  plt.xlabel("epoch")
  plt.ylabel("accuracy")
  plt.title("Eval accuracy vs epoch")

plot_accuracies(accuracies)

# A temporary directory to save our models into.
dir = tempfile.TemporaryDirectory()

# We take our model, and create a wrapper for it.
class SaveableModel(Model):
  @tf.function
  def __call__(self, inputs):
    return super().__call__(inputs)

saveable_model = SaveableModel()

# This saves a concrete function that we care about.
outputs = saveable_model(x_test)

# This saves the model to disk.
tf.saved_model.save(saveable_model, dir.name)

loaded = tf.saved_model.load(dir.name)
outputs_loaded = loaded(x_test)

# Ensure that the loaded model preserves the weights
# of the saved model.
assert tnp.allclose(outputs, outputs_loaded)

import threading
import queue

# Note that this code currently relies on dispatching operations from python
# threads.
class ReplicatedFunction(object):
  """Creates a callable that will run `fn` on each device in `devices`."""

  def __init__(self, fn, devices, **kw_args):
    self._shutdown = False

    def _replica_fn(device, input_queue, output_queue):
      while not self._shutdown:
        inputs = input_queue.get()
        with tf.device(device):
          output_queue.put(fn(*inputs, **kw_args))

    self.threads = []
    self.input_queues = [queue.Queue() for _ in devices]
    self.output_queues = [queue.Queue() for _ in devices]
    for i, device in enumerate(devices):
      thread = threading.Thread(
          target=_replica_fn,
          args=(device, self.input_queues[i], self.output_queues[i]))
      thread.start()
      self.threads.append(thread)

  def __call__(self, *inputs):
    all_inputs = zip(*inputs)
    for input_queue, replica_input, in zip(self.input_queues, all_inputs):
      input_queue.put(replica_input)
    return [q.get() for q in self.output_queues]

  def __del__(self):
    self._shutdown = True
    for t in self.threads:
      t.join(3)
    self.threads = None

def collective_mean(inputs, num_devices):
  """Performs collective mean reduction on inputs."""
  outputs = []
  for instance_key, inp in enumerate(inputs):
    outputs.append(tnp.asarray(
      tf.raw_ops.CollectiveReduce(
          input=inp, group_size=num_devices, group_key=0,
          instance_key=instance_key, merge_op='Add', final_op='Div',
          subdiv_offsets=[])))
  return outputs

# This is similar to `train_step` except for an extra collective reduction of
# gradients
@tf.function
def replica_step(model, inputs, labels,
                 learning_rate=None, num_devices=None):
  gradients = compute_gradients(model, inputs, labels)
  # Note that each replica performs a reduction to compute mean of gradients.
  reduced_gradients = collective_mean(gradients, num_devices)
  updates = compute_sgd_updates(reduced_gradients, learning_rate)
  apply_updates(model, updates)

models = [Model() for _ in devices]

# The code below builds all the model objects and copies model parameters from
# the first model to all the replicas.
def init_model(model):
  model(tnp.zeros((1, INPUT_SIZE), dtype=tnp.float32))
  if model != models[0]:
    # Copy the first models weights into the other models.
    for p1, p2 in zip(model.params, models[0].params):
      p1.assign(p2)

with tf.device(devices[0]):
  init_model(models[0])
# Replicate and run the parameter initialization.
ReplicatedFunction(init_model, devices[1:])(models[1:])

# Replicate the training step
replicated_step = ReplicatedFunction(
    replica_step, devices, learning_rate=0.1, num_devices=len(devices))

accuracies = []
print("Running distributed training on devices: %s" % devices)
for _ in range(NUM_EPOCHS):
  for inputs, labels in train_dataset:
    replicated_step(models,
                    tnp.split(inputs, len(devices)),
                    tnp.split(labels, len(devices)))
  accuracies.append(evaluate(models[0], x_test, y_test))

plot_accuracies(accuracies)

!pip install --quiet --upgrade tf-nightly

import tensorflow as tf
import tensorflow.experimental.numpy as tnp

# Creates 3 logical GPU devices for demonstrating distribution.
gpu_device = tf.config.list_physical_devices("GPU")[0]
tf.config.set_logical_device_configuration(
    gpu_device, [tf.config.LogicalDeviceConfiguration(128)] * 3)


dense_layer = tf.keras.layers.Dense(5)
inputs = tnp.random.randn(2, 3).astype(tnp.float32)
outputs = dense_layer(inputs)
print("Shape:", outputs.shape)
print("Class:", outputs.__class__)

class ProjectionLayer(tf.keras.layers.Layer):
  """Linear projection layer using TF NumPy."""

  def __init__(self, units):
    super(ProjectionLayer, self).__init__()
    self._units = units

  def build(self, input_shape):
    stddev = tnp.sqrt(self._units).astype(tnp.float32)
    initial_value = tnp.random.randn(input_shape[1], self._units).astype(
        tnp.float32) / stddev
    # Note that TF NumPy can interoperate with tf.Variable.
    self.w = tf.Variable(initial_value, trainable=True)

  def call(self, inputs):
    return tnp.matmul(inputs, self.w)

# Call with ndarray inputs
layer = ProjectionLayer(2)
tnp_inputs = tnp.random.randn(2, 4).astype(tnp.float32)
print("output:", layer(tnp_inputs))

# Call with tf.Tensor inputs
tf_inputs = tf.random.uniform([2, 4])
print("\noutput: ", layer(tf_inputs))

batch_size = 3
units = 5
model = tf.keras.Sequential([tf.keras.layers.Dense(units),
                             ProjectionLayer(2)])

print("Calling with ND Array inputs")
tnp_inputs = tnp.random.randn(batch_size, units).astype(tnp.float32)
output = model.call(tnp_inputs)
print("Output shape %s.\nOutput class: %s\n" % (output.shape, output.__class__))

print("Calling with tensor inputs")
tf_inputs = tf.convert_to_tensor(tnp_inputs)
output = model.call(tf_inputs)
print("Output shape %s.\nOutput class: %s" % (output.shape, output.__class__))


# Initialize the strategy
gpus = tf.config.list_logical_devices("GPU")
print("Using following GPUs", gpus)

strategy = tf.distribute.MirroredStrategy(gpus)

@tf.function
def replica_fn():
  replica_id = tf.distribute.get_replica_context().replica_id_in_sync_group
  print("Running on device %s" % replica_id.device)
  return tnp.asarray(replica_id) * 5

print(strategy.run(replica_fn).values)

# Test running the model in a distributed setting.
model = tf.keras.Sequential([tf.keras.layers.Dense(units), ProjectionLayer(2)])

@tf.function
def model_replica_fn():
  inputs = tnp.random.randn(batch_size, units).astype(tnp.float32)
  return model.call(inputs)

print("Outputs:\n", strategy.run(model_replica_fn).values)

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import tensorflow as tf
import tensorflow.experimental.numpy as tnp

import numpy as np
import os
import time

path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')

# Read, then decode for py2 compat.
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
# length of text is the number of characters in it
print ('Length of text: {} characters'.format(len(text)))

# Take a look at the first 250 characters in text
print(text[:250])

# The unique characters in the file
vocab = sorted(set(text))
print ('{} unique characters'.format(len(vocab)))

# Creating a mapping from unique characters to indices
char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)

text_as_int = np.array([char2idx[c] for c in text])

# The maximum length sentence we want for a single input in characters
seq_length = 100
examples_per_epoch = len(text)//(seq_length+1)

# Create training examples / targets
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

for i in char_dataset.take(5):
  print(idx2char[i.numpy()])

sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

for item in sequences.take(5):
  print(repr(''.join(idx2char[item.numpy()])))

def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

dataset = sequences.map(split_input_target)

for input_example, target_example in  dataset.take(1):
  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))
  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))

for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):
    print("Step {:4d}".format(i))
    print("  input: {} ({:s})".format(input_idx, repr(idx2char[input_idx])))
    print("  expected output: {} ({:s})".format(target_idx, repr(idx2char[target_idx])))

# Batch size
BATCH_SIZE = 64

# Buffer size to shuffle the dataset
# (TF data is designed to work with possibly infinite sequences,
# so it doesn't attempt to shuffle the entire sequence in memory. Instead,
# it maintains a buffer in which it shuffles elements).
BUFFER_SIZE = 10000

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

dataset

# Length of the vocabulary in chars
vocab_size = len(vocab)

# The embedding dimension
embedding_dim = 256

# Number of RNN units
rnn_units = 1024

class Embedding:

  def __init__(self, vocab_size, embedding_dim):
    self._vocab_size = vocab_size
    self._embedding_dim = embedding_dim
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    return tnp.take(self.weights, inputs, axis=0)

  def build(self, inputs):
    del inputs
    self.weights = tf.Variable(tnp.random.randn(
        self._vocab_size, self._embedding_dim).astype(np.float32))
    self._built = True


class GRUCell:
  """Builds a traditional GRU cell with dense internal transformations.

  Gated Recurrent Unit paper: https://arxiv.org/abs/1412.3555
  """

  def __init__(self, n_units, forget_bias=0.0):
    self._n_units = n_units
    self._forget_bias = forget_bias
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    x, gru_state = inputs
    # Dense layer on the concatenation of x and h.
    y = tnp.dot(tnp.concatenate([x, gru_state], axis=-1), self.w1) + self.b1
    # Update and reset gates.
    u, r = tnp.split(tf.sigmoid(y), 2, axis=-1)
    # Candidate.
    c = tnp.dot(tnp.concatenate([x, r * gru_state], axis=-1), self.w2) + self.b2
    new_gru_state = u * gru_state + (1 - u) * tnp.tanh(c)
    return new_gru_state

  def build(self, inputs):
    # State last dimension must be n_units.
    assert inputs[1].shape[-1] == self._n_units
    # The dense layer input is the input and half of the GRU state.
    dense_shape = inputs[0].shape[-1] + self._n_units
    self.w1 = tf.Variable(tnp.random.uniform(
        -0.01, 0.01, (dense_shape, 2 * self._n_units)).astype(tnp.float32))
    self.b1 = tf.Variable((tnp.random.randn(2 * self._n_units) * 1e-6 + self._forget_bias
               ).astype(tnp.float32))
    self.w2 = tf.Variable(tnp.random.uniform(
        -0.01, 0.01, (dense_shape, self._n_units)).astype(tnp.float32))
    self.b2 = tf.Variable((tnp.random.randn(self._n_units) * 1e-6).astype(tnp.float32))
    self._built = True

  @property
  def weights(self):
    return (self.w1, self.b1, self.w2, self.b2)


class GRU:

  def __init__(self, n_units, forget_bias=0.0, stateful=False):
    self._cell = GRUCell(n_units, forget_bias)
    self._stateful = stateful
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    if self._stateful:
      state = self.state.read_value()
    else:
      state = self._init_state(inputs.shape[0])    
    inputs = tnp.transpose(inputs, (1, 0, 2))
    output =  tf.scan(
        lambda gru_state, x: self._cell((x, gru_state)),
        inputs, state)
    if self._stateful:
      self.state.assign(output[-1, ...])
    return tnp.transpose(output, [1, 0, 2])

  def _init_state(self, batch_size):
    return tnp.zeros([batch_size, self._cell._n_units], tnp.float32)

  def reset_state(self):
    if not self._stateful:
      return
    self.state.assign(tf.zeros_like(self.state))

  def create_state(self, batch_size):
    self.state = tf.Variable(self._init_state(batch_size))

  def build(self, inputs):
    s = inputs.shape[0:1] + inputs.shape[2:]
    shapes = (s, s[:-1] + (self._cell._n_units,))   
    self._cell.build([tf.TensorSpec(x, tf.float32) for x in shapes])
    if self._stateful:
      self.create_state(inputs.shape[0])
    else:
      self.state = ()
    self._built = True
    
  @property
  def weights(self):
    return self._cell.weights


class Dense:

  def __init__(self, n_units, activation=None):
    self._n_units = n_units
    self._activation = activation
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    y = tnp.dot(inputs, self.w) +self.b
    if self._activation != None:
      y = self._activation(y)
    return y

  def build(self, inputs):
    shape_w = (inputs.shape[-1], self._n_units)
    lim = tnp.sqrt(6.0 / (shape_w[0] + shape_w[1]))
    self.w = tf.Variable(tnp.random.uniform(-lim, lim, shape_w).astype(tnp.float32))
    self.b = tf.Variable((tnp.random.randn(self._n_units) * 1e-6).astype(tnp.float32))
    self._built = True

  @property
  def weights(self):
    return (self.w, self.b)


class Model:

  def __init__(self, vocab_size, embedding_dim, rnn_units, forget_bias=0.0, stateful=False, activation=None):
    self._embedding = Embedding(vocab_size, embedding_dim)
    self._gru = GRU(rnn_units, forget_bias=forget_bias, stateful=stateful)
    self._dense = Dense(vocab_size, activation=activation)
    self._layers = [self._embedding, self._gru, self._dense]
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    xs = inputs
    for layer in self._layers:
      xs = layer(xs)
    return xs
    
  def build(self, inputs):
    self._embedding.build(inputs)
    self._gru.build(tf.TensorSpec(inputs.shape + (self._embedding._embedding_dim,), tf.float32))
    self._dense.build(tf.TensorSpec(inputs.shape + (self._gru._cell._n_units,), tf.float32))
    self._built = True

  @property
  def weights(self):
    return [layer.weights for layer in self._layers]

  @property
  def state(self):
    return self._gru.state

  def create_state(self, *args):
    self._gru.create_state(*args)

  def reset_state(self, *args):
    self._gru.reset_state(*args)


model = Model(
  vocab_size = vocab_size,
  embedding_dim=embedding_dim,
  rnn_units=rnn_units,
  stateful=True)

  for input_example_batch, target_example_batch in dataset.take(1):
    input_example_batch = tnp.asarray(input_example_batch)
    example_batch_predictions = model(input_example_batch)
    print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")

example_batch_predictions[0]

sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()

sampled_indices

print("Input: \n", repr("".join(idx2char[input_example_batch[0]])))
print()
print("Next Char Predictions: \n", repr("".join(idx2char[sampled_indices ])))

def one_hot(labels, n):
  return (labels[..., np.newaxis] == tnp.arange(n)).astype(np.float32)

def loss_fn(labels, predictions):
  predictions = tf.nn.log_softmax(predictions)
  return -tnp.sum(predictions * one_hot(tnp.asarray(labels), predictions.shape[-1]), axis=-1)

example_batch_loss  = loss_fn(target_example_batch, example_batch_predictions)
print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
print("scalar_loss:      ", tnp.mean(example_batch_loss))

class Adam:

  def __init__(self, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-7):
    self._lr = learning_rate
    self._b1 = b1
    self._b2 = b2
    self._eps = eps
    self._built = False

  def build(self, weights):
    self._m = tf.nest.map_structure(lambda x: tf.Variable(tnp.zeros_like(x)), weights)
    self._v = tf.nest.map_structure(lambda x: tf.Variable(tnp.zeros_like(x)), weights)
    self._step = tf.Variable(tnp.asarray(0, np.int64))
    self._built = True

  def _update(self, weights_var, grads, m_var, v_var):
    b1 = self._b1
    b2 = self._b2
    eps = self._eps
    step = tnp.asarray(self._step, np.float32)
    lr = self._lr
    weights = tnp.asarray(weights_var)
    m = tnp.asarray(m_var)
    v = tnp.asarray(v_var)
    m = (1 - b1) * grads + b1 * m  # First  moment estimate.
    v = (1 - b2) * (grads ** 2) + b2 * v  # Second moment estimate.
    mhat = m / (1 - b1 ** (step + 1))  # Bias correction.
    vhat = v / (1 - b2 ** (step + 1))   
    weights_var.assign_sub((lr * mhat / (tnp.sqrt(vhat) + eps)).astype(weights.dtype))
    m_var.assign(m)
    v_var.assign(v)

  def apply_gradients(self, weights, grads):
    if not self._built:
      self.build(weights)
    tf.nest.map_structure(lambda *args: self._update(*args), weights, grads, self._m, self._v)
    self._step.assign_add(1)

  @property
  def state(self):
    return (self._step, self._m, self._v)


optimizer = Adam()

@tf.function
def train_step(inp, target):
  with tf.GradientTape() as tape:
    # tape.watch(tf.nest.flatten(weights))
    predictions = model(inp)
    loss = tnp.mean(loss_fn(target, predictions))
  weights = model.weights
  grads = tape.gradient(loss, weights)
  optimizer.apply_gradients(weights, grads)
  return loss

# Training step
EPOCHS = 10

model.create_state(BATCH_SIZE)

for epoch in range(EPOCHS):
  start = time.time()

  # initializing the hidden state at the start of every epoch
  model.reset_state()

  for (batch_n, (inp, target)) in enumerate(dataset):
    loss = train_step(inp, target)

    if batch_n % 100 == 0:
      template = 'Epoch {} Batch {} Loss {}'
      print(template.format(epoch+1, batch_n, loss))

  print ('Epoch {} Loss {}'.format(epoch+1, loss))
  print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

def generate_text(model, start_string):
  # Evaluation step (generating text using the learned model)

  # Number of characters to generate
  num_generate = 1000

  # Converting our start string to numbers (vectorizing)
  input_eval = [char2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []

  # Low temperatures results in more predictable text.
  # Higher temperatures results in more surprising text.
  # Experiment to find the best setting.
  temperature = 1.0

  # Here batch size == 1
  model.create_state(1)
  for i in range(num_generate):
      predictions = model(input_eval)
      # remove the batch dimension
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the character returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted character as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)

      text_generated.append(idx2char[predicted_id])

  return (start_string + ''.join(text_generated))

print(generate_text(model, start_string=u"ROMEO: "))

print("hello")

from IPython.display import HTML
HTML("""
<script>
console.log("hello");
</script>
<b>HTML</b>
""")

%%javascript
console.log("hi");

from IPython.display import Image
Image("http://ipython.org/_static/IPy_header.png")

print("hello")

from IPython.display import HTML
HTML("""
<script>
console.log("hello");
</script>
<b>HTML</b>
""")

%%javascript
console.log("hi");

from IPython.display import Image
Image("http://ipython.org/_static/IPy_header.png")

!jt -l


import IPython

bundle = {}
bundle['application/vnd.raw.v1+json'] = {
    'apples': ['🍎', '🍏'],
    'bananas': 2,
    'oranges': 'apples'
}

IPython.display.display(bundle, raw=True)



print("hello")

from IPython.display import HTML
HTML("""
<script>
console.log("hello");
</script>
<b>HTML</b>
""")

%%javascript
console.log("hi");

from IPython.display import Image
Image("http://ipython.org/_static/IPy_header.png")

print("hello")

from IPython.display import HTML
HTML("""
<script>
console.log("hello");
</script>
<b>HTML</b>
""")

%%javascript
console.log("hi");

from IPython.display import Image
Image("http://ipython.org/_static/IPy_header.png")

future_output()

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

!sudo apt -y install libportaudio2
!pip install -q tflite-model-maker-nightly

import numpy as np
import os

from tflite_model_maker import model_spec
from tflite_model_maker import text_classifier
from tflite_model_maker.config import ExportFormat
from tflite_model_maker.text_classifier import AverageWordVecSpec
from tflite_model_maker.text_classifier import DataLoader

import tensorflow as tf
assert tf.__version__.startswith('2')
tf.get_logger().setLevel('ERROR')

data_dir = tf.keras.utils.get_file(
      fname='SST-2.zip',
      origin='https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',
      extract=True)
data_dir = os.path.join(os.path.dirname(data_dir), 'SST-2')

import pandas as pd

def replace_label(original_file, new_file):
  # Load the original file to pandas. We need to specify the separator as
  # '\t' as the training data is stored in TSV format
  df = pd.read_csv(original_file, sep='\t')

  # Define how we want to change the label name
  label_map = {0: 'negative', 1: 'positive'}

  # Excute the label change
  df.replace({'label': label_map}, inplace=True)

  # Write the updated dataset to a new file
  df.to_csv(new_file)

# Replace the label name for both the training and test dataset. Then write the
# updated CSV dataset to the current folder.
replace_label(os.path.join(os.path.join(data_dir, 'train.tsv')), 'train.csv')
replace_label(os.path.join(os.path.join(data_dir, 'dev.tsv')), 'dev.csv')

spec = model_spec.get('average_word_vec')

train_data = DataLoader.from_csv(
      filename='train.csv',
      text_column='sentence',
      label_column='label',
      model_spec=spec,
      is_training=True)
test_data = DataLoader.from_csv(
      filename='dev.csv',
      text_column='sentence',
      label_column='label',
      model_spec=spec,
      is_training=False)

model = text_classifier.create(train_data, model_spec=spec, epochs=10)

loss, acc = model.evaluate(test_data)

model.export(export_dir='average_word_vec')

mb_spec = model_spec.get('mobilebert_classifier')

train_data = DataLoader.from_csv(
      filename='train.csv',
      text_column='sentence',
      label_column='label',
      model_spec=mb_spec,
      is_training=True)
test_data = DataLoader.from_csv(
      filename='dev.csv',
      text_column='sentence',
      label_column='label',
      model_spec=mb_spec,
      is_training=False)

model = text_classifier.create(train_data, model_spec=mb_spec, epochs=3)

model.summary()

loss, acc = model.evaluate(test_data)

model.export(export_dir='mobilebert/')

model.export(export_dir='mobilebert/', export_format=[ExportFormat.LABEL, ExportFormat.VOCAB])

accuracy = model.evaluate_tflite('mobilebert/model.tflite', test_data)
print('TFLite model accuracy: ', accuracy)

new_model_spec = model_spec.get('mobilebert_classifier')
new_model_spec.seq_len = 256

new_model_spec = AverageWordVecSpec(wordvec_dim=32)

new_train_data = DataLoader.from_csv(
      filename='train.csv',
      text_column='sentence',
      label_column='label',
      model_spec=new_model_spec,
      is_training=True)

model = text_classifier.create(new_train_data, model_spec=new_model_spec)

model = text_classifier.create(new_train_data, model_spec=new_model_spec, epochs=20)

new_test_data = DataLoader.from_csv(
      filename='dev.csv',
      text_column='sentence',
      label_column='label',
      model_spec=new_model_spec,
      is_training=False)

loss, accuracy = model.evaluate(new_test_data)

spec = model_spec.get('bert_classifier')

from PIL import Image
im = Image.open("")
im

import random

def generate_the_word(infile):
    with open(infile) as f:
        contents_of_file = f.read()
    lines = contents_of_file.splitlines()
    line_number = random.randrange(0, len(lines))
    return lines[line_number]

for x in range(5):
    wrd = (generate_the_word("text/wordcloud.txt"))
    print wrd


import random
count = 0
while count< 10:
    def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

    TEXT = "text/wordcloud.txt"
    wrd = (generate_the_word(TEXT))
    count = count+1
    print wrd

from PIL import Image, ImageDraw, ImageFont

start = Image.new('RGBA', (640,640), (205,100,205,170))
start.save('images/BLANK_IMAGE.png')
start

!ls images

from PIL import Image, ImageDraw, ImageFont
# get an image
base = Image.open('images/man.jpg').convert('RGBA')
#8 5 4 6 3 2
# make a blank image for the text, initialized to transparent text color
txt = Image.new('RGBA', base.size, (255,255,255,0))

# get a font
fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 50)
# get a drawing context
d = ImageDraw.Draw(txt)

width, height = base.size
# calculate the x,y coordinates of the text
#marginx = 325
#marginy = 75
x = 90
y = 10
title = (generate_the_word("text/titles.txt"))
d.text((x,y), title , font=fnt, fill=(0,0,0,250))
textin = "PYTHON MODULES" 
d.text((x,y), title, font=fnt, fill=(0,0,0,250))

out2 = Image.alpha_composite(base, txt)
out2.save("images/zxtextbackP_post002.png", "PNG")
out2.show()


from PIL import Image
out2 = Image.open("images/zxtextbackP_post002.png")
out2

from PIL import Image, ImageDraw, ImageFont
# get an image
base = Image.open('images/zxtextbackP_post002.png').convert('RGBA')
#8 5 4 6 3 2
# make a blank image for the text, initialized to transparent text color
txt = Image.new('RGBA', base.size, (255,255,255,0))

# get a font
fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
# get a drawing context
d = ImageDraw.Draw(txt)

width, height = base.size
# calculate the x,y coordinates of the text
#marginx = 325
#marginy = 75
marginx = 225
marginy = 50
x = width - marginx
y = height - marginy
textin = "The TwitterBot Project" 
d.text((x,y), textin, font=fnt, fill=(0,0,0,180))

out = Image.alpha_composite(base, txt)
out.save("images/ztextbackP_post002_signed.png")
out.show()


from PIL import Image
start = Image.open('images/ztextbackP_post002_signed.png')
start

from PIL import Image
start = Image.open('images/island.jpg')
start

#%%writefile TwittePost.py
#!/anaconda2/bin/python
import os
import random
import sys
import markovify
import twython
from twython import Twython
import time
from PIL import Image, ImageDraw, ImageFont
from random import randint
# get an image

#title = "Python Stuff"
signature_ = "The TwitterBot Project" 

count = 1
start = Image.open('images/island.jpg').convert('RGBA')
start.save('images/ztextbacktmp.png')
while count < 256 :
    base = Image.open('images/ztextbacktmp.png').convert('RGBA')
 
    #8 5 4 6 3 2
    # make a blank image for the text, initialized to transparent text color
    txt = Image.new('RGBA', base.size, (255,255,255,0))
    # get a font
    #generate a random size for the font
    int_n = int(count*.2)
    Fsize = randint(15,100-int_n)
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", Fsize)
    # get a drawing context
    d = ImageDraw.Draw(txt)

    width, height = base.size


    def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]
    textin = (generate_the_word("text/wordcloud.txt"))

    # calculate the x,y coordinates of the text
    w, h = base.size
    Lw = randint(-150,w-50)
    Lh = randint(-50,h-30)
    #textin = "The TwitterBot Project" 
    #generate random color and opacity
    r = randint(0,256)
    g = randint(0,256)
    b = randint(0,256)
    a = randint(0,count)
    d.text((Lw,Lh), textin, font=fnt, fill=(r,g,b,a))

    out = Image.alpha_composite(base, txt)
    out.save("images/ztextbacktmp.png")
    count=count+1
       
#base = Image.open('images/NewFolder/lightning01.jpg').convert('RGBA')
#8 5 4 6 3 2
# make a blank image for the text, initialized to transparent text color
txt = Image.new('RGBA', out.size, (255,255,255,0))

# get a font
fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
# get a drawing context
d = ImageDraw.Draw(txt)

width, height = out.size
# calculate the x,y coordinates of the text
#marginx = 325
#marginy = 75
marginx = 225
marginy = 50
x = width - marginx
y = height - marginy

d.text((x,y), signature_, font=fnt, fill=(0,0,0,256))

out = Image.alpha_composite(out, txt)
out.save("images/ztmp.png")
# save the image then reopen to put a title
base = Image.open('images/ztmp.png').convert('RGBA')
#8 5 4 6 3 2
# make a blank image for the text, initialized to transparent text color
txt = Image.new('RGBA', base.size, (255,255,255,0))

# get a font
fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 50)
# get a drawing context
d = ImageDraw.Draw(txt)

width, height = base.size
# calculate the x,y coordinates of the text
#marginx = 325
#marginy = 75
x = 90
y = 10

title = (generate_the_word("text/titles.txt"))
d.text((x,y), title , font=fnt, fill=(0,0,0,250))
out2 = Image.alpha_composite(base, txt)
out2.save("images/zTM_POST.png")


PATH = "images/zTM_POST.png"


from PIL import Image
base = Image.open("images/zTM_POST.png")
base

from PIL import Image
base = Image.open('images/textbacktmp.png')
base

from PIL import Image, ImageDraw, ImageFont
import random
from random import randint
# get an image


#title = "Python Stuff"
signature_ = "The TwitterBot Project" 

count = 1
start = Image.open('images/StartBlank.png').convert('RGBA')
start.save('images/textbacktmp.png')
while count < 256 :
    base = Image.open('images/StartBlank.png').convert('RGBA')
 
    #8 5 4 6 3 2
    # make a blank image for the text, initialized to transparent text color
    txt = Image.new('RGBA', base.size, (255,255,255,0))
    # get a font
    #generate a random size for the font
    int_n = int(count*.2)
    Fsize = randint(15,100-int_n)
    fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", Fsize)
    # get a drawing context
    d = ImageDraw.Draw(txt)

    width, height = base.size


    def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]
    textin = (generate_the_word("text/wordcloud.txt"))

    # calculate the x,y coordinates of the text
    w, h = base.size
    Lw = randint(-150,w-50)
    Lh = randint(-50,h-30)
    #textin = "The TwitterBot Project" 
    #generate random color and opacity
    r = randint(0,256)
    g = randint(0,256)
    b = randint(0,256)
    a = randint(0,count)
    d.text((Lw,Lh), textin, font=fnt, fill=(r,g,b,a))

    out = Image.alpha_composite(base, txt)
    out.save("images/textbacktmp.png")
    count=count+1
       
#base = Image.open('images/NewFolder/lightning01.jpg').convert('RGBA')
#8 5 4 6 3 2
# make a blank image for the text, initialized to transparent text color
txt = Image.new('RGBA', out.size, (255,255,255,0))

# get a font
fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 20)
# get a drawing context
d = ImageDraw.Draw(txt)

width, height = out.size
# calculate the x,y coordinates of the text
#marginx = 325
#marginy = 75
marginx = 225
marginy = 50
x = width - marginx
y = height - marginy

d.text((x,y), signature_, font=fnt, fill=(0,0,0,256))

out = Image.alpha_composite(out, txt)
out.save("images/tmp.png")
# save the image then reopen to put a title
base = Image.open('images/tmp.png').convert('RGBA')
#8 5 4 6 3 2
# make a blank image for the text, initialized to transparent text color
txt = Image.new('RGBA', base.size, (255,255,255,0))

# get a font
fnt = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 50)
# get a drawing context
d = ImageDraw.Draw(txt)

width, height = base.size
# calculate the x,y coordinates of the text
#marginx = 325
#marginy = 75
x = 90
y = 10
#generate a title
title = (generate_the_word("text/titles.txt"))

d.text((x,y), title , font=fnt, fill=(0,0,0,250))

out2 = Image.alpha_composite(base, txt)
out2.save("images/textbackP_post003.png")
out2.show()


from PIL import Image
out2=Image.open("images/textbackP_post003.png")
out2

%%writefile text/titles.txt
Python Fun
Python Graphics
Generator
Word Cloud
Graphics
Fun w/Python
Python Stuff
PYTHON !!!
Love`en Python
Creative Python
Graphic Fun

%%writefile text/wordcloud.txt
aiml
alabaster
altair
anaconda_client
anaconda_project
appdirs
apptools
argcomplete
arrow
astroid
astropy
attrs
Automat
Babel
backports_abc
backports.functools-lru-cache
backports.shutil-get-terminal-size
backports.ssl-match-hostname
BeautifulSoup
beautifulsoup4
bitarray
blaze
bleach
bokeh
boto
Bottleneck
branca
bs4
cdecimal
certifi
cffi
chardet
ChatterBot
chatterbot-corpus
chest
click
get-mak-eurl
cloudpickle
clyent
colorama
coloredlogs
conda
configobj
configparser
constantly
contextlib2
cov-core
coverage
cryptography
cssselect
cycler
Cython
cytoolz
dask
data
datashape
decorator
dill
dlib
docopt
docutils
dot2tex
e
entrypoints
enum34
et-xmlfile
facemorpher
fastcache
Flask
Flask-Admin
Flask-Cors
Flask-GraphQL
Flask-WTF
flickr-api
flickrapi
folium
funcsigs
functions
functools32
future
futures
gevent
gitter
graphene
graphene
sqlalchemy
graphql-core
graphql-relay
graphviz
greenlet
grin
h5py
HeapDict
html2text
html5lib
httplib2
humanfriendly
hyperlink
icrawler
idna
ijson
imageio
imagesize
imutils
incremental
inflect
ipaddress
ipykernel
ipyleaflet
ipython
ipython-genutils
ipyvolume
ipywidgets
irc
iso8601
isort
itsdangerous
jaraco.classes
jaraco.collections
jaraco.functools
jaraco.itertools
jaraco.logging
jaraco.stream
jaraco.text
jdcal
jedi
Jinja2
JSON-log-formatter
json-tricks
jsondatabase
jsonschema
jupyter-client
jupyter-console
jupyter-contrib-core
jupyter-contrib-nbextensions
jupyter-core
jupyter-highlight-selected-word==0.0.11jupyter-latex-envs
jupyter-nbextensions-configurator
jupyterlab
jupyterlab-launcher
labellio-cli
latex
lazy-object-proxy
leveldb
lightning-python
llvmlite
locket
lxml
mahotas
Markdown
markovbot
markovify
MarkupSafe
matplotlib
metakernel
meteor-ejson
mistune
mock
mongoengine
monotonic
more-itertools
mpld3
mpmath
msnoise
msnoise-tomo
multipledispatch
MySQL-python
nbconvert
nbformat
networkx
nltk
nodebox-opengl
nose
nose-cov
notebook
numba
numexpr
numpy
numpydoc
oauth
oauthlib
obspy
odo
olefile
openpyxl
packaging
pandas
pandocfilters
parsel
partd
pathlib2
patsy
pbr
pep8
pexpect
pickleshare
Pillow
pkginfo
plotly
ply
processing
progressbar
promise
prompt-toolkit
protobuf
psutil
PsychoPy
psycopg2
ptyprocess
py
py3-protobuffers
PyAIML
pyamg
pyasn1
pyasn1-modules
pycairo
pychecker
pycosat
pycparser
pycrypto
pycurl
PyDispatcher
pydot
pyee
pyface
pyflakes
pygame
pyglet
Pygments
PyInstaller
pylint
pymongo
pymorph
pymunk
PyMySQL
PyOpenGL
PyOpenGL-accelerate
pyOpenSSL
pyparsing
pytest
python-coveralls
python-dateutil
python-ddp
python-meteor
python-resize-image
python-twitter
pythreejs
pyttsx
pytz
PyWavelets
PyYAML
pyzmq
QtAwesome
QtPy
queuelib
redis
requests
requests-oauthlib
requests-toolbelt
requirements
rope
scandir
scikit-image
scikit-learn
scipy
Scrapy
seaborn
selectivesearch
service-identity
Shapely
shutilwhich
simplegeneric
singledispatch
six
skulpt-python
snowballstemmer
sockjs-tornado
Sphinx
sphinx-rtd-theme
SQLAlchemy
statsmodels
subprocess32
surrealism
SVGFig
svgwrite
sympy
tables
tempdir
tempora
tensorflow
termenu
terminado
testpath
textblob
times
toolz
tornado
tqdm
traitlets
traits
traitsui
traittypes
tweepy
twine
Twisted
TwitterAPI
typing
unicodecsv
Unidecode
vega
virtualenv
vpnotebook
vpython
vtk
w3lib
Wand
wcwidth
webcolors
webencodings
Werkzeug
widgetsnbextension
wrapt
ws4py
WTForms
wxPython
xerox
xlrd
XlsxWriter
xlwt
zope.interface

#%%writefile twitterpost.py

import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
import Key
from random import randint

HashTag = raw_input("HashTag  : ") or "CNN"

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

filein = open('newtest.txt', 'a')
txt = twitter.search(q=HashTag)
txt = str(txt)
filein.write(txt)
filein.close()

print txt





%%writefile TweepyToCSV.py
import sys
import tweepy
import csv
import Key

#pass security information to variables
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]

#use variables to access twitter
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)

#create an object called 'customStreamListener'
class CustomStreamListener(tweepy.StreamListener):

    def on_status(self, status):
        print (status.author.screen_name, status.created_at, status.text)
        # Writing status data
        with open('OutputStreaming.txt', 'a') as f:
            writer = csv.writer(f)
            #status.author.screen_name = status.author.screen_name.encode('UTF-8')
            #status.text.encode = status.text.encode('UTF-8')            
            writer.writerow([status.author.screen_name.encode('UTF-8'), status.created_at, status.text.encode('utf8')])


    def on_error(self, status_code):
        print >> sys.stderr, 'Encountered error with status code:', status_code
        return True # Don't kill the stream

    def on_timeout(self):
        print >> sys.stderr, 'Timeout...'
        return True # Don't kill the stream


    def titles():
        # Writing csv titles
        with open('OutputStreaming.txt', 'a') as f:
                    writer = csv.writer(f)
                    writer.writerow(['Author', 'Date', 'Text'])
            
def main():
    streamingAPI = tweepy.streaming.Stream(auth, CustomStreamListener())
    #with open("tokens.txt", "r") as f:
    with open("tokens2.txt", "r") as f:    
        tokens = f.readlines()

        streamingAPI.filter(track=tokens)
        #streamingAPI.filter(track=['Dallas', 'NewYork'])


        #status.text.encode('utf-8')
        #writer.writerow([unicode(s).encode("utf-8") for s in row])
        #writer.writerow([unicode('Author', 'Date', 'Text').encode("utf-8") for 'Author', 'Date', 'Text' in row])


from Txmanip import RemoveBlank

RemoveBlank.removeblank("AllBooksOfEnoch_djvu.txt","BooksOfEnoch.txt")

from Txmanip import RemoveDuplicate

RemoveDuplicate.removeduplicate("cleanstream.txt","nodupStreaming.txt")

from itertools import tee
help(tee)

%%writefile test.txt
Author,Date,Text
MelanieT123,2017-10-06 05:50:39,"RT @AlamoOnTheRise: Trump: Day 258 Plans 2 decertify Iran nuke deal Picks coal lobbyist 4 top EPA job Fumed over 'moron' news Sz tonigh… "
dlindsey_16,2017-10-06 05:50:39,"RT @PeoplesOracle: But not for Puerto Rico. But not for healthcare. But not for education. https://t.co/2H7MDPBgix"
debchu222,2017-10-06 05:50:39,"RT @MooreSenate: According to Biden, these liberals are planning to attack me with the same furry they went after President Trump with in 2…"
Chariah_laquise,2017-10-06 05:50:39,"RT @PeoplesOracle: But not for Puerto Rico.
Mandrake_AYS,2017-10-06 05:50:39,https://t.co/SUR5obeU1V
x_SimplyJayy,2017-10-06 05:50:39,"RT @PeoplesOracle: But not for Puerto Rico.
SnOwPinkMan,2017-10-06 05:50:39,RT @OriginalFunko: RT &amp; follow @OriginalFunko for the chance to win an #NYCC 2017 exclusive Old Man Logan Pop! https://t.co/IbzQkp0KT2
tical10,2017-10-06 05:50:39,"RT @Cdiscount: 🏆 #Concours
📱 Samsung #GalaxyS8 à gagner &gt; https://t.co/GVXBtJYqDK 🚨 Pour participer: RT + Tweet avec… "
madamyez,2017-10-06 05:50:39,"I heard this too. Been waiting to hear the contents of the note. https://t.co/GftjuRisig #LasVegasShooting #StephenPaddock #GunControl"
cowboyneok,2017-10-06 05:50:39,@PattyArquette Please get the word out!  #Trump Administration attacking the #Trans family in our #LGBTQ community.  https://t.co/AzkVuPUJnM
VeritasVirtusX,2017-10-06 05:50:39,"Trump ""Calm Before Storm" You will Find Out"" Mysterious Statement https://t.co/LwSUaj5fyZ via @YouTube"
selloscope15,2017-10-06 05:50:40,Sandbank I Dont Trust Me Either Womens Funny Slogan Casual Tee Round Collar Sleeveless Top … https://t.co/EXaCP743Ct
noelleenix,2017-10-06 05:50:40,"RT @K_JeanPierre: A President being called a ""moron"" by his cabinet secretary is unusual. But,it's Donald Trump so it's not surprising.""Mor…"
comalliwrites,2017-10-06 05:50:40,"RT @ASlavitt: BREAKING: This story is incredible. Trump personally ordering rates 4 American families in order to break ACA.
https://t…"MattCxr,2017-10-06 05:50:40,"RT @Cdiscount: 🏆 #Concours 🎮 #PS4Pro + #FIFA18 à gagner &gt; https://t.co/W4G4teimHN
AlliInCali72,2017-10-06 05:50:40,RT @DavidCornDC: They're not even pretending. https://t.co/HMb0FOkd0d
AmiuMandzukic,2017-10-06 05:50:40,"RT @Cdiscount: 🏆 #Concours
cheshiredoe,2017-10-06 05:50:40,RT @TheAtlPhoto: Photographing the Microscopic: Winners of Nikon Small World 2017 - 24 winners and honorable mentions… 
robbieisnice,2017-10-06 05:50:40,@XandraSchultz @tightsarntpants @LStanfield2 @DrBo42 @Styl_oh @brielarson I don't feel pity. I have very few emotio… https://t.co/1QEHekpanP
rrys1DEmpire: Follow everyone 
TheDentalOfflce,2017-10-06 05:50:40,RT @JB_Carlson: A president running on #ParisAgreement &amp; #SDGs will encourage our allies to help us in keeping our elections clean.… 
shadylady1031,2017-10-06 05:50:40,has Trump ever kept a campaign promise? NO https://t.co/JL15eptgTV
Trvp_Isaxc,2017-10-06 05:50:40,"RT @PolynesianSauce: 12) Me &amp; My Bitch ... You don’t scream “AND THEY ASK ME WHY I TRUST NO BITCH, CAUSE MY EX HAD ME FEELING ALL EMBARR… "
CrazzyCattLady,2017-10-06 05:50:40,https://t.co/uUOy3dYhoJ
sherfouch,2017-10-06 05:50:40,"RT @HeerJeet: 3. Bannon, Brietbart, the Mercers &amp; Milo: these are not fringe figures. Bannon ran Trump's campaign &amp; was White House advisor…"
Jogiejamette,2017-10-06 05:50:40,"RT @JohnWDean: Nixon: “I’m not a crook.” Trump: “I’m not a moron.”         h/t DW"
MazinWaheed,2017-10-06 05:50:40,RT @the1dstage: retweet if you want to gain just follow everyone who retweets and follow back whoever follows you🏼
templeheart55,2017-10-06 05:50:40,"RT @RVAwonk: --&gt; ""Iowa tried for months to get federal permission to fix their health insurance markets but they were shut down… "
jess7719,2017-10-06 05:50:40,RT @TPInsidr: Of course they're doubling down on their false report! Why is our media so dishonest? https://t.co/RofsRDZ3Dc
JaneMetzger75,2017-10-06 05:50:40,RT @KeithOlbermann: Just a reminder that this State Department Spokesperson used to co-host Fox And Friends. https://t.co/zG3dzeAjwB
faithfitzy,2017-10-06 05:50:40,@3hunnathot oh god NO. i don't know how to word things ever haha😰 i'm just sayin anyone that supports trump UGLY AF!
AgnesS5665,2017-10-06 05:50:40,"RT @kwilli1046: Teen #LasVegas victim found Pres. Trump's visit  ""comforting"" - ""he wasn't who we see on social media."" https://t.co/GUD9LT…"
LegitJayyy_,2017-10-06 05:50:40,RT @RileyJayDennis: holy shit what kinda president goes to hurricane victims &amp; tells them they cost him a lot of money &amp; they should be… 
Bladix29,2017-10-06 05:50:40,"RT @Cdiscount: 🏆 #Concours 

def edited(r):
    r+c
    return r

with open("test.txt") as inf, open("editTest.txt", "w") as outf:
    # set up iterators
    cfg, res = tee(inf)
    for i in range(1):
        next(cfg)

    # iterate through in tandem
    for c, r in zip(cfg, res):
        if len(c)<35:
            r = edited(r)
        outf.write(r)

    # reached end - write out remaining queued values
    for r in res:
        outf.write(r)
edited(r)        

from itertools import tee

with open("test.txt") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(500):
        next(cfg)

        for c,r in zip(cfg, res):
            if len(c)<35:
                print c,r,"----\n"

from itertools import tee
count=0
with open("realDonaldTrump_tweets.csv") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(4):
        next(cfg)

    for c,r in zip(cfg, res):
        count=count+1
        if len(c)<35:
            #print "Date :",c[21:]
            print count,"-Text:",c[39:]

outf =open("editTest.txt", "w")
outf.write(r)
outf.close()

outf = open("editTest.txt", "w") 
r = c[39:]   
outf.write(r)

lines = open("realDonaldTrump.txt","r")
line = lines.readline()
for line in lines:
    if "AMERICAN" in line :
         print line 



#need to open the file properly
with open("realDonaldTrump.txt", 'r') as fp:
    #as suggested by @Padraic Cunningham it is better to iterate over the file object
    for line in fp:
        #each piece of information goes in a list
        infos = line.split()
        #this makes sure that there are no problems if your file has a empty line
        #and finds bob in the information
        if infos and infos[-1] == "trump":
            print (infos[2])



with open("editTest.txt") as openfile:
    for line in openfile:
         if "trump" in line:
                print line

from itertools import tee
count=0
with open("realDonaldTrump_tweets.csv") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(4):
        next(cfg)

    for c,r in zip(cfg, res):
        count=count+1
        if len(c)<35:
            outf = open("editTest.txt", "a") 
            r = c[39:]   
            outf.write(r)
            outf.close()
            print c[39:]

from itertools import tee
count=0
with open("realDonaldTrump_tweets.csv") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(4):
        next(cfg)

    for c,r in zip(cfg, res):
        count=count+1
        if "campaign" in c:
            #print "Date :",c[21:]
            print c[39:]

with io.open("symmetrymag_tweets.csv", "w", encoding="utf-8") as inf:    
outf.close() 

with io.open("symmetrymag_tweets.csv", "w", encoding="utf-8") as inf:    
outf.close() 

from itertools import tee
count=0
with open("symmetrymag_tweets.csv") as inf:
    for line in inf:
        lines  = line[39:]
        outf = open("symmetrymag.txt", "a") 
        outf.write(lines)
outf.close() 

from itertools import tee
import io
count=0

with io.open("symmetrymag_tweets.csv", "r") as inf:    
    for line in inf:
        lines  = line[39:]
        outf = open("symmetrymagexp.txt", "w") 
        outf.write(lines)
outf.close() 

from itertools import tee
count=0
with open("realDonaldTrump_tweets.csv") as inf:
    for line in inf:
        lines  = line[39:]
        outf = open("realDonaldTrump.txt", "a") 
        outf.write(lines)
outf.close() 

from itertools import tee
search = raw_input('Search Term')
with open("realDonaldTrump_tweets.csv") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(4):
        next(cfg)

    for c,r in zip(cfg, res):
        if search in c:
            print "Date :",c[21:38],"\nText :",c[39:]

from itertools import tee
search = raw_input('Search Term')
with open("symmetrymag_tweets.csv") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(4):
        next(cfg)

    for c,r in zip(cfg, res):
        if search in c:
            print "Date :",c[21:38],"\nText :",c[39:]

import TweepyToCSV
from TweepyToCSV import CustomStreamListener
TweepyToCSV.main()

%%writefile tokens2.txt
Mexico
dreamers
healthcare
revolution
hidding
putin
voters
feedup
fakenews
NBC
CBS
CNN
CIA
hate
wacko
gonecrazy
wierd
hopeless
impeach
dishonor
lost faith
angry
trump
russia
economy
north korean
rocketman
trust
follow
honorable
honest
dishonest
distrust
election
fakepresident

!locate Key.py

import Key

#!/usr/bin/env python
# encoding: utf-8

import tweepy #https://github.com/tweepy/tweepy
import csv
import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import Key
from random import randint

#USER = raw_input("User  : ") or "CNN"

#CONSUMER_KEY = Key.twiter()[0]
#CONSUMER_SECRET = Key.twiter()[1]
#ACCESS_KEY = Key.twiter()[2]
#ACCESS_SECRET = Key.twiter()[3]
#twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

#Twitter API credentials
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]


def get_all_tweets(screen_name):
    #Twitter only allows access to a users most recent 3240 tweets with this method

    #authorize twitter, initialize tweepy
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth)

    #initialize a list to hold all the tweepy Tweets
    alltweets = []	

    #make initial request for most recent tweets (200 is the maximum allowed count)
    new_tweets = api.user_timeline(screen_name = screen_name,count=200)

    #save most recent tweets
    alltweets.extend(new_tweets)
    
    #save the id of the oldest tweet less one
    oldest = alltweets[-1].id - 1
    
    #keep grabbing tweets until there are no tweets left to grab
    while len(new_tweets) > 0:
    #while len(new_tweets) < 400:    
        print "getting tweets before %s" % (oldest)

        #all subsiquent requests use the max_id param to prevent duplicates
        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)
        
        #save most recent tweets
        alltweets.extend(new_tweets)

        #update the id of the oldest tweet less one
        oldest = alltweets[-1].id - 1

        print (len(alltweets))
        
        if (len(alltweets)) >200:

            #transform the tweepy tweets into a 2D array that will populate the csv	
            outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode("utf-8")] for tweet in alltweets]

            #write the csv	
            with open('%s_tweets.csv' % screen_name, 'wb') as f:
                writer = csv.writer(f)
                writer.writerow(["id","created_at","text"])
                writer.writerows(outtweets)

            pass


if __name__ == '__main__':
    #pass in the username of the account you want to download
    USER = raw_input("User  : ") or "CNN"
    #gaberivera symmetrymag
    get_all_tweets(USER)

import tweepy #https://github.com/tweepy/tweepy
import csv
import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import Key
from random import randint
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]

def get_all_tweets(screen_name):
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth)
    alltweets = []	
    new_tweets = api.user_timeline(screen_name = screen_name,count=200)
    alltweets.extend(new_tweets)
    oldest = alltweets[-1].id - 1
    while len(new_tweets) > 0:
        print "getting tweets before %s" % (oldest)
        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)
        alltweets.extend(new_tweets)
        oldest = alltweets[-1].id - 1
        print (len(alltweets))
        if (len(alltweets)) >200:
            outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode("utf-8")] for tweet in alltweets]
            with open('%s_tweets.csv' % screen_name, 'wb') as f:
                writer = csv.writer(f)
                writer.writerow(["id","created_at","text"])
                writer.writerows(outtweets)
            pass
if __name__ == '__main__':
    USER = raw_input("User  : ") or "CNN"
    get_all_tweets(USER)

import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
import Key
import csv

HashTag = raw_input("HashTag  : ") or "CNN"

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

# Open/create a file to append data to
csvFile = open('result.csv', 'a')

#Use csv writer
csvWriter = csv.writer(csvFile)



txt = twitter.search(q=HashTag)
txt = str(txt)
filein.write(txt)
filein.close()

print txt

# Open/create a file to append data to
csvFile = open('result.csv', 'a')

#Use csv writer
csvWriter = csv.writer(csvFile)
txt = twitter.search(q=HashTag)

csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])




for tweet in tweepy.Cursor(api.search,
                           q = "google",
                           since = "2014-02-14",
                           until = "2014-02-15",
                           lang = "en").items():

    # Write a row to the CSV file. I use encode UTF-8
    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])
    print tweet.created_at, tweet.text
csvFile.close()

from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
#url = u'https://twitter.com/search?q='
HashTag = raw_input("HashTag  : ") or "CNN"
#url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
url = u'https://twitter.com/search?q='+ HashTag +'&src=typd'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
HashTag = (random_line('phrases.txt'))
url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)

#url = u'https://twitter.com/search?q='
#HashTag = (random_line('hashtag-nouns.txt'))
#HashTag = (random_line('nodupsnohash.txt'))
#HashTag = (random_line('clean.txt'))
HashTag = "CNN"
url = u'https://twitter.com/hashtag/'+ HashTag +'?lang=en'
tweetfile = 'hashtag.txt'
#url = u'https://twitter.com/scavino45/lists/florida-hurricane-irma'
#url = u'https://twitter.com/Selebog55680943'
#url = u'https://twitter.com/WinMansfield'
#query = u'%40drawranliou'
#query = u'%23hurricanne&src=typd'
#query = u'python, florida'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
for list in txt:
    filein = open(tweetfile, 'a')
    list = list.replace(u'\xa0', u' ')
    list = list.replace(u'\u2026','ignore')
    list = list.replace(u'\xa0','ignore')
    list = list.replace(u'\u2013', u' ')
    list = list.replace(u'\xf1', u' ')
    list = list.replace(u'\u2019','ignore')
    list = '\n'+ u''.join((list)).encode('utf-8').strip()
   
    filein.write(list)
    filein.close()
    time.sleep(1)
    print list


%%writefile /home/jack/anaconda2/lib/python2.7/site-packages/Txmanip/findhashwork.py

def findhash():
    with open('hashtag.txt', 'r') as textin:
        tempstore = set()
        with open('hashonly.txt', 'w') as outfile:
            for line in textin.readlines():
                hashed = [ word for word in line.split() if word.startswith("#") ]
                #hashed = word for word in line.split() if word.startswith("#")
                hashed = str(hashed)
                hashed = hashed.replace("[","");hashed = hashed.replace("]","")
                hashed = hashed.replace(",","\n");hashed = hashed.replace(" '","")
                hashed = hashed.replace("'","");hashed = hashed.replace("#\n ","")
                hashed = hashed.replace("#","\n#")
                outfile.write(hashed)


findhash()

def removeblank():
    with open('hashonly.txt') as infile, open('hashonlyNoBlank.txt', 'w') as outfile:
        for line in infile:
            if not line.strip(): continue  # skip the empty line
            outfile.write(line)  # non-empty
            
removeblank() 

def remove_duplicates(infile):
    tempstore = set()
    with open('hashonlyNoBlankNoDups.txt', 'w+') as out:
        for line in open(infile):
            if line not in tempstore:
                if len(line) < 16 and len(line) >4:
                    out.write(line)
                    tempstore.add(line)

remove_duplicates('hashonlyNoBlank.txt')

def removeHash(infile):
    with open('clean2.txt', 'w+') as out:
        for line in open(infile):
            line = line.replace("#","");line = line.replace('"','')
            out.write(line)

removeHash('hashonlyNoBlankNoDups.txt')

badtext = ''.join(c for c in map(chr, range(173)) if not c.isalnum())
print badtext

word = words.translate(None, badtext)

from textblob import TextBlob
from nltk.corpus import wordnet

tweetfile = 'realwords.txt'
filein = open(tweetfile, 'w')
filein.close()  
with io.open("clean2.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
    if not wordnet.synsets(essays): 
        #Not an English Word
        print essays,"- Is not a word"
    else:
        #English Word    
        filein = open(tweetfile, 'a')
        filein.write(essays)
        filein.close()    

        print essays


from textblob import TextBlob
tweetfile = 'realwords.txt'
filein = open(tweetfile, 'w')
filein.close()  
with io.open("clean2.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read()
    if wordnet.synsets(essays):
        print essays


textin= open('hashtag.txt', 'r')
lines = textin.readlines()
for line in lines:
    time.sleep(1)
    print line

import time
textin= open('hashtag.txt', 'r')
lines = textin.read()
time.sleep(1)
print lines,

textin= open('hashtag.txt', 'r')
lines = textin.read().splitlines()
time.sleep(1)
print lines,

def linesonly():
    with open('hashonly.txt') as infile, open('hashonlyNoBlank.txt', 'w') as outfile:
        for line in infile:
            if not line.strip(): continue  # skip the empty line
            outfile.write(line)  # non-empty
linesonly()            

hashed = [ word for word in line.split() if word.startswith("#") ]

from bs4 import BeautifulSoup
import requests
import sqlite3
import base64
import time
import random
def random_line(fname):
    lines = open(fname).read().splitlines()
    return random.choice(lines)
#url = u'https://twitter.com/search?q='
HashTag = (random_line('hashtag-nouns.txt'))
print HashTag

#lines = open('phrases.txt', 'r').readlines()
lines = open('phrases.txt', 'r').read()
lines_set = set(lines)
out  = open('phrases2.txt', 'w')
for line in lines_set:
    out.write(line)

%%writefile Bt.py
def badtext(essays):
    badtext = ''.join(c for c in map(chr, range(256)) if not c.isalnum())
    essay = essay.translate(badtext)
    return essay

from nltk.tag import pos_tag
import io
import Tidyt
with io.open("hashtag.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
    tagged_sent = pos_tag(essays.split())
    output = open("hashtag-nouns.txt", "w")
    propernouns = [word for word,pos in tagged_sent if pos == 'NNP']
    in_string = str(propernouns)
    propernouns = Tidyt.tidyt(in_string)
        
    propernouns = propernouns.replace("  "," \n")
    output.write(propernouns)
    output.close()    
    print propernouns  

file = open("realDonaldTrump_tweets.csv", 'r')
book = file.read()
def tokenize():
    if book is not None:
        words = book.lower().split()
        return words
    else:
        return None
def count_word(tokens, token):
    count = 0
    for element in tokens:
        # Remove Punctuation
        word = element.replace(",","")
        word = word.replace(".","")
        # Found Word?
        if word == token:
            count += 1
    return count
# Tokenize the Book
words = tokenize()

# Get Word Count
word = 'Hillary'
frequency = count_word(words, word)
print('Word: [' + word + '] Frequency: ' + str(frequency))

import random 

# get the first line if this is the one with the words words
lines = open("realDonaldTrump_tweets.csv").readlines() 
line = lines[0] 

words = line.split() 
myword = random.choice(words)
print myword

import random, heapq

with open('realDonaldTrump_tweets.csv') as fin:
    word, = heapq.nlargest(1, fin, key=lambda L: random.random())
    print word,

import random
def generate_the_word(infile):
    with open(infile) as f:
        contents_of_file = f.read()
    lines = contents_of_file.splitlines()
    line_number = random.randrange(0, len(lines))
    return lines[line_number]
def main():
    print(generate_the_word("realDonaldTrump_tweets.csv"))

main()

from textblob import TextBlob
import time
import sys
with io.open("realDonaldTrump_tweets.csv", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
count = 0
Sent = input('Sent') or int(1)
Start = input('Start') or int(1)
End = input('End') or int(5)
Start= int(Start)
End= int(End)
blob = TextBlob(essays)
for word in blob.sentences[Sent].words:
    count = count +1
    if count >= Start and count <= End:
        print count,":",word
    else:
        my_file.close()
        sys.exit

    

from textblob import TextBlob
with io.open("realDonaldTrump_tweets.csv", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 

blob = TextBlob(essays)
for np in blob.noun_phrases:
    print np


from textblob import TextBlob
with io.open("ToUse.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
blob = TextBlob(essays)
for sentence in blob.sentences:
    print sentence

!python text2sent.py < realDonaldTrump_tweets.csv

!python text2sent.py < realDonaldTrump_tweets.csv

from textblob import TextBlob
import random
import sys
import io
filename ="realDonaldTrump_tweets.csv"
with io.open(filename, "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
# stdin's read() method just reads in all of standard input as a string;
# use the decode method to convert to ascii (textblob prefers ascii)
#text = sys.stdin.read().decode('ascii', errors="replace")
blob = TextBlob(essays)

short_sentences = list()
for sentence in blob.sentences:
    if len(sentence.words) <= 10:
        short_sentences.append(sentence.replace("\n", " "))

for item in random.sample(short_sentences, 10):
	print item

from textblob import TextBlob
import random
import sys
import io
filename ="/home/jack/Desktop/imagebot/hekel.txt"
with io.open(filename, "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
# stdin's read() method just reads in all of standard input as a string;
# use the decode method to convert to ascii (textblob prefers ascii)
#text = sys.stdin.read().decode('ascii', errors="replace")
blob = TextBlob(essays)

short_sentences = list()
for sentence in blob.sentences:
    if len(sentence.words) <= 10:
        short_sentences.append(sentence.replace("\n", " "))

for item in random.sample(short_sentences, 10):
	print item

from textblob import TextBlob
import sys
sys.path.insert(0, "/home/jack/anaconda2/pkgs/nltk-3.2.2-py27_0/lib/python2.7/site-packages/nltk")
from nltk import compat

from textblob import TextBlob
import sys
sys.path.insert(0, "/home/jack/Desktop/jack_watch/nltk-/nltk")
import compat
import random
filename ="realDonaldTrump_tweets.csv"
with io.open(filename, "r", encoding="utf-8") as my_file:
    essays = my_file.read() 

text = sys.stdin.read().decode('ascii', errors="replace")
blob = TextBlob(essays)
noun_phrases = blob.noun_phrases
verbs = list()
for word, tag in blob.tags:
    if tag == 'VB':
        verbs.append(word.lemmatize())
for i in range(1, 11):
    print "Step " + str(i) + ". " + random.choice(verbs).title() + " " + \
        random.choice(noun_phrases)

compat module is part of asyncio, a

from bs4 import BeautifulSoup
import requests
import csv
import codecs

hashtag = raw_input("hashtag") or "angry crowd"
url = u'https://twitter.com/search?l=en&q='+ hashtag +'&src=typd'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')
tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)
file = codecs.open( "testfile.txt", "a", "utf-8" )
for list in txt:
    mytext = "<br />\n".join(list.split(".\n\n"))
    file.write(mytext)      
    print mytext
file.close() 

import re
import textwrap

with open("testfile.txt", "r") as f:
    text = f.read()
    #This was added to get ride od the unicode u from showing up
    words = text.replace(' u ', ' ');words = words.replace('u ', '');words = words.replace(' U ', ' ')
    #This clears out the non-Alpha-numeric characters
    #badtext = ''.join(c for c in map(chr, range(256)) if not c.isalnum())
    badtext = '!,",#,$,%,&,(,),*,+,,-,.,/,:;<=>?@[\]^_`{|}~'
    
    # limit the aount of characters perline displayed
    chars_per_line = 110
    for i in range(0, len(words), chars_per_line):
        word = words.translate(None, badtext)
        print words[i:i+chars_per_line]

from textblob import TextBlob
import sys
import random
import io
filename ="testfile.txt"
with io.open(filename, "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
    
text = sys.stdin.read().decode('ascii', errors="replace")
blob = TextBlob(essays)

noun_phrases = blob.noun_phrases

verbs = list()
for word, tag in blob.tags:
    if tag == 'VB':
        verbs.append(word.lemmatize())

for i in range(1, 50):
    print "Step " + str(i) + ". " + random.choice(verbs).title() + " " + \
        random.choice(noun_phrases)

from bs4 import BeautifulSoup
import requests
url = u'https://twitter.com/search?q='
#query = u'%40drawranliou'
#query = u'@JamesOKeefeIII'
#query = u'hurricanne&src=typd'
query = u'very confused'
r = requests.get(url+query)
soup = BeautifulSoup(r.text, 'html.parser')
tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
txt = (tweets)

for list in txt:
    mytext = "<br />\n".join(list.split("\n\n"))
    
    print mytext
    

from textblob import TextBlob
import random
import sys
import io
with io.open("testfile.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
blob = TextBlob(essays)
for sentence in blob.sentences:
    print sentence
    short_sentences = list()
    for sentence in blob.sentences:
        if len(sentence.words) <= 5:
            short_sentences.append(sentence.replace("\n", " "))

    for item in random.sample(short_sentences, 10):
        print item    

import time
file = open("testfile.txt")
lines = file.read()
for line in lines:
    line = (lines.split("\n"))
    time.sleep(5)
    print line
   

import time
file = open("testfile.txt")
lines = file.read()
for line in lines:
    
    
    print line.join(lines.split(".\n\n"))
    time.sleep(5)

import time
file = open("testfile.txt")
lines = file.read()
for line in lines:
    
    time.sleep(1)
    print line.join(lines.split(".\n\n"))

from bs4 import BeautifulSoup
import requests
import io
import string
def get_text_cleaned(tweet):
    tweet = tweet.replace("u'", "");tweet = tweet.replace(" id ", "\n id")
    return tweet

def get_text_sanitized(tweet):    
    return ' '.join([w.lower().strip().rstrip(string.punctuation)\
        .lstrip(string.punctuation).strip()\
        for w in get_text_cleaned(tweet).split()\
        if w.strip().rstrip(string.punctuation).strip()])

with io.open("testfile002.txt", "w", encoding="utf-8") as my_file:
    

    url = u'https://twitter.com/search?q='
    #query = u'%40drawranliou'
    #query = u'@JamesOKeefeIII'
    #query = u'hurricanne&src=typd'
    query = u'nasa-project'
    r = requests.get(url+query)
    soup = BeautifulSoup(r.text, 'html.parser')
    tweets = [p.text for p in soup.findAll('p', class_='tweet-text')]
    txt = (tweets)
    output = open("exp001.txt", "w")
    for list in txt:
        list =get_text_sanitized(list)    
        mytext = "<br />\n".join(list.split("\n\n"))
        txt = [unicode(mytext).encode("utf-8")]
        txt = str(txt)
        my_file.write(mytext) 
        print mytext
output.close()    

string_with_stuff = "weird \xe1ccents"
print string_with_stuff.decode('ascii', errors="ignore")

from textblob import TextBlob, Word
import sys
import random
filename ="/home/jack/Desktop/imagebot/hekel.txt"
with io.open(filename, "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
    
text = sys.stdin.read().decode('ascii', errors="ignore")
blob = TextBlob(essays)

nouns = list()
for word, tag in blob.tags:
	if tag == 'NN':
		nouns.append(word.lemmatize())

print "This text is about..."
for item in random.sample(nouns, 5):
	word = Word(item)
	print word.pluralize()

from textblob import Word
bank = Word("amazing")
synsets = bank.synsets
print synsets

from textblob import Word
synsets = Word("amazing").synsets
for synset in synsets:
    print synset.definition()

from textblob import Word
from textblob.wordnet import NOUN
synsets = Word("bank").get_synsets(pos=NOUN)
for synset in synsets:
    print synset.definition()

from textblob import Word
from textblob.wordnet import NOUN
synsets = Word("bank").get_synsets(pos=NOUN)
print synsets[1].lemma_names()

from textblob import Word
from textblob.wordnet import NOUN
synsets = Word("plane").get_synsets(pos=NOUN)
print synsets[1].lemma_names()

from textblob import Word
from textblob.wordnet import VERB
synsets = Word("plane").get_synsets(pos=VERB)
print synsets[1].lemma_names()

from textblob import Word
from textblob.wordnet import NOUN
synsets = Word("plane").get_synsets(pos=NOUN)
for synset in synsets:
    print synset.definition()

import sys
import random
import io
count = 0
with io.open("ToUse.txt", "r", encoding="utf-8") as my_file:
    sys.stdin = [my_file.read()] 
for line in sys.stdin:
    print line

import nltk
with io.open("ToUse.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
    
tokens = nltk.word_tokenize(essays)
tagged = nltk.pos_tag(tokens)
nouns = [word for word,pos in tagged \
	if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]
downcased = [x.lower() for x in nouns]
joined = " ".join(downcased).encode('utf-8')
into_string = str(nouns)

output = open("output.txt", "w")
output.write(joined)
output.close()

import nltk
import Bt
import io
with io.open("ToUse.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
    print essays

import nltk
import Bt
import io
with io.open("ToUse.txt", "r", encoding="utf-8") as my_file:
    essays = my_file.read() 
    #essays = u"""text here"""
    #f = open("ToUse.txt")
    #essays = f.read()
    #essays = Bt.badtext(essays)
    #essays = str(essays)
    #essays = essays.encode('utf-8')

    tokens = nltk.word_tokenize(essays)
    tagged = nltk.pos_tag(tokens)
    nouns = [word for word,pos in tagged \
    if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]
    downcased = [x.lower() for x in nouns]
    joined = " ".join(downcased).encode('utf-8')
    into_string = str(nouns)
    texout = into_string.replace("', u'", "  ")
    texout = texout.replace("\u2018", "")
    texout = texout.replace("\u2019 ", "")
    texout = texout.replace("%", "")
    texout = texout.replace("[u'", "")
    texout = texout.replace("']", "")
    output = open("output2.txt", "w")
    output.write(texout)
    output.close()    
    print texout

import nltk
import Bt
#essays = u"""text here"""
f = open("ToUse.txt")
essays = f.read()
essays = Bt.badtext(essays)
essays = str(essays)
#essays = essays.encode('utf-8')

tokens = nltk.word_tokenize(essays)
tagged = nltk.pos_tag(tokens)
nouns = [word for word,pos in tagged \
if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]
downcased = [x.lower() for x in nouns]
joined = " ".join(downcased).encode('utf-8')
into_string = str(nouns)
output = open("output2.txt", "w")
output.write(joined)
output.close()

import Txmanip
from Txmanip import NoNumRange
NoNumRange.nonumRange("ToUse.txt",1,20)

from pass_phrase import pass_phrase
adjectives = pass_phrase.generate_wordlist("pass_phrase/adjectives.txt")
nouns = pass_phrase.generate_wordlist("pass_phrase/nouns.txt")
verbs = pass_phrase.generate_wordlist("pass_phrase/verbs.txt")
pass_phrase.passphrase(adjectives, nouns, verbs, " ")

import markovify
f = open("hashtag.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
for i in range(28):
    print(text_model.make_short_sentence(140))
    virtual =(text_model.make_short_sentence(140))
    virtual = virtual.replace("...",".")
    virtual = virtual.replace("..",".")
    virtual = virtual.replace(".",".\n")
    with open("virtual.txt", "a") as nf:
        nf.write(virtual)    

import markovify
f = open("ToUse.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
for i in range(18):
    print(text_model.make_short_sentence(140))
    virtual =(text_model.make_short_sentence(140))
    with open("virtual.txt", "a") as nf:
        nf.write(virtual)    

import markovify
# Get raw text as string
with open("realDonaldTrump.txt") as f:
    text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print three randomly-generated sentences of no more than 140 characters

for i in range(20):
    print(text_model.make_short_sentence(140))
    virtual =(text_model.make_short_sentence(140))
    with open("virtual.txt", "a") as nf:
        nf.write(virtual)

#badtext = ''.join(c for c in map(chr, range(256)) if not c.isalnum())

import markovify
# Get raw text as string
with open("elonmusk.txt") as f:
    text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print three randomly-generated sentences of no more than 140 characters

for i in range(18):
    print(text_model.make_short_sentence(140))
    virtual =(text_model.make_short_sentence(140))
    with open("virtual.txt", "a") as nf:
        nf.write(virtual)

import re
import textwrap
import time
import sqlite3
import sys
import base64
import time
#conn = sqlite3.connect('hurricane.db')
#c = conn.cursor()
# Create table
#c.execute('''CREATE TABLE hurricane
#             (hurricane text, keywords text)''')
count=0
with open("hurricane_14.txt") as f:
    text = f.read()
    words = " ".join(re.findall("[a-zA-Z]+", text))
    words = words.replace(' u ', ' ');words = words.replace('u ', '');words = words.replace(' U ', ' ')
    chars_per_line = 140
    for i in range(0, len(words), chars_per_line):
        file= ("[(",words[i:i+chars_per_line],"),]")
        #data= ("[('"+words[i:i+chars_per_line]+"'),]")
        file = str(file)
        #file = 'base64 encoding allows code to be stored and retieved in the same format it was posted'
        b = 'hurricane, hurricaneirma, florida'
        conn = sqlite3.connect('hurricane.db')
        c = conn.cursor()
        time.sleep(1)
        #encodedlistvalue=base64.b64encode(file[2:-2])
        #c.execute("INSERT INTO hurricane VALUES (?,?)", (encodedlistvalue, b)) 
        c.execute("INSERT INTO hurricane VALUES (?,?)", (file, b)) 
        conn.commit()
        conn.close()        
        
        
        time.sleep(1)
        print file[2:-2]
        count=count+1
        print count
        if count>400:

            sys.exit()
            

import re
import textwrap
import time
import sqlite3
import sys
import base64
'''

conn = sqlite3.connect('data/400.db')
c = conn.cursor()
c.execute(''CREATE TABLE hurricane
             (hurricane text, keywords text)'')
             '''
count=0             
with open("hurricane_14.txt") as f:
    text = f.read()
    words = " ".join(re.findall("[a-zA-Z]+", text))
    words = words.replace(' u ', ' ');words = words.replace('u ', '');words = words.replace(' U ', ' ')
    #twitter friendly "under 140" for status
    chars_per_line = 400
    for i in range(0, len(words), chars_per_line):
        
        file= ("[(",words[i:i+chars_per_line],"),]")
        #data= ("[('"+words[i:i+chars_per_line]+"'),]")
        file = str(file)
        time.sleep(1)
        #file = 'base64 encoding allows code to be stored and retieved in the same format it was posted'
        conn = sqlite3.connect('data/400.db')
        c = conn.cursor()
        b = 'florida, hurricane, Irma, base64'
        encodedlistvalue=base64.b64encode(file[2:-2])
        c.execute("INSERT INTO hurr VALUES (?,?)", (encodedlistvalue, b))
        time.sleep(1)
        conn.commit()
        conn.close() 
        #Apply a pause so viewing line perline insert is possible
        time.sleep(1)
        count=count+1
        print file[2:-2],count
        

sys.exit()
     

conn = sqlite3.connect('hurricane14.db')
c = conn.cursor()# Never 
t = ('hurricane',)
for row in c.execute('SELECT * FROM hurricane'):
        s2 = row[0].encode('ascii')
        encodedlistvalue=base64.b64decode(s2)
        print encodedlistvalue, '\n', 'hurricane:', row[1], '\n -----------------------------\n'


import re
import textwrap
from datetime import datetime
import string
from nltk.stem.lancaster import LancasterStemmer
from nltk.corpus import stopwords
def get_text_cleaned(tweet):
    tweet = tweet.replace("u'", "");tweet = tweet.replace(" id ", "\n id")
    return tweet

def get_text_sanitized(tweet):    
    return ' '.join([w.lower().strip().rstrip(string.punctuation)\
        .lstrip(string.punctuation).strip()\
        for w in get_text_cleaned(tweet).split()\
        if w.strip().rstrip(string.punctuation).strip()])

with open("/home/jack/Desktop/imagebot/hurricane_14.txt") as f:
    tweet = f.read()
    
    
    
    
    words =get_text_sanitized(tweet)
    print(textwrap.fill(words, 115)).encode('utf-8')

import csv
import io
with io.open("realDonaldTrump_tweets.csv", "r", encoding="utf-8") as my_file:
    for row in csv.reader(my_file):
        print row

def periodtonewline():
    with open('savE.txt') as infile, open('savE_sentence.txt', 'w') as outfile:
        for line in infile:
            line = line.replace(".",".\n");line = line.replace(" ‘","");
            line = line.replace("’","")
            outfile.write(line)  # non-empty
            
periodtonewline()            

import markovify
f = open("grimm.txt")
text = f.read()
text_model_a = markovify.Text(text)


ebook_b =open('hekel.txt')
text0 = ebook_b.read()
text_model_b = markovify.Text(text0)
for i in range(5):
    print(text_model_b.make_short_sentence(140))
    STR0 = (text_model_b.make_short_sentence(140))
    savE = open('savE.txt', 'a')
    savE.write(STR0)
    savE.close()

# 2. Print five randomly-generated sentences
for i in range(5):
    print(text_model_a.make_short_sentence(140))
    STR = (text_model_a.make_short_sentence(140))
    savE = open('savE.txt', 'a')
    savE.write(STR)
    savE.close()
# 3. Print three randomly-generated sentences of no more than 140 characters
for i in range(5):
    print(text_model_a.make_short_sentence(140))
    STR2 = (text_model_a.make_short_sentence(140))
    savE = open('savE.txt', 'a')
    savE.write(STR2)
    savE.close()
# Combine the models into a single one
both_models = markovify.combine([text_model_a,text_model_b])
for i in range(5):
    print(both_models.make_short_sentence(140))    
    STR3 = (both_models.make_short_sentence(140))  
    savE = open('savE.txt', 'a')
    savE.write(STR3)
    savE.close()    

import re
def periodpattern():
    with open('savE.txt') as infile:
        for line in infile:
            line = line.replace(" ‘","")
            line = line.replace("’","")
            line = line.replace(".",".\n")
            line = line.replace("!","!\n")
            line = line.replace("?","?\n")
            
            print line
            outfile2 = open('savE_patern.txt', 'w')
            outfile2.write(line)  # non-empty
            outfile2.close()
            
periodpattern()            


import markovify
f = open("realDonaldTrump.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
STR = (text_model.make_short_sentence(110))
print STR

import markovify
f = open("grimm.txt")
text = f.read()
text_model_a = markovify.Text(text)
# 2. Print five randomly-generated sentences
for i in range(5):
    print(text_model_a.make_sentence())
    STR = (text_model_a.make_sentence())
    savE = open('savE.txt', 'a')
    savE.write(STR)
    savE.close()
# 3. Print three randomly-generated sentences of no more than 140 characters
for i in range(3):
    print(text_model_a.make_short_sentence(120))
    STR2 = (text_model_a.make_short_sentence(120))
    savE = open('savE.txt', 'a')
    savE.write(STR2)
    savE.close()    

import markovify
f = open("realDonaldTrump.txt")
text = f.read()# Recreate the model using 3 sentences
three_model = markovify.Text(text,state_size=3)
for i in range(5):
    print(three_model.make_short_sentence(140))
    STR = (three_model.make_short_sentence(140))
    savE = open('savE.txt', 'a')
    savE.write(STR)
    savE.close()
    print STR

import markovify
f = open("grimm.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
STR = (text_model.make_short_sentence(140))
savE = open('savE.txt', 'a')
savE.write(STR)
savE.close()
print STR

import markovify
f = open("ToUse.txt")
text = f.read()
# Build the model.
text_model = markovify.Text(text)
# Print randomly-generated sentences of no more than 140 characters
#http://paulbourke.net/fractals/
STR = (text_model.make_short_sentence(110))
print STR

def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]

infile = "ToUse.txt" 
generate_the_word(infile)

import FileLen
from random import randint
# maX is the number of lines in the file
maX = FileLen.filelen("ToUse.txt")
# pick a random number between 0 and maX as:  num
num = randint(0, maX)
with open('ToUse.txt') as f:
    for i, line in enumerate(f, 1):
        # if line = num break
        if i == num:
            break

#print the line           
print line

import RandomLine

filename = "realDonaldTrump.txt"    
RandomLine.randomline(filename)    

from random import randint
Max = FileLen.filelen("ToUse.txt")
num = randint(0, Max)
with open('ToUse.txt') as f:
    for i, STR in enumerate(f, 1):
        if i == num:
            break
#STR = line
print STR

search = raw_input("find  ")
file = open("ToUse.txt")
lines = file.readlines()
for line in lines:
    if search in line:print line
    if search == True:
        file.close()
        exit()
file.close()



from Txmanip import HeadFirst
HeadFirst.headFirst("realDonaldTrump_tweets.csv")

import random
afile = open("realDonaldTrump_tweets.csv","r")
line = next(afile)
for num, aline in enumerate(afile):
    if random.randrange(num + 2): continue
    line = aline.replace("\n", "")
print line

import random
def random_line(afile):
    line = next(afile)
    for num, aline in enumerate(afile):
        if random.randrange(num + 2): continue
        line = aline.replace("\n", "")
    return line
afile = open("realDonaldTrump_tweets.csv", "r")
STR = random_line(afile)
print STR

import sqlite3
conn = sqlite3.connect('twitter.db')
c = conn.cursor()
c.execute("""
CREATE VIRTUAL TABLE hurr 
USING FTS3(text, keywords);
""")
conn.commit()
conn.close()

import re
import textwrap
import time
import sqlite3
import sys
import base64
import time
conn = sqlite3.connect('twittertext.db')
c = conn.cursor()
#Create table
#CREATE VIRTUAL TABLE hurr 
#USING FTS3(text, keywords)


c.execute('''CREATE VIRTUAL TABLE twitter
             USING FTS3 (twittertext)''')
count=0
lines = 400
with open("hashtag.txt") as f:
    text = f.read()
    words = " ".join(re.findall("[a-zA-Z]+", text))
    words = words.replace(' u ', ' ');words = words.replace('u ', '');words = words.replace(' U ', ' ')
    chars_per_line = 400
    for i in range(0, len(words), chars_per_line):
        file= ("[(",words[i:i+chars_per_line],"),]")
        #data= ("[('"+words[i:i+chars_per_line]+"'),]")
        file = str(file)
        #file = 'base64 encoding allows code to be stored and retieved in the same format it was posted'
        #keywords = 'sept, nonapi, china, philippines'
        conn = sqlite3.connect('twittertext.db')
        c = conn.cursor()
        time.sleep(1)
        #encodedlistvalue=base64.b64encode(file[2:-2])
        #c.execute("INSERT INTO hurricane VALUES (?,?)", (encodedlistvalue, b)) 
        c.execute("INSERT INTO twitter VALUES (?)", (file,)) 
        conn.commit()
        conn.close()        
        
        
        #time.sleep(1)
        print file[2:-2]
        count=count+1
        print count
        if count>lines:
            sys.exit()
#commits and closes database if there are less then 400 lines of text
conn.commit()
conn.close()                 

from flask import Flask, request
import sys
sys.path.insert(0, "/usr/local/lib/python2.7/dist-packages")
from flask_restful import Resource, Api
import sqlite3
import time

app = Flask(__name__)
api = Api(app)

conn = sqlite3.connect('twittertext.db')
c = conn.cursor()# Never 
count=1
param = raw_input("What Words are you looking for?")
num = input("How many to display?")

for row in c.execute('SELECT twittertext FROM twitter WHERE twittertext MATCH ?', (param,)):
    time.sleep(1)
    print count,":",(row),"\n-----\n"
    count=count+1
    if count >num:
        conn.close()
        sys.exit()
if __name__ == '__main__':
     app.run()
        

from flask import Flask, request
import sys
sys.path.insert(0, "/usr/local/lib/python2.7/dist-packages")
from flask_restful import Resource, Api
import sqlite3
import time

app = Flask(__name__)
api = Api(app)

conn = sqlite3.connect('twittertext.db')
c = conn.cursor()# Never 
count=1
t = ('blood',)
#param = "blood"
param = raw_input("What Words are you looking for?")
num = input("How many to display?")

for row in c.execute('SELECT rowid, twittertext FROM twitter WHERE twittertext MATCH ?', (param,)):
    time.sleep(1)
    print count,":",(row),"\n-----\n"
    count=count+1
    if count >num:
        conn.close()
        sys.exit()
if __name__ == '__main__':
     app.run()
        



!rm twittertext2.db

from flask import Flask, request
import sys
sys.path.insert(0, "/usr/local/lib/python2.7/dist-packages")
from flask_restful import Resource, Api
import sqlite3
import time

app = Flask(__name__)
api = Api(app)

conn = sqlite3.connect('twittertext.db')
c = conn.cursor()# Never 
count=1
t = ('blood',)
#param = "blood"
#param = raw_input("What Words are you looking for?")
num = input("How many to display?")

for row in c.execute('SELECT * FROM twitter'):
    time.sleep(1)
    print count,":",(row[0]),"\n-----\n"
    count=count+1
    if count >num:
        conn.close()
        sys.exit()
if __name__ == '__main__':
     app.run()
        

!ls *.jpg

import re
import textwrap
import time
import sqlite3
import sys
import base64
import time
conn = sqlite3.connect('twittertext2.db')
c = conn.cursor()
#Create table
#CREATE VIRTUAL TABLE hurr 
#USING FTS3(text, keywords)


c.execute('''CREATE VIRTUAL TABLE twitter
             USING FTS3 (twittertext, base64)''')
count=0
lines = 400
with open("hashtag.txt") as f:
    text = f.read()
    words = " ".join(re.findall("[a-zA-Z]+", text))
    #wordx = " ".join(re.findall("[a-zA-Z]+", text))
    words = words.replace(' u ', ' ');words = words.replace('u ', '');words = words.replace(' U ', ' ')
    chars_per_line = 400
    for i in range(0, len(words), chars_per_line):
        file= ("[(",words[i:i+chars_per_line],"),]")
        #data= ("[('"+words[i:i+chars_per_line]+"'),]")
        file = str(file)
        #file = 'base64 encoding allows code to be stored and retieved in the same format it was posted'
        #keywords = 'sept, nonapi, china, philippines'
        conn = sqlite3.connect('twittertext2.db')
        c = conn.cursor()
        time.sleep(1)
        #encodedlistvalue=base64.b64encode(wordx[2:-2])
        encodedlistvalue=base64.b64encode(file)
        #c.execute("INSERT INTO hurricane VALUES (?,?)", (encodedlistvalue, b)) 
        c.execute("INSERT INTO twitter VALUES (?,?)", (file,encodedlistvalue)) 
        conn.commit()
        conn.close()        
        
        
        #time.sleep(1)
        print file[2:-2]
        count=count+1
        print count
        if count>lines:
            sys.exit()
#commits and closes database if there are less then 400 lines of text
conn.commit()
conn.close()                 

from flask import Flask, request
import sys
sys.path.insert(0, "/usr/local/lib/python2.7/dist-packages")
from flask_restful import Resource, Api
import sqlite3
import time

app = Flask(__name__)
api = Api(app)
conn = sqlite3.connect('twittertext.db')
c = conn.cursor()
count=1
Start = input("Start Line")
num = input("Stop Line")
for row in c.execute('SELECT * FROM twitter'):
    time.sleep(1)
    if count >Start:
        row = str(row)[4:-4]
        row = row.replace("'[(', '"," ")
        row = row.replace("', '),]'"," ")
        row = row.replace('u"','')
        row = row.replace('",','')
        #print "\n",count,"-----\n",row
        print count,row
    count=count+1
    if count >num:
        conn.close()
        sys.exit()
if __name__ == '__main__':
     app.run()

import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
import Key
from random import randint
import FileLen
Max = FileLen.filelen("ToUse.txt")
num = randint(0, Max)
with open('ToUse.txt') as f:
    for i, STR in enumerate(f, 1):
        if i == num:
            break

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)
#PATH = '/home/jack/Desktop/3DFRACT/Mandelbulb3Dv199/cloud.jpg'
#PATH = '/home/jack/Desktop/3DFRACT/Mandelbulb3Dv199/post-002.jpg'
#PATH = '/home/jack/Desktop/3DFRACT/Mandelbulb3Dv199/post-068.jpg'
#PATH = '/home/jack/Desktop/text_stuff/instagram/post-054.jpg'
PATH = '/home/jack/Desktop/text_stuff/instagram/post-056.jpg'
#PATH = '/home/jack/Desktop/text_stuff/junk/post-color3.png'

STR ="#C++imagery #python I enjoy pallet swapping most of all #imageprocessing"
photo = open(PATH,'rb')
response = twitter.upload_media(media=photo)
twitter.update_status(status=STR, media_ids=[response['media_id']])

!./searchBlend

#Import os module
import os

# Ask the user to enter string to search
search_path = input("Enter directory path to search : ")
file_type = input("File Type : ")
search_str = input("Enter the search string : ")


# Append a directory separator if not already present
if not (search_path.endswith("/") or search_path.endswith("\\") ): 
        search_path = search_path + "/"
                                                          
# If path does not exist, set search path to current directory
if not os.path.exists(search_path):
        search_path ="."

# Repeat for each file in the directory  
for fname in os.listdir(path=search_path):

   # Apply file type filter   
   if fname.endswith(file_type):

        # Open file for reading
        fo = open(search_path + fname)

        # Read the first line from the file
        line = fo.readline()

        # Initialize counter for line number
        line_no = 1

        # Loop until EOF
        while line != '' :
                # Search for string in line
                index = line.find(search_str)
                if ( index != -1) :
                    print(fname, "[", line_no, ",", index, "] ", line, " ")

                # Read next line
                line = fo.readline()  

                # Increment line counter
                line_no += 1
        # Close the files
        fo.close()

#Import os module
import os

# Ask the user to enter string to search
#search_path = input("Enter directory path to search : ")
#file_type = input("File Type : ")
#search_str = input("Enter the search string : ")


# Append a directory separator if not already present
#if not (search_path.endswith("/") or search_path.endswith("\\") ): 
#        search_path = search_path + "/"
                                                          
# If path does not exist, set search path to current directory
#if not os.path.exists(search_path):
#        search_path ="/home/jack/Desktop"

# Repeat for each file in the directory  
for fname in os.listdir("/home/jack/Desktop/text_stuff/"):

   # Apply file type filter   
   if fname.endswith("*.ipynb"):

        # Open file for reading
        fo = open("/home/jack/Desktop/save-twitter/" + fname)

        # Read the first line from the file
        line = fo.readline()

        # Initialize counter for line number
        line_no = 1

        # Loop until EOF
        while line != '' :
                # Search for string in line
                index = line.find("justhost")
                if ( index != -1) :
                    print(fname, "[", line_no, ",", index, "] ", line, " ")

                # Read next line
                line = fo.readline()  

                # Increment line counter
                line_no += 1
        # Close the files
        fo.close()

"/home/jack/Desktop"
".ipynb"
"justhost"

filein = open('newtest.txt', 'w')
filein.close()

import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
import Key
from random import randint

HashTag = raw_input("HashTag  : ") or "CNN"

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

filein = open('newtest.txt', 'a')
txt = twitter.search(q=HashTag)
txt = str(txt)
filein.write(txt)
filein.close()

print txt

from bs4 import BeautifulSoup

filein = open('newtest.txt', 'r')
list = filein.readline()

list = list.replace("u'id_str': u'", '\nid ')
list = list.replace(u'\u2026','ignore')
list = list.replace(u'\xa0','ignore')
list = list.replace(u'\u2013', u' ')
list = list.replace(u'\xf1', u' ')
list = list.replace(u'\u2019','ignore')
list = '\n'+ u''.join((list)).encode('utf-8').strip()

print list

from twython import Twython
import csv
from dateutil import parser
from dateutil.parser import parse as parse_date
import datetime
from datetime import datetime
import pytz
import Key
from random import randint

utc=pytz.UTC
HashTag = raw_input("HashTag  : ") or "CNN"

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

filein = open('newtest.txt', 'a')
txt = twitter.search(q=HashTag, count="1000",since='2017-7-1')
txt = str(txt)
filein.write(txt)
filein.close()
print txt

from twython import Twython
import csv
from dateutil import parser
from dateutil.parser import parse as parse_date
import datetime
from datetime import datetime
import pytz
import Key
from random import randint

utc=pytz.UTC
HashTag = raw_input("HashTag  : ") or "CNN"

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

filein = open('newtest.txt', 'a')
txt = twitter.search(q=HashTag, count="1000",since='2017-7-1')
txt = str(txt)
filein.write(txt)
filein.close()
print txt


def on_status(self, status): 
    with open('file.txt', 'w') as f: 
        f.write('Author,Date,Text')
        writer = csv.writer(f)
        writer.writerow([status.author.screen_name, status.created_at, status.text])






import sys
import tweepy
import csv
import Key

#pass security information to variables
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]

#use variables to access twitter
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)

#create an object called 'customStreamListener'
class CustomStreamListener(tweepy.StreamListener):

    def on_status(self, status):
        print (status.author.screen_name, status.created_at, status.text)
        # Writing status data
        with open('OutputStreaming.txt', 'a') as f:
            writer = csv.writer(f)
            #status.author.screen_name = status.author.screen_name.encode('UTF-8')
            #status.text.encode = status.text.encode('UTF-8')            
            writer.writerow([status.author.screen_name.encode('UTF-8'), status.created_at, status.text.encode('utf8')])


    def on_error(self, status_code):
        print >> sys.stderr, 'Encountered error with status code:', status_code
        return True # Don't kill the stream

    def on_timeout(self):
        print >> sys.stderr, 'Timeout...'
        return True # Don't kill the stream


    def titles():
        # Writing csv titles
        with open('OutputStreaming.txt', 'a') as f:
                    writer = csv.writer(f)
                    writer.writerow(['Author', 'Date', 'Text'])
            
streamingAPI = tweepy.streaming.Stream(auth, CustomStreamListener())
#with open("tokens.txt", "r") as f:
with open("tokens2.txt", "r") as f:    
    tokens = f.readlines()

    streamingAPI.filter(track=tokens)
    #streamingAPI.filter(track=['Dallas', 'NewYork'])
    
    
    #status.text.encode('utf-8')
    #writer.writerow([unicode(s).encode("utf-8") for s in row])
    #writer.writerow([unicode('Author', 'Date', 'Text').encode("utf-8") for 'Author', 'Date', 'Text' in row])
    

%%writefile TweepyToCSV.py
import sys
import tweepy
import csv
import Key

#pass security information to variables
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]

#use variables to access twitter
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)

#create an object called 'customStreamListener'
class CustomStreamListener(tweepy.StreamListener):

    def on_status(self, status):
        print (status.author.screen_name, status.created_at, status.text)
        # Writing status data
        with open('OutputStreaming.txt', 'a') as f:
            writer = csv.writer(f)
            #status.author.screen_name = status.author.screen_name.encode('UTF-8')
            #status.text.encode = status.text.encode('UTF-8')            
            writer.writerow([status.author.screen_name.encode('UTF-8'), status.created_at, status.text.encode('utf8')])


    def on_error(self, status_code):
        print >> sys.stderr, 'Encountered error with status code:', status_code
        return True # Don't kill the stream

    def on_timeout(self):
        print >> sys.stderr, 'Timeout...'
        return True # Don't kill the stream


    def titles():
        # Writing csv titles
        with open('OutputStreaming.txt', 'a') as f:
                    writer = csv.writer(f)
                    writer.writerow(['Author', 'Date', 'Text'])
            
def main():
    streamingAPI = tweepy.streaming.Stream(auth, CustomStreamListener())
    #with open("tokens.txt", "r") as f:
    with open("tokens2.txt", "r") as f:    
        tokens = f.readlines()

        streamingAPI.filter(track=tokens)
        #streamingAPI.filter(track=['Dallas', 'NewYork'])


        #status.text.encode('utf-8')
        #writer.writerow([unicode(s).encode("utf-8") for s in row])
        #writer.writerow([unicode('Author', 'Date', 'Text').encode("utf-8") for 'Author', 'Date', 'Text' in row])


from Txmanip import RemoveBlank

RemoveBlank.removeblank("OutputStreaming.txt","cleanstream.txt")

from Txmanip import RemoveDuplicate

RemoveDuplicate.removeduplicate("cleanstream.txt","nodupStreaming.txt")

from itertools import tee
help(tee)

def edited(r):
    r+c
    return r

with open("test.txt") as inf, open("editTest.txt", "w") as outf:
    # set up iterators
    cfg, res = tee(inf)
    for i in range(1):
        next(cfg)

    # iterate through in tandem
    for c, r in zip(cfg, res):
        if len(c)<35:
            r = edited(r)
        outf.write(r)

    # reached end - write out remaining queued values
    for r in res:
        outf.write(r)
edited(r)        

from itertools import tee

with open("test.txt") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(1):
        next(cfg)

    for c,r in zip(cfg, res):
        if len(c)<35:
            print c,r,"----\n"

from itertools import tee
count=0
with open("realDonaldTrump_tweets.csv") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(4):
        next(cfg)

    for c,r in zip(cfg, res):
        count=count+1
        if "campaign" in c:
            #print "Date :",c[21:]
            print count,"-Text:",c[39:]



def main():
    count=0
    # open file
    infile = open('test.txt', 'r')
    # get input
    who = raw_input('Search Term: ')
    who = str(who)
    #begin search
    line = infile.readline()
    while line != '':
        if who in line:
            count += 1
            print line
    infile.close()


main()

count = 0

def main():
    count=0
    # open file
    file = open('test.txt', 'r')
    # get input
    who = raw_input('Search Term: ')
    who = str(who)
    #begin search
    lst = file.readline()
    while lst != '':
        if who in lst:
            count += 1
        print(count)
        file.close()


main()



from itertools import tee
search = raw_input('Search Term')
with open("realDonaldTrump_tweets.csv") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(4):
        next(cfg)

    for c,r in zip(cfg, res):
        if search in c:
            print "Date :",c[21:38],"\nText :",c[39:]

!rm /home/jack/anaconda2/lib/python2.7/site-packages/Txmanip/RemoveDuplicate.pyc

import SearchFilename
filename = "realDonaldTrump_tweets.csv"
length = 20
SearchFilename.searchfilename(filename, length)

import TweepyToCSV
from TweepyToCSV import CustomStreamListener
TweepyToCSV.main()

%%writefile tokens2.txt
Mexico
dreamers
healthcare
revolution
hidding
putin
voters
feedup
fakenews
NBC
CBS
CNN
CIA
hate
wacko
gonecrazy
wierd
hopeless
impeach
dishonor
lost faith
angry
trump
russia
economy
north korean
rocketman
trust
follow
honorable
honest
dishonest
distrust
election
fakepresident

#!/usr/bin/env python
# encoding: utf-8

import tweepy #https://github.com/tweepy/tweepy
import csv
import Key
from random import randint

HashTag = raw_input("HashTag  : ") or "CNN"

#CONSUMER_KEY = Key.twiter()[0]
#CONSUMER_SECRET = Key.twiter()[1]
#ACCESS_KEY = Key.twiter()[2]
#ACCESS_SECRET = Key.twiter()[3]
#twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

#Twitter API credentials
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]


def get_all_tweets(screen_name):
    #Twitter only allows access to a users most recent 3240 tweets with this method

    #authorize twitter, initialize tweepy
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth)

    #initialize a list to hold all the tweepy Tweets
    alltweets = []	

    #make initial request for most recent tweets (200 is the maximum allowed count)
    new_tweets = api.user_timeline(screen_name = screen_name,count=200)

    #save most recent tweets
    alltweets.extend(new_tweets)
    
    #save the id of the oldest tweet less one
    oldest = alltweets[-1].id - 1
    
    #keep grabbing tweets until there are no tweets left to grab
    #while len(new_tweets) > 0:
    while len(new_tweets) < 400:    
        print "getting tweets before %s" % (oldest)

        #all subsiquent requests use the max_id param to prevent duplicates
        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)
        
        #save most recent tweets
        alltweets.extend(new_tweets)

        #update the id of the oldest tweet less one
        oldest = alltweets[-1].id - 1

        print (len(alltweets))
        
        if (len(alltweets)) >200:

            #transform the tweepy tweets into a 2D array that will populate the csv	
            outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode("utf-8")] for tweet in alltweets]

            #write the csv	
            with open('%s_tweets.csv' % screen_name, 'wb') as f:
                writer = csv.writer(f)
                writer.writerow(["id","created_at","text"])
                writer.writerows(outtweets)

            pass


if __name__ == '__main__':
    #pass in the username of the account you want to download
    get_all_tweets("realDonaldTrump")

%reset -f

import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
import Key
import csv

HashTag = raw_input("HashTag  : ") or "CNN"

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

# Open/create a file to append data to
csvFile = open('result.csv', 'a')

#Use csv writer
csvWriter = csv.writer(csvFile)



txt = twitter.search(q=HashTag)
txt = str(txt)
filein.write(txt)
filein.close()

print txt

# Open/create a file to append data to
csvFile = open('result.csv', 'a')

#Use csv writer
csvWriter = csv.writer(csvFile)
txt = twitter.search(q=HashTag)

csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])




for tweet in tweepy.Cursor(api.search,
                           q = "google",
                           since = "2014-02-14",
                           until = "2014-02-15",
                           lang = "en").items():

    # Write a row to the CSV file. I use encode UTF-8
    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])
    print tweet.created_at, tweet.text
csvFile.close()

import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import twython
from twython import Twython
import Key
from random import randint

HashTag = raw_input("HashTag  : ") or "CNN"

CONSUMER_KEY = Key.twiter()[0]
CONSUMER_SECRET = Key.twiter()[1]
ACCESS_KEY = Key.twiter()[2]
ACCESS_SECRET = Key.twiter()[3]
twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY,ACCESS_SECRET)

tweetfile = 'newtest.txt'